<bug_data>
<bug id='3104' author='dalmia' open_date='2020-08-22T19:24:01Z' closed_time='2020-10-06T17:54:38Z'>
 	<summary>TPU available: true when there are no TPUs</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 I am using a DGX machine (and so, no TPUs), but on initiating Trainer, it logs TPU available: True. This ends up returning Missing XLA configuration when I run my script.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 Simply running the following lines on my machine:
 &gt;&gt; trainer = pl.Trainer(gpus=[0])                                                                                                                 
 GPU available: True, used: True
 TPU available: True, using: 0 TPU cores
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 &gt;&gt; trainer = pl.Trainer(gpus=[0])                                                                                                                 
 GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;* CUDA:
         - GPU:
                 - Tesla V100-SXM2-32GB
         - available:         True
         - version:           10.2
 * Packages:
         - numpy:             1.18.2
         - pyTorch_debug:     False
         - pyTorch_version:   1.6.0
         - pytorch-lightning: 0.9.0
         - tensorboard:       2.2.0
         - tqdm:              4.45.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 - 
         - processor:         x86_64
         - python:            3.6.9
         - version:           #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='dalmia' date='2020-08-24T23:10:11Z'>
 		sounds like some misconfiguration issue, are interested in sending a PR? üê∞
 		</comment>
 		<comment id='2' author='dalmia' date='2020-08-28T01:39:04Z'>
 		Sure. I realized that the bug is in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/8ebf4fe1739aae04c14ddb3ad572a57775018673/pytorch_lightning/trainer/distrib_parts.py&gt;this&lt;/denchmark-link&gt;
  script.
 Specifically:
 &lt;denchmark-code&gt;try:
     import torch_xla.core.xla_model as xm
 except ImportError:
     XLA_AVAILABLE = False
 else:
     XLA_AVAILABLE = True
 &lt;/denchmark-code&gt;
 
 So, if the environment has  installed but no TPU, then this error is thrown. If I use an environment without , it works fine. So, is this something that should be fixed in the codebase or something that the user should take care of? &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='dalmia' date='2020-08-28T07:35:19Z'>
 		yes, we had the XLA detection as a temporal solution as we did not expect someone would install XLA without having TPU...
 so, pls send a PR, I think that we have this patter in several files...
 		</comment>
 		<comment id='4' author='dalmia' date='2020-10-01T18:35:37Z'>
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		</comment>
 	</comments>
 </bug>
<commit id='69833dad5b2a0e7e68ed60a91a5a8c32ae22f707' author='Lezwon Castelino' date='2020-10-06 19:54:37+02:00'>
 	<dmm_unit complexity='1.0' interfacing='0.8703703703703703' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>34,35</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\accelerators\tpu_backend.py' new_name='pytorch_lightning\accelerators\tpu_backend.py'>
 		<file_info nloc='187' complexity='45' token_count='1439'></file_info>
 		<method name='setup' parameters='self,model'>
 				<method_info nloc='8' complexity='2' token_count='52' nesting_level='1' start_line='46' end_line='60'></method_info>
 			<added_lines>49,50</added_lines>
 			<deleted_lines>50</deleted_lines>
 		</method>
 		<method name='to_device' parameters='self,batch'>
 				<method_info nloc='8' complexity='2' token_count='39' nesting_level='1' start_line='160' end_line='181'></method_info>
 			<added_lines>174</added_lines>
 			<deleted_lines>174</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>24,27,29,30,31,36</added_lines>
 			<deleted_lines>24,25,29,34,35,36,37</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\early_stopping.py' new_name='pytorch_lightning\callbacks\early_stopping.py'>
 		<file_info nloc='145' complexity='27' token_count='741'></file_info>
 		<method name='_run_early_stopping_check' parameters='self,trainer,pl_module'>
 				<method_info nloc='21' complexity='7' token_count='161' nesting_level='1' start_line='170' end_line='204'></method_info>
 			<added_lines>188</added_lines>
 			<deleted_lines>189</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>22,23,30,31,32,33,37</added_lines>
 			<deleted_lines>28,32,33,34,35,36,37,38</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\lightning.py' new_name='pytorch_lightning\core\lightning.py'>
 		<file_info nloc='1119' complexity='100' token_count='2640'></file_info>
 		<modified_lines>
 			<added_lines>33,34,48,49,50</added_lines>
 			<deleted_lines>46,48,49,50,51</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\data_loading.py' new_name='pytorch_lightning\trainer\data_loading.py'>
 		<file_info nloc='256' complexity='49' token_count='1587'></file_info>
 		<modified_lines>
 			<added_lines>30,33,39</added_lines>
 			<deleted_lines>32,38,41,42,43,44,45</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='pytorch_lightning\utilities\xla_device_utils.py'>
 		<file_info nloc='60' complexity='11' token_count='254'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_tpu.py' new_name='tests\models\test_tpu.py'>
 		<file_info nloc='214' complexity='17' token_count='1415'></file_info>
 		<modified_lines>
 			<added_lines>2,9,13,18,19,20</added_lines>
 			<deleted_lines>15,20,21,22,23,219</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\utilities\test_xla_device_utils.py'>
 		<file_info nloc='20' complexity='3' token_count='137'></file_info>
 	</modification>
 </commit>
</bug_data>
