<bug_data>
<bug id='3549' author='pbmstrk' open_date='2020-09-18T13:00:13Z' closed_time='2020-09-20T00:00:51Z'>
 	<summary>Bug in validation_epoch_end</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 In the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html?highlight=validation_epoch_end#validation-epoch-end&gt;documentation&lt;/denchmark-link&gt;
 ,  is described as running at the end of a validation epoch and does not need to necessarily return anything.
 When running a slightly modified version of the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/new_project.html&gt;example&lt;/denchmark-link&gt;
  in the docs,  seemingly runs once on a single batch of validation data and returns an error if nothing is returned.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 A MWE:
 &lt;denchmark-code&gt;import os
 import torch
 import torch.nn.functional as F
 from torchvision.datasets import MNIST
 from torchvision import transforms
 from torch.utils.data import DataLoader
 import pytorch_lightning as pl
 from torch.utils.data import random_split
 
 class LitModel(pl.LightningModule):
 
     def __init__(self):
         super().__init__()
         self.layer_1 = torch.nn.Linear(28 * 28, 128)
         self.layer_2 = torch.nn.Linear(128, 10)
 
     def forward(self, x):
         x = x.view(x.size(0), -1)
         x = self.layer_1(x)
         x = F.relu(x)
         x = self.layer_2(x)
         return x
 
     def configure_optimizers(self):
         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
         return optimizer
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
 
         return {'loss': loss}
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         
         return {"val_loss": loss}
 
     def validation_epoch_end(self, validation_step_outputs):
         print("Length of outputs: {}".format(len(validation_step_outputs)))
         # a dummy return value to avoid error
         return {"val_loss": 0}
 
 model = LitModel()
 
 dataset = MNIST(os.getcwd(), download=True, train=False, transform=transforms.ToTensor())
 train, val = random_split(dataset, [9000, 1000])
 
 train_loader = DataLoader(train)
 val_loader = DataLoader(val)
 
 # train
 trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)
 trainer.fit(model, train_loader, val_loader)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Output&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 CUDA_VISIBLE_DEVICES: [0]
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
   warnings.warn(*args, **kwargs)
 Length of outputs: 1
 Length of outputs: 1000
 Length of outputs: 1000
 Length of outputs: 1000
 Length of outputs: 1000
 Saving latest checkpoint..
 Length of outputs: 1000
 &lt;/denchmark-code&gt;
 
 If no return value is given in validation_epoch_end, the following error occurs
 &lt;denchmark-code&gt;---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 &lt;ipython-input-183-013dcacba267&gt; in &lt;module&gt;()
      10 # train
      11 trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)
 ---&gt; 12 trainer.fit(model, train_loader, val_loader)
 
 7 frames
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)
      46             if entering is not None:
      47                 self.state = entering
 ---&gt; 48             result = fn(self, *args, **kwargs)
      49 
      50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
    1071             self.accelerator_backend = GPUBackend(self)
    1072             model = self.accelerator_backend.setup(model)
 -&gt; 1073             results = self.accelerator_backend.train(model)
    1074 
    1075         elif self.use_tpu:
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/gpu_backend.py in train(self, model)
      49 
      50     def train(self, model):
 ---&gt; 51         results = self.trainer.run_pretrain_routine(model)
      52         return results
      53 
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
    1222 
    1223         # run a few val batches before training starts
 -&gt; 1224         self._run_sanity_check(ref_model, model)
    1225 
    1226         # clear cache before training
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model, model)
    1255             num_loaders = len(self.val_dataloaders)
    1256             max_batches = [self.num_sanity_val_steps] * num_loaders
 -&gt; 1257             eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)
    1258 
    1259             # allow no returns from eval
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in _evaluate(self, model, dataloaders, max_batches, test_mode)
     397 
     398         # log callback metrics
 --&gt; 399         self.__update_callback_metrics(eval_results, using_eval_result)
     400 
     401         # Write predictions to disk if they're available.
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in __update_callback_metrics(self, eval_results, using_eval_result)
     429                         flat = {'val_loss': eval_result}
     430                     else:
 --&gt; 431                         flat = flatten_dict(eval_result)
     432                     self.callback_metrics.update(flat)
     433             else:
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/parsing.py in flatten_dict(source, result)
     113         result = {}
     114 
 --&gt; 115     for k, v in source.items():
     116         if isinstance(v, dict):
     117             _ = flatten_dict(v, result)
 
 AttributeError: 'NoneType' object has no attribute 'items'
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The outputs in validation_epoch_end should contain results from all batches. Is a test validation batch running in the background, which could explain why the results of only one batch are included in the first loop.
 Return values in  were addressed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2438&gt;#2438&lt;/denchmark-link&gt;
  (with the same error) and from what I can tell included in the 0.8.4 release - perhaps I am missing something?
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch: 1.6.0
 pytorch-lightning: 0.9.0
 
 	</description>
 	<comments>
 		<comment id='1' author='pbmstrk' date='2020-09-18T13:01:10Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='pbmstrk' date='2020-09-18T15:17:40Z'>
 		Hi there! What you are seeing in the first output is caused by our "validation sanity check". We run a configurable number of validation steps before training starts to ensure the user doesn't run into any issues after a potentially timely series of training steps. This can be configured (or turned off) with pl.Trainer(num_sanity_val_steps=0).
 Regarding returning None from validation_epoch_end, it appears that functionality has recently been removed, I will look into whether that was intentional or not and update the code/docs accordingly.
 		</comment>
 		<comment id='3' author='pbmstrk' date='2020-09-18T19:10:08Z'>
 		&lt;denchmark-link:https://github.com/teddykoker&gt;@teddykoker&lt;/denchmark-link&gt;
  That makes sense. Thanks for the pointer for configuring this!
 		</comment>
 	</comments>
 </bug>
<commit id='9acee67c31c84dac74cc6169561a483d3b9c9f9d' author='William Falcon' date='2020-09-19 20:00:50-04:00'>
 	<dmm_unit complexity='0.8846153846153846' interfacing='0.8846153846153846' size='0.19230769230769232'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\connectors\logger_connector.py' new_name='pytorch_lightning\trainer\connectors\logger_connector.py'>
 		<file_info nloc='185' complexity='56' token_count='1372'></file_info>
 		<method name='_log_on_evaluation_epoch_end_metrics' parameters='self,eval_results,using_eval_result'>
 				<method_info nloc='23' complexity='10' token_count='156' nesting_level='1' start_line='106' end_line='131'></method_info>
 			<added_lines>107,108,109</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\logging.py' new_name='pytorch_lightning\trainer\logging.py'>
 		<file_info nloc='92' complexity='35' token_count='605'></file_info>
 		<method name='process_output' parameters='self,output,train'>
 				<method_info nloc='49' complexity='22' token_count='301' nesting_level='1' start_line='55' end_line='150'></method_info>
 			<added_lines>76,77,78,79,140</added_lines>
 			<deleted_lines>76,77,78,139</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\base\model_valid_epoch_ends.py' new_name='tests\base\model_valid_epoch_ends.py'>
 		<file_info nloc='39' complexity='18' token_count='324'></file_info>
 		<method name='validation_epoch_end_return_none._mean' parameters='res,key'>
 				<method_info nloc='2' complexity='3' token_count='41' nesting_level='2' start_line='45' end_line='47'></method_info>
 			<added_lines>45,46,47</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='validation_epoch_end_return_none' parameters='self,outputs'>
 				<method_info nloc='3' complexity='1' token_count='12' nesting_level='1' start_line='36' end_line='49'></method_info>
 			<added_lines>36,37,38,39,40,41,42,43,44,45,46,47,48,49</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>50</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_trainer_steps_result_return.py' new_name='tests\trainer\test_trainer_steps_result_return.py'>
 		<file_info nloc='453' complexity='29' token_count='3001'></file_info>
 		<method name='test_eval_loop_return_none' parameters='tmpdir'>
 				<method_info nloc='18' complexity='1' token_count='88' nesting_level='0' start_line='628' end_line='649'></method_info>
 			<added_lines>628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>626,627</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
