<bug_data>
<bug id='1442' author='alexeykarnachev' open_date='2020-04-10T12:52:39Z' closed_time='2020-04-10T15:43:07Z'>
 	<summary>Failed to configure_optimizers from dictionary without lr_scheduler field presented</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Optimizer is failed to be configured from the dictionary without lr_sheduler field.
 Consider an example of the Module configure_optimizers method:
         def configure_optimizers(self):
             config = {
                 'optimizer': torch.optim.SGD(params=self.parameters(), lr=1e-03)
             }
             return config
 Then, we run a simple trainer:
     trainer_options = dict(default_save_path=tmpdir, max_epochs=1)
     trainer = Trainer(**trainer_options)
     _ = trainer.fit(model)
 And we fail with an error:
 &lt;denchmark-code&gt;UnboundLocalError: local variable 'lr_schedulers' referenced before assignment
 &lt;/denchmark-code&gt;
 
 I believe, that the reason is that lr_schedulers local variable is not determined here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/optimizers.py
 
 
         Lines 36 to 42
       in
       8dd9b80
 
 
 
 
 
 
  # single dictionary 
 
 
 
  elif isinstance(optim_conf, dict): 
 
 
 
  optimizer = optim_conf["optimizer"] 
 
 
 
  lr_scheduler = optim_conf.get("lr_scheduler", []) 
 
 
 
  if lr_scheduler: 
 
 
 
  lr_schedulers = self.configure_schedulers([lr_scheduler]) 
 
 
 
  return [optimizer], lr_schedulers, [] 
 
 
 
 
 
 I think, it could be fixed like this:
         # single dictionary
         elif isinstance(optim_conf, dict):
             optimizer = optim_conf["optimizer"]
             lr_scheduler = optim_conf.get("lr_scheduler", [])
             if lr_scheduler:
                 lr_schedulers = self.configure_schedulers([lr_scheduler])
             else:
                 lr_schedulers = []
             return [optimizer], lr_schedulers, []
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Create a simple Module with configure_optimizers which looks like above.
 Run the fit Trainer method with the model.
 See error
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-link:https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc&gt;https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 I suppose, that such the configuration: {"optimizer": ...}, without "lr_scheduler" must be a valid one, and this error must not be occurred.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 OS: Linux
 architecture: 64bit
 processor: x86_64
 python: 3.7.6
 version: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/97&gt;#97&lt;/denchmark-link&gt;
 ~16.04.1-Ubuntu SMP Wed Apr 1 03:03:31 UTC 2020
 pytorch-lightning: 0.7.3rc1
 	</description>
 	<comments>
 		<comment id='1' author='alexeykarnachev' date='2020-04-10T13:21:59Z'>
 		yeah, agreed that dict should work without the scheduler. mind submitting a PR?
 		</comment>
 	</comments>
 </bug>
<commit id='4c34d16a349bc96a717be5674606c2577fab8946' author='Alexey Karnachev' date='2020-04-10 11:43:06-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>15</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\optimizers.py' new_name='pytorch_lightning\trainer\optimizers.py'>
 		<file_info nloc='100' complexity='15' token_count='603'></file_info>
 		<modified_lines>
 			<added_lines>42,43</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\supporters.py' new_name='pytorch_lightning\trainer\supporters.py'>
 		<file_info nloc='54' complexity='14' token_count='289'></file_info>
 		<modified_lines>
 			<added_lines>23</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='752' complexity='75' token_count='3127'></file_info>
 		<method name='add_argparse_args.allowed_type' parameters='x'>
 				<method_info nloc='2' complexity='1' token_count='17' nesting_level='5' start_line='557' end_line='558'></method_info>
 			<added_lines>557,558</added_lines>
 			<deleted_lines>557</deleted_lines>
 		</method>
 		<method name='add_argparse_args' parameters='cls,ArgumentParser'>
 				<method_info nloc='28' complexity='8' token_count='124' nesting_level='1' start_line='536' end_line='568'></method_info>
 			<added_lines>557,558</added_lines>
 			<deleted_lines>557</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_optimizers.py' new_name='tests\trainer\test_optimizers.py'>
 		<file_info nloc='205' complexity='34' token_count='1485'></file_info>
 		<method name='test_configure_optimizer_from_dict' parameters='tmpdir'>
 				<method_info nloc='9' complexity='1' token_count='60' nesting_level='0' start_line='280' end_line='300'></method_info>
 			<added_lines>280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_configure_optimizer_from_dict.configure_optimizers' parameters='self'>
 				<method_info nloc='5' complexity='1' token_count='33' nesting_level='2' start_line='286' end_line='290'></method_info>
 			<added_lines>286,287,288,289,290</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>278,279</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
