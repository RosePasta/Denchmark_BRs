<bug_data>
<bug id='2539' author='YuxianMeng' open_date='2020-07-07T11:30:39Z' closed_time='2020-07-10T01:28:12Z'>
 	<summary>TPU fp16 requires apex installed</summary>
 	<description>
 When I tried to use precision=16 on TPU, pytorch-lightning is trying to find amp, which is unnecessary.
 The backtrace is
 &lt;denchmark-code&gt;GPU available: False, used: False
 TPU available: True, using: 8 TPU cores
 Traceback (most recent call last):
   File "bert_ner/light/fp16_debug.py", line 16, in &lt;module&gt;
     trainer = pl.Trainer(tpu_cores=8, precision=16)
   File "/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 607, in __init__
     self.init_amp()
   File "/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/auto_mix_precision.py", line 27, in init_amp
     "You set `use_amp=True` but do not have apex installed."
 ModuleNotFoundError: You set `use_amp=True` but do not have apex installed.Install apex first using this guide and rerun with use_amp=True:https://github.com/NVIDIA/apex#linux his run will NOT use 16 bit precision
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 build a whatever Trainer in TPU and use fp16
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import pytorch_lightning as pl
 
 trainer = pl.Trainer(tpu_cores=8, precision=16)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Should have nothing error.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch Version (e.g., 1.5.0):
 OS (e.g., Linux): Linux
 How you installed PyTorch (conda, pip, source): conda
 Build command you used (if compiling from source):
 Python version:
 CUDA/cuDNN version:
 GPU models and configuration:
 Any other relevant information: actually I directly use pytorch-xla-1.5 docker on Google Cloud
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='YuxianMeng' date='2020-07-07T11:31:35Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='YuxianMeng' date='2020-07-07T12:26:33Z'>
 		If you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.
 I could get this working using nightly version of pytorch:
 &lt;denchmark-code&gt;pl.Trainer(precision=16, tpu_cores=8)
 &gt;&gt;&gt;GPU available: False, used: False
 &gt;&gt;&gt;TPU available: True, using: 8 TPU cores
 &gt;&gt;&gt;Using native 16bit precision.
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='YuxianMeng' date='2020-07-08T03:23:51Z'>
 		
 If you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.
 I could get this working using nightly version of pytorch:
 pl.Trainer(precision=16, tpu_cores=8)
 &gt;&gt;&gt;GPU available: False, used: False
 &gt;&gt;&gt;TPU available: True, using: 8 TPU cores
 &gt;&gt;&gt;Using native 16bit precision.
 
 
 Thanks for the quick reply. But &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/apex.html&gt;the document&lt;/denchmark-link&gt;
  does not point out that I must have nightly version of pytorch installed or have apex installed when training on TPU with fp16. Maybe it's better to revise that part of document?
 		</comment>
 		<comment id='4' author='YuxianMeng' date='2020-07-08T09:56:58Z'>
 		Yes, I agree that from the documentation it would look like it is only a requirement for gpu training. I guess that the specific requirement for TPU is to have pytorch version 1.6 or higher.
 		</comment>
 	</comments>
 </bug>
<commit id='e068af9ea8c86df8ed5eb20e57a36fbb38c70462' author='William Falcon' date='2020-07-09 21:28:11-04:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\memory.py' new_name='pytorch_lightning\core\memory.py'>
 		<file_info nloc='307' complexity='59' token_count='1426'></file_info>
 		<method name='_forward_example_input' parameters='self'>
 				<method_info nloc='19' complexity='7' token_count='132' nesting_level='1' start_line='204' end_line='226'></method_info>
 			<added_lines>212</added_lines>
 			<deleted_lines>212</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_parts.py' new_name='pytorch_lightning\trainer\distrib_parts.py'>
 		<file_info nloc='373' complexity='104' token_count='2201'></file_info>
 		<method name='dp_train' parameters='self,model'>
 				<method_info nloc='26' complexity='11' token_count='213' nesting_level='1' start_line='229' end_line='273'></method_info>
 			<added_lines>243,250</added_lines>
 			<deleted_lines>243,250</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\evaluation_loop.py' new_name='pytorch_lightning\trainer\evaluation_loop.py'>
 		<file_info nloc='338' complexity='39' token_count='1250'></file_info>
 		<modified_lines>
 			<added_lines>289</added_lines>
 			<deleted_lines>289</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='1111' complexity='131' token_count='4693'></file_info>
 		<method name='__test_using_best_weights' parameters='self,ckpt_path,test_dataloaders'>
 				<method_info nloc='30' complexity='9' token_count='202' nesting_level='1' start_line='1287' end_line='1330'></method_info>
 			<added_lines>1303,1304,1305,1306,1307</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='run_pretrain_routine' parameters='self,LightningModule'>
 				<method_info nloc='56' complexity='29' token_count='431' nesting_level='1' start_line='1104' end_line='1213'></method_info>
 			<added_lines>1121</added_lines>
 			<deleted_lines>1121</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_io.py' new_name='pytorch_lightning\trainer\training_io.py'>
 		<file_info nloc='351' complexity='87' token_count='1748'></file_info>
 		<method name='dump_checkpoint' parameters='self,bool'>
 				<method_info nloc='43' complexity='18' token_count='295' nesting_level='1' start_line='316' end_line='381'></method_info>
 			<added_lines>361</added_lines>
 			<deleted_lines>361</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='652' complexity='163' token_count='3372'></file_info>
 		<method name='run_batch_backward_pass' parameters='self,split_batch,batch_idx,opt_idx,optimizer'>
 				<method_info nloc='12' complexity='6' token_count='92' nesting_level='1' start_line='690' end_line='714'></method_info>
 			<added_lines>705</added_lines>
 			<deleted_lines>705</deleted_lines>
 		</method>
 		<method name='optimizer_closure' parameters='self,split_batch,batch_idx,opt_idx,optimizer,hiddens'>
 				<method_info nloc='50' complexity='13' token_count='364' nesting_level='1' start_line='762' end_line='847'></method_info>
 			<added_lines>770,820</added_lines>
 			<deleted_lines>770,820</deleted_lines>
 		</method>
 		<method name='call_optimizer_step' parameters='self,optimizer,opt_idx,batch_idx,split_batch'>
 				<method_info nloc='28' complexity='10' token_count='192' nesting_level='1' start_line='716' end_line='760'></method_info>
 			<added_lines>753</added_lines>
 			<deleted_lines>753</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
