<bug_data>
<bug id='1156' author='LucFrachon' open_date='2020-03-15T15:03:34Z' closed_time='2020-06-05T10:28:30Z'>
 	<summary>ReduceLROnPlateau does not recognise val_loss despite progress_bar dict</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 When training my model, I get the following message:
 &lt;denchmark-code&gt;  File "C:\Users\Luc\Miniconda3\envs\pytorch\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 371, in train
     raise MisconfigurationException(m)
 pytorch_lightning.utilities.debugging.MisconfigurationException: ReduceLROnPlateau conditioned on metric val_loss which is not available. Available metrics are: loss
 &lt;/denchmark-code&gt;
 
 Ihis is similar to #321for instance, but I definitely return a progress_bar dict with a val_loss key in it (see code below).
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;  def training_step(self, batch, batch_idx):
        z, y_true = batch
        y_pred = self.forward(z)
        loss_val = self.loss_function(y_pred, y_true)
        return {'loss': loss_val.sqrt()}
 
    def validation_step(self, batch, batch_idx):
        z, y_true = batch
        lr = torch.tensor(self.optim.param_groups[0]['lr'])
        y_pred = self.forward(z)
        loss_val = self.loss_function(y_pred, y_true)
        return {'val_loss': loss_val.sqrt(), 'lr': lr}
 
    def validation_epoch_end(self, outputs):
        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()
        lr = outputs[-1]['lr']
        logs = {'val_loss': val_loss_mean, 'lr': lr}
        return {'val_loss': val_loss_mean, 'progress_bar': logs, 'log': logs}
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The val_loss value should be picked up by the progress bar.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch Version (e.g., 1.0): 1.4.0
 OS (e.g., Linux): Windows 10
 How you installed PyTorch (conda, pip, source): pip
 Python version: 3.6.10
 CUDA/cuDNN version: 10
 GPU models and configuration: 1070Ti x 1
 Any other relevant information:
 
 	</description>
 	<comments>
 		<comment id='1' author='LucFrachon' date='2020-03-16T14:07:38Z'>
 		Actually, ReduceLROnPlateau lr schedulers are conditioned on callback_metrics, see here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 717 to 726
       in
       774d9be
 
 
 
 
 
 
  if lr_scheduler['reduce_on_plateau']: 
 
 
 
  monitor_key = lr_scheduler['monitor'] 
 
 
 
  monitor_val = self.callback_metrics.get(monitor_key) 
 
 
 
  if monitor_val is None: 
 
 
 
  avail_metrics = ','.join(list(self.callback_metrics.keys())) 
 
 
 
  m = f'ReduceLROnPlateau conditioned on metric {monitor_key} ' \ 
 
 
 
  f'which is not available. Available metrics are: {avail_metrics}. ' \ 
 
 
 
  'Condition can be set using `monitor` key in lr scheduler dict' 
 
 
 
  raise MisconfigurationException(m) 
 
 
 
  lr_scheduler['scheduler'].step(monitor_val) 
 
 
 
 
 
 and these values are everything not defined in 'progress_bar', 'log' and 'hidden', see here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/logging.py
 
 
         Lines 106 to 109
       in
       774d9be
 
 
 
 
 
 
  callback_metrics = {} 
 
 
 
  for k, v in output.items(): 
 
 
 
  if k not in ['progress_bar', 'log', 'hiddens']: 
 
 
 
  callback_metrics[k] = v 
 
 
 
 
 
 since you are returning 'val_loss' as a separate key this should however not be the problem.
 Since only 'loss' is available as a callback_metrics it seems that your lr scheduler is called before your validation data is processed. What does your configure_optimizers() look like?
 		</comment>
 		<comment id='2' author='LucFrachon' date='2020-03-16T15:24:11Z'>
 		Thanks for your answer. If I understand correctly, ReduceLROnPlateau is the only LR scheduler that does not use values from progress_bar?
 Here's configure_optimizers():
 &lt;denchmark-code&gt;    def configure_optimizers(self):
         self.optim = torch.optim.AdamW(self.parameters(), lr=self.hp.lr_ini, eps=1.e-4)
         self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optim, 'max', factor=self.hp.lr_reduce_factor,
                                                                     patience=self.hp.lr_reduce_patience,
                                                                     min_lr=self.hp.lr_min)
         return [self.optim], [self.scheduler]
 &lt;/denchmark-code&gt;
 
 And here is the Trainer definition:
 &lt;denchmark-code&gt;trainer = pl.Trainer(gpus=-1, early_stop_callback=None, max_epochs=stgs.PRED_HPARAMS['max_epochs'])
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='LucFrachon' date='2020-03-17T14:04:20Z'>
 		Only ReduceLROnPlateau schedulers are conditioned on some value. All other LR schedulers have nothing to do with callback_metrics or progress_bar. Hope this answers your question.
 I don't see any obvious mistakes in your code. As a said in my earlier comment, my best guess is that your ReduceLROnlr scheduler is called before val_loss` is actually calculated. You probably need to check this.
 		</comment>
 		<comment id='4' author='LucFrachon' date='2020-03-17T18:32:55Z'>
 		I have the same issue. That's how I deal with it:
     def training_step(self, batch, batch_idx):
         loss, lm_loss, mc_loss = self.forward(batch)
         lr = self.trainer.optimizers[0].param_groups[0]['lr']
         lr = torch.tensor(lr).unsqueeze(0).to(loss.device) if len(loss.size()) else lr
         log = {
             'MC-Loss/train': mc_loss,
             'LM-Loss/train': lm_loss,
             'Learning-Rate': lr
         }
         # Set up placeholders for valid metrics.
         if self.trainer.global_step == 0:
             log.update({'LM-Loss/valid': np.inf})
 
         return {'loss': loss, 'log': log}
 here I check if self.trainer.global_step == 0 and if yes, I set up a placeholder.
 &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
  maybe you can advice a better way
 		</comment>
 		<comment id='5' author='LucFrachon' date='2020-03-17T22:04:57Z'>
 		
 Only ReduceLROnPlateau schedulers are conditioned on some value. All other LR schedulers have nothing to do with callback_metrics or progress_bar. Hope this answers your question.
 I don't see any obvious mistakes in your code. As a said in my earlier comment, my best guess is that your ReduceLROnlr scheduler is called before val_loss` is actually calculated. You probably need to check this.
 
 &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
  Not sure I follow you. Maybe I misunderstood, but doesn't PL automatically handle calls to the scheduler? Doesn't it defeat (some of) the purpose of PL if I have to call it myself?
 &lt;denchmark-link:https://github.com/alexeykarnachev&gt;@alexeykarnachev&lt;/denchmark-link&gt;
  Thanks, I'll try that, but maybe there is a less hacky way...?
 		</comment>
 		<comment id='6' author='LucFrachon' date='2020-03-19T15:05:06Z'>
 		&lt;denchmark-link:https://github.com/LucFrachon&gt;@LucFrachon&lt;/denchmark-link&gt;
  sorry, I must have misinterpret what you asked about. Yes, PL automatically calls the scheduler you define. Sorry for the confusion.
 &lt;denchmark-link:https://github.com/alexeykarnachev&gt;@alexeykarnachev&lt;/denchmark-link&gt;
  I think that indicates that there is some bug here, since you need to setup a placeholder for the first step. I will see if I can come up with a more permanent solution.
 		</comment>
 		<comment id='7' author='LucFrachon' date='2020-03-22T12:43:02Z'>
 		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
  , btw, the trick:
         if self.trainer.global_step == 0:
             log.update({'Loss/valid': np.inf})
 doesn't help in case of warm start from checkpoint, because in warm start, the global_step is never equal to 0 :)
 For now I did it like this:
         # Set up placeholders for valid metrics.
         if not self._valid_metrics_patched:
             log.update({'Loss/valid': np.inf})
             self._valid_metrics_patched = True
 		</comment>
 		<comment id='8' author='LucFrachon' date='2020-03-23T14:31:40Z'>
 		&lt;denchmark-link:https://github.com/alexeykarnachev&gt;@alexeykarnachev&lt;/denchmark-link&gt;
  I have a hard time reproducing this bug, it seems to be a very corner case. Do you have a simple model that I can reproduce?
 		</comment>
 		<comment id='9' author='LucFrachon' date='2020-03-23T19:36:37Z'>
 		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 
 Yes, sure:
 &lt;denchmark-link:https://gist.github.com/alexeykarnachev/efbd40f0ff0cfcd1e324de044a802c25&gt;https://gist.github.com/alexeykarnachev/efbd40f0ff0cfcd1e324de044a802c25&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='10' author='LucFrachon' date='2020-03-25T15:40:46Z'>
 		Okay, after looking at your code &lt;denchmark-link:https://github.com/alexeykarnachev&gt;@alexeykarnachev&lt;/denchmark-link&gt;
 , this does not seems to be a bug. When you set  you are calling the  method for  after each batch and it therefore makes complete sense that no  is calculated yet. If you really want to do something like this, you need to set  in the  construction to a number lower than  in the scheduler construction. In this way  will be calculated before  is called.
 		</comment>
 		<comment id='11' author='LucFrachon' date='2020-03-26T10:09:51Z'>
 		Thx, now it's more clear.
 Does it mean, that I can't use a schedulers, which are depend on some valid metric with interval="step"?
 For instance, I have a custom reduce on plateau scheduler, which just ignores nan metric values, that's why I need to set a placeholder for valid metric (like in my first example).
 		</comment>
 		<comment id='12' author='LucFrachon' date='2020-03-30T12:23:25Z'>
 		I do not think it is possible just out of the box. However, if you configure your scheduler correctly, then it should be possible. For example, if I initialize my Trainer as
 trainer = Trainer(val_check_interval=50)
 and initialize my scheduler as
 &lt;denchmark-code&gt;scheduler = {
     'schduler': ReduceLROnPlateau(optimizer, mode, factor, patience),
     'interval': 'step',
     'frequency': 100
 }
 &lt;/denchmark-code&gt;
 
 it should work (not tested), since val_loss will be created every 50 steps but the scheduler will first be called after 100 steps.
 		</comment>
 		<comment id='13' author='LucFrachon' date='2020-03-30T13:32:50Z'>
 		oh, right, I missed the frequency argument. Totally forgot about it. I'll try it. Thank you!
 		</comment>
 		<comment id='14' author='LucFrachon' date='2020-05-29T14:54:40Z'>
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		</comment>
 		<comment id='15' author='LucFrachon' date='2020-06-05T10:05:23Z'>
 		shall be fixed with &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1989&gt;#1989&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='711892a0a293f7c7f951eba0907e1c0ccd2b37d8' author='authman' date='2020-03-19 09:22:29-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\source\optimizers.rst' new_name='docs\source\optimizers.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>22,23,24,25,26,27,28,29,30,31,32,33,34,35</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\lightning.py' new_name='pytorch_lightning\core\lightning.py'>
 		<file_info nloc='1407' complexity='56' token_count='1746'></file_info>
 		<modified_lines>
 			<added_lines>945,947,949,950,951,953,955,957,958,959,960,961,962,963,964,965</added_lines>
 			<deleted_lines>945,946,948,949,951,952,953,954,955,956,957,959,960,962,963,965,966</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
