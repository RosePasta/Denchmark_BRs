<bug_data>
<bug id='2372' author='xiadingZ' open_date='2020-06-26T13:35:38Z' closed_time='2020-06-30T14:03:50Z'>
 	<summary>training_epoch_end's outputs doesn't have 'loss' key</summary>
 	<description>
 pytorch-lightning: build from master
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "main.py", line 140, in &lt;module&gt;
     main(hparams)
   File "main.py", line 72, in main
     trainer.fit(model)
   File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 881, in fit
     self.ddp_train(task, model)
   File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 539, in ddp_train
     self.run_pretrain_routine(model)
   File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1091, in run_pretrain_routine
     self.train()
   File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 376, in train
     self.run_training_epoch()
   File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 510, in run_training_epoch
     self.run_training_epoch_end(epoch_output)
   File "/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 535, in run_training_epoch_end
     epoch_output = model.training_epoch_end(epoch_output)
   File "/mnt/lustre/maxiao1/PVM/models/baseline.py", line 335, in training_epoch_end
     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
   File "/mnt/lustre/maxiao1/PVM/models/baseline.py", line 335, in &lt;listcomp&gt;
     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
 KeyError: 'loss'
 &lt;/denchmark-code&gt;
 
 This is my code:
 &lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
         ...
         return {'loss': loss, "train_acc": acc}
 
     def training_epoch_end(self, outputs):
         avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
         avg_acc = torch.stack([x['train_acc'] for x in outputs]).mean()
         logs = {'loss': avg_loss, 'train_acc': avg_acc}
         progress_bar = {'train_loss': avg_loss, 'train_acc': avg_acc}
         results = {
             'log': logs,
             'progress_bar': progress_bar
         }
         return results
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='xiadingZ' date='2020-06-26T21:54:01Z'>
 		Try: avg_loss = torch.stack([x['batch_loss'] for x in outputs]).mean()
 		</comment>
 		<comment id='2' author='xiadingZ' date='2020-06-27T01:35:18Z'>
 		Thanksï¼Œ it works
 but 'train_acc' key doesn't exist, neither do batch_train_acc. How to access other keys returned in training_step?
 		</comment>
 		<comment id='3' author='xiadingZ' date='2020-06-27T11:35:13Z'>
 		As of now in lightning you can access them using x['callback_metrics']['loss'] and x['callback_metrics']['train_acc'], but I think it should be handled in a similar way we do this with validation_epoch_end and test_epoch_end.
 		</comment>
 		<comment id='4' author='xiadingZ' date='2020-06-29T16:47:24Z'>
 		Hi! One hint: for me it works with "loss" under windows but not under ubuntu.
 		</comment>
 		<comment id='5' author='xiadingZ' date='2020-06-29T17:20:30Z'>
 		Weird!! Why is this think platform dependent?? ðŸ¤”
 		</comment>
 		<comment id='6' author='xiadingZ' date='2020-06-30T07:45:37Z'>
 		&lt;denchmark-link:https://github.com/Pet222&gt;@Pet222&lt;/denchmark-link&gt;
  , are u sure that versions on ubuntu and windows are same?
 		</comment>
 		<comment id='7' author='xiadingZ' date='2020-06-30T10:29:19Z'>
 		Hey &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  is this intended behaviour? I was surprised to see this breaking change being introduced with no warning.
 If it is intended, why not have consistent behaviour over  and .
 If it is not intended, as it seems due to the "bug fix" tag, are you working on it or should I make a PR for this?
 		</comment>
 		<comment id='8' author='xiadingZ' date='2020-06-30T11:43:59Z'>
 		what is the behavior? that the "loss" key is not in training_epoch_end? If so, that's a bug because it should be there
 		</comment>
 		<comment id='9' author='xiadingZ' date='2020-06-30T11:49:52Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  , on the latest version, the  key was changed to the . I think it was changed &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/0f073819d3e0df8db7602eab489b1bad0fc0949c#diff-c45bd21c331565cbe62aaa12fa43aa0aR717&gt;here&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='10' author='xiadingZ' date='2020-06-30T11:50:10Z'>
 		Yes, the fact that you need to access it through 'callback metrics'.
 Got it!
 On Tue, 30 Jun 2020 at 12:44, William Falcon ***@***.***&gt; wrote:
  what is the behavior? that the "loss" key is not in training_epoch_end? If
  so, that's a bug because it should be there
 
  â€”
  You are receiving this because you commented.
  Reply to this email directly, view it on GitHub
  &lt;&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2372#issuecomment-651740702&gt;#2372 (comment)&lt;/denchmark-link&gt;
 &gt;,
  or unsubscribe
  &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ&gt;https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ&lt;/denchmark-link&gt;
 &gt;
  .
 
 -- 
 Best Regards,
 Miguel Vera
 
 +351 915 198 452
 miguel.coimbra.vera@protonmail.com
 Github/Captainvera &lt;&lt;denchmark-link:http://www.github.com/captainvera&gt;http://www.github.com/captainvera&lt;/denchmark-link&gt;
 &gt;
 		</comment>
 		<comment id='11' author='xiadingZ' date='2020-06-30T12:19:01Z'>
 		&lt;denchmark-link:https://github.com/captainvera&gt;@captainvera&lt;/denchmark-link&gt;
  would love a PR :)
 		</comment>
 		<comment id='12' author='xiadingZ' date='2020-06-30T13:24:38Z'>
 		&lt;denchmark-link:https://github.com/captainvera&gt;@captainvera&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/xiadingZ&gt;@xiadingZ&lt;/denchmark-link&gt;
  sorry about that! it was a bad bug.
 Made a PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2428&gt;#2428&lt;/denchmark-link&gt;
   and added tests to make sure this doesn't happen again!
 try master now!
 weâ€™ll push a new minor again since this is a key bug (and we have a few other key bugs)
 		</comment>
 		<comment id='13' author='xiadingZ' date='2020-06-30T14:11:11Z'>
 		Well, that was fast, thanks!
 		</comment>
 	</comments>
 </bug>
<commit id='a42a0e16ddd75dd7199ecefe4d10c2941c17ba76' author='William Falcon' date='2020-06-30 10:03:49-04:00'>
 	<dmm_unit complexity='0.0' interfacing='0.9166666666666666' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='636' complexity='154' token_count='3246'></file_info>
 		<method name='optimizer_closure' parameters='self,split_batch,batch_idx,opt_idx,optimizer,hiddens'>
 				<method_info nloc='40' complexity='7' token_count='280' nesting_level='1' start_line='759' end_line='828'></method_info>
 			<added_lines>779,792</added_lines>
 			<deleted_lines>791</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\base\deterministic_model.py' new_name='tests\base\deterministic_model.py'>
 		<file_info nloc='100' complexity='26' token_count='1007'></file_info>
 		<method name='training_step_end_dict' parameters='self,output'>
 				<method_info nloc='13' complexity='1' token_count='117' nesting_level='1' start_line='76' end_line='96'></method_info>
 			<added_lines>92,93,96</added_lines>
 			<deleted_lines>92,93,96</deleted_lines>
 		</method>
 		<method name='training_epoch_end_dict' parameters='self,outputs'>
 				<method_info nloc='17' complexity='6' token_count='138' nesting_level='1' start_line='98' end_line='118'></method_info>
 			<added_lines>107,108,109,110,114</added_lines>
 			<deleted_lines>107,108,112</deleted_lines>
 		</method>
 		<method name='training_step_dict_return' parameters='self,batch,batch_idx'>
 				<method_info nloc='6' complexity='1' token_count='114' nesting_level='1' start_line='54' end_line='61'></method_info>
 			<added_lines>61</added_lines>
 			<deleted_lines>61</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_trainer_steps.py' new_name='tests\trainer\test_trainer_steps.py'>
 		<file_info nloc='100' complexity='8' token_count='662'></file_info>
 		<method name='test_training_step_dict' parameters='tmpdir'>
 				<method_info nloc='27' complexity='2' token_count='161' nesting_level='0' start_line='5' end_line='41'></method_info>
 			<added_lines>34,35,36,37,38,39</added_lines>
 			<deleted_lines>34</deleted_lines>
 		</method>
 		<method name='test_train_step_epoch_end' parameters='tmpdir'>
 				<method_info nloc='23' complexity='2' token_count='166' nesting_level='0' start_line='118' end_line='152'></method_info>
 			<added_lines>149,150</added_lines>
 			<deleted_lines>141</deleted_lines>
 		</method>
 		<method name='training_step_with_step_end' parameters='tmpdir'>
 				<method_info nloc='21' complexity='2' token_count='147' nesting_level='0' start_line='44' end_line='74'></method_info>
 			<added_lines>67,68,70,71,72,73,74</added_lines>
 			<deleted_lines>62,63,65,66,67</deleted_lines>
 		</method>
 		<method name='test_full_training_loop_dict' parameters='tmpdir'>
 				<method_info nloc='27' complexity='2' token_count='172' nesting_level='0' start_line='77' end_line='115'></method_info>
 			<added_lines>109,110,112,113,114,115</added_lines>
 			<deleted_lines>102,103,105,106,107</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
