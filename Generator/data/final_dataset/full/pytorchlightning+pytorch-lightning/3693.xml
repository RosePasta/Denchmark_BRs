<bug_data>
<bug id='3693' author='chrismaliszewski' open_date='2020-09-28T04:34:38Z' closed_time='2020-10-02T19:46:47Z'>
 	<summary>Missing attribute "training_step_output_for_epoch_end"</summary>
 	<description>
 I used the documentation way of stopping the training (&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html#enable-early-stopping-using-callbacks-on-epoch-end&gt;https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html#enable-early-stopping-using-callbacks-on-epoch-end&lt;/denchmark-link&gt;
 ).
 If on_bath_start method returns -1 at the very beginning of an epoch, the titled AttributeError exception.
 The problem is in training_loop.py line 496 (batch_output.training_step_output_for_epoch_end).
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 Use the method and run your code:
     def on_batch_start(self, batch):
         return -1
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Check batch_output value if equals -1 before running trainin_loop.py line 495.
 The early stopping method achieved the same way the documentation specifies should not throw an exception but rather simply stop the training.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           None
 
 
 Packages:
 
 numpy:             1.19.1
 pyTorch_debug:     False
 pyTorch_version:   1.6.0
 pytorch-lightning: 0.9.0
 tqdm:              4.49.0
 
 
 System:
 
 OS:                Windows
 architecture:
 
 64bit
 WindowsPE
 
 
 processor:         Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
 python:            3.8.5
 version:           10.0.18362
 
 
 
 	</description>
 	<comments>
 		<comment id='1' author='chrismaliszewski' date='2020-09-28T04:35:19Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='chrismaliszewski' date='2020-10-03T04:24:44Z'>
 		&lt;denchmark-link:https://github.com/chrismaliszewski&gt;@chrismaliszewski&lt;/denchmark-link&gt;
  can you confirm this now stops the training epoch?
 		</comment>
 		<comment id='3' author='chrismaliszewski' date='2020-10-03T07:01:54Z'>
 		Should I update in conda command line, nothing has changed:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "XXX\__main__train.py", line 54, in &lt;module&gt;
     trainer.fit(model)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\states.py", line 48, in wrapped_fn
     result = fn(self, *args, **kwargs)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1084, in fit
     results = self.accelerator_backend.train(model)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\cpu_backend.py", line 39, in train
     results = self.trainer.run_pretrain_routine(model)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 1239, in run_pretrain_routine
     self.train()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 394, in train
     self.run_training_epoch()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 496, in run_training_epoch
     batch_output.training_step_output_for_epoch_end,
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\utilities\parsing.py", line 144, in __getattr__
     raise AttributeError(f'Missing attribute "{key}"')
 AttributeError: Missing attribute "training_step_output_for_epoch_end"
 Epoch 0:   0%|          | 0/4 [00:00&lt;?, ?it/s]
 
 Process finished with exit code 1
 &lt;/denchmark-code&gt;
 
 Or should I update directly from GitHub, i.e. using the method provided here: &lt;denchmark-link:https://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github&gt;https://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github&lt;/denchmark-link&gt;
 ?
 		</comment>
 		<comment id='4' author='chrismaliszewski' date='2020-10-03T07:06:01Z'>
 		Yes please, it's fixed on master, it hasn't been released yet. You can do the following in your conda or pip env
 &lt;denchmark-code&gt;pip install git+https://github.com/PyTorchLightning/pytorch-lightning.git@master --upgrade
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='chrismaliszewski' date='2020-10-03T07:27:42Z'>
 		After the update you suggested I have MAJOR problems, even crashing errors.
 In regards to the issue I posted, I have the following error, no matter if I return -1 or anything else:
 &lt;denchmark-code&gt;File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 442, in fit
     results = self.accelerator_backend.train()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\cpu_backend.py", line 47, in train
     results = self.train_or_test()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\base_backend.py", line 43, in train_or_test
     results = self.trainer.train()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 489, in train
     self.train_loop.run_training_epoch()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 516, in run_training_epoch
     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 617, in run_training_batch
     response = self.trainer.call_hook('on_batch_start')
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 807, in call_hook
     output = hook_fx(*args, **kwargs)
 TypeError: on_batch_start() missing 1 required positional argument: 'batch'
 Epoch 0:   0%|          | 0/4 [00:00&lt;?, ?it/s]
 &lt;/denchmark-code&gt;
 
 I haven't changed anything in the definition of my function and it looks as follows:
 &lt;denchmark-code&gt;    def on_batch_start(self, batch):
         return -1
         if self.get_early_stop(self.hparams['early_stop_path']):
             return -1
         else:
             return batch
 &lt;/denchmark-code&gt;
 
 where get_early_stop returns Boolean if the training should early stop at any given time.
 For the unknown reason, the args and kwargs in the line output = hook_fx(*args, **kwargs) are empty.
 If I remove method on_batch_start, the code follows further but crashes elsewhere, read the next comment.
 If you need further information, let me know and I try helping.
 		</comment>
 		<comment id='6' author='chrismaliszewski' date='2020-10-03T07:39:45Z'>
 		In terms of other problems with the version.
 I have a crashing error or a warning is being displayed. Messages exclude each other in the potential resolving way.
 Error message 1.
 &lt;denchmark-code&gt; File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 442, in fit
     results = self.accelerator_backend.train()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\cpu_backend.py", line 47, in train
     results = self.train_or_test()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\base_backend.py", line 43, in train_or_test
     results = self.trainer.train()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 489, in train
     self.train_loop.run_training_epoch()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py", line 539, in run_training_epoch
     self.trainer.run_evaluation(test_mode=False)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 604, in run_evaluation
     self.evaluation_loop.on_evaluation_epoch_end()
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\evaluation_loop.py", line 298, in on_evaluation_epoch_end
     self.trainer.call_hook('on_validation_epoch_end', *args, **kwargs)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 800, in call_hook
     trainer_hook(*args, **kwargs)
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\callback_hook.py", line 87, in on_validation_epoch_end
     callback.on_validation_epoch_end(self, self.get_model())
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\callbacks\early_stopping.py", line 152, in on_validation_epoch_end
     if self._validate_condition_metric(trainer.logger_connector.callback_metrics):
   File "YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\callbacks\early_stopping.py", line 116, in _validate_condition_metric
     raise RuntimeError(error_msg)
 RuntimeError: Early stopping conditioned on metric `val_loss` which is not available. Either add `val_loss` to the return of `validation_epoch_end` or modify your `EarlyStopping` callback to use any of the following: ``
 &lt;/denchmark-code&gt;
 
 Note the part Either add val_loss to the return of validation_epoch_end and that the error message is cut with nothing after the following:.
 Warning message 2.
 UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule
 after I remove return {'val_loss': loss} leaving just self.log("val_loss", loss). So which one should I do? return or not return?
 		</comment>
 		<comment id='7' author='chrismaliszewski' date='2020-10-03T07:50:12Z'>
 		Okay. Regarding the first issue,  is deprecated in 0.9 and will be removed in 1.0.
 &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_batch_start&gt;https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_batch_start&lt;/denchmark-link&gt;
 
 Please use .
 &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_train_batch_start&gt;https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_train_batch_start&lt;/denchmark-link&gt;
 
 Also you probably don't want to stop training epoch just at the start of the training, I assume.
     def on_train_batch_start(self, batch, batch_idx, dataloader_idx):
         if self.get_early_stop(self.hparams['early_stop_path']):
             return -1
         else:
             return batch
 		</comment>
 		<comment id='8' author='chrismaliszewski' date='2020-10-03T07:53:21Z'>
 		The method you provided works without any errors. Thank you for the advice.
 		</comment>
 		<comment id='9' author='chrismaliszewski' date='2020-10-03T07:56:57Z'>
 		Regarding the second issue, you can use  to use  in the early stop callback.
 For now, you can ignore the below warning, currently at fixing at &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3812&gt;#3812&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='10' author='chrismaliszewski' date='2020-10-03T07:58:29Z'>
 		&lt;denchmark-link:https://github.com/ydcjeff&gt;@ydcjeff&lt;/denchmark-link&gt;
 , I'll report you on that later. It's 9PM my time. Thank you.
 		</comment>
 		<comment id='11' author='chrismaliszewski' date='2020-10-03T07:59:54Z'>
 		Okay. Feel free to create an another issue, if something doesn't work with earlystopping
 		</comment>
 		<comment id='12' author='chrismaliszewski' date='2020-10-07T10:36:08Z'>
 		Forgot to report. The code you suggested works. Thank you again, &lt;denchmark-link:https://github.com/ydcjeff&gt;@ydcjeff&lt;/denchmark-link&gt;
 .
 		</comment>
 	</comments>
 </bug>
<commit id='9942f3ebdf14d0139b1b156dd56662b425f3c777' author='Jeff Yang' date='2020-10-02 21:46:46+02:00'>
 	<dmm_unit complexity='0.9375' interfacing='0.8125' size='0.9375'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>62,63</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\early_stopping.rst' new_name='docs\source\early_stopping.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>13</added_lines>
 			<deleted_lines>13</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='463' complexity='141' token_count='3659'></file_info>
 		<method name='run_training_epoch' parameters='self'>
 				<method_info nloc='45' complexity='10' token_count='324' nesting_level='1' start_line='494' end_line='594'></method_info>
 			<added_lines>518,519,520,521</added_lines>
 			<deleted_lines>530,531,532</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_hooks.py' new_name='tests\models\test_hooks.py'>
 		<file_info nloc='97' complexity='14' token_count='692'></file_info>
 		<method name='test_on_train_batch_start_hook.on_train_batch_start' parameters='self,batch,batch_idx,dataloader_idx'>
 				<method_info nloc='3' complexity='2' token_count='19' nesting_level='2' start_line='119' end_line='121'></method_info>
 			<added_lines>119,120,121</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_on_train_batch_start_hook' parameters='max_epochs,batch_idx_'>
 				<method_info nloc='12' complexity='2' token_count='98' nesting_level='0' start_line='117' end_line='131'></method_info>
 			<added_lines>117,118,119,120,121,122,123,124,125,126,127,128,129,130,131</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>111,112,113,114,115,116</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
