<bug_data>
<bug id='2551' author='moi90' open_date='2020-07-08T10:57:09Z' closed_time='2020-10-05T03:02:36Z'>
 	<summary>TrainerEvaluationLoopMixin activates model.train() at the end</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 According to the &lt;denchmark-link:https://github.com/jbschiratti/pytorch-lightning/blob/master/pl_examples/domain_templates/computer_vision_fine_tuning.py&gt;example on fine-tuning&lt;/denchmark-link&gt;
 , it is important to set the frozen sub-modules to eval mode. This is sensitive because when in training mode, BatchNorm and Dropout change state.
 However, at the end of TrainerEvaluationLoopMixin._evaluate there is following code:
 # enable train mode again
 model.train()
 So after the first validation run, the model is again completely in training mode and the freezing is partially undone (for layers like BatchNorm and Dropout).
 	</description>
 	<comments>
 		<comment id='1' author='moi90' date='2020-07-09T07:05:14Z'>
 		I agree, this should not happen in the validation loop. Only the training loop should switch the model to training mode.
 		</comment>
 		<comment id='2' author='moi90' date='2020-07-09T10:57:04Z'>
 		so, we have to track the state of all the frozen modules before?
 		</comment>
 		<comment id='3' author='moi90' date='2020-07-09T15:33:30Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  When accepting that the training mode could have been customized, not even the training loop should change it carelessly.
 &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  That would be one solution.
 Another would be to just advice people to set the mode in on_epoch_start. (Would this be late enough so it is not reset by the Trainer?)
 		</comment>
 		<comment id='4' author='moi90' date='2020-09-07T16:13:44Z'>
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		</comment>
 		<comment id='5' author='moi90' date='2020-09-18T00:35:29Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/moi90&gt;@moi90&lt;/denchmark-link&gt;
  what if we just made this a hook? The default hook will implement what we have today, set the model to train or eval mode. But if user wants the fine tuning use case, they can override the hook and just set their layers to train/eval mode manually.
 
 old behaviour is preserved
 no complicated tracking needed
 in the finetuning case, it will be fully transparent to the reader of the code which layers are eval and which are training
 easy to implement
 
 		</comment>
 		<comment id='6' author='moi90' date='2020-09-19T07:29:58Z'>
 		Where and when would this hook be called? How would this be different from using on_epoch_start?
 		</comment>
 		<comment id='7' author='moi90' date='2020-09-28T04:30:46Z'>
 		
 Where and when would this hook be called?
 
 Wherever we call model.eval / model.train today in the training loop, we would call the hook instead, which by default also just does that same thing as before, unless user overrides it.
 
 How would this be different from using on_epoch_start?
 
 I think this would be different from an on epoch start because it would allow you to prevent exactly what happens as described in the title: activating model.train for all layers at the end of an epoch.
 		</comment>
 		<comment id='8' author='moi90' date='2020-10-02T20:49:22Z'>
 		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  can this be added post v1?
 		</comment>
 		<comment id='9' author='moi90' date='2020-10-02T21:30:08Z'>
 		Yes
 		</comment>
 		<comment id='10' author='moi90' date='2020-10-05T08:51:34Z'>
 		Thank you, this is much appreciated!
 		</comment>
 		<comment id='11' author='moi90' date='2020-12-10T15:41:42Z'>
 		Shouldn't the hooks be defined like:
 &lt;denchmark-code&gt;def on_validation_model_eval_end(self) -&gt; None:
   """
   Called in the validation loop after the model is set to eval
   """
   # do something when the model is set to eval
 &lt;/denchmark-code&gt;
 
 How do we use these hooks in the current version? If I don't want to switch to eval mode ever (a pretty common case with GANs), do I just override
 &lt;denchmark-code&gt;class DummyCallback(pl.Callback):
   def on_validation_model_eval():
     pass
 &lt;/denchmark-code&gt;
 
 or do I call on_validation_model_train() and then the eval hook won't be called?
 		</comment>
 	</comments>
 </bug>
<commit id='f58c7604093fc37c765ac88e46aaf52b403332fe' author='William Falcon' date='2020-10-04 23:02:35-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.21621621621621623' size='0.5405405405405406'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\hooks.py' new_name='pytorch_lightning\core\hooks.py'>
 		<file_info nloc='554' complexity='39' token_count='639'></file_info>
 		<method name='on_test_model_eval' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='3' nesting_level='1' start_line='211' end_line='212'></method_info>
 			<added_lines>211,212</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='on_validation_model_train' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='3' nesting_level='1' start_line='151' end_line='152'></method_info>
 			<added_lines>151,152</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='on_validation_model_eval' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='3' nesting_level='1' start_line='143' end_line='144'></method_info>
 			<added_lines>143,144</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='on_test_model_train' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='3' nesting_level='1' start_line='219' end_line='220'></method_info>
 			<added_lines>219,220</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>145,146,147,148,149,150,153,154,155,156,157,158,213,214,215,216,217,218,221,222,223,224,225,226</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\evaluation_loop.py' new_name='pytorch_lightning\trainer\evaluation_loop.py'>
 		<file_info nloc='236' complexity='85' token_count='1768'></file_info>
 		<method name='on_evaluation_model_eval' parameters='self,args,kwargs'>
 				<method_info nloc='6' complexity='2' token_count='37' nesting_level='1' start_line='91' end_line='96'></method_info>
 			<added_lines>91,92,93,94,95,96</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='on_evaluation_model_train' parameters='self,args,kwargs'>
 				<method_info nloc='6' complexity='2' token_count='37' nesting_level='1' start_line='98' end_line='103'></method_info>
 			<added_lines>98,99,100,101,102,103</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>97,104</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='595' complexity='60' token_count='2975'></file_info>
 		<method name='run_evaluation' parameters='self,bool,max_batches'>
 				<method_info nloc='45' complexity='8' token_count='362' nesting_level='1' start_line='537' end_line='625'></method_info>
 			<added_lines>546,547,619</added_lines>
 			<deleted_lines>547,618</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='tests\trainer\model_hooks\__init__.py' new_name='tests\trainer\model_hooks\__init__.py'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\trainer\model_hooks\test_model_hooks.py'>
 		<file_info nloc='24' complexity='1' token_count='133'></file_info>
 	</modification>
 </commit>
</bug_data>
