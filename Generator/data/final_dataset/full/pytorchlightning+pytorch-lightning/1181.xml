<bug_data>
<bug id='1181' author='TevenLeScao' open_date='2020-03-18T15:42:43Z' closed_time='2020-04-02T09:41:56Z'>
 	<summary>Additional dataloader created and discarded when training with reload_dataloaders_every_epoch</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 I am training with reload_dataloaders_every_epoch and I've noticed it instantiates an extra DataLoader before training for which nothing is run. This is an issue for me as I am training with chunks that get loaded every epoch and it is messing with the order I load them in especially if I reload a checkpoint; it would be an issue for people that generate a new dataset every epoch as they waste computation. The tqdm bar also keeps the information of the first, discarded DataLoader (in the screenshot, the number of iterations is the same for both whereas they should be different sizes)
 &lt;denchmark-link:https://user-images.githubusercontent.com/26709476/76968780-be917200-6929-11ea-82c5-6850a6f6e678.png&gt;&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Run the code sample below, which runs for one epoch and displays a message every time a DataLoader is created.
 A DataLoader gets instantiated a first time line 286 in training_loop.py outside of the epoch loop (that's the usual time it gets instantiated when not reloading every epoch. Then when using reload_dataloaders_every_epoch another one is created at the start of every epoch line 386, inside the loop, so for the first epoch there's an extra one.
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import torch
 import pytorch_lightning as pl
 from torch.utils.data import DataLoader, Dataset
 from time import sleep
 
 class MinimalDataset(Dataset):
 
     def __init__(self, index):
         self.data = torch.Tensor(64 * index, 1024)
 
     def __getitem__(self, item):
         return self.data[item]
 
     def __len__(self):
         return len(self.data)
 
 class MinimalModule(pl.LightningModule):
 
     def __init__(self):
         super(MinimalModule, self).__init__()
         self.nn = torch.nn.Linear(1024, 1)
         self.current_index = 0
 
     def forward(self, batch):
         return self.nn(batch)
 
     def training_step(self, batch, batch_idx):
         sleep(0.1)
         loss = self.nn(batch)[0]
         return {'loss': loss}
 
     def validation_step(self, batch, batch_idx):
         loss = self.nn(batch)[0]
         return {'val_loss': loss}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.01)
 
     def train_dataloader(self):
         # REQUIRED
         self.current_index += 1
         print(f"initializing DataLoader n{self.current_index}")
         data_loader = DataLoader(MinimalDataset(self.current_index))
         return data_loader
     
 model = MinimalModule()
 trainer = pl.Trainer(reload_dataloaders_every_epoch=True, num_sanity_val_steps=0, val_check_interval=8, max_epochs=1)
 
 trainer.fit(model)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Only one dataloader should be created; two are. The tqdm bar should show 128 iterations as that is the dataset size the second time; but it shows 64 instead (I added the sleep(0.1) to leave time to observe that)
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 PyTorch version: 1.4.0
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 OS: Ubuntu 18.04.4 LTS
 GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
 CMake version: Could not collect
 Python version: 3.6
 Is CUDA available: Yes
 CUDA runtime version: Could not collectepoch_end
 GPU models and configuration: GPU 0: GeForce RTX 2070 with Max-Q Design
 Nvidia driver version: 435.21
 cuDNN version: Could not collect
 Versions of relevant libraries:
 [pip3] numpy==1.18.1
 [pip3] pytorch-lightning==0.7.1
 [pip3] torch==1.4.0
 [pip3] torchvision==0.4.2
 [conda] Could not collect
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='TevenLeScao' date='2020-03-18T15:43:24Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 	</comments>
 </bug>
<commit id='04935ea7184a50d535af96dd85a58fdc43a659b8' author='Teven' date='2020-04-02 11:41:56+02:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>46</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\evaluation_loop.py' new_name='pytorch_lightning\trainer\evaluation_loop.py'>
 		<file_info nloc='324' complexity='58' token_count='1218'></file_info>
 		<method name='run_evaluation' parameters='self,bool'>
 				<method_info nloc='56' complexity='18' token_count='350' nesting_level='1' start_line='322' end_line='413'></method_info>
 			<added_lines>341,348,402,403,404,405,406,407,408,409,410</added_lines>
 			<deleted_lines>341,348</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='762' complexity='70' token_count='3376'></file_info>
 		<modified_lines>
 			<added_lines>321,322</added_lines>
 			<deleted_lines>277</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='522' complexity='121' token_count='2557'></file_info>
 		<method name='run_training_epoch' parameters='self'>
 				<method_info nloc='58' complexity='34' token_count='468' nesting_level='1' start_line='386' end_line='499'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>397,398,399,400</deleted_lines>
 		</method>
 		<method name='train' parameters='self'>
 				<method_info nloc='61' complexity='23' token_count='413' nesting_level='1' start_line='285' end_line='389'></method_info>
 			<added_lines>293,294,295,311,312,313</added_lines>
 			<deleted_lines>293</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
