<bug_data>
<bug id='2254' author='s-rog' open_date='2020-06-19T02:37:22Z' closed_time='2020-07-10T01:20:18Z'>
 	<summary>Single node DDP: "Default process group is not initialized"</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Unable to start single node ddp training on 0.8.0
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 was going to run the gpu_template but... #2235
 both methods of running the template result in the same error
 &lt;denchmark-code&gt;$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp_spawn
 $ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 CUDA_VISIBLE_DEVICES: [0,1,2,3]
 Traceback (most recent call last):
   File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
     "__main__", mod_spec)
   File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
     exec(code, run_globals)
   File "/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py", line 80, in &lt;module&gt;
     main(hyperparams)
   File "/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py", line 41, in main
     trainer.fit(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 860, in fit
     self.barrier('fit_prepare_data')
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1261, in barrier
     torch_distrib.barrier()
   File "/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 1484, in barrier
     _check_default_pg()
   File "/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 187, in _check_default_pg
     "Default process group is not initialized"
 AssertionError: Default process group is not initialized
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='s-rog' date='2020-06-19T02:47:50Z'>
 		can you post code to reproduce? just a minimal example that breaks
 BTW, the GPU template is fixed...
 		</comment>
 		<comment id='2' author='s-rog' date='2020-06-19T02:50:00Z'>
 		done, let me post my env as well
 		</comment>
 		<comment id='3' author='s-rog' date='2020-06-19T02:50:36Z'>
 		ok wait... i think i see it. one sec
 		</comment>
 		<comment id='4' author='s-rog' date='2020-06-19T04:50:07Z'>
 		I just tested the merged changes with both ddp and ddp_spawn again got this:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py", line 80, in &lt;module&gt;
     main(hyperparams)
   File "/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py", line 41, in main
     trainer.fit(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 891, in fit
 Traceback (most recent call last):
   File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
     "__main__", mod_spec)
   File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
     self.ddp_train(task, model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 479, in ddp_train
     exec(code, run_globals)
   File "/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py", line 80, in &lt;module&gt;
     main(hyperparams)
   File "/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py", line 41, in main
     trainer.fit(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 907, in fit
     self.setup()
 TypeError: setup() missing 1 required positional argument: 'stage'
     self.spawn_ddp_children(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 441, in spawn_ddp_children
     self.ddp_train(local_rank, model, is_master=True)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 479, in ddp_train
     self.setup()
 TypeError: setup() missing 1 required positional argument: 'stage'
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='s-rog' date='2020-06-19T05:14:30Z'>
 		try again. that was a typo
 		</comment>
 		<comment id='6' author='s-rog' date='2020-06-19T05:47:52Z'>
 		cheers, works now!
 		</comment>
 		<comment id='7' author='s-rog' date='2020-06-23T05:35:19Z'>
 		Still having the Default process group is not initialized issue when using trainer.test
 		</comment>
 		<comment id='8' author='s-rog' date='2020-06-23T06:30:56Z'>
 		
 Still having the Default process group is not initialized issue when using trainer.test
 
 I still have this bug as well. One temporary solution is creating a new single GPU trainer to do the test.
 Like
 &lt;denchmark-code&gt;trainer = Trainer(gpus=1, deterministic=True, logger=logger)
 trainer.model = model
 trainer.test()
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='9' author='s-rog' date='2020-06-23T19:57:28Z'>
 		Right, I know it works on single gpu. I have a large test set and ideally want faster inference using multiple gpus.
 		</comment>
 		<comment id='10' author='s-rog' date='2020-07-02T15:11:23Z'>
 		Can we re-open this issue? I am still having the Default process group is not initialized issue when I hit trainer.test() with ddp (with any number of gpus, even 1). I'm using the latest release from yesterday.
 		</comment>
 		<comment id='11' author='s-rog' date='2020-07-02T15:33:13Z'>
 		+1, doesn't look like the issue is resolved yet.
 		</comment>
 		<comment id='12' author='s-rog' date='2020-07-04T05:32:04Z'>
 		having the same problem..... I also tried to downgrade pl to an older version, like 0.7.5, and try to using the older version to do the inference. But, the model trained and saved using the 0.8.x seems to not directly be compatible with older version.
 		</comment>
 		<comment id='13' author='s-rog' date='2020-07-09T12:11:00Z'>
 		version: 0.8.4  train with ddp,  Got "Default process group is not initialized" when run trainer.test()
 		</comment>
 		<comment id='14' author='s-rog' date='2020-07-09T12:18:32Z'>
 		could you try master? this is fixed there
 		</comment>
 		<comment id='15' author='s-rog' date='2020-07-09T19:06:49Z'>
 		Just tried it, it works fine now! Thank you!
 		</comment>
 		<comment id='16' author='s-rog' date='2020-08-17T19:13:27Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  Trying 0.8.5
 Trained with ddp, and testing with ddp, but got the following error message:
 &lt;denchmark-code&gt;AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.
 &lt;/denchmark-code&gt;
 
 Any idea?
 Thanks!
 		</comment>
 	</comments>
 </bug>
<commit id='57d5f6e74a3bcd8f5c73211ba3a4e2480fcc1114' author='William Falcon' date='2020-06-19 00:42:20-04:00'>
 	<dmm_unit complexity='0.0' interfacing='0.5882352941176471' size='0.29411764705882354'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\source\trainer.rst' new_name='docs\source\trainer.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>12</added_lines>
 			<deleted_lines>12</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\hooks.py' new_name='pytorch_lightning\core\hooks.py'>
 		<file_info nloc='169' complexity='20' token_count='303'></file_info>
 		<method name='teardown' parameters='self,str'>
 				<method_info nloc='1' complexity='1' token_count='10' nesting_level='1' start_line='28' end_line='34'></method_info>
 			<added_lines>33</added_lines>
 			<deleted_lines>33</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_data_parallel.py' new_name='pytorch_lightning\trainer\distrib_data_parallel.py'>
 		<file_info nloc='434' complexity='98' token_count='2040'></file_info>
 		<method name='ddp_train' parameters='self,process_idx,model,is_master,proc_offset'>
 				<method_info nloc='45' complexity='15' token_count='373' nesting_level='1' start_line='443' end_line='530'></method_info>
 			<added_lines>478,479,480,481,482</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\distrib_parts.py' new_name='pytorch_lightning\trainer\distrib_parts.py'>
 		<file_info nloc='327' complexity='88' token_count='1953'></file_info>
 		<method name='tpu_train' parameters='self,tpu_core_idx,model'>
 				<method_info nloc='20' complexity='7' token_count='179' nesting_level='1' start_line='178' end_line='215'></method_info>
 			<added_lines>179,180,181,182,183</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='horovod_train' parameters='self,model'>
 				<method_info nloc='33' complexity='12' token_count='289' nesting_level='1' start_line='262' end_line='322'></method_info>
 			<added_lines>263,264,265,266,267</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='dp_train' parameters='self,model'>
 				<method_info nloc='25' complexity='9' token_count='203' nesting_level='1' start_line='217' end_line='260'></method_info>
 			<added_lines>218,219,220,221</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='single_gpu_train' parameters='self,model'>
 				<method_info nloc='11' complexity='4' token_count='108' nesting_level='1' start_line='157' end_line='176'></method_info>
 			<added_lines>158,159,160,161,162</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='1006' complexity='103' token_count='4304'></file_info>
 		<modified_lines>
 			<added_lines>894,899,906,929,930,931,948,949,950,951,952</added_lines>
 			<deleted_lines>860,861,862,863,864,865,900,905,912</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='557' complexity='131' token_count='2728'></file_info>
 		<method name='is_function_implemented' parameters='self,args'>
 				<method_info nloc='1' complexity='1' token_count='9' nesting_level='1' start_line='261' end_line='262'></method_info>
 			<added_lines>262</added_lines>
 			<deleted_lines>261</deleted_lines>
 		</method>
 		<method name='run_training_teardown' parameters='self'>
 				<method_info nloc='13' complexity='7' token_count='92' nesting_level='1' start_line='702' end_line='723'></method_info>
 			<added_lines>705,706,707,708,709</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='is_function_implemented' parameters='self,args,kwargs'>
 				<method_info nloc='1' complexity='1' token_count='12' nesting_level='1' start_line='262' end_line='263'></method_info>
 			<added_lines>262</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>156</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
