<bug_data>
<bug id='3424' author='GimmickNG' open_date='2020-09-09T16:18:23Z' closed_time='2020-10-01T08:33:13Z'>
 	<summary>DataModule with lr_find not supported</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Create a Trainer (trainer), a LightningModule (model) and a DataModule (data_module)
 Call trainer.lr_find(model, datamodule=data_module)
 Error: TypeError: lr_find() got an unexpected keyword argument 'datamodule'
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import torch
 import torch.nn as nn
 import pytorch_lightning as pl
 from torch.utils.data import DataLoader
 class DataModule(pl.LightningDataModule):
     def __init__(self):
         super().__init__()
     def gen_set(self, start, end):
         vals = torch.tensor([float(i) for i in range(0, 800)])
         return torch.stack([torch.sin(vals), torch.cos(vals)], 1)
     def train_dataloader(self):
         return DataLoader(dataset=self.gen_set(0, 800))
     def val_dataloader(self):
         return DataLoader(dataset=self.gen_set(800, 900))
     def test_dataloader(self):
         return DataLoader(dataset=self.gen_set(900, 1000))
 
 class LitModel(pl.LightningModule):
     def __init__(self, in_features):
         super().__init__()
         self.in_layer = nn.Linear(in_features, in_features//2)
         self.out_layer = nn.Linear(in_features//2, in_features)
         self.criterion = nn.MSELoss()
         self.lr = 0.001
     def forward(self, inp):
         inp = torch.tanh(self.in_layer(inp))
         return self.out_layer(inp)
     def training_step(self, batch, batch_idx):
         x = batch
         y_hat = self(x)
         loss = self.criterion(y_hat, x.float())
         return pl.TrainResult(minimize=loss)
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=(self.lr or self.learning_rate))
     
 data_module = DataModule()
 trainer = pl.Trainer()
 model = LitModel(in_features=2)
 lr_finder = trainer.lr_find(model, datamodule=data_module)   #error
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The lr_find() function should work as if it were passed a train_loader and test_loader when invoking it, just like fit()
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.5.1
 pytorch-lightning: 0.9.0
 tensorboard:       2.2.0
 tqdm:              4.45.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 processor:         x86_64
 python:            3.7.6
 
 
 
 	</description>
 	<comments>
 		<comment id='1' author='GimmickNG' date='2020-09-09T16:19:18Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='GimmickNG' date='2020-09-10T17:13:45Z'>
 		&lt;denchmark-link:https://github.com/nateraw&gt;@nateraw&lt;/denchmark-link&gt;
  take a look?
 		</comment>
 	</comments>
 </bug>
<commit id='e4e60e9b82adc48482db4721ce3e1fdc3ab6d6fe' author='GimmickNG' date='2020-10-01 10:33:12+02:00'>
 	<dmm_unit complexity='1.0' interfacing='0.875' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>30,31</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\tuner\lr_finder.py' new_name='pytorch_lightning\tuner\lr_finder.py'>
 		<file_info nloc='357' complexity='46' token_count='1753'></file_info>
 		<method name='lr_find' parameters='trainer,LightningModule,None,DataLoader,None,float,float,int,str,float,None'>
 				<method_info nloc='11' complexity='1' token_count='77' nesting_level='0' start_line='66' end_line='76'></method_info>
 			<added_lines>76</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14,16,17,18,20,22,23,24,25,87,104,105,106,107,108,109,179,180</added_lines>
 			<deleted_lines>16,21,22,23,24,25,28,84,170</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\tuner\tuning.py' new_name='pytorch_lightning\tuner\tuning.py'>
 		<file_info nloc='52' complexity='6' token_count='300'></file_info>
 		<method name='lr_find' parameters='self,LightningModule,None,DataLoader,None,float,float,int,str,float,None'>
 				<method_info nloc='11' complexity='1' token_count='76' nesting_level='1' start_line='44' end_line='54'></method_info>
 			<added_lines>54</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>18,65,66</added_lines>
 			<deleted_lines>63</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_lr_finder.py' new_name='tests\trainer\test_lr_finder.py'>
 		<file_info nloc='158' complexity='18' token_count='922'></file_info>
 		<method name='test_datamodule_parameter' parameters='tmpdir'>
 				<method_info nloc='14' complexity='1' token_count='80' nesting_level='0' start_line='156' end_line='177'></method_info>
 			<added_lines>156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>8,178,179</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
