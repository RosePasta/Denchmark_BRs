<bug_data>
<bug id='2484' author='wietsedv' open_date='2020-07-03T09:48:35Z' closed_time='2020-08-19T23:01:56Z'>
 	<summary>Trainer.scale_batch_size requires model.batch_size instead of model.hparams.batch_size</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Trainer.scale_batch_size only works if a model has the batch_size property and does not work with model.hparams.batch_size even though all documentation points to the reverse.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 All of my hyperparameters are available as model.hparams like suggested in the documentation: (&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/hyperparameters.html#lightningmodule-hyperparameters&gt;hyperparameters, option 3&lt;/denchmark-link&gt;
 .
 This means that my  is available as .
 This should be fully compatible with the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/training_tricks.html#auto-scaling-of-batch-size&gt;documented example code&lt;/denchmark-link&gt;
  of  since that code also uses  instead of .
 However, when I put my model in Trainer.scale_batch_size, I get the following error:
 &lt;denchmark-code&gt;pytorch_lightning.utilities.exceptions.MisconfigurationException: Field batch_size not found in `model.hparams`
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Example code&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;class LitModel(pl.LightningModule):
     def __init__(self, hparams):
         super().__init__()
         self.hparams = args
 
 model = LitModel(args)
 trainer = Trainer()
 trainer.scale_batch_size(model)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Either  should work with  or the error message, linked documentation examples and docstrings should all change (i.e. &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L139&gt;here&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L228&gt;here&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L233&gt;here&lt;/denchmark-link&gt;
 ).
 (I would prefer the second option. I think that it should work with both model.batch_size and model.hparams.batch_size.)
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 pytorch-lightning         0.8.4
 
 	</description>
 	<comments>
 		<comment id='1' author='wietsedv' date='2020-07-03T09:49:38Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='wietsedv' date='2020-07-03T10:53:40Z'>
 		it seems like a nice first issue, &lt;denchmark-link:https://github.com/wietsedv&gt;@wietsedv&lt;/denchmark-link&gt;
  mind send a PR? 
 		</comment>
 		<comment id='3' author='wietsedv' date='2020-07-04T05:10:40Z'>
 		Appears to be the same with the learning rate parameter.
 		</comment>
 		<comment id='4' author='wietsedv' date='2020-07-20T19:05:23Z'>
 		A clean fix on the user side while waiting for the PR is to actually use self.hparams.batch_size and define self.batch_size as a property of your module:
 @property
 def batch_size(self):
     return self.hparams.batch_size
 
 @batch_size.setter
 def batch_size(self, batch_size):
     self.hparams.batch_size = batch_size
 That way you keep your hyper parameters together in case you want to dump them somewhere without having to add specific code.
 		</comment>
 		<comment id='5' author='wietsedv' date='2020-07-24T15:29:35Z'>
 		From &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1896&gt;#1896&lt;/denchmark-link&gt;
  it seems that the problem is rather on the docs side than 's implementation.
  is the correct location to look for the parameter, not .
 My above fix is thus also obsolete.
 		</comment>
 		<comment id='6' author='wietsedv' date='2020-07-24T15:30:51Z'>
 		
 From #1896 it seems that the problem is rather on the docs side than scale_batch_size()'s implementation.
 My above fix is thus also obsolete.
 
 I just tried your fix and it seemed to work :)
 		</comment>
 		<comment id='7' author='wietsedv' date='2020-07-24T15:37:53Z'>
 		Yes it does work, but from what they said in the PR I linked, hparams was just there as a temporary solution, and all hyper parameters are intended to be set as direct instance attributes in __init__.
 My fix is obsolete regarding the intended usage of PL.
 		</comment>
 	</comments>
 </bug>
<commit id='7b917de94642f63eedaffde79fb973705d2288dd' author='Adrian W√§lchli' date='2020-08-19 19:01:55-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.7619047619047619' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>151,152,155</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_tricks.py' new_name='pytorch_lightning\trainer\training_tricks.py'>
 		<file_info nloc='250' complexity='41' token_count='1489'></file_info>
 		<modified_lines>
 			<added_lines>27,30,162,163,164,165,166,167,168,169,170,276,278,286</added_lines>
 			<deleted_lines>27,161,162,163,164,165,271,272,273,274,276,277,278,279,287</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\test_trainer_tricks.py' new_name='tests\trainer\test_trainer_tricks.py'>
 		<file_info nloc='193' complexity='19' token_count='1530'></file_info>
 		<method name='test_auto_scale_batch_size_duplicate_attribute_warning' parameters='tmpdir'>
 				<method_info nloc='8' complexity='1' token_count='62' nesting_level='0' start_line='240' end_line='249'></method_info>
 			<added_lines>240,241,242,243,244,245,246,247,248,249</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_trainer_arg' parameters='tmpdir,scale_arg'>
 				<method_info nloc='14' complexity='1' token_count='70' nesting_level='0' start_line='200' end_line='218'></method_info>
 			<added_lines>200,201,206,213,214,215,216,217,218</added_lines>
 			<deleted_lines>200,201,203,206,208,209,210,211,212,213,214</deleted_lines>
 		</method>
 		<method name='test_auto_scale_batch_size_trainer_arg' parameters='tmpdir,scale_arg'>
 				<method_info nloc='10' complexity='1' token_count='69' nesting_level='0' start_line='200' end_line='210'></method_info>
 			<added_lines>200,201,206</added_lines>
 			<deleted_lines>200,201,203,206,208,209,210</deleted_lines>
 		</method>
 		<method name='test_auto_scale_batch_size_set_model_attribute' parameters='tmpdir,use_hparams'>
 				<method_info nloc='12' complexity='3' token_count='89' nesting_level='0' start_line='214' end_line='237'></method_info>
 			<added_lines>214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237</added_lines>
 			<deleted_lines>214</deleted_lines>
 		</method>
 		<method name='test_auto_scale_batch_size_set_model_attribute.dataloader' parameters='self,args,kwargs'>
 				<method_info nloc='5' complexity='1' token_count='39' nesting_level='2' start_line='223' end_line='229'></method_info>
 			<added_lines>223,224,225,226,227,228,229</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>199,238,239,250,251</added_lines>
 			<deleted_lines>199</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
