<bug_data>
<bug id='67' author='qiulesun' open_date='2018-06-09T01:28:43Z' closed_time='2018-09-26T22:59:45Z'>
 	<summary>No module named cpp_extension</summary>
 	<description>
 Hi, I got the error named No module named cpp_extension (from torch.utils.cpp_extension import load) when I run the quick demo &lt;denchmark-link:http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package&gt;http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package&lt;/denchmark-link&gt;
 . The version of python and torch are 2.7 and 0.3.1 respectively. How can I handle it?
 	</description>
 	<comments>
 		<comment id='1' author='qiulesun' date='2018-06-09T18:18:37Z'>
 		0.3.1 is way too old. Please install PyTorch master branch &gt; 0.5.0
 		</comment>
 		<comment id='2' author='qiulesun' date='2018-06-15T06:22:06Z'>
 		The version of python and torch are updated to 3.6 and 0.4.0 respectively. Follow the link you provided &lt;denchmark-link:https://www.claudiokuenzler.com/blog/756/install-newer-ninja-build-tools-ubuntu-14.04-trusty#.WxYrvFMvzJw&gt;https://www.claudiokuenzler.com/blog/756/install-newer-ninja-build-tools-ubuntu-14.04-trusty#.WxYrvFMvzJw&lt;/denchmark-link&gt;
 , I install ninja 1.8.2. However, when I run again the quick demo &lt;denchmark-link:http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package&gt;http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package&lt;/denchmark-link&gt;
 , I got another error. How can I solve it? I believe your papers and code can make me interested in semantic segmentation tasks.
 root@hh-Z97X-UD3H:/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master# python quick_demo.py
 Traceback (most recent call last):
 File "/usr/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py", line 576, in _build_extension_module
 ['ninja', '-v'], stderr=subprocess.STDOUT, cwd=build_directory)
 File "/usr/anaconda3/lib/python3.6/subprocess.py", line 336, in check_output
 **kwargs).stdout
 File "/usr/anaconda3/lib/python3.6/subprocess.py", line 418, in run
 output=stdout, stderr=stderr)
 subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.
 During handling of the above exception, another exception occurred:
 Traceback (most recent call last):
 File "demo.py", line 2, in 
 import encoding
 File "/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/init.py", line 13, in 
 from . import nn, functions, dilated, parallel, utils, models, datasets
 File "/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/nn/init.py", line 12, in 
 from .encoding import *
 File "/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/nn/encoding.py", line 18, in 
 from ..functions import scaledL2, aggregate
 File "/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/functions/init.py", line 2, in 
 from .encoding import *
 File "/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/functions/encoding.py", line 13, in 
 from .. import lib
 File "/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/init.py", line 12, in 
 ], build_directory=cpu_path, verbose=False)
 File "/usr/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py", line 501, in load
 _build_extension_module(name, build_directory)
 File "/usr/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py", line 582, in _build_extension_module
 name, error.output.decode()))
 RuntimeError: Error building extension 'enclib_cpu': [1/2] c++ -MMD -MF roi_align_cpu.o.d -DTORCH_EXTENSION_NAME=enclib_cpu -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/anaconda3/include/python3.6m -fPIC -std=c++11 -c /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp -o roi_align_cpu.o
 FAILED: roi_align_cpu.o
 c++ -MMD -MF roi_align_cpu.o.d -DTORCH_EXTENSION_NAME=enclib_cpu -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/anaconda3/include/python3.6m -fPIC -std=c++11 -c /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp -o roi_align_cpu.o
 In file included from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/ArrayRef.h:18:0,
 from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/ScalarType.h:5,
 from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Scalar.h:11,
 from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/ATen.h:6,
 from /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:1:
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp: In function ‘at::Tensor ROIAlignForwardCPU(const at::Tensor&amp;, const at::Tensor&amp;, int64_t, int64_t, double, int64_t)’:
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:388:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(input.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:388:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(input.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:389:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:389:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:390:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(input.ndimension() == 4);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:390:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(input.ndimension() == 4);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:391:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.ndimension() == 2);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:391:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.ndimension() == 2);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:392:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.size(1) == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:392:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.size(1) == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:404:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(roi_cols == 4 || roi_cols == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:404:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(roi_cols == 4 || roi_cols == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:409:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(input.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:409:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(input.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:410:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:410:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp: In function ‘at::Tensor ROIAlignBackwardCPU(const at::Tensor&amp;, const at::Tensor&amp;, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, double, int64_t)’:
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:444:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:444:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:445:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.ndimension() == 2);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:445:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.ndimension() == 2);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:446:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.size(1) == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:446:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.size(1) == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:451:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(roi_cols == 4 || roi_cols == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:451:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(roi_cols == 4 || roi_cols == 5);
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before ‘(’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:456:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before ‘)’ token
 throw at::Error({func, FILE, LINE}, VA_ARGS)
 ^
 /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro ‘AT_ERROR’
 AT_ERROR(VA_ARGS);   
 ^
 /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:456:3: note: in expansion of macro ‘AT_ASSERT’
 AT_ASSERT(bottom_rois.is_contiguous());
 ^
 ninja: build stopped: subcommand failed.
 		</comment>
 		<comment id='3' author='qiulesun' date='2018-06-15T06:26:36Z'>
 		This package depend on a slightly higher version than PyTroch 0.4.0. Please follow the instructions to install pytorch from source &lt;denchmark-link:https://github.com/pytorch/pytorch#from-source&gt;https://github.com/pytorch/pytorch#from-source&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='qiulesun' date='2018-06-19T07:53:14Z'>
 		In your paper, the sentence ''The ground truth labels for SE-loss are generated by “unique” operation finding the categories presented in the given ground-truth segmentation mask.'' means that every input image has multiple labels. As far as I know, the binary cross entroy loss can handle binary class or multi-class task rather than multi-labels.
 		</comment>
 		<comment id='5' author='qiulesun' date='2018-06-19T13:26:54Z'>
 		I didn’t get the difference between multi class and multi labels. Could you please explain in detail?
 Btw, the NN already has sigmoid activation
 		</comment>
 		<comment id='6' author='qiulesun' date='2018-06-20T00:58:53Z'>
 		Multiclass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.
 Multilabel classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these.
 I note that the  NN has sigmoid activation. I hold the question that, in your case, the input image has multiple labels or one.
 		</comment>
 		<comment id='7' author='qiulesun' date='2018-06-20T01:27:55Z'>
 		The presence of the object categories is indeed a multi-label task. Each category is predicted independently using a binary prediction. I hope it can address your concern.
 		</comment>
 		<comment id='8' author='qiulesun' date='2018-06-20T01:32:39Z'>
 		Please refer to the docs for binary cross entropy loss &lt;denchmark-link:https://pytorch.org/docs/stable/nn.html?highlight=bceloss#torch.nn.BCELoss&gt;https://pytorch.org/docs/stable/nn.html?highlight=bceloss#torch.nn.BCELoss&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='9' author='qiulesun' date='2018-06-20T03:48:33Z'>
 		In binary classification, the number of classes equals 2. The object categories in an input image are more than 2 (figure 2 in paper). So I don't understand why binary cross entropy loss is empolyed and ''Each category is predicted independently using a binary prediction. ''
 		</comment>
 		<comment id='10' author='qiulesun' date='2018-06-20T04:42:55Z'>
 		Each category is a binary classification problem. For 150 categories, there 150 individual binary classification problem. I hope this explanation  is clear enough. If you still have difficulties, feel free to ask questions in Chinese.
 		</comment>
 		<comment id='11' author='qiulesun' date='2018-06-20T07:31:07Z'>
 		Thank you for your patience.  Your explanation is clear. The binary cross entropy loss can handle the multi-label classification task. Its target is something like [1,0,0,1,0...]. Sigmoid, unlike softmax don't give probability distribution around NCLASS as output, but independent probabilities.
 		</comment>
 		<comment id='12' author='qiulesun' date='2018-06-20T13:23:14Z'>
 		You’re welcome. That is correct.
 		</comment>
 		<comment id='13' author='qiulesun' date='2018-06-24T07:58:39Z'>
 		I am really sorroy for disturbing you again. I shouldn't ask the question about installation PyTorch from source, but I have no idea to solve it. Can you help me to fix it out?
 System Info：
 How you installed PyTorch (conda, pip, source): source
 Build command you used (if compiling from source): python setup.py install
 OS: ubuntu14.04
 PyTorch version: master
 Python version: 3.6
 CUDA/cuDNN version: cuda8.0+cudnn5.0
 GPU models and configuration: GTX1080Ti
 GCC version (if compiling from source): 4.9.4
 CMake version: 3.7.2
 ############################################################
 Issue description：
 3 errors detected in the compilation of "/tmp/tmpxft_00002a14_00000000-7_THCTensorMath.cpp1.ii".
 CMake Error at caffe2_gpu_generated_THCTensorMath.cu.o.Release.cmake:279 (message):
 Error generating file
 /media/hh/pytorch_dir/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCTensorMath.cu.o
 make[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCTensorMath.cu.o] Error 1
 make[1]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/all] Error 2
 make: *** [all] Error 2
 Failed to run 'bash tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn nccl caffe2 nanopb libshm gloo THD c10d'
 		</comment>
 		<comment id='14' author='qiulesun' date='2018-06-24T16:51:34Z'>
 		Try install the dependencies as following first:
 &lt;denchmark-code&gt;export CMAKE_PREFIX_PATH="$(dirname $(which conda))/../" # [anaconda root directory]
 
 # Install basic dependencies
 conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing
 conda install -c mingfeima mkldnn
 
 # Add LAPACK support for the GPU
 conda install -c pytorch magma-cuda80 # or magma-cuda90 if CUDA 9
 &lt;/denchmark-code&gt;
 
 You may want to ask on PyTorch repo for further help
 		</comment>
 		<comment id='15' author='qiulesun' date='2018-06-26T12:20:01Z'>
 		Are the models you released (model_zoo.py) all trained with two Context Encoding Modules?  Can you detail the MS evaluation in the table 1?
 &lt;denchmark-code&gt;models = {
      'encnet_resnet50_pcontext': get_encnet_resnet50_pcontext,
     'encnet_resnet101_pcontext': get_encnet_resnet101_pcontext,
     'encnet_resnet50_ade': get_encnet_resnet50_ade,
     }
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='16' author='qiulesun' date='2018-06-26T15:16:17Z'>
 		We only use one Context Encoding Module now, which is more efficient and makes the model compatible with EncNetV2.
 		</comment>
 		<comment id='17' author='qiulesun' date='2018-07-01T06:30:43Z'>
 		Can Ubuntu, Mac and Windows os all run the released codes?
 		</comment>
 		<comment id='18' author='qiulesun' date='2018-07-01T19:16:58Z'>
 		It mainly depends on the PyTorch. If the pytorch is compiled successfully on your system, there won't be a problem. I am using both Mac and Ubuntu. Note that PyTorch master branch is required.
 		</comment>
 		<comment id='19' author='qiulesun' date='2018-07-03T03:03:14Z'>
 		The comand (e.g., CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset PContext --model EncNet --aux --se-loss --backbone resnet101)  for training the model means training resnet101 from scratch or finetuning resnet101?
 		</comment>
 		<comment id='20' author='qiulesun' date='2018-07-03T03:10:46Z'>
 		resnet101 is pretrained from ImageNet.
 		</comment>
 		<comment id='21' author='qiulesun' date='2018-07-03T09:46:27Z'>
 		I used the comand (CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset PContext --model EncNet --aux --se-loss) for training the model resnet50. However, when it ran to the epoch12, I stopped it. Next, I restart it and find unluckily it has ran from epoch0 rather than epoch12. What should I do to run it from epoch12?
 		</comment>
 		<comment id='22' author='qiulesun' date='2018-07-03T14:50:26Z'>
 		Please resume by adding command --resume path/to/checkpoint.pth.tar
 		</comment>
 		<comment id='23' author='qiulesun' date='2018-07-06T03:21:47Z'>
 		Thank you. I have another interest. When does PyTroch 0.4.0 meets the requirements of running released code ?
 		</comment>
 		<comment id='24' author='qiulesun' date='2018-07-06T18:34:43Z'>
 		This package won't be compatible with PyTroch 0.4.0, but it will be compatible with next stable release.
 		</comment>
 		<comment id='25' author='qiulesun' date='2018-07-13T02:32:31Z'>
 		Question about selayer, why does the selayer have no sigmoid activation function?
 (encmodule): EncModule(
 (encoding): Sequential(
 (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
 (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 (2): ReLU(inplace)
 (3): Encoding(N x 512=&gt;32x512)
 (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 (5): ReLU(inplace)
 (6): Mean()
 )
 (fc): Sequential(
 (0): Linear(in_features=512, out_features=512, bias=True)
 (1): Sigmoid()
 )
 (selayer): Linear(in_features=512, out_features=59, bias=True)
 )
 		</comment>
 		<comment id='26' author='qiulesun' date='2018-07-13T02:35:51Z'>
 		That is the prediction layer for minimizing SE-Loss.
 The  function is applied during the loss calculation &lt;denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/customize.py#L65&gt;https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/customize.py#L65&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='27' author='qiulesun' date='2018-08-03T17:22:58Z'>
 		Sorry for bothering you agian, I have no idea with next errors when I run CUDA_VISIBLE_DEVICES=0,1 python train.py --dataset pcontext --model encnet --aux --se-loss.
 And import encoding gets similar errors.
 OS: ubuntu14.04
 Pytorch version: 0.5.0 (from source)
 Python version: 3.6
 CUDA: 8.0
 cudnn: 6.0.21
 GPU: 2 1080
 /usr/local/anaconda3/bin/python3.6 /media/cv-pc-00/QL_480G/sql/pytorch_dir/PyTorch-Encoding/experiments/segmentation/train.py --dataset PContext --model EncNet --se-loss
 ——————————————————————————————————————————————
 Traceback (most recent call last):
 File "/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py", line 742, in _build_extension_module
 ['ninja', '-v'], stderr=subprocess.STDOUT, cwd=build_directory)
 File "/usr/local/anaconda3/lib/python3.6/subprocess.py", line 336, in check_output
 **kwargs).stdout
 File "/usr/local/anaconda3/lib/python3.6/subprocess.py", line 418, in run
 output=stdout, stderr=stderr)
 subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.
 During handling of the above exception, another exception occurred:
 Traceback (most recent call last):
 File "/media/cv-pc-00/QL_480G/sql/pytorch_dir/PyTorch-Encoding/experiments/segmentation/train.py", line 17, in 
 import encoding.utils as utils
 File "/usr/local/anaconda3/lib/python3.6/site-packages/encoding/init.py", line 13, in 
 from . import nn, functions, dilated, parallel, utils, models, datasets
 File "/usr/local/anaconda3/lib/python3.6/site-packages/encoding/nn/init.py", line 12, in 
 from .encoding import *
 File "/usr/local/anaconda3/lib/python3.6/site-packages/encoding/nn/encoding.py", line 18, in 
 from ..functions import scaledL2, aggregate, pairwise_cosine
 File "/usr/local/anaconda3/lib/python3.6/site-packages/encoding/functions/init.py", line 2, in 
 from .encoding import *
 File "/usr/local/anaconda3/lib/python3.6/site-packages/encoding/functions/encoding.py", line 14, in 
 from .. import lib
 File "/usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/init.py", line 20, in 
 ], build_directory=gpu_path, verbose=False)
 File "/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py", line 496, in load
 with_cuda=with_cuda)
 File "/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py", line 664, in _jit_compile
 _build_extension_module(name, build_directory)
 File "/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py", line 748, in _build_extension_module
 name, error.output.decode()))
 RuntimeError: Error building extension 'enclib_gpu': [1/4] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu -o roi_align_kernel.cuda.o
 FAILED: roi_align_kernel.cuda.o
 /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu -o roi_align_kernel.cuda.o
 nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(373): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(373): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(420): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(420): error: class "at::Context" has no member "getCurrentCUDAStream"
 4 errors detected in the compilation of "/tmp/tmpxft_0000662c_00000000-7_roi_align_kernel.cpp1.ii".
 [2/4] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu -o encoding_kernel.cuda.o
 FAILED: encoding_kernel.cuda.o
 /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu -o encoding_kernel.cuda.o
 nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(315): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(341): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(364): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(391): error: class "at::Context" has no member "getCurrentCUDAStream"
 4 errors detected in the compilation of "/tmp/tmpxft_00006623_00000000-7_encoding_kernel.cpp1.ii".
 [3/4] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu -o syncbn_kernel.cuda.o
 FAILED: syncbn_kernel.cuda.o
 /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu -o syncbn_kernel.cuda.o
 nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(183): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(217): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(249): error: class "at::Context" has no member "getCurrentCUDAStream"
 /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(272): error: class "at::Context" has no member "getCurrentCUDAStream"
 4 errors detected in the compilation of "/tmp/tmpxft_00006627_00000000-7_syncbn_kernel.cpp1.ii".
 ninja: build stopped: subcommand failed.
 Process finished with exit code 1
 		</comment>
 		<comment id='28' author='qiulesun' date='2018-08-03T17:40:59Z'>
 		Hi, That is because the PyTorch updates in backend.
 
 Could you change at::Context:: getCurrentCUDAStream  to cudaStream_t stream = at::cuda::getCurrentCUDAStream();
 Also add #include &lt;ATen/cuda/CUDAContext.h&gt;
 
 This will be fixed in next version.
 		</comment>
 		<comment id='29' author='qiulesun' date='2018-08-04T08:42:53Z'>
 		Thanks for your attention. It does work! However, three warnings occur,  do that matter?
 
 
 /usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1940:
 UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.   warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
 
 
 /usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1025:
 UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
 warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
 
 
 /usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52:
 UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.
 warnings.warn(warning.format(ret))
 
 
 		</comment>
 		<comment id='30' author='qiulesun' date='2018-08-05T17:52:39Z'>
 		The deprecate warning is okay for now.
 		</comment>
 		<comment id='31' author='qiulesun' date='2018-08-07T03:08:49Z'>
 		Problem with debugging the backward method of Function class
 Hi, aggregate(A, X, C) and scaledL2(X, C, S) in encoding.functions.encoding.py implement the forward and backwark of your custom function. I want to debug their forward and backwark and the pycharm-community-2018.1.4 I used on Ubuntu 16.04 LTS has allowed me debug the forward step by step. However, I could not debug backward function like forward equipped with 2 1080 GPU.
 Could you tell me is it possilbe and how to address it? (ps: for my own  custom functions based on your codes, I also face the same problem)
 		</comment>
 		<comment id='32' author='qiulesun' date='2018-08-07T17:54:45Z'>
 		You can directly call the backend function for debugging &lt;denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/functions/encoding.py#L77&gt;https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/functions/encoding.py#L77&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='33' author='qiulesun' date='2018-08-08T13:40:24Z'>
 		For my special case, I want to run the codes with one GPU (ps: my machine is equipped with 2 GPUs), for example debugging the codes, etc.
 Do the codes support a single GPU operation even if the machine is equipped with 2 GPUs?
 Is the default multi GPU running if the machine is equipped with multiple  GPUs?
 		</comment>
 		<comment id='34' author='qiulesun' date='2018-08-08T17:38:20Z'>
 		CUDA_VISIBLE_DEVICES=0 python train.py ...
 		</comment>
 		<comment id='35' author='qiulesun' date='2018-08-09T00:50:45Z'>
 		Question 1
 I use pycharm-community-2018.1.4 to make it easier to debug the codes and CUDA_VISIBLE_DEVICES=0 --dataset PContext --model EncNet --se-loss is given in debug configurations.
 However, I get the error  train.py: error: unrecognized arguments: CUDA_VISIBLE_DEVICES=0
 When I use the pycharm-community-2018.1.4 to debug the codes with a single GPU, I should do what next ?
 Connected to pydev debugger (build 181.5087.37)
 usage: train.py [-h] [--model MODEL] [--backbone BACKBONE] [--dataset DATASET]
 [--data-folder DATA_FOLDER] [--workers N] [--aux] [--se-loss]
 [--epochs N] [--start_epoch N] [--batch-size N]
 [--test-batch-size N] [--lr LR] [--lr-scheduler LR_SCHEDULER]
 [--momentum M] [--weight-decay M] [--no-cuda] [--seed S]
 [--resume RESUME] [--checkname CHECKNAME]
 [--model-zoo MODEL_ZOO] [--ft] [--pre-class PRE_CLASS] [--ema]
 [--eval] [--no-val] [--test-folder TEST_FOLDER]
 train.py: error: unrecognized arguments: CUDA_VISIBLE_DEVICES=0
 Question 2
 args.lr = lrs[args.dataset.lower()] / 16 * args.batch_size in option.py means that the lr is relate to batch_size you give. Is that the lr not fixed depending on the batch_size (GPU memory)?
 In my experiments, I set the args.lr = lrs[args.dataset.lower()], is it reasonable and feasible, does it respect your paper and intentions?
 Question 3
 For multi-size evaluation, the 27th line base_size=576, crop_size=608 (base_size less than crop_size) in encoding/models/base.py should be base_size=608, crop_size=576?
 Previously, you set base_size=520, crop_size=480 and now you change them to base_size=576, crop_size=608. I hold the view that crop_size less than base_size seems reasonable. What settings should I follow to reproduce your results?
 I am looking forward to your reply.
 		</comment>
 		<comment id='36' author='qiulesun' date='2018-08-09T16:41:27Z'>
 		Q1: please use the terminal to launch the program.
 Q2: That is a kind of standard setting for LR. When increasing the batch size, people typically increase the LR accordingly.
 Q3. That is a bug. It will be fixed in next release.
 		</comment>
 		<comment id='37' author='qiulesun' date='2018-08-09T17:43:41Z'>
 		For the Q2 above, due to the limited GPU memory, the batch size has to be small (typically less than 16) unfortunately. It means that  I have to use smaller LR according to the standard setting, i.e., args.lr = lrs[args.dataset.lower()] / 16 * args.batch_size ?
 		</comment>
 		<comment id='38' author='qiulesun' date='2018-08-09T17:46:48Z'>
 		Yes. If the batch size is too small, the model will get worse result, because the working batch size for batch normalization is small.
 		</comment>
 		<comment id='39' author='qiulesun' date='2018-08-10T09:36:22Z'>
 		I only have 2 1080 GPUs with a total of 16G memory. The batch size is small less than 16 in my experiments. Can I alleviate this side effect (the model will get worse result you said) by using larger LR and set args.lr = lrs[args.dataset.lower()], independent of batch size?
 		</comment>
 		<comment id='40' author='qiulesun' date='2018-08-10T19:01:38Z'>
 		The batch size matters for segmentation task, due to working batch size for the Synchronize Batch Normalization. For batch size =16 yields the best performance.
 		</comment>
 		<comment id='41' author='qiulesun' date='2018-08-12T12:36:08Z'>
 		What is the main difference between encoding.nn.BatchNorm1d and  encoding.nn.BatchNorm2d?
 		</comment>
 		<comment id='42' author='qiulesun' date='2018-08-13T17:19:39Z'>
 		same as torch.nn.BatchNorm1d and  torch.nn.BatchNorm2d
 		</comment>
 		<comment id='43' author='qiulesun' date='2018-08-24T03:16:41Z'>
 		I have two questions.
 (1) For cos ans poly lr schedules, every batch (iter) has a different lr rather than them in one epoch has same lr. Is that right?
 (2) For cifar10 recognition, the scaling factor s_k is not learnt but randomly sampling from a uniform distribution between 0 and 1, which is different from segmentation tasks. Is that right?
 		</comment>
 		<comment id='44' author='qiulesun' date='2018-09-19T09:04:56Z'>
 		I'm sorry for disturbing you again.
 Your work is very encouraging to me. I notice that the scaled_l2 and aggregate opertors of the proposed encoding layer are implemented by C++  language.  Duo to I am not good at it, could you share the corresponding implementation using python code  if you want?
 		</comment>
 		<comment id='45' author='qiulesun' date='2018-09-19T13:58:13Z'>
 		We change LR every iter.
 The cifar experiment use shake-out like regularization.
 Scaled L2 and aggregate are easy to implement in python, but that will be memory consuming.
 		</comment>
 		<comment id='46' author='qiulesun' date='2018-09-21T03:00:47Z'>
 		question 1:
 Sorry to ask the stupid question.
 The augmented pascal voc 2012 has 11533 images in trainval.txt rather than 10582 used in paper.  It's troubled me. And I do not get the information about how to augment the 1464 trainging images of pascal voc 2012 to result in 10582 ones. In other words, I do not get the relationship between the pascal voc 2012 and its augmented  version. Could I fortunately know your opinion?
 If you think this question is not worth answering, I can understand completely.
 
 As far as I known, Group norm (&lt;denchmark-link:https://arxiv.org/pdf/1803.08494.pdf&gt;https://arxiv.org/pdf/1803.08494.pdf&lt;/denchmark-link&gt;
 ) is independent of batch size, much suitable for semantic segmentation task, which requires small batches constrained by memory consumption.
 Could you consider employing it in your updated version?
 		</comment>
 		<comment id='47' author='qiulesun' date='2018-09-21T17:37:03Z'>
 		Q1. For VOC experiments, first pretrained on COCO, then finetune on "pascal_aug" and finally on "pascal_voc". I am releasing the training detail for reproducing VOC experiments this weekend.
 Q2. Group Norm still has inferior performance comparing to BN. You can easily use that by changing the code a little bit.
 		</comment>
 		<comment id='48' author='qiulesun' date='2018-09-24T13:55:12Z'>
 		
 I see base_size=608 and crop_size=576 in the training log of EncNet_ResNet50_ADE, (&lt;denchmark-link:https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log&gt;https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log&lt;/denchmark-link&gt;
 ), however, the base_size and crop_size are set to 520 and 480 respectively in &lt;denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/datasets/base.py#L17&gt;https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/datasets/base.py#L17&lt;/denchmark-link&gt;
 .
 It's troubled me. Does the special case for ADE20K use base_size=608 and crop_size=576 and use  base_size=520 and crop_size=480 for PASCAL Context and PASCAL VOC12 ?
 
 Besides, base_size=576 and crop_size=608 in &lt;denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/models/base.py#L27&gt;https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/models/base.py#L27&lt;/denchmark-link&gt;
  is only to multiscale test ?
 		</comment>
 		<comment id='49' author='qiulesun' date='2018-09-24T16:51:37Z'>
 		There are some bugs in existing code. I am updating them soon.
 		</comment>
 		<comment id='50' author='qiulesun' date='2018-09-26T01:17:42Z'>
 		
 As mentioned above,  there are some bugs in existing code. I still have a question.
 The EncNet_ResNet50_ADE achieves 79.9 pixAcc and 41.2 mIoU at the last row in the table  (&lt;denchmark-link:https://hangzhang.org/PyTorch-Encoding/experiments/segmentation.html&gt;https://hangzhang.org/PyTorch-Encoding/experiments/segmentation.html&lt;/denchmark-link&gt;
 ), however, from the training log file (&lt;denchmark-link:https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log&gt;https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log&lt;/denchmark-link&gt;
 )  I see that it obtains 78.0 pixAcc and 40.2 mIoU lower than the results you reported.
 Is this because you use the multi-scale testing strategy on ADE20K val set?  Or something else ?
 		</comment>
 		<comment id='51' author='qiulesun' date='2018-09-26T17:12:44Z'>
 		The validation during the training is using center crop, only for monitoring the training process.
 		</comment>
 	</comments>
 </bug>
<commit id='f8919197bd53b027b131a891dd7b917e615a6743' author='Hang Zhang' date='2018-09-26 15:59:44-07:00'>
 	<dmm_unit complexity='0.6579634464751958' interfacing='0.49477806788511747' size='0.4151436031331593'></dmm_unit>
 	<modification change_type='ADD' old_name='None' new_name='docs\source\_static\js\hidebib.js'>
 		<file_info nloc='38' complexity='10' token_count='231'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\_templates\layout.html' new_name='docs\source\_templates\layout.html'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>6</added_lines>
 			<deleted_lines>6</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\experiments\segmentation.rst' new_name='docs\source\experiments\segmentation.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>26,27,28,29,34,41,42,43,44,45,46,47,48,49,50,51,52,53,82,83,84,85,86,87,88,89,90,91,92,93,94,140,142,143,144,145,147</added_lines>
 			<deleted_lines>30,37,38,39,40,41,42,43,44,45,119,121,122,123,125</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\experiments\texture.rst' new_name='docs\source\experiments\texture.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>20,22,23,24,25,26,27,34,42</added_lines>
 			<deleted_lines>20,22,29,37,59,60</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\source\functions.rst' new_name='docs\source\functions.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>23,26</added_lines>
 			<deleted_lines>23,26</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\__init__.py' new_name='encoding\__init__.py'>
 		<file_info nloc='3' complexity='0' token_count='24'></file_info>
 		<modified_lines>
 			<added_lines>13</added_lines>
 			<deleted_lines>13</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\datasets\__init__.py' new_name='encoding\datasets\__init__.py'>
 		<file_info nloc='17' complexity='1' token_count='85'></file_info>
 		<modified_lines>
 			<added_lines>2,7,10,15</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\datasets\ade20k.py' new_name='encoding\datasets\ade20k.py'>
 		<file_info nloc='92' complexity='18' token_count='731'></file_info>
 		<method name='_get_ade20k_pairs' parameters='folder,split'>
 				<method_info nloc='25' complexity='3' token_count='207' nesting_level='0' start_line='72' end_line='111'></method_info>
 			<added_lines>93,94,99,101,108,109,110</added_lines>
 			<deleted_lines>104,105</deleted_lines>
 		</method>
 		<method name='_mask_transform' parameters='self,mask'>
 				<method_info nloc='3' complexity='1' token_count='29' nesting_level='1' start_line='60' end_line='62'></method_info>
 			<added_lines>61,62</added_lines>
 			<deleted_lines>61,62</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\datasets\base.py' new_name='encoding\datasets\base.py'>
 		<file_info nloc='86' complexity='19' token_count='807'></file_info>
 		<method name='make_pred' parameters='self,x'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='1' start_line='40' end_line='41'></method_info>
 			<added_lines>40,41</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_sync_transform' parameters='self,img,mask'>
 				<method_info nloc='32' complexity='7' token_count='377' nesting_level='1' start_line='61' end_line='99'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>78,79,80,81</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>42</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\datasets\cityscapes.py'>
 		<file_info nloc='148' complexity='34' token_count='1514'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\datasets\coco.py' new_name='encoding\datasets\coco.py'>
 		<file_info nloc='116' complexity='14' token_count='825'></file_info>
 		<method name='__getitem__' parameters='self,index'>
 				<method_info nloc='21' complexity='5' token_count='189' nesting_level='1' start_line='39' end_line='61'></method_info>
 			<added_lines>46,47</added_lines>
 			<deleted_lines>39,41,44,46,47,48,51,52,53</deleted_lines>
 		</method>
 		<method name='_gen_seg_mask' parameters='self,target,h,w'>
 				<method_info nloc='16' complexity='4' token_count='160' nesting_level='1' start_line='66' end_line='81'></method_info>
 			<added_lines>73,74</added_lines>
 			<deleted_lines>71,72</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,root'>
 				<method_info nloc='2' complexity='1' token_count='35' nesting_level='1' start_line='13' end_line='14'></method_info>
 			<added_lines>14</added_lines>
 			<deleted_lines>13,14</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>10,11,12,16,19,21,22,23,26,27,28,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126</added_lines>
 			<deleted_lines>9,10,11,12,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,98,99</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\datasets\pascal_aug.py' new_name='encoding\datasets\pascal_aug.py'>
 		<file_info nloc='63' complexity='10' token_count='562'></file_info>
 		<method name='__init__' parameters='self,root,split,mode,transform,target_transform,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='26' nesting_level='1' start_line='18' end_line='19'></method_info>
 			<added_lines>18,19</added_lines>
 			<deleted_lines>18,19</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,root'>
 				<method_info nloc='2' complexity='1' token_count='35' nesting_level='1' start_line='18' end_line='19'></method_info>
 			<added_lines>18,19</added_lines>
 			<deleted_lines>18,19</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\datasets\pascal_voc.py' new_name='encoding\datasets\pascal_voc.py'>
 		<file_info nloc='73' complexity='11' token_count='631'></file_info>
 		<method name='pred_offset' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='7' nesting_level='1' start_line='82' end_line='83'></method_info>
 			<added_lines>82,83</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,root,split,mode,transform,target_transform,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='26' nesting_level='1' start_line='19' end_line='20'></method_info>
 			<added_lines>19,20</added_lines>
 			<deleted_lines>19,20</deleted_lines>
 		</method>
 		<method name='__getitem__' parameters='self,index'>
 				<method_info nloc='19' complexity='7' token_count='167' nesting_level='1' start_line='51' end_line='73'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>68,71</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,root'>
 				<method_info nloc='2' complexity='1' token_count='35' nesting_level='1' start_line='19' end_line='20'></method_info>
 			<added_lines>19,20</added_lines>
 			<deleted_lines>19,20</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>80,81</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\datasets\pcontext.py' new_name='encoding\datasets\pcontext.py'>
 		<file_info nloc='85' complexity='15' token_count='797'></file_info>
 		<method name='__getitem__' parameters='self,index'>
 				<method_info nloc='22' complexity='7' token_count='185' nesting_level='1' start_line='71' end_line='97'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>92,95</deleted_lines>
 		</method>
 		<method name='_class_to_index' parameters='self,mask'>
 				<method_info nloc='6' complexity='2' token_count='71' nesting_level='1' start_line='48' end_line='55'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>51</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>42,43</added_lines>
 			<deleted_lines>29</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\dilated\resnet.py' new_name='encoding\dilated\resnet.py'>
 		<file_info nloc='215' complexity='28' token_count='1820'></file_info>
 		<method name='__init__' parameters='self,block,layers,num_classes,dilated,norm_layer'>
 				<method_info nloc='29' complexity='5' token_count='376' nesting_level='1' start_line='138' end_line='167'></method_info>
 			<added_lines>139,140,141,142,143,144,145,146,147,148,149,150,151,152,167</added_lines>
 			<deleted_lines>138,139,141,142,143,158</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,block,layers,num_classes,dilated,deep_base,norm_layer'>
 				<method_info nloc='2' complexity='1' token_count='27' nesting_level='1' start_line='135' end_line='136'></method_info>
 			<added_lines>135,136</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='resnet152' parameters='pretrained,root,kwargs'>
 				<method_info nloc='7' complexity='2' token_count='69' nesting_level='0' start_line='274' end_line='285'></method_info>
 			<added_lines>282,283,284</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>137</added_lines>
 			<deleted_lines>13,14,15,273</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\functions\__init__.py' new_name='encoding\functions\__init__.py'>
 		<file_info nloc='4' complexity='0' token_count='16'></file_info>
 		<modified_lines>
 			<added_lines>4</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\functions\customize.py'>
 		<file_info nloc='42' complexity='2' token_count='66'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\functions\encoding.py' new_name='encoding\functions\encoding.py'>
 		<file_info nloc='91' complexity='12' token_count='431'></file_info>
 		<method name='scaledL2' parameters='X,C,S'>
 				<method_info nloc='14' complexity='1' token_count='22' nesting_level='0' start_line='83' end_line='96'></method_info>
 			<added_lines>83,95</added_lines>
 			<deleted_lines>83,84,96</deleted_lines>
 		</method>
 		<method name='forward' parameters='ctx,X,C,S'>
 				<method_info nloc='7' complexity='2' token_count='60' nesting_level='1' start_line='65' end_line='71'></method_info>
 			<added_lines>69</added_lines>
 			<deleted_lines>69</deleted_lines>
 		</method>
 		<method name='forward' parameters='ctx,A,X,C'>
 				<method_info nloc='7' complexity='2' token_count='58' nesting_level='1' start_line='20' end_line='27'></method_info>
 			<added_lines>26</added_lines>
 			<deleted_lines>26</deleted_lines>
 		</method>
 		<method name='backward' parameters='ctx,gradSL'>
 				<method_info nloc='7' complexity='2' token_count='75' nesting_level='1' start_line='74' end_line='80'></method_info>
 			<added_lines>79</added_lines>
 			<deleted_lines>79</deleted_lines>
 		</method>
 		<method name='backward' parameters='ctx,gradE'>
 				<method_info nloc='7' complexity='2' token_count='69' nesting_level='1' start_line='30' end_line='36'></method_info>
 			<added_lines>35</added_lines>
 			<deleted_lines>35</deleted_lines>
 		</method>
 		<method name='scaled_l2' parameters='X,C,S'>
 				<method_info nloc='14' complexity='1' token_count='22' nesting_level='0' start_line='82' end_line='95'></method_info>
 			<added_lines>82,83,95</added_lines>
 			<deleted_lines>82,83,84</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>16,63</added_lines>
 			<deleted_lines>16,63</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\functions\syncbn.py' new_name='encoding\functions\syncbn.py'>
 		<file_info nloc='60' complexity='10' token_count='325'></file_info>
 		<method name='forward' parameters='ctx,input,mean,std,gamma,beta'>
 				<method_info nloc='7' complexity='2' token_count='74' nesting_level='1' start_line='44' end_line='50'></method_info>
 			<added_lines>49</added_lines>
 			<deleted_lines>49</deleted_lines>
 		</method>
 		<method name='forward' parameters='ctx,input'>
 				<method_info nloc='7' complexity='2' token_count='48' nesting_level='1' start_line='24' end_line='30'></method_info>
 			<added_lines>29</added_lines>
 			<deleted_lines>29</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\__init__.py' new_name='encoding\lib\__init__.py'>
 		<file_info nloc='22' complexity='0' token_count='217'></file_info>
 		<modified_lines>
 			<added_lines>9,10,11,12,14,18,21,24</added_lines>
 			<deleted_lines>9,10,15</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='encoding\lib\cpu\.ninja_deps' new_name='encoding\lib\cpu\.ninja_deps'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\cpu\.ninja_log'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\cpu\build.ninja'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\cpu\encoding_cpu.cpp'>
 		<file_info nloc='43' complexity='4' token_count='570'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='encoding\lib\cpu\encoding_cpu.o' new_name='encoding\lib\cpu\encoding_cpu.o'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\cpu\nms_cpu.cpp'>
 		<file_info nloc='82' complexity='15' token_count='853'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\cpu\operator.cpp'>
 		<file_info nloc='14' complexity='1' token_count='142'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\cpu\operator.h'>
 		<file_info nloc='63' complexity='0' token_count='359'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='encoding\lib\cpu\operator.o' new_name='encoding\lib\cpu\operator.o'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\cpu\roi_align_cpu.cpp' new_name='encoding\lib\cpu\roi_align_cpu.cpp'>
 		<file_info nloc='385' complexity='48' token_count='2615'></file_info>
 		<method name='ROIAlignForwardCPU' parameters='input,bottom_rois,pooled_height,pooled_width,spatial_scale,sampling_ratio'>
 				<method_info nloc='38' complexity='2' token_count='266' nesting_level='0' start_line='380' end_line='429'></method_info>
 			<added_lines>380,412</added_lines>
 			<deleted_lines>380,412</deleted_lines>
 		</method>
 		<method name='ROIAlign_Forward_CPU' parameters='input,bottom_rois,pooled_height,pooled_width,spatial_scale,sampling_ratio'>
 				<method_info nloc='38' complexity='2' token_count='266' nesting_level='0' start_line='380' end_line='429'></method_info>
 			<added_lines>380,412</added_lines>
 			<deleted_lines>380,412</deleted_lines>
 		</method>
 		<method name='ROIAlignBackwardCPU' parameters='bottom_rois,grad_output,b_size,channels,height,width,pooled_height,pooled_width,spatial_scale,sampling_ratio'>
 				<method_info nloc='38' complexity='2' token_count='225' nesting_level='0' start_line='432' end_line='476'></method_info>
 			<added_lines>432,458</added_lines>
 			<deleted_lines>432,458</deleted_lines>
 		</method>
 		<method name='ROIAlign_Backward_CPU' parameters='bottom_rois,grad_output,b_size,channels,height,width,pooled_height,pooled_width,spatial_scale,sampling_ratio'>
 				<method_info nloc='38' complexity='2' token_count='225' nesting_level='0' start_line='432' end_line='476'></method_info>
 			<added_lines>432,458</added_lines>
 			<deleted_lines>432,458</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\cpu\setup.py' new_name='encoding\lib\cpu\setup.py'>
 		<file_info nloc='16' complexity='0' token_count='51'></file_info>
 		<modified_lines>
 			<added_lines>8,10,11,12</added_lines>
 			<deleted_lines>8</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\cpu\syncbn_cpu.cpp'>
 		<file_info nloc='50' complexity='7' token_count='409'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='encoding\lib\cpu\syncbn_cpu.o' new_name='encoding\lib\cpu\syncbn_cpu.o'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\gpu\common.h' new_name='encoding\lib\gpu\common.h'>
 		<file_info nloc='174' complexity='50' token_count='1375'></file_info>
 		<method name='reduceN' parameters='op,b,k,d,N'>
 				<method_info nloc='27' complexity='8' token_count='198' nesting_level='0' start_line='117' end_line='150'></method_info>
 			<added_lines>117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='reduceK' parameters='op,b,i,d,K'>
 				<method_info nloc='27' complexity='8' token_count='198' nesting_level='0' start_line='153' end_line='186'></method_info>
 			<added_lines>153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='reduceD' parameters='op,b,i,k,D'>
 				<method_info nloc='27' complexity='8' token_count='198' nesting_level='0' start_line='81' end_line='114'></method_info>
 			<added_lines>81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='reduceBN' parameters='op,k,d,B,N'>
 				<method_info nloc='30' complexity='9' token_count='214' nesting_level='0' start_line='189' end_line='224'></method_info>
 			<added_lines>189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>80,115,116,151,152,187,188</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\gpu\encoding_kernel.cu' new_name='encoding\lib\gpu\encoding_kernel.cu'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>2,3,82,101,169,195,218,245</added_lines>
 			<deleted_lines>1,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,228,247,315,341,364,391,396,397</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\gpu\encodingv2_kernel.cu'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='encoding\lib\gpu\nms_kernel.cu'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\gpu\operator.cpp' new_name='encoding\lib\gpu\operator.cpp'>
 		<file_info nloc='22' complexity='1' token_count='214'></file_info>
 		<method name='PYBIND11_MODULE' parameters='TORCH_EXTENSION_NAME,m'>
 				<method_info nloc='21' complexity='1' token_count='212' nesting_level='0' start_line='3' end_line='23'></method_info>
 			<added_lines>4,5,6,15,16,17,18,19,20,21,22</added_lines>
 			<deleted_lines>4,5</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\gpu\operator.h' new_name='encoding\lib\gpu\operator.h'>
 		<file_info nloc='96' complexity='0' token_count='570'></file_info>
 		<modified_lines>
 			<added_lines>4,12,24,25,26,27,28,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113</added_lines>
 			<deleted_lines>4,12</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\gpu\roi_align_kernel.cu' new_name='encoding\lib\gpu\roi_align_kernel.cu'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>2,350,374,379,396,421,426</added_lines>
 			<deleted_lines>349,373,378,395,420,425</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\gpu\setup.py' new_name='encoding\lib\gpu\setup.py'>
 		<file_info nloc='17' complexity='0' token_count='53'></file_info>
 		<modified_lines>
 			<added_lines>10,13</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\lib\gpu\syncbn_kernel.cu' new_name='encoding\lib\gpu\syncbn_kernel.cu'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>2,3,184,218,250,253,273,276</added_lines>
 			<deleted_lines>1,183,217,249,252,272,275</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\models\base.py' new_name='encoding\models\base.py'>
 		<file_info nloc='162' complexity='31' token_count='1695'></file_info>
 		<method name='__init__' parameters='self,nclass,backbone,aux,se_loss,dilated,norm_layer,base_size,crop_size,mean,456,std,224,root'>
 				<method_info nloc='3' complexity='1' token_count='59' nesting_level='1' start_line='26' end_line='28'></method_info>
 			<added_lines>27</added_lines>
 			<deleted_lines>27</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,nclass,backbone,aux,se_loss,dilated,norm_layer,base_size,crop_size,mean,456,std,224,root'>
 				<method_info nloc='3' complexity='1' token_count='59' nesting_level='1' start_line='26' end_line='28'></method_info>
 			<added_lines>27</added_lines>
 			<deleted_lines>27</deleted_lines>
 		</method>
 		<method name='parallel_forward' parameters='self,inputs,kwargs'>
 				<method_info nloc='11' complexity='7' token_count='160' nesting_level='1' start_line='87' end_line='104'></method_info>
 			<added_lines>102,103</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\models\encnet.py' new_name='encoding\models\encnet.py'>
 		<file_info nloc='225' complexity='16' token_count='1255'></file_info>
 		<method name='get_encnet_resnet101_pcontext' parameters='pretrained,root,kwargs'>
 				<method_info nloc='19' complexity='1' token_count='44' nesting_level='0' start_line='172' end_line='190'></method_info>
 			<added_lines>189,190</added_lines>
 			<deleted_lines>189</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,in_channels,nclass,ncodes,se_loss,norm_layer'>
 				<method_info nloc='16' complexity='2' token_count='157' nesting_level='1' start_line='44' end_line='61'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>46,47</deleted_lines>
 		</method>
 		<method name='get_encnet_resnet50_pcontext' parameters='pretrained,root,kwargs'>
 				<method_info nloc='19' complexity='1' token_count='44' nesting_level='0' start_line='152' end_line='170'></method_info>
 			<added_lines>169,170</added_lines>
 			<deleted_lines>170</deleted_lines>
 		</method>
 		<method name='get_encnet_resnet101_ade' parameters='pretrained,root,kwargs'>
 				<method_info nloc='19' complexity='1' token_count='44' nesting_level='0' start_line='212' end_line='230'></method_info>
 			<added_lines>212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_encnet_resnet152_ade' parameters='pretrained,root,kwargs'>
 				<method_info nloc='19' complexity='1' token_count='44' nesting_level='0' start_line='232' end_line='250'></method_info>
 			<added_lines>232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_encnet_resnet50_ade' parameters='pretrained,root,kwargs'>
 				<method_info nloc='19' complexity='1' token_count='44' nesting_level='0' start_line='192' end_line='210'></method_info>
 			<added_lines>209,210</added_lines>
 			<deleted_lines>208</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>17,18,142,144,211,231</added_lines>
 			<deleted_lines>17,143,145</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\models\fcn.py' new_name='encoding\models\fcn.py'>
 		<file_info nloc='125' complexity='9' token_count='523'></file_info>
 		<modified_lines>
 			<added_lines>104</added_lines>
 			<deleted_lines>104,105</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\models\model_store.py' new_name='encoding\models\model_store.py'>
 		<file_info nloc='88' complexity='12' token_count='469'></file_info>
 		<method name='get_model_file' parameters='name,root'>
 				<method_info nloc='46' complexity='6' token_count='232' nesting_level='0' start_line='33' end_line='81'></method_info>
 			<added_lines>59,60,62</added_lines>
 			<deleted_lines>55,57</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>10,11,12,13,17,18,19,20,21,22</added_lines>
 			<deleted_lines>10,11,12,16,17,18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\models\model_zoo.py' new_name='encoding\models\model_zoo.py'>
 		<file_info nloc='21' complexity='2' token_count='122'></file_info>
 		<method name='get_model' parameters='name,kwargs'>
 				<method_info nloc='16' complexity='2' token_count='96' nesting_level='0' start_line='11' end_line='42'></method_info>
 			<added_lines>33,36</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\models\psp.py' new_name='encoding\models\psp.py'>
 		<file_info nloc='71' complexity='8' token_count='505'></file_info>
 		<modified_lines>
 			<added_lines>61</added_lines>
 			<deleted_lines>61</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\nn\customize.py' new_name='encoding\nn\customize.py'>
 		<file_info nloc='151' complexity='22' token_count='1243'></file_info>
 		<method name='__init__' parameters='self,se_loss,se_weight,nclass,aux,aux_weight,weight,size_average,ignore_index'>
 				<method_info nloc='3' complexity='1' token_count='43' nesting_level='1' start_line='42' end_line='44'></method_info>
 			<added_lines>43</added_lines>
 			<deleted_lines>43</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,se_loss,se_weight,nclass,aux,aux_weight,weight,size_average,ignore_index'>
 				<method_info nloc='3' complexity='1' token_count='43' nesting_level='1' start_line='42' end_line='44'></method_info>
 			<added_lines>43</added_lines>
 			<deleted_lines>43</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,inputs'>
 				<method_info nloc='21' complexity='5' token_count='251' nesting_level='1' start_line='53' end_line='73'></method_info>
 			<added_lines>65,72</added_lines>
 			<deleted_lines>65,72</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\nn\encoding.py' new_name='encoding\nn\encoding.py'>
 		<file_info nloc='259' complexity='24' token_count='1299'></file_info>
 		<method name='forward' parameters='self,X'>
 				<method_info nloc='12' complexity='3' token_count='134' nesting_level='1' start_line='90' end_line='106'></method_info>
 			<added_lines>93,95,98,102,103</added_lines>
 			<deleted_lines>94,95,98,99,103,104</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>18,151</added_lines>
 			<deleted_lines>18,152</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\utils\__init__.py' new_name='encoding\utils\__init__.py'>
 		<file_info nloc='10' complexity='0' token_count='62'></file_info>
 		<modified_lines>
 			<added_lines>13</added_lines>
 			<deleted_lines>13</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\utils\metrics.py' new_name='encoding\utils\metrics.py'>
 		<file_info nloc='90' complexity='14' token_count='758'></file_info>
 		<method name='batch_pix_accuracy' parameters='output,target'>
 				<method_info nloc='9' complexity='1' token_count='95' nesting_level='0' start_line='64' end_line='79'></method_info>
 			<added_lines>64,70,71,72,73,74</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get' parameters='self'>
 				<method_info nloc='5' complexity='1' token_count='60' nesting_level='1' start_line='50' end_line='54'></method_info>
 			<added_lines>50,51,52,53,54</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='batch_intersection_union' parameters='output,target,nclass'>
 				<method_info nloc='16' complexity='1' token_count='182' nesting_level='0' start_line='82' end_line='105'></method_info>
 			<added_lines>82,89,93,94</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='reset' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='26' nesting_level='1' start_line='56' end_line='61'></method_info>
 			<added_lines>56,57,58,59,60,61</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='batch_intersection_union' parameters='predict,target,nclass'>
 				<method_info nloc='16' complexity='1' token_count='172' nesting_level='0' start_line='30' end_line='53'></method_info>
 			<added_lines>30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53</added_lines>
 			<deleted_lines>30,37,41,42</deleted_lines>
 		</method>
 		<method name='batch_pix_accuracy' parameters='predict,target'>
 				<method_info nloc='9' complexity='1' token_count='85' nesting_level='0' start_line='14' end_line='27'></method_info>
 			<added_lines>15,16,17,18,19,20,21,22,23,24,25,26,27</added_lines>
 			<deleted_lines>14,20,21,22</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,nclass'>
 				<method_info nloc='4' complexity='1' token_count='26' nesting_level='1' start_line='18' end_line='21'></method_info>
 			<added_lines>18,19,20,21</added_lines>
 			<deleted_lines>20,21</deleted_lines>
 		</method>
 		<method name='update' parameters='self,labels,preds'>
 				<method_info nloc='15' complexity='6' token_count='101' nesting_level='1' start_line='23' end_line='48'></method_info>
 			<added_lines>23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48</added_lines>
 			<deleted_lines>30,37,41,42</deleted_lines>
 		</method>
 		<method name='update.evaluate_worker' parameters='self,label,pred'>
 				<method_info nloc='11' complexity='1' token_count='59' nesting_level='2' start_line='24' end_line='34'></method_info>
 			<added_lines>24,25,26,27,28,29,30,31,32,33,34</added_lines>
 			<deleted_lines>30</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>11,55,62,63</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='encoding\utils\pallete.py' new_name='encoding\utils\pallete.py'>
 		<file_info nloc='32' complexity='8' token_count='2707'></file_info>
 		<method name='get_mask_pallete' parameters='npimg,dataset'>
 				<method_info nloc='11' complexity='5' token_count='81' nesting_level='0' start_line='13' end_line='26'></method_info>
 			<added_lines>24</added_lines>
 			<deleted_lines>24</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>28</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\recognition\dataset\minc.py' new_name='experiments\recognition\dataset\minc.py'>
 		<file_info nloc='103' complexity='17' token_count='957'></file_info>
 		<method name='__init__' parameters='self,args'>
 				<method_info nloc='29' complexity='2' token_count='271' nesting_level='1' start_line='78' end_line='108'></method_info>
 			<added_lines>97,99</added_lines>
 			<deleted_lines>97,99</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\recognition\main.py' new_name='experiments\recognition\main.py'>
 		<file_info nloc='138' complexity='19' token_count='1055'></file_info>
 		<method name='main' parameters=''>
 				<method_info nloc='57' complexity='10' token_count='432' nesting_level='0' start_line='33' end_line='171'></method_info>
 			<added_lines>99</added_lines>
 			<deleted_lines>99</deleted_lines>
 		</method>
 		<method name='main.train' parameters='epoch'>
 				<method_info nloc='23' complexity='3' token_count='216' nesting_level='1' start_line='81' end_line='105'></method_info>
 			<added_lines>99</added_lines>
 			<deleted_lines>99</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\recognition\model\download_models.py' new_name='experiments\recognition\model\download_models.py'>
 		<file_info nloc='4' complexity='0' token_count='24'></file_info>
 		<modified_lines>
 			<added_lines>5</added_lines>
 			<deleted_lines>5</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\recognition\model\mynn.py' new_name='experiments\recognition\model\mynn.py'>
 		<file_info nloc='208' complexity='27' token_count='1799'></file_info>
 		<method name='__init__' parameters='self,channel,K,reduction'>
 				<method_info nloc='13' complexity='1' token_count='122' nesting_level='1' start_line='90' end_line='102'></method_info>
 			<added_lines>90,91,92,93,94,95,96,97,98,99,100,101,102</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='forward' parameters='self,x'>
 				<method_info nloc='4' complexity='1' token_count='43' nesting_level='1' start_line='104' end_line='107'></method_info>
 			<added_lines>104,105,106,107</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>89,103,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\recognition\option.py' new_name='experiments\recognition\option.py'>
 		<file_info nloc='53' complexity='3' token_count='491'></file_info>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='45' complexity='1' token_count='452' nesting_level='1' start_line='15' end_line='67'></method_info>
 			<added_lines>43,44</added_lines>
 			<deleted_lines>43,44</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\segmentation\option.py' new_name='experiments\segmentation\option.py'>
 		<file_info nloc='99' complexity='7' token_count='799'></file_info>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='66' complexity='1' token_count='600' nesting_level='1' start_line='12' end_line='86'></method_info>
 			<added_lines>28,30,32,33,37,38,41,42</added_lines>
 			<deleted_lines>28,30,71,72,73,75,76</deleted_lines>
 		</method>
 		<method name='parse' parameters='self'>
 				<method_info nloc='29' complexity='6' token_count='186' nesting_level='1' start_line='88' end_line='117'></method_info>
 			<added_lines>94,95,99,108,109,116</added_lines>
 			<deleted_lines>96</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\segmentation\test.py' new_name='experiments\segmentation\test.py'>
 		<file_info nloc='71' complexity='12' token_count='692'></file_info>
 		<method name='test.eval_batch' parameters='image,dst,evaluator,eval_mode'>
 				<method_info nloc='24' complexity='5' token_count='218' nesting_level='1' start_line='71' end_line='97'></method_info>
 			<added_lines>71,72,73,74,75,76,78,79,80,81,82</added_lines>
 			<deleted_lines>71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,87,88,89,90,91,92,96,97</deleted_lines>
 		</method>
 		<method name='test' parameters='args'>
 				<method_info nloc='53' complexity='12' token_count='576' nesting_level='0' start_line='24' end_line='85'></method_info>
 			<added_lines>63,64,65,67,70,71,72,73,74,75,76,78,79,80,81,82</added_lines>
 			<deleted_lines>24,25,26,27,67,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='experiments\segmentation\train.py' new_name='experiments\segmentation\train.py'>
 		<file_info nloc='148' complexity='19' token_count='1323'></file_info>
 		<method name='__init__' parameters='self,args'>
 				<method_info nloc='57' complexity='10' token_count='607' nesting_level='1' start_line='30' end_line='96'></method_info>
 			<added_lines>39,63,64,67,68,69,90,91,92</added_lines>
 			<deleted_lines>39,63,64,65,66,67,68,69,72</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>175,176,177,179</added_lines>
 			<deleted_lines>175,176,177,179</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='scripts\prepare_cityscapes.py'>
 		<file_info nloc='40' complexity='4' token_count='252'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\prepare_coco.py' new_name='scripts\prepare_coco.py'>
 		<file_info nloc='48' complexity='5' token_count='269'></file_info>
 		<method name='download_coco' parameters='path,overwrite'>
 				<method_info nloc='14' complexity='2' token_count='78' nesting_level='0' start_line='19' end_line='37'></method_info>
 			<added_lines>27,28,29,30</added_lines>
 			<deleted_lines>27,28,29,30</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='scripts\prepare_minc.py'>
 		<file_info nloc='35' complexity='3' token_count='249'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='setup.py' new_name='setup.py'>
 		<file_info nloc='74' complexity='3' token_count='357'></file_info>
 		<modified_lines>
 			<added_lines>21</added_lines>
 			<deleted_lines>21</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\unit_test\test_function.py'>
 		<file_info nloc='188' complexity='14' token_count='2596'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\unit_test\test_module.py' new_name='tests\unit_test\test_module.py'>
 		<file_info nloc='75' complexity='17' token_count='855'></file_info>
 		<method name='test_sum_square' parameters=''>
 				<method_info nloc='7' complexity='1' token_count='83' nesting_level='0' start_line='63' end_line='69'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>63,64,65,66,67,68,69</deleted_lines>
 		</method>
 		<method name='testSyncBN._check_batchnorm_result' parameters='bn1,bn2,input,is_train,cuda'>
 				<method_info nloc='21' complexity='3' token_count='202' nesting_level='1' start_line='48' end_line='84'></method_info>
 			<added_lines>48</added_lines>
 			<deleted_lines>48,49,50,51,52,62,63,64,65,66,67,68,69,70,71</deleted_lines>
 		</method>
 		<method name='testSyncBN' parameters=''>
 				<method_info nloc='10' complexity='2' token_count='130' nesting_level='0' start_line='47' end_line='95'></method_info>
 			<added_lines>48,94,95</added_lines>
 			<deleted_lines>47,48,49,50,51,52,62,63,64,65,66,67,68,69,70,71,85,86,87,88,89,90,91,92,93,94,95</deleted_lines>
 		</method>
 		<method name='test_scaledL2' parameters=''>
 				<method_info nloc='11' complexity='1' token_count='150' nesting_level='0' start_line='40' end_line='50'></method_info>
 			<added_lines>48</added_lines>
 			<deleted_lines>40,41,42,43,44,45,46,47,48,49,50</deleted_lines>
 		</method>
 		<method name='test_syncbn_func' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='211' nesting_level='0' start_line='103' end_line='116'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>103,104,105,106,107,108,109,110,111,112,113,114,115,116</deleted_lines>
 		</method>
 		<method name='test_aggregate' parameters=''>
 				<method_info nloc='11' complexity='1' token_count='154' nesting_level='0' start_line='27' end_line='37'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>27,28,29,30,31,32,33,34,35,36,37</deleted_lines>
 		</method>
 		<method name='testSyncBN._checkBatchNormResult' parameters='bn1,bn2,input,is_train,cuda'>
 				<method_info nloc='21' complexity='3' token_count='202' nesting_level='1' start_line='120' end_line='156'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>120</deleted_lines>
 		</method>
 		<method name='test_syncbn' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='130' nesting_level='0' start_line='86' end_line='100'></method_info>
 			<added_lines>94,95</added_lines>
 			<deleted_lines>86,87,88,89,90,91,92,93,94,95,96,97,98,99,100</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>12,15,38,39,101,102,117,118,166,167</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\unit_test\test_utils.py'>
 		<file_info nloc='23' complexity='1' token_count='256'></file_info>
 	</modification>
 </commit>
</bug_data>
