<bug_data>
<bug id='1556' author='ctlaltdefeat' open_date='2020-12-14T16:26:23Z' closed_time='2020-12-14T19:50:37Z'>
 	<summary>Tacotron2 bug when batch dimension is 1</summary>
 	<description>
 Describe the bug
 Training (or just passing input with ground-truth spectrogram) Tacotron2 with a batch dimension of 1 does not work due to a dimension problem when aggregating the gate outputs.
 The line here
 
 
 
 NeMo/nemo/collections/tts/modules/tacotron2.py
 
 
          Line 256
       in
       d01b7bb
 
 
 
 
 
 
  gate_outputs = torch.stack(gate_outputs).squeeze(-1).transpose(0, 1) 
 
 
 
 
 
 should have unsqueeze instead of squeeze when the batch dimension is 1.
 (I didn't want to submit a pull request as I'm not familiar with the conventions of the project)
 	</description>
 	<comments>
 		<comment id='1' author='ctlaltdefeat' date='2020-12-14T19:50:55Z'>
 		Thanks for catching this bug. It should be fixed now
 		</comment>
 	</comments>
 </bug>
<commit id='9da1d27ddd7f1d5d9b6cd75f7308787976680dd8' author='Jason' date='2020-12-14 14:50:36-05:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='nemo\collections\tts\modules\tacotron2.py' new_name='nemo\collections\tts\modules\tacotron2.py'>
 		<file_info nloc='325' complexity='35' token_count='2497'></file_info>
 		<method name='train_forward' parameters='self,memory,decoder_inputs,memory_lengths'>
 				<method_info nloc='15' complexity='2' token_count='164' nesting_level='1' start_line='310' end_line='328'></method_info>
 			<added_lines>324</added_lines>
 			<deleted_lines>324</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
