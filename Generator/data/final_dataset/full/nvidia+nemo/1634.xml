<bug_data>
<bug id='1634' author='kshtzgupta' open_date='2021-01-15T00:14:16Z' closed_time='2021-01-20T04:28:32Z'>
 	<summary>Incorrect usage of nn.LSTM in NeMo Models Tutorial</summary>
 	<description>
 Under the Neural Types section, for Case 2 LSTM [ERROR CELL], the tutorial gives the wrong reason for the TypeError: Number of output arguments provided (2) is not as expected (1). Specifically, it states:
 
 What exactly is going on here? Well, inside our LSTMModule class, we declare the output types to be a single NeuralType - an EncodedRepresentation of shape [B, T, C].
 
 
 But the output of an LSTM layer is a tuple of two state values - the hidden state h and the cell state c!
 
 But actually from the nn.LSTM &lt;denchmark-link:https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html&gt;docs&lt;/denchmark-link&gt;
  we see that &lt;denchmark-link:https://user-images.githubusercontent.com/76540498/104661588-adb2a980-567d-11eb-9a18-6f845b6c92a0.png&gt;&lt;/denchmark-link&gt;
  it outputs a tuple  whose first element  contains the output features from the last layer of the LSTM for all timesteps. With  this should be of the shape . The second element of the tuple is again itself a tuple of the final hidden state () and final cell state (). So the axes in  is also wrong. Particularly, even with  in the LSTM  and  are of shape . Since  and  are the final states there should be no Time (T) or seq_len dim in their shape. Here we are using unidirectional LSTM and representing batch with axis B and hidden_size with axis C so shape becomes  not  as implemented in 
 &lt;denchmark-link:https://user-images.githubusercontent.com/76540498/104663013-e902a780-5680-11eb-8a9c-57f41d19d9a0.png&gt;&lt;/denchmark-link&gt;
 .
 Also wondering, is there an AxisKind to represent  or ? Couldn't find a suitable AxisKind in &lt;denchmark-link:https://github.com/NVIDIA/NeMo/blob/main/nemo/core/neural_types/axes.py&gt;https://github.com/NVIDIA/NeMo/blob/main/nemo/core/neural_types/axes.py&lt;/denchmark-link&gt;
 . Maybe "Any" should be used?
 To give a more concrete example of the LSTM output. Let (h_n, c_n) be the tuple of final hidden and cell state and y_t be the outputs for all timestamps then
 &lt;denchmark-code&gt;lstm = torch.nn.LSTM(1, 30, batch_first=True)
 x3 = torch.rand(2, 5, 1)
 # x3 shape is [batch_size x time stamps x dimension]
 y_t, (h_n, c_n) = lstm(x3)
 &lt;/denchmark-code&gt;
 
 y_t is of shape ([2, 5, 30]) i.e batch_size x time stamps x dimension
 h_n is of shape ([1, 2, 30]) i.e. num_layers x batch_size x dimension
 c_n is of shape ([1, 2, 30]) i.e. num_layers x batch_size x dimension
 	</description>
 	<comments>
 		<comment id='1' author='kshtzgupta' date='2021-01-15T05:30:22Z'>
 		This is a good find, I keep forgetting TF and PT have different semantics for their RNNs.
 NeMo currently ignores nested structures (h, c) (though we plan to add this), so that will be a todo for a subsequent patch. In the diagram above, "h" corresponds to "y_t" and "c" corresponds to both (h_n, c_n), so a bit of renaming will be necessary as well.
 As to the axes, we cant detach the num_directions * num_layers since it flattens out into a single axis. The axiskind most appropriate to that would probably be Channel/Dimension rather than Any. Any is a bit too generic for an RNN state axis.
 		</comment>
 	</comments>
 </bug>
<commit id='eb927fe6aa69f8d73705dcb785a2afb95e4c279d' author='Somshubra Majumdar' date='2021-01-19 20:28:31-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tutorials\01_NeMo_Models.ipynb' new_name='tutorials\01_NeMo_Models.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>583,601,602,603,607,608,622,623,648,650,651,652,1118,1122,1124,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202</added_lines>
 			<deleted_lines>583,601,618,619,645,1111,1115,1117,1181</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
