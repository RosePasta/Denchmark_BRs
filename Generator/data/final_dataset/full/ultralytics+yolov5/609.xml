<bug_data>
<bug id='609' author='ZeKunZhang1998' open_date='2020-08-03T02:09:06Z' closed_time='2020-08-05T20:41:43Z'>
 	<summary>Possible Bug Training on Empty Batch?</summary>
 	<description>
 &lt;denchmark-h:h2&gt;‚ùîQuestion&lt;/denchmark-h&gt;
 
 Traceback (most recent call last):
 File "train.py", line 463, in 
 train(hyp, tb_writer, opt, device)
 File "train.py", line 286, in train
 loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size
 File "/content/drive/My Drive/yolov5/utils/utils.py", line 443, in compute_loss
 tcls, tbox, indices, anchors = build_targets(p, targets, model)  # targets
 File "/content/drive/My Drive/yolov5/utils/utils.py", line 542, in build_targets
 b, c = t[:, :2].long().T  # image, class
 ValueError: too many values to unpack (expected 2)
 &lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='ZeKunZhang1998' date='2020-08-03T18:34:22Z'>
 		Hello, thank you for your interest in our work! This issue seems to lack the minimum requirements for a proper response, or is insufficiently detailed for us to help you. Please note that most technical problems are due to:
 
 Your changes to the default repository. If your issue is not reproducible in a new git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:
 
 sudo rm -rf yolov5  # remove existing
 git clone https://github.com/ultralytics/yolov5 &amp;&amp; cd yolov5 # clone latest
 python detect.py  # verify detection
 # CODE TO REPRODUCE YOUR ISSUE HERE
 
 
 Your custom data. If your issue is not reproducible with COCO or COCO128 data we can not debug it. Visit our Custom Training Tutorial for guidelines on training your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
 
 
 Your environment. If your issue is not reproducible in one of the verified environments below we can not debug it. If you are running YOLOv5 locally, ensure your environment meets all of the requirements.txt dependencies specified below.
 
 
 If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
 &lt;denchmark-h:h2&gt;Requirements&lt;/denchmark-h&gt;
 
 Python 3.8 or later with all &lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/requirements.txt&gt;requirements.txt&lt;/denchmark-link&gt;
  dependencies installed, including . To install run:
 $ pip install -U -r requirements.txt
 &lt;denchmark-h:h2&gt;Environments&lt;/denchmark-h&gt;
 
 YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including &lt;denchmark-link:https://developer.nvidia.com/cuda&gt;CUDA&lt;/denchmark-link&gt;
 /&lt;denchmark-link:https://developer.nvidia.com/cudnn&gt;CUDNN&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://www.python.org/&gt;Python&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://pytorch.org/&gt;PyTorch&lt;/denchmark-link&gt;
  preinstalled):
 
 Google Colab Notebook with free GPU: 
 Kaggle Notebook with free GPU: https://www.kaggle.com/ultralytics/yolov5
 Google Cloud Deep Learning VM. See GCP Quickstart Guide
 Docker Image https://hub.docker.com/r/ultralytics/yolov5. See Docker Quickstart Guide 
 
 &lt;denchmark-h:h2&gt;Current Status&lt;/denchmark-h&gt;
 
 &lt;denchmark-link:https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg&gt;&lt;/denchmark-link&gt;
 
 If this badge is green, all &lt;denchmark-link:https://github.com/ultralytics/yolov5/actions&gt;YOLOv5 GitHub Actions&lt;/denchmark-link&gt;
  Continuous Integration (CI) test are passing. These tests evaluate proper operation of basic YOLOv5 functionality, including training (&lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/train.py&gt;train.py&lt;/denchmark-link&gt;
 ), testing (&lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/test.py&gt;test.py&lt;/denchmark-link&gt;
 ), inference (&lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/detect.py&gt;detect.py&lt;/denchmark-link&gt;
 ) and export (&lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/models/export.py&gt;export.py&lt;/denchmark-link&gt;
 ) on MacOS, Windows, and Ubuntu.
 		</comment>
 		<comment id='2' author='ZeKunZhang1998' date='2020-08-03T21:23:06Z'>
 		Changing batchsize will solve this issue, it will occur when a batch of images contain no object.
 		</comment>
 		<comment id='3' author='ZeKunZhang1998' date='2020-08-03T22:06:20Z'>
 		&lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  ah I see. I remember a similar issue about testing with no targets, but I believed this was resolved. Does this occur when training or testing? Can you supply code to reproduce?
 		</comment>
 		<comment id='4' author='ZeKunZhang1998' date='2020-08-03T22:12:49Z'>
 		While changing the batchsize helped prolong the learning process, this issue still occurs for me. By printing the paths of the images in each batch i can check to see if they have an object, and there's definitely at least one object in each occasion (my dataset doesn't have images with empty label files) of the crash.
 		</comment>
 		<comment id='5' author='ZeKunZhang1998' date='2020-08-03T22:31:06Z'>
 		&lt;denchmark-link:https://github.com/MiiaBestLamia&gt;@MiiaBestLamia&lt;/denchmark-link&gt;
  can you supply exact steps and code to reproduce this issue following the steps outlined before (current repo, valid environment, common dataset?)
 		</comment>
 		<comment id='6' author='ZeKunZhang1998' date='2020-08-03T22:39:02Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  i'm using the most recent repository, all of the requirements are satisfied except the Python version, which might be the reason for the issue, though it seems strange that the learning process works for a bit, then crashes (I'm using 3.6.9). I have altered only one line of code in the repository, the one that prints paths of the files used in the batch. Using a dataset that I may not be allowed to share, so I cannot provide you with the files.
 I'm using the .txt option of providing the train and val sets, i have only one class, using image size 800 with batch size 4, providing yolov5l.pt weights (that I downloaded from the google drive), would be nice to see what &lt;denchmark-link:https://github.com/ZeKunZhang1998&gt;@ZeKunZhang1998&lt;/denchmark-link&gt;
  is working with, maybe that can narrow down the problem.
 		</comment>
 		<comment id='7' author='ZeKunZhang1998' date='2020-08-03T22:49:48Z'>
 		&lt;denchmark-link:https://github.com/MiiaBestLamia&gt;@MiiaBestLamia&lt;/denchmark-link&gt;
  I would verify your issue is reproducible in one of the environments above. That's what they're there for.
 		</comment>
 		<comment id='8' author='ZeKunZhang1998' date='2020-08-03T23:58:45Z'>
 		
 Changing batchsize will solve this issue, it will occur when a batch of images contain no object.
 
 when I change to batch size = 1,another picture is something wrong.
 		</comment>
 		<comment id='9' author='ZeKunZhang1998' date='2020-08-04T00:05:07Z'>
 		
 @glenn-jocher i'm using the most recent repository, all of the requirements are satisfied except the Python version, which might be the reason for the issue, though it seems strange that the learning process works for a bit, then crashes (I'm using 3.6.9). I have altered only one line of code in the repository, the one that prints paths of the files used in the batch. Using a dataset that I may not be allowed to share, so I cannot provide you with the files.
 I'm using the .txt option of providing the train and val sets, i have only one class, using image size 800 with batch size 4, providing yolov5l.pt weights (that I downloaded from the google drive), would be nice to see what @ZeKunZhang1998 is working with, maybe that can narrow down the problem.
 
 I use a private dataset.When I jump the wrong batch , the problem will be solved.The wrong batch has object, it is not an empty picture.
 		</comment>
 		<comment id='10' author='ZeKunZhang1998' date='2020-08-04T03:30:49Z'>
 		Maybe i am wrong about this issue, but this issue disappeared when i changed to another batchsize.
 I've added print(targets.shape) in build_targets, i got this Tensor.Size([0, 6]) when ValueError: too many values to unpack (expected 2)
 		</comment>
 		<comment id='11' author='ZeKunZhang1998' date='2020-08-04T04:14:26Z'>
 		&lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  the only thing we can act on is a reproducible example in one of the verified environments.
 		</comment>
 		<comment id='12' author='ZeKunZhang1998' date='2020-08-04T04:40:57Z'>
 		
 @acai66 the only thing we can act on is a reproducible example in one of the verified environments.
 
 I will reinstall pytorch from source code and fetch latest yolov5, i will upload my datasets if this issue occurs again.
 		</comment>
 		<comment id='13' author='ZeKunZhang1998' date='2020-08-04T07:33:03Z'>
 		I meet the same problem too.
 I think the reason is that after box_candidates function there are no targets.
 		</comment>
 		<comment id='14' author='ZeKunZhang1998' date='2020-08-04T08:32:15Z'>
 		After changing some of the hyperparameters in train.py (lr0:0.001, scale:0.2), moving the project to a computer with a beefier GPU (went from 2060S to 2080Ti, using Python 3.6.9) and increasing the batch size to 8, training is functioning properly for 4 epochs now. I can still reproduce the issue with my data on the 2080Ti by launching train.py with a batch size of 4, so I suppose this issue is caused by peculiarities in data, not problems with the network/code.
 		</comment>
 		<comment id='15' author='ZeKunZhang1998' date='2020-08-04T09:52:45Z'>
 		
 @acai66 the only thing we can act on is a reproducible example in one of the verified environments.
 
 here is my datasets,
 &lt;denchmark-link:https://drive.google.com/file/d/12epqSyYELm7c4mXXIIJos33KDcj1l67f/view?usp=sharing&gt;https://drive.google.com/file/d/12epqSyYELm7c4mXXIIJos33KDcj1l67f/view?usp=sharing&lt;/denchmark-link&gt;
 
 data yaml:
 &lt;denchmark-link:https://github.com/ultralytics/yolov5/files/5021341/2020.yaml.txt&gt;2020.yaml.txt&lt;/denchmark-link&gt;
 
 models yaml:
 &lt;denchmark-link:https://github.com/ultralytics/yolov5/files/5021344/yolov5x_2020.yaml.txt&gt;yolov5x_2020.yaml.txt&lt;/denchmark-link&gt;
 
 train command:
 python train.py --cfg models/yolov5x_2020.yaml --data data/2020.yaml --epochs 300 --batch-size 8 --img-size 512 512 --cache-images --weights '' --name "yolov5x_2020_default" --single-cls
 This issue disappeared when changed batchsize to 12.
 		</comment>
 		<comment id='16' author='ZeKunZhang1998' date='2020-08-04T13:37:33Z'>
 		When I use batch size = 4, it works.
 		</comment>
 		<comment id='17' author='ZeKunZhang1998' date='2020-08-04T13:38:37Z'>
 		
 @acai66 the only thing we can act on is a reproducible example in one of the verified environments.
 
 Hi,I think some data augmentation make the boxes disappear, right? If you use bigger batch size , this issue will disappear.
 		</comment>
 		<comment id='18' author='ZeKunZhang1998' date='2020-08-04T18:59:38Z'>
 		On a private dataset I have also had this issue with batch size = 16. Haven't fully tested further, but the dataset does include a fair amount of images without objects. For what that's worth.
 		</comment>
 		<comment id='19' author='ZeKunZhang1998' date='2020-08-05T03:08:02Z'>
 		It means that whole dataset must have object, if an image isnt labelled then wrong, isnt it?
 		</comment>
 		<comment id='20' author='ZeKunZhang1998' date='2020-08-05T04:20:52Z'>
 		&lt;denchmark-link:https://github.com/buimanhlinh96&gt;@buimanhlinh96&lt;/denchmark-link&gt;
  that is not correct. COCO has over a thousand images without labels.
 		</comment>
 		<comment id='21' author='ZeKunZhang1998' date='2020-08-05T04:24:17Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  so what happended with this issue?  It might in a batch, we must have at least one image is labelled?
 		</comment>
 		<comment id='22' author='ZeKunZhang1998' date='2020-08-05T04:56:49Z'>
 		
 
 @acai66 the only thing we can act on is a reproducible example in one of the verified environments.
 
 here is my datasets,
 https://drive.google.com/file/d/12epqSyYELm7c4mXXIIJos33KDcj1l67f/view?usp=sharing
 data yaml:
 2020.yaml.txt
 models yaml:
 yolov5x_2020.yaml.txt
 train command:
 python train.py --cfg models/yolov5x_2020.yaml --data data/2020.yaml --epochs 300 --batch-size 8 --img-size 512 512 --cache-images --weights '' --name "yolov5x_2020_default" --single-cls
 This issue disappeared when changed batchsize to 12.
 
 &lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  thank you! I think I can work with this. I can only debug official models though, so I will use yolov5x.yaml in place of yours. Do you yourself see the error when running on the default models?
 		</comment>
 		<comment id='23' author='ZeKunZhang1998' date='2020-08-05T05:00:06Z'>
 		&lt;denchmark-link:https://github.com/buimanhlinh96&gt;@buimanhlinh96&lt;/denchmark-link&gt;
  I don't know, I have not tried to reproduce yet. I know test.py operates correctly on datasets without labels, I don't know about train.py. Can you provide minimum viable code to reproduce your specific issue?
 		</comment>
 		<comment id='24' author='ZeKunZhang1998' date='2020-08-05T05:01:48Z'>
 		&lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  also are you able to reproduce in one of the verified environments?
 		</comment>
 		<comment id='25' author='ZeKunZhang1998' date='2020-08-05T06:32:21Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  I try some experiments and come up with the batch-size should be more than or equal 8
 		</comment>
 		<comment id='26' author='ZeKunZhang1998' date='2020-08-05T06:34:42Z'>
 		&lt;denchmark-link:https://github.com/buimanhlinh96&gt;@buimanhlinh96&lt;/denchmark-link&gt;
  there is no constraint on batch size, so you should be able to use batch size 1 to batch size x, whatever your hardware can handle. If this is not the case then there is a bug.
 		</comment>
 		<comment id='27' author='ZeKunZhang1998' date='2020-08-05T09:47:50Z'>
 		
 @acai66 also are you able to reproduce in one of the verified environments?
 
 you can try default yolov5x.yaml. I just change nc to 1 in yolov5x_2020_default.yaml actually.
 		</comment>
 		<comment id='28' author='ZeKunZhang1998' date='2020-08-05T10:24:49Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  Yes. Hopefully we can fix it ASAP. Love yolov5
 		</comment>
 		<comment id='29' author='ZeKunZhang1998' date='2020-08-05T17:07:08Z'>
 		&lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  ah I see, of course. We actually updated train.py a few weeks back to inherit  from the data.yaml in case of a mismatch with the model yaml , so you should be able to use your command with the default 80 class yolov5x.yaml as well, and it will still operate correctly.
 Ok, I will try to reproduce this in a colab notebook today if I have time.
 		</comment>
 		<comment id='30' author='ZeKunZhang1998' date='2020-08-05T17:53:45Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  I'm in the same boat.
 
 Colab notebook environment with torch 1.6.0
 yolov5x yaml changing NC
 private dataset
 
 For me the bug hits right after the first epoch (which successfully completes), when moving to the second epoch.
 It seems fixed by moving the batch size from 4 to 12 as suggested above (Colab runs out of memory on this dataset at 16).
 		</comment>
 		<comment id='31' author='ZeKunZhang1998' date='2020-08-05T18:02:38Z'>
 		&lt;denchmark-link:https://github.com/Jacobsolawetz&gt;@Jacobsolawetz&lt;/denchmark-link&gt;
  hmm ok. Do you have a pretty sparse dataset, do you think it's possible a whole batch of 4 images might have no labels? Does the bug happen during training or testing?
 		</comment>
 		<comment id='32' author='ZeKunZhang1998' date='2020-08-05T20:11:05Z'>
 		&lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  I'm able to reproduce this in a colab notebook:
 &lt;denchmark-link:https://colab.research.google.com/drive/1bCFd_1fyFG8pkXkQ8MubvRSgFsb9ZPhu#scrollTo=-AVqcyhjO89V&gt;https://colab.research.google.com/drive/1bCFd_1fyFG8pkXkQ8MubvRSgFsb9ZPhu#scrollTo=-AVqcyhjO89V&lt;/denchmark-link&gt;
 
 I see this midway through the first epoch:
 &lt;denchmark-code&gt;     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
      0/299     4.78G   0.07214   0.01437         0   0.08651         3       512:  70% 105/150 [00:58&lt;00:22,  2.02it/s]Traceback (most recent call last):
   File "train.py", line 477, in &lt;module&gt;
     train(hyp, opt, device, tb_writer)
   File "train.py", line 300, in train
     loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size
   File "/content/yolov5/utils/general.py", line 446, in compute_loss
     tcls, tbox, indices, anchors = build_targets(p, targets, model)  # targets
   File "/content/yolov5/utils/general.py", line 545, in build_targets
     b, c = t[:, :2].long().T  # image, class
 ValueError: too many values to unpack (expected 2)
      0/299     4.78G   0.07214   0.01437         0   0.08651         3       512:  70% 105/150 [00:58&lt;00:25,  1.79it/s]
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='33' author='ZeKunZhang1998' date='2020-08-05T20:38:36Z'>
 		&lt;denchmark-link:https://github.com/ZeKunZhang1998&gt;@ZeKunZhang1998&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/mrk230&gt;@mrk230&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/Jacobsolawetz&gt;@Jacobsolawetz&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/buimanhlinh96&gt;@buimanhlinh96&lt;/denchmark-link&gt;
  this issue should be resolved now in &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/7eaf225d558c6495190e0c79a56553633a065c49&gt;7eaf225&lt;/denchmark-link&gt;
 . Please  to receive the latest updates and try again.
 Let us know if you run into anymore problems, and good luck!
 		</comment>
 		<comment id='34' author='ZeKunZhang1998' date='2020-08-05T20:40:55Z'>
 		&lt;denchmark-link:https://github.com/acai66&gt;@acai66&lt;/denchmark-link&gt;
  for your dataset I would recommend several changes:
 
 You have very small objects, you need to train at the highest viable resolution, even if it means using a smaller model.
 Start from pretrained weights for best results, but also try training from scratch to compare.
 Use the largest batch size that will fit into RAM.
 Your dataset is different enough from COCO that it may benefit from substantially different hyperparameters. See hyperparameter evolution tutorial: https://github.com/ultralytics/yolov5#tutorials
 
 		</comment>
 		<comment id='35' author='ZeKunZhang1998' date='2020-08-05T21:20:36Z'>
 		
 @acai66 for your dataset I would recommend several changes:
 
 You have very small objects, you need to train at the highest viable resolution, even if it means using a smaller model.
 Start from pretrained weights for best results, but also try training from scratch to compare.
 Use the largest batch size that will fit into RAM.
 Your dataset is different enough from COCO that it may benefit from substantially different hyperparameters. See hyperparameter evolution tutorial: https://github.com/ultralytics/yolov5#tutorials
 
 
 Thank you very much for your recommendation, and i will try to do that,. This issue was solved after git pull latest commits.
 		</comment>
 		<comment id='36' author='ZeKunZhang1998' date='2020-08-05T22:41:19Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  yes... after introspection, there are maybe 6 or so images in the dataset of 500 that do not have annotations. A random grouping of those may have caused the cough.
 Thanks for fixing this bug so quickly!
 		</comment>
 		<comment id='37' author='ZeKunZhang1998' date='2020-08-05T23:48:30Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  Thank you very much!!!!!!!
 		</comment>
 		<comment id='38' author='ZeKunZhang1998' date='2020-08-06T02:37:56Z'>
 		[&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 ] I am also facing the same issue (cloned latest code). Maybe the bug still remain, it quite strange because it can train to final epoch before the error happens. I trained with yolov5-s.yaml, batch-size=100 (maybe it is too large ?) on 2 GPU RTX 2080Ti. Every image contain at least one object
 &lt;denchmark-link:https://user-images.githubusercontent.com/30823943/89484395-a16e1a80-d7c8-11ea-9b9c-15014309e329.png&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='39' author='ZeKunZhang1998' date='2020-08-06T04:35:16Z'>
 		&lt;denchmark-link:https://github.com/anhnktp&gt;@anhnktp&lt;/denchmark-link&gt;
  no, you are incorrect, you are not using the latest code. L545 no longer contains the same code, so the error message you see is not possible to produce in origin/master.
 		</comment>
 		<comment id='40' author='ZeKunZhang1998' date='2020-08-06T04:49:24Z'>
 		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  oh, I see. It is yolov5 version 2 days ago. You added some code. I'll recheck again. Thank you
 		</comment>
 	</comments>
 </bug>
<commit id='7eaf225d558c6495190e0c79a56553633a065c49' author='Glenn Jocher' date='2020-08-05 13:35:31-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='utils\general.py' new_name='utils\general.py'>
 		<file_info nloc='811' complexity='189' token_count='10548'></file_info>
 		<method name='build_targets' parameters='p,targets,model'>
 				<method_info nloc='40' complexity='4' token_count='573' nesting_level='0' start_line='506' end_line='560'></method_info>
 			<added_lines>526,542,543,544</added_lines>
 			<deleted_lines>527</deleted_lines>
 		</method>
 		<method name='compute_loss' parameters='p,targets,model'>
 				<method_info nloc='39' complexity='7' token_count='570' nesting_level='0' start_line='443' end_line='503'></method_info>
 			<added_lines>499</added_lines>
 			<deleted_lines>499,500</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
