<bug_data>
<bug id='1099' author='ildoonet' open_date='2019-05-28T01:10:09Z' closed_time='2019-05-30T01:58:02Z'>
 	<summary>Duplicated Synchronization with Gradient Clipping?</summary>
 	<description>
 Environment:
 
 Framework: pytorch
 Framework version: 1.0
 Horovod version: latest
 MPI version: -
 CUDA version: -
 NCCL version: -
 Python version: 3.6
 OS and version: Ubuntu 16.04
 
 &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/torch/__init__.py#L174&gt;https://github.com/horovod/horovod/blob/master/horovod/torch/__init__.py#L174&lt;/denchmark-link&gt;
 
 If you see this documentation, when using gradient clipping, there is duplicated(twice) synchronization. Once is when 'synchronize' is called and second one is when 'step' is called.
 After I followed this documentation with gradient clipping, training speed is slower.
 Any solusions?
 	</description>
 	<comments>
 		<comment id='1' author='ildoonet' date='2019-05-29T03:34:08Z'>
 		&lt;denchmark-link:https://github.com/ildoonet&gt;@ildoonet&lt;/denchmark-link&gt;
 , thanks for raising this!  This regression has been introduced by &lt;denchmark-link:https://github.com/horovod/horovod/pull/597&gt;#597&lt;/denchmark-link&gt;
 .
 While we're thinking about a proper solution, can you specify  after  wraps original optimizer, &lt;denchmark-link:https://gist.github.com/alsrgv/0713add50fe49a409316832a31612dde&gt;like this&lt;/denchmark-link&gt;
 ?
 		</comment>
 		<comment id='2' author='ildoonet' date='2019-05-29T05:08:47Z'>
 		&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
  Thanks for the tip. I will try with it and wait for the proper solutions.
 		</comment>
 		<comment id='3' author='ildoonet' date='2019-05-30T01:58:55Z'>
 		&lt;denchmark-link:https://github.com/ildoonet&gt;@ildoonet&lt;/denchmark-link&gt;
 , the fix was merged into master.  You can reinstall Horovod from master (or wait a bit for 0.16.3), and use , as new documentation prescribes.
 		</comment>
 	</comments>
 </bug>
<commit id='60506fb1571c89b5584280ba87522b047d6e9a0c' author='Alex Sergeev' date='2019-05-29 18:58:01-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.8035714285714286' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='horovod\torch\__init__.py' new_name='horovod\torch\__init__.py'>
 		<file_info nloc='208' complexity='58' token_count='1623'></file_info>
 		<method name='step' parameters='self,closure'>
 				<method_info nloc='3' complexity='1' token_count='28' nesting_level='1' start_line='149' end_line='151'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>149,150</deleted_lines>
 		</method>
 		<method name='step' parameters='self,closure,synchronize'>
 				<method_info nloc='11' complexity='3' token_count='55' nesting_level='1' start_line='154' end_line='164'></method_info>
 			<added_lines>154,155,156,157,158,159,160,161,162,163</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='synchronize' parameters='self'>
 				<method_info nloc='16' complexity='5' token_count='155' nesting_level='1' start_line='135' end_line='152'></method_info>
 			<added_lines>152</added_lines>
 			<deleted_lines>149,150</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>20,21,79,153,181,182,190,191</added_lines>
 			<deleted_lines>175,176</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\test_torch.py' new_name='test\test_torch.py'>
 		<file_info nloc='973' complexity='226' token_count='8233'></file_info>
 		<method name='test_gradient_clipping' parameters='self'>
 				<method_info nloc='24' complexity='2' token_count='243' nesting_level='1' start_line='1227' end_line='1256'></method_info>
 			<added_lines>1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_synchronize_step_warning' parameters='self'>
 				<method_info nloc='23' complexity='2' token_count='203' nesting_level='1' start_line='1258' end_line='1286'></method_info>
 			<added_lines>1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1226,1257</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
