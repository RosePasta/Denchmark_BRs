<bug_data>
<bug id='1669' author='ahmadki' open_date='2020-01-14T21:15:07Z' closed_time='2020-07-13T18:30:00Z'>
 	<summary>mxnet allgather is borken. it crashed when using GPU and produces wrong results when using CPU</summary>
 	<description>
 Environment:
 
 Framework: MXNet
 Framework version: 1.5.1
 Horovod version: built from source
 MPI version: 3.1.1
 CUDA version: 10.2
 NCCL version: 2.5.6
 Python version: 3.6.9
 OS and version: Linux Ubuntu 18.04.3
 GCC version: 7.4.0
 
 Checklist:
 
 Did you search issues to find if somebody asked this question before?
 If your question is about hang, did you read this doc? Yes
 If your question is about docker, did you read this doc?
 Did you check if you question is answered in the troubleshooting guide?
 
 
 Following PR &lt;denchmark-link:https://github.com/horovod/horovod/pull/1639&gt;#1639&lt;/denchmark-link&gt;
 ,  in mxnet is still borken. It produces incorrect results when using CPU and crashes when using GPUs
 I installed horovod from source:
 git clone --recurse-submodules -j8 https://github.com/horovod/horovod.git
 cd horovod
 export HOROVOD_GPU_ALLREDUCE=NCCL
 export HOROVOD_NCCL_INCLUDE=/usr/include
 export HOROVOD_NCCL_LIB=/usr/lib/x86_64-linux-gnu
 export HOROVOD_NCCL_LINK=SHARED
 export HOROVOD_WITHOUT_PYTORCH=1
 export HOROVOD_WITHOUT_TENSORFLOW=1
 export HOROVOD_WITH_MXNET=1
 export HOROVOD_WITH_MPI=1
 ln -s /usr/local/cuda/lib64/stubs/libcuda.so ./libcuda.so.1
 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD
 python setup.py install
 Then ran the following sample code:
 # CUDA_VISIBLE_DEVICES=0,1 mpiexec -n 2 --allow-run-as-root python all_gather_bug.py
 import mxnet as mx
 from mxnet import ndarray
 import horovod.mxnet as hvd
 from mpi4py import MPI
 
 use_gpu = False
 use_mpi4py = False
 
 if use_mpi4py:
     comm = MPI.COMM_WORLD
     hvd.init(comm=comm)
     local_rank = comm.Get_rank()
 else:
     hvd.init()
     local_rank = hvd.local_rank()
 
 ctx = mx.gpu(local_rank) if use_gpu else mx.cpu(local_rank)
 mx.random.seed(local_rank)
 
 a = ndarray.random.uniform(shape=[10, 100, 100], ctx=ctx)
 print("(before) a.shape = {}".format(a.shape))
 print("(before) a[0]={}".format(a[0]))
 
 a = hvd.allgather(a)
 print("(after) a.shape = {}".format(a.shape))
 print("(after) a[0]={}".format(a[0]))
 
 mx.nd.waitall()
 hvd.shutdown()
 When using CPU, it looks like rank 0 is overwritting other ranks (ie broadcast instead of allgather):
 &lt;denchmark-code&gt;(before) a.shape = (10, 100, 100)
 (before) a.shape = (10, 100, 100)
 (before) a[0]=
 [[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01
   4.1702199e-01 9.9718481e-01]
  [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01
   2.5926229e-02 9.3154085e-01]
  [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01
   2.9090473e-01 1.2132858e-01]
  ...
  [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01
   4.6623814e-01 1.4771296e-01]
  [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02
   5.1639861e-01 5.5254018e-01]
  [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01
   6.7598689e-01 4.6260124e-01]]
 &lt;NDArray 100x100 @cpu(0)&gt;
 (after) a.shape = (10, 100, 100)
 (before) a[0]=
 [[0.52383333 0.05501367 0.03996297 ... 0.57466674 0.86663705 0.89201903]
  [0.26314485 0.8478666  0.13140848 ... 0.47916418 0.26825976 0.62353116]
  [0.60529524 0.56331843 0.08485638 ... 0.32175666 0.5547923  0.8856785 ]
  ...
  [0.8128833  0.50981677 0.89495724 ... 0.37793323 0.36698005 0.35633445]
  [0.12747145 0.10071229 0.580568   ... 0.38354123 0.4238108  0.21727636]
  [0.54174274 0.33404106 0.6885549  ... 0.9920475  0.7385572  0.32574102]]
 &lt;NDArray 100x100 @cpu(1)&gt;
 (after) a.shape = (10, 100, 100)
 (after) a[0]=
 [[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01
   4.1702199e-01 9.9718481e-01]
  [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01
   2.5926229e-02 9.3154085e-01]
  [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01
   2.9090473e-01 1.2132858e-01]
  ...
  [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01
   4.6623814e-01 1.4771296e-01]
  [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02
   5.1639861e-01 5.5254018e-01]
  [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01
   6.7598689e-01 4.6260124e-01]]
 &lt;NDArray 100x100 @cpu(0)&gt;
 (after) a[0]=
 [[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01
   4.1702199e-01 9.9718481e-01]
  [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01
   2.5926229e-02 9.3154085e-01]
  [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01
   2.9090473e-01 1.2132858e-01]
  ...
  [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01
   4.6623814e-01 1.4771296e-01]
  [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02
   5.1639861e-01 5.5254018e-01]
  [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01
   6.7598689e-01 4.6260124e-01]]
 &lt;NDArray 100x100 @cpu(1)&gt;
 &lt;/denchmark-code&gt;
 
 When using GPUs the operation fails:
 &lt;denchmark-code&gt;(before) a.shape = (10, 100, 100)
 (before) a[0]=
 [[0.6686509  0.17409194 0.3850025  ... 0.43011498 0.0661214  0.2502998 ]
  [0.7005292  0.19000232 0.6673837  ... 0.27718288 0.16084558 0.223108  ]
  [0.96042585 0.81086403 0.54152083 ... 0.5650488  0.5196334  0.6767488 ]
  ...
  [0.96879214 0.9387428  0.04036242 ... 0.13176239 0.3436321  0.47154343]
  [0.8069018  0.91234195 0.01141495 ... 0.35816687 0.57390726 0.68393874]
  [0.72049534 0.67948174 0.44702923 ... 0.87448525 0.63809574 0.7006303 ]]
 &lt;NDArray 100x100 @gpu(0)&gt;
 (after) a.shape = (10, 100, 100)
 (before) a.shape = (10, 100, 100)
 (before) a[0]=
 [[0.7685592  0.10232276 0.8685353  ... 0.93769354 0.62144864 0.21535844]
  [0.85973674 0.3420865  0.6202223  ... 0.5464046  0.41442537 0.32170743]
  [0.11786121 0.23281038 0.95843846 ... 0.17739207 0.5901362  0.28355032]
  ...
  [0.70749086 0.61171615 0.37854642 ... 0.3485002  0.29636437 0.7359518 ]
  [0.4345038  0.90834665 0.5242443  ... 0.0793817  0.40161872 0.6579807 ]
  [0.8531954  0.18177992 0.7053579  ... 0.5257004  0.24457276 0.74836564]]
 &lt;NDArray 100x100 @gpu(1)&gt;
 (after) a.shape = (10, 100, 100)
 Traceback (most recent call last):
   File "all_gather_bug.py", line 29, in &lt;module&gt;
     print("(after) a[0]={}".format(a[0]))
   File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 194, in __repr__
     return '\n%s\n&lt;%s %s @%s&gt;' % (str(self.asnumpy()),
   File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
     ctypes.c_size_t(data.size)))
   File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
     raise MXNetError(py_str(_LIB.MXGetLastError()))
 mxnet.base.MXNetError: MPI_Allgatherv failed, see MPI output for details.
 Traceback (most recent call last):
   File "all_gather_bug.py", line 29, in &lt;module&gt;
     print("(after) a[0]={}".format(a[0]))
   File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 194, in __repr__
     return '\n%s\n&lt;%s %s @%s&gt;' % (str(self.asnumpy()),
   File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
     ctypes.c_size_t(data.size)))
   File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
     raise MXNetError(py_str(_LIB.MXGetLastError()))
 mxnet.base.MXNetError: MPI_Allgatherv failed, see MPI output for details.
 --------------------------------------------------------------------------
 Primary job  terminated normally, but 1 process returned
 a non-zero exit code. Per user-direction, the job has been aborted.
 --------------------------------------------------------------------------
 --------------------------------------------------------------------------
 mpiexec detected that one or more processes exited with non-zero status, thus causing
 the job to be terminated. The first process to do so was:
 
   Process name: [[59002,1],1]
   Exit code:    1
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='ahmadki' date='2020-01-14T21:25:28Z'>
 		There is a known issue with MXNet allgather. &lt;denchmark-link:https://github.com/horovod/horovod/issues/1409&gt;#1409&lt;/denchmark-link&gt;
 
 We are currently working to resolve it. Will keep you posted. Thanks!
 		</comment>
 		<comment id='2' author='ahmadki' date='2020-01-15T00:19:44Z'>
 		I believe I found the issue.
 The operation receive buffer is defined to have the same size as the send buffer (instead of the sum of sizes from all ranks):
 &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155&gt;https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155&lt;/denchmark-link&gt;
 
 This obviously results in truncated results on CPU and illegal memory access in CUDA.
 I'm working on a PR.
 		</comment>
 		<comment id='3' author='ahmadki' date='2020-01-15T06:01:48Z'>
 		Thanks. That'll be really great. I would love to review when it's ready.
 		</comment>
 		<comment id='4' author='ahmadki' date='2020-01-28T19:30:31Z'>
 		&lt;denchmark-link:https://github.com/ahmadki&gt;@ahmadki&lt;/denchmark-link&gt;
  Do you need any help with this PR? Thanks!
 		</comment>
 		<comment id='5' author='ahmadki' date='2020-01-28T19:31:41Z'>
 		There is another report of this issue (&lt;denchmark-link:https://github.com/horovod/horovod/issues/1519&gt;#1519&lt;/denchmark-link&gt;
 ). It would be great if you could provide an ETA for this fix. Thanks!
 		</comment>
 		<comment id='6' author='ahmadki' date='2020-01-30T23:10:48Z'>
 		Sorry for going  AWOL. Incorrect output tensor shape was only the symptom, and the problem is much deeper than I originally though. I got tied up with other things at work and I won't be able to take a second crack at a solution before next week. A summary of my findings so far:
 The allgather output nd-tensor is defined &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155&gt;here&lt;/denchmark-link&gt;
 , This by it self is not an issue since MPI wrappers  reshape and reallocate the output memory buffers as needed.
 However, because hvd.allgather() is async, the original output tensor is returned immediately without modifications. We would expect subsequent calls to the output tensor to be blocking until it is re-evaluated but this is not the case.
 The output tensor buffer is shallow copied &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.cc#L127&gt;here&lt;/denchmark-link&gt;
  and changes made to it by &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/common/ops/collective_operations.h#L91&gt;AllocateOutput&lt;/denchmark-link&gt;
  are not reflected on the original tensor.
 So a solution (probably) should include the creation of a null tensor using ctypes to keep memory ownership at the python level ? and removing the shallow copies.
 		</comment>
 		<comment id='7' author='ahmadki' date='2020-06-24T04:13:22Z'>
 		Any updates here ? Got the same problem recently.
 		</comment>
 	</comments>
 </bug>
<commit id='cf022be959a7c9431a8415729758b26dec1a87e5' author='romerojosh' date='2020-07-13 11:29:59-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.7835051546391752' size='0.061855670103092786'></dmm_unit>
 	<modification change_type='MODIFY' old_name='horovod\mxnet\mpi_ops.cc' new_name='horovod\mxnet\mpi_ops.cc'>
 		<file_info nloc='228' complexity='35' token_count='1607'></file_info>
 		<method name='horovod::mxnet::DoHorovodOperationCudaOnCPU' parameters='on_complete_ptr,param'>
 				<method_info nloc='37' complexity='4' token_count='253' nesting_level='2' start_line='150' end_line='189'></method_info>
 			<added_lines>156,158</added_lines>
 			<deleted_lines>153,155</deleted_lines>
 		</method>
 		<method name='horovod::mxnet::PushHorovodOperation' parameters='op_type,input,output,name,priority,root_rank'>
 				<method_info nloc='22' complexity='2' token_count='183' nesting_level='2' start_line='119' end_line='147'></method_info>
 			<added_lines>130,131,132</added_lines>
 			<deleted_lines>128,129</deleted_lines>
 		</method>
 		<method name='horovod::mxnet::PushHorovodOperationCudaOnCPU' parameters='op_type,input,output,name,priority,root_rank'>
 				<method_info nloc='29' complexity='2' token_count='276' nesting_level='2' start_line='191' end_line='232'></method_info>
 			<added_lines>197,199,200,201,202,205,207,208,209,210,211,212,213,214,215,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231</added_lines>
 			<deleted_lines>194,196,197,200,202,203,204,205,206,208,209</deleted_lines>
 		</method>
 		<method name='horovod::mxnet::DoHorovodOperation' parameters='on_complete_ptr,param'>
 				<method_info nloc='45' complexity='5' token_count='326' nesting_level='2' start_line='68' end_line='117'></method_info>
 			<added_lines>73,74,75,77,79,86,102</added_lines>
 			<deleted_lines>72,73,75,77,84,100</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='horovod\mxnet\mpi_ops.h' new_name='horovod\mxnet\mpi_ops.h'>
 		<file_info nloc='69' complexity='3' token_count='330'></file_info>
 		<method name='horovod::mxnet::MpiOpsParam::MpiOpsParam' parameters='input_tensor,output_tensor,output,cpu_input_tensor,cpu_output_tensor,op_type,op_name,root_rank'>
 				<method_info nloc='16' complexity='1' token_count='75' nesting_level='3' start_line='50' end_line='65'></method_info>
 			<added_lines>52,53,54,59,60,61</added_lines>
 			<deleted_lines>54,63</deleted_lines>
 		</method>
 		<method name='horovod::mxnet::CreateMpiOpsParam' parameters='input_tensor,output_tensor,cpu_tensor,op_type,op_name,root_rank'>
 				<method_info nloc='9' complexity='1' token_count='45' nesting_level='2' start_line='61' end_line='69'></method_info>
 			<added_lines>61</added_lines>
 			<deleted_lines>63,67,68</deleted_lines>
 		</method>
 		<method name='horovod::mxnet::CreateMpiOpsParam' parameters='input_tensor,output_tensor,output,cpu_input_tensor,cpu_output_tensor,op_type,op_name,root_rank'>
 				<method_info nloc='11' complexity='1' token_count='56' nesting_level='2' start_line='68' end_line='78'></method_info>
 			<added_lines>70,71,72,76,77</added_lines>
 			<deleted_lines>68</deleted_lines>
 		</method>
 		<method name='horovod::mxnet::MpiOpsParam::MpiOpsParam' parameters='input_tensor,output_tensor,cpu_tensor,op_type,op_name,root_rank'>
 				<method_info nloc='12' complexity='1' token_count='58' nesting_level='3' start_line='47' end_line='58'></method_info>
 			<added_lines>52,53,54</added_lines>
 			<deleted_lines>49,54</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2,43,44,45</added_lines>
 			<deleted_lines>42</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='horovod\mxnet\mpi_ops.py' new_name='horovod\mxnet\mpi_ops.py'>
 		<file_info nloc='93' complexity='11' token_count='729'></file_info>
 		<method name='allgather' parameters='tensor,name,priority'>
 				<method_info nloc='14' complexity='2' token_count='123' nesting_level='0' start_line='126' end_line='167'></method_info>
 			<added_lines>152,153,154,164,165,166</added_lines>
 			<deleted_lines>151</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\test_mxnet.py' new_name='test\test_mxnet.py'>
 		<file_info nloc='508' complexity='99' token_count='4252'></file_info>
 		<method name='test_horovod_allgather_type_error' parameters='self'>
 				<method_info nloc='17' complexity='4' token_count='119' nesting_level='1' start_line='582' end_line='605'></method_info>
 			<added_lines>582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_horovod_allgather_variable_size' parameters='self'>
 				<method_info nloc='25' complexity='4' token_count='245' nesting_level='1' start_line='524' end_line='557'></method_info>
 			<added_lines>524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_horovod_allgather_error' parameters='self'>
 				<method_info nloc='15' complexity='3' token_count='98' nesting_level='1' start_line='559' end_line='580'></method_info>
 			<added_lines>559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_horovod_allgather' parameters='self'>
 				<method_info nloc='18' complexity='3' token_count='183' nesting_level='1' start_line='501' end_line='522'></method_info>
 			<added_lines>501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>523,558,581,606</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
