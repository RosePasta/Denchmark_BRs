<bug_data>
<bug id='605' author='jotterbach' open_date='2018-11-01T22:51:26Z' closed_time='2018-11-03T01:28:25Z'>
 	<summary>pytorch + horovod 0.15.1 distributed optimizer not working anymore</summary>
 	<description>
 I just upgraded horovod 0.15.0 -&gt; 0.15.1 on a ubuntu image 4.4.0-137-generic #163-Ubuntu SMP Mon Sep 24 13:14:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. When using the DistributedOptimizer from horovod.torch I now encounter the error
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "train.py", line 641, in &lt;module&gt;
     train_images(hps)
   File "train.py", line 444, in train_images
     train_step(batch, batch_idx, epoch, hps, model, opt, train_logger)
   File "train.py", line 457, in train_step
     opt.step()
   File "/opt/conda/lib/python3.6/site-packages/horovod/torch/__init__.py", line 97, in step
     return super(self.__class__, self).step(closure)
   File "/opt/conda/lib/python3.6/site-packages/torch/optim/adamax.py", line 75, in step
     exp_avg.mul_(beta1).add_(1 - beta1, grad)
 TypeError: mul_() received an invalid combination of arguments - got (numpy.float32), but expected one of:
  * (Tensor other)
       didn't match because some of the arguments have invalid types: (numpy.float32)
  * (float other)
       didn't match because some of the arguments have invalid types: (numpy.float32)
 &lt;/denchmark-code&gt;
 
 Downgrading to 0.15.0 fixes the issue. The behavior is independent of CPU, GPU or MultipleGPU training.
 	</description>
 	<comments>
 		<comment id='1' author='jotterbach' date='2018-11-01T23:48:12Z'>
 		Hey &lt;denchmark-link:https://github.com/jotterbach&gt;@jotterbach&lt;/denchmark-link&gt;
 , what version of PyTorch are you using?
 		</comment>
 		<comment id='2' author='jotterbach' date='2018-11-02T00:27:49Z'>
 		Sorry, I forgot to specify that. It's 0.4.0.
 		</comment>
 		<comment id='3' author='jotterbach' date='2018-11-02T16:20:06Z'>
 		That's curious.  I haven't been able to repro yet, but it looks like the value of beta1 is being set to a numpy float32 when a pure Python float is expected.  Do you have a snippet of code that initializes the Adamax optimizer?
 		</comment>
 		<comment id='4' author='jotterbach' date='2018-11-02T19:51:14Z'>
 		Sure thing &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
  . The code is fairly straight forward:
 &lt;denchmark-code&gt;if hps.cuda:
     model = model.cuda()
 opt_s = torch.optim.Adamax(model.parameters(), hps.lr)
 opt = hvd.DistributedOptimizer(opt_s,
                                named_parameters=model.named_parameters())
 
 hvd.broadcast_parameters(model.state_dict(), root_rank=0)
 hvd.broadcast_optimizer_state(opt, root_rank=0)
 &lt;/denchmark-code&gt;
 
 The relevant code in the training loop looks like:
 &lt;denchmark-code&gt;model.train()
 opt.zero_grad()
 if hps.cuda:
     batch = batch.cuda()
 loss = model(batch)
 
 loss.backward()
 opt.synchronize()
 opt.step()
 &lt;/denchmark-code&gt;
 
 The optimizer is called nowhere else.
 		</comment>
 		<comment id='5' author='jotterbach' date='2018-11-02T20:45:18Z'>
 		Ah, okay, I see what's going on here.  This is the result of a change we made to hvd.broadcast_optimizer_state() to broadcast the scalar options.  Looks like beta1 is getting coerced into a numpy.float32, which is causing your issue.
 I'll start working on a fix for this.  Thanks for reporting!
 		</comment>
 	</comments>
 </bug>
<commit id='62d2869047ee8ccab3d559bee35b8f5e392936fb' author='Travis Addair' date='2018-11-02 18:28:24-07:00'>
 	<dmm_unit complexity='0.6' interfacing='1.0' size='0.6'></dmm_unit>
 	<modification change_type='MODIFY' old_name='horovod\torch\__init__.py' new_name='horovod\torch\__init__.py'>
 		<file_info nloc='157' complexity='53' token_count='1307'></file_info>
 		<method name='broadcast_optimizer_state._recursive_cast' parameters='x,dtype'>
 				<method_info nloc='7' complexity='3' token_count='61' nesting_level='1' start_line='238' end_line='244'></method_info>
 			<added_lines>238,239,240,241,242,243,244</added_lines>
 			<deleted_lines>239,241</deleted_lines>
 		</method>
 		<method name='broadcast_optimizer_state.broadcast_optimizer_state._create_option_callback._from_tensor' parameters=''>
 				<method_info nloc='2' complexity='1' token_count='27' nesting_level='2' start_line='256' end_line='257'></method_info>
 			<added_lines>257</added_lines>
 			<deleted_lines>257</deleted_lines>
 		</method>
 		<method name='broadcast_optimizer_state._create_option_callback' parameters='index,option_key,option_tensor,dtypes'>
 				<method_info nloc='3' complexity='1' token_count='15' nesting_level='1' start_line='255' end_line='258'></method_info>
 			<added_lines>255,257</added_lines>
 			<deleted_lines>255,257</deleted_lines>
 		</method>
 		<method name='broadcast_optimizer_state._get_types' parameters='x'>
 				<method_info nloc='5' complexity='3' token_count='38' nesting_level='1' start_line='231' end_line='235'></method_info>
 			<added_lines>231,232,233,234,235</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='broadcast_optimizer_state' parameters='optimizer,root_rank'>
 				<method_info nloc='45' complexity='15' token_count='352' nesting_level='0' start_line='185' end_line='301'></method_info>
 			<added_lines>230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,255,257,271,273</added_lines>
 			<deleted_lines>239,241,255,257</deleted_lines>
 		</method>
 		<method name='broadcast_optimizer_state._create_option_callback' parameters='index,option_key,option_tensor,dtype'>
 				<method_info nloc='3' complexity='1' token_count='15' nesting_level='1' start_line='239' end_line='242'></method_info>
 			<added_lines>239,240,241,242</added_lines>
 			<deleted_lines>239,241</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\test_torch.py' new_name='test\test_torch.py'>
 		<file_info nloc='799' complexity='189' token_count='6749'></file_info>
 		<method name='test_broadcast_state_options' parameters='self'>
 				<method_info nloc='43' complexity='11' token_count='447' nesting_level='1' start_line='868' end_line='935'></method_info>
 			<added_lines>876,878,927,930,931,932,933,934,935</added_lines>
 			<deleted_lines>876,878</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>936</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
