<bug_data>
<bug id='2367' author='hhaoyan' open_date='2020-10-13T03:48:21Z' closed_time='2020-11-02T00:54:34Z'>
 	<summary>Horovod throws UnicodeDecodeError when using tqdm</summary>
 	<description>
 Environment:
 
 Framework: PyTorch
 Framework version: 1.2.0
 Horovod version: 0.19.5
 MPI version: None
 CUDA version: 10.0
 NCCL version: 2.7.8.1
 Python version: 3.6.12
 Spark / PySpark version: None
 OS and version: Linux 4.14.105-1-tlinux3-0010
 GCC version: 5.2.0
 CMake version: 2.8.12.2
 
 What happened?
 When using horovod together with tqdm, horovod throws UnicodeDecodeError when tqdm is progressing the bar display:
 &lt;denchmark-code&gt;Tue Oct 13 10:54:25 2020[0]&lt;stderr&gt;:[10/13/2020 10:54:25 - INFO - __main__ -   start running validation...
 Tue Oct 13 10:54:25 2020[0]&lt;stderr&gt;:  0%|          | 0/1327 [00:00&lt;?, ?it/s]
 Tue Oct 13 10:55:56 2020[0]&lt;stderr&gt;:
 Exception in thread Thread-17:derr&gt;: 65%|██████▌   | 866/1327 [01:17&lt;00:47,  9.80it/s]
 Traceback (most recent call last):
   File "/data/anaconda3/envs/vcr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
     self.run()
   File "/data/anaconda3/envs/vcr/lib/python3.6/threading.py", line 864, in run
     self._target(*self._args, **self._kwargs)
   File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/runner/common/util/safe_shell_exec.py", line 104, in forward_stream
     text = text.decode('utf-8')
 UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 999: unexpected end of data
 
 Tue Oct 13 10:58:56 2020[3]&lt;stderr&gt;:[2020-10-13 10:58:56.365049: E /dockerdata/app/tmp/pip-install-998q9kn4/horovod/horovod/common/operations.cc:525] Horovod background loop uncaught exception: [/dockerdata/app/tmp/pip-install-998q9kn4/horovod/third_party/compatible_gloo/gloo/transport/tcp/unbound_buffer.cc:84] Timed out waiting 30000ms for recv operation to complete
 Tue Oct 13 10:58:56 2020[2]&lt;stderr&gt;:[2020-10-13 10:58:56.366492: E /dockerdata/app/tmp/pip-install-998q9kn4/horovod/horovod/common/operations.cc:525] Horovod background loop uncaught exception: [/dockerdata/app/tmp/pip-install-998q9kn4/horovod/third_party/compatible_gloo/gloo/transport/tcp/unbound_buffer.cc:84] Timed out waiting 30000ms for recv operation to complete
 Tue Oct 13 10:58:56 2020[1]&lt;stderr&gt;:[2020-10-13 10:58:56.366674: E /dockerdata/app/tmp/pip-install-998q9kn4/horovod/horovod/common/operations.cc:525] Horovod background loop uncaught exception: [/dockerdata/app/tmp/pip-install-998q9kn4/horovod/third_party/compatible_gloo/gloo/transport/tcp/unbound_buffer.cc:136] Timed out waiting 30000ms for send operation to complete
 Tue Oct 13 10:58:56 2020[3]&lt;stderr&gt;:Traceback (most recent call last):
 Tue Oct 13 10:58:56 2020[1]&lt;stderr&gt;:Traceback (most recent call last):
 Tue Oct 13 10:58:56 2020[3]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 622, in synchronize
 Tue Oct 13 10:58:56 2020[2]&lt;stderr&gt;:Traceback (most recent call last):
 Tue Oct 13 10:58:56 2020[3]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
 Tue Oct 13 10:58:56 2020[2]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 622, in synchronize
 Tue Oct 13 10:58:56 2020[3]&lt;stderr&gt;:RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
 Tue Oct 13 10:58:56 2020[1]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 622, in synchronize
 Tue Oct 13 10:58:56 2020[3]&lt;stderr&gt;:
 Tue Oct 13 10:58:56 2020[1]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
 Tue Oct 13 10:58:56 2020[2]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:During handling of the above exception, another exception occurred:
 Tue Oct 13 10:58:57 2020[2]&lt;stderr&gt;:RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:
 Tue Oct 13 10:58:57 2020[2]&lt;stderr&gt;:
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:Traceback (most recent call last):
 Tue Oct 13 10:58:57 2020[2]&lt;stderr&gt;:During handling of the above exception, another exception occurred:
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:During handling of the above exception, another exception occurred:
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:  File "train_vcr.py", line 332, in &lt;module&gt;
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:    main(parse_cmd_args(TrainingOpts))
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:Traceback (most recent call last):
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:  File "train_vcr.py", line 260, in main
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:  File "train_vcr.py", line 332, in &lt;module&gt;
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:    val_log = validate(model, val_dataloader)
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:    main(parse_cmd_args(TrainingOpts))
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:  File "train_vcr.py", line 260, in main
 Tue Oct 13 10:58:57 2020[3]&lt;stderr&gt;:    return func(*args, **kwargs)
 Tue Oct 13 10:58:57 2020[1]&lt;stderr&gt;:    val_log = validate(model, val_dataloader)
 Tue Oct 13 10:58:58 2020[3]&lt;stderr&gt;:  File "train_vcr.py", line 310, in validate
 Tue Oct 13 10:58:58 2020[1]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
 Tue Oct 13 10:58:57 2020[2]&lt;stderr&gt;:
 Tue Oct 13 10:58:58 2020[3]&lt;stderr&gt;:    qa_loss = sum(all_gather_list(qa_loss))
 Tue Oct 13 10:58:58 2020[1]&lt;stderr&gt;:    return func(*args, **kwargs)
 Tue Oct 13 10:58:58 2020[2]&lt;stderr&gt;:Traceback (most recent call last):
 Tue Oct 13 10:58:58 2020[1]&lt;stderr&gt;:  File "train_vcr.py", line 310, in validate
 Tue Oct 13 10:58:58 2020[2]&lt;stderr&gt;:  File "train_vcr.py", line 332, in &lt;module&gt;
 Tue Oct 13 10:58:58 2020[3]&lt;stderr&gt;:  File "/data/cdp_algo_ceph_ssd/users/haoyanhuo/vcr/utils/distributed.py", line 233, in all_gather_list
 Tue Oct 13 10:58:58 2020[2]&lt;stderr&gt;:    main(parse_cmd_args(TrainingOpts))
 Tue Oct 13 10:58:58 2020[3]&lt;stderr&gt;:    max_size = hvd.allgather(torch.tensor([enc_size]).cuda()).max().item()
 Tue Oct 13 10:58:58 2020[1]&lt;stderr&gt;:    qa_loss = sum(all_gather_list(qa_loss))
 Tue Oct 13 10:58:58 2020[2]&lt;stderr&gt;:  File "train_vcr.py", line 260, in main
 Tue Oct 13 10:58:58 2020[3]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 365, in allgather
 Tue Oct 13 10:58:58 2020[2]&lt;stderr&gt;:    val_log = validate(model, val_dataloader)
 Tue Oct 13 10:58:58 2020[3]&lt;stderr&gt;:    return HorovodAllgather.apply(tensor, name)
 Tue Oct 13 10:58:58 2020[1]&lt;stderr&gt;:  File "/data/cdp_algo_ceph_ssd/users/haoyanhuo/vcr/utils/distributed.py", line 233, in all_gather_list
 Tue Oct 13 10:58:58 2020[2]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
 Tue Oct 13 10:58:58 2020[1]&lt;stderr&gt;:    max_size = hvd.allgather(torch.tensor([enc_size]).cuda()).max().item()
 Tue Oct 13 10:58:58 2020[3]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 328, in forward
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:    return func(*args, **kwargs)
 Tue Oct 13 10:58:59 2020[3]&lt;stderr&gt;:    return synchronize(handle)
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:  File "train_vcr.py", line 310, in validate
 Tue Oct 13 10:58:59 2020[3]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 626, in synchronize
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:    qa_loss = sum(all_gather_list(qa_loss))
 Tue Oct 13 10:58:59 2020[3]&lt;stderr&gt;:    raise HorovodInternalError(e)
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:  File "/data/cdp_algo_ceph_ssd/users/haoyanhuo/vcr/utils/distributed.py", line 233, in all_gather_list
 Tue Oct 13 10:58:59 2020[3]&lt;stderr&gt;:horovod.common.exceptions.HorovodInternalError: Horovod has been shut down. This was causedby an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finishedexecution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
 Tue Oct 13 10:58:59 2020[1]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 365, in allgather
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:    max_size = hvd.allgather(torch.tensor([enc_size]).cuda()).max().item()
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 365, in allgather
 Tue Oct 13 10:58:59 2020[1]&lt;stderr&gt;:    return HorovodAllgather.apply(tensor, name)
 Tue Oct 13 10:58:59 2020[1]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 328, in forward
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:    return HorovodAllgather.apply(tensor, name)
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 328, in forward
 Tue Oct 13 10:58:59 2020[1]&lt;stderr&gt;:    return synchronize(handle)
 Tue Oct 13 10:58:59 2020[1]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 626, in synchronize
 Tue Oct 13 10:58:59 2020[2]&lt;stderr&gt;:    return synchronize(handle)
 Tue Oct 13 10:58:59 2020[1]&lt;stderr&gt;:    raise HorovodInternalError(e)
 Tue Oct 13 10:59:00 2020[2]&lt;stderr&gt;:  File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 626, in synchronize
 Tue Oct 13 10:59:00 2020[1]&lt;stderr&gt;:horovod.common.exceptions.HorovodInternalError: Horovod has been shut down. This was causedby an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finishedexecution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
 Tue Oct 13 10:59:00 2020[2]&lt;stderr&gt;:    raise HorovodInternalError(e)
 Tue Oct 13 10:59:00 2020[2]&lt;stderr&gt;:horovod.common.exceptions.HorovodInternalError: Horovod has been shut down. This was causedby an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finishedexecution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.
 &lt;/denchmark-code&gt;
 
 Reasons for this issue
 This is due to a truncated (since horovod only captures 1000 bytes) UTF8 code. For example, the above error was due to a truncated black box character 0xe2 0x96 0x88 used by tqdm to indicate progress. The following is an example of the bytes produced by tqdm. Pay attention to bytes 0x00059960-0x00059970:
 &lt;denchmark-code&gt;...
 00059930  30 5d 3c 73 74 64 65 72  72 3e 3a 0d 54 75 65 20  |0]&lt;stderr&gt;:.Tue |
 00059940  4f 63 74 20 31 33 20 31  30 3a 35 35 3a 35 35 20  |Oct 13 10:55:55 |
 00059950  32 30 32 30 5b 30 5d 3c  73 74 64 65 72 72 3e 3a  |2020[0]&lt;stderr&gt;:|
 00059960  20 36 35 25 7c e2 96 88  e2 96 88 e2 96 88 e2 96  | 65%|...........|
 00059970  88 e2 96 88 e2 96 88 e2  96 8d 20 20 20 7c 20 38  |..........   | 8|
 00059980  35 38 2f 31 33 32 37 20  5b 30 31 3a 31 37 3c 30  |58/1327 [01:17&lt;0|
 00059990  30 3a 34 34 2c 20 31 30  2e 34 33 69 74 2f 73 5d  |0:44, 10.43it/s]|
 000599a0  1b 5b 41 1b 5b 41 0a 54  75 65 20 4f 63 74 20 31  |.[A.[A.Tue Oct 1|
 000599b0  33 20 31 30 3a 35 35 3a  35 35 20 32 30 32 30 5b  |3 10:55:55 2020[|
 000599c0  30 5d 3c 73 74 64 65 72  72 3e 3a 0a 54 75 65 20  |0]&lt;stderr&gt;:.Tue |
 ...
 &lt;/denchmark-code&gt;
 
 I think horovod needs a better way of handling program outputs, either pipe them in binary or delay the decoding of partially captured UTF8 codes.
 	</description>
 	<comments>
 		<comment id='1' author='hhaoyan' date='2020-10-13T07:02:40Z'>
 		how to solve this bug?
 		</comment>
 		<comment id='2' author='hhaoyan' date='2020-10-13T07:15:47Z'>
 		
 how to solve this bug?
 
 A quick fix: tqdm(ascii=True) to eliminate UTF8 outputs.
 The better way: Replace all strs in forward_stream with bytes.
 		</comment>
 		<comment id='3' author='hhaoyan' date='2020-10-13T07:18:41Z'>
 		
 
 how to solve this bug?
 
 A quick fix: tqdm(ascii=True) to eliminate UTF8 outputs.
 The better way: Replace all strs in forward_stream with bytes.
 
 thanks!
 		</comment>
 		<comment id='4' author='hhaoyan' date='2020-10-14T13:24:03Z'>
 		Thanks for raising this issue &lt;denchmark-link:https://github.com/hhaoyan&gt;@hhaoyan&lt;/denchmark-link&gt;
 .  Do you have a small repro script I can run to demonstrate the error?
 		</comment>
 		<comment id='5' author='hhaoyan' date='2020-10-15T02:50:30Z'>
 		
 Thanks for raising this issue @hhaoyan. Do you have a small repro script I can run to demonstrate the error?
 
 Here is a simple script that reproduces this error:
 from horovod import torch as hvd
 import time
 
 hvd.init()
 while True:
     print(b'\xe2\x96\x88'.decode('utf8')*334)
     time.sleep(1)
 From terminal, run:
 $ horovodrun -n 2 pythontest.py
 Exception in thread Thread-6:
 Traceback (most recent call last):
   File "/data/anaconda3/envs/vcr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
     self.run()
   File "/data/anaconda3/envs/vcr/lib/python3.6/threading.py", line 864, in run
     self._target(*self._args, **self._kwargs)
   File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/run/common/util/safe_shell_exec.py", line 98, in forward_stream
     text = text.decode('utf-8')
 UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 999: unexpected end of data
 Exception in thread Thread-5:
 Traceback (most recent call last):
   File "/data/anaconda3/envs/vcr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
     self.run()
   File "/data/anaconda3/envs/vcr/lib/python3.6/threading.py", line 864, in run
     self._target(*self._args, **self._kwargs)
   File "/data/anaconda3/envs/vcr/lib/python3.6/site-packages/horovod/run/common/util/safe_shell_exec.py", line 98, in forward_stream
     text = text.decode('utf-8')
 UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 999: unexpected end of data
 		</comment>
 		<comment id='6' author='hhaoyan' date='2020-10-16T20:50:34Z'>
 		
 how to solve this bug?
 
 Alternative would be not to truncate on 1000 bytes but 1000 characters. Where is this 1000 truncation happening?
 		</comment>
 		<comment id='7' author='hhaoyan' date='2020-10-18T03:15:54Z'>
 		
 
 how to solve this bug?
 
 Alternative would be not to truncate on 1000 bytes but 1000 characters. Where is this 1000 truncation happening?
 
 &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/runner/common/util/safe_shell_exec.py#L99&gt;https://github.com/horovod/horovod/blob/master/horovod/runner/common/util/safe_shell_exec.py#L99&lt;/denchmark-link&gt;
 
 If subprocess throws some illegal UTF8 bytes, then I guess there is no way to truncate in characters?
 I temporarily fixed it by using bytes IO for all pipes, stdout/stderr and it seems to be working for me.
 		</comment>
 		<comment id='8' author='hhaoyan' date='2020-10-18T14:26:41Z'>
 		I think the simplest fix is to use  with , which makes it ignore incomplete / invalid UTF8 characters: &lt;denchmark-link:https://docs.python.org/3.6/howto/unicode.html#the-string-type&gt;https://docs.python.org/3.6/howto/unicode.html#the-string-type&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/runner/common/util/safe_shell_exec.py#L104&gt;https://github.com/horovod/horovod/blob/master/horovod/runner/common/util/safe_shell_exec.py#L104&lt;/denchmark-link&gt;
  would then read:
 &lt;denchmark-code&gt;text = text.decode('utf-8', errors='ignore')
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='9' author='hhaoyan' date='2020-10-19T19:15:58Z'>
 		Good suggestion, &lt;denchmark-link:https://github.com/EnricoMi&gt;@EnricoMi&lt;/denchmark-link&gt;
 .  Do you have the bandwidth to put together a PR?
 		</comment>
 		<comment id='10' author='hhaoyan' date='2020-10-19T21:30:06Z'>
 		Sure, give me a day or two, I would also add a unit test for that.
 		</comment>
 		<comment id='11' author='hhaoyan' date='2020-10-20T12:48:30Z'>
 		After looking at the forward_stream method a bit closer, using errors='ignore' is not the right thing to do here. We are not truncating the stream at 1000 bytes, but we are chunking it into 1000 bytes chunks to line buffer it. We would lose the character at the chunk boundary. I will rework the line buffering.
 		</comment>
 		<comment id='12' author='hhaoyan' date='2020-10-21T20:20:12Z'>
 		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 : is it required for  to work with both, byte streams and text streams? Given we are calling this from  on pipe connections, we can guarantee it is one of them. Would simplify things.
 &lt;denchmark-code&gt;        if not isinstance(text, str):
             text = text.decode('utf-8')
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/runner/common/util/safe_shell_exec.py#L103&gt;https://github.com/horovod/horovod/blob/master/horovod/runner/common/util/safe_shell_exec.py#L103&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='13' author='hhaoyan' date='2020-10-22T13:06:06Z'>
 		&lt;denchmark-link:https://github.com/EnricoMi&gt;@EnricoMi&lt;/denchmark-link&gt;
  it may not be necessary to decode the stream data, I do not remember why we needed to do that originally, but we can always decode on the other end of the stream when necessary.
 		</comment>
 		<comment id='14' author='hhaoyan' date='2020-10-22T19:15:40Z'>
 		We have to encode it in forward_stream to be able to prepend timestamp, prefix and index to each line:
 &lt;denchmark-code&gt;return '{time}[{rank}]&lt;{prefix}&gt;:{line}'.format(...)
 &lt;/denchmark-code&gt;
 
 I think this justifies expecting the stream to be a text stream.
 		</comment>
 		<comment id='15' author='hhaoyan' date='2020-10-22T19:46:30Z'>
 		Waiting for &lt;denchmark-link:https://github.com/horovod/horovod/pull/2388&gt;#2388&lt;/denchmark-link&gt;
  to land which simplifies unit test assertions without timestamps.
 		</comment>
 		<comment id='16' author='hhaoyan' date='2020-10-23T18:18:29Z'>
 		
 We have to encode it in forward_stream to be able to prepend timestamp, prefix and index to each line:
 return '{time}[{rank}]&lt;{prefix}&gt;:{line}'.format(...)
 
 I think this justifies expecting the stream to be a text stream.
 
 Maybe the prefix strings can be formatted and encoded into UTF8, then we can simply join two byte strings?
 		</comment>
 		<comment id='17' author='hhaoyan' date='2020-10-25T17:17:03Z'>
 		That is a good idea, that would then only require the source stream to be split on new lines, no matter if being a byte or text stream. If a byte stream, prepend each line as described by you, if a text stream, prepend with the prefix as a string. Will try that.
 		</comment>
 		<comment id='18' author='hhaoyan' date='2020-10-26T15:12:50Z'>
 		Well, since we fall back to sys.stdout and sys.stderr when no stdout and stderr are given, respectively, stdout and stderrmust be text streams only. So forward_stream will only be called with text streams and therefore dst_stream is also text stream. This simplifies code.
 		</comment>
 		<comment id='19' author='hhaoyan' date='2020-10-27T15:13:54Z'>
 		PR &lt;denchmark-link:https://github.com/horovod/horovod/pull/2398&gt;#2398&lt;/denchmark-link&gt;
  ready for review.
 		</comment>
 		<comment id='20' author='hhaoyan' date='2020-10-31T12:03:19Z'>
 		The correct and minimal fix is to use an incremental UTF8 decoder rather than decoding that byte chunk on its own:
 &lt;denchmark-code&gt;# the incremental encoder allows us to decode chunks of utf8 bytes
 # with utf8 characters spread across the boundary chunks
 decoder = codecs.getincrementaldecoder('utf8')()
 &lt;/denchmark-code&gt;
 
 and
 &lt;denchmark-code&gt;# turn the bytes into string decoding them as utf8
 # we need to use an incremental decoder as characters can span multiple bytes
 # where the last character might not be completely in buf
 # see https://github.com/horovod/horovod/issues/2367
 text = decoder.decode(buf or b'', final=not buf)
 &lt;/denchmark-code&gt;
 
 The incremental decoder has an internal state and keeps the non-decoded bytes at the end of the chunk for decoding the next chunk.
 		</comment>
 	</comments>
 </bug>
<commit id='8396f5a929112fc8c2e9bbde0245a193743779c5' author='Enrico Minack' date='2020-11-01 16:54:33-08:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.8055555555555556'></dmm_unit>
 	<modification change_type='MODIFY' old_name='horovod\runner\common\util\safe_shell_exec.py' new_name='horovod\runner\common\util\safe_shell_exec.py'>
 		<file_info nloc='138' complexity='30' token_count='842'></file_info>
 		<method name='forward_stream.prepend_context' parameters='line,rank,prefix'>
 				<method_info nloc='8' complexity='2' token_count='54' nesting_level='1' start_line='82' end_line='89'></method_info>
 			<added_lines>83,84,85,86,87,88,89</added_lines>
 			<deleted_lines>82,84,87,88</deleted_lines>
 		</method>
 		<method name='prefix_connection.write' parameters='text'>
 				<method_info nloc='6' complexity='3' token_count='41' nesting_level='1' start_line='105' end_line='110'></method_info>
 			<added_lines>106,107,108</added_lines>
 			<deleted_lines>105,106,107,108</deleted_lines>
 		</method>
 		<method name='forward_stream' parameters='src_stream,dst_stream,prefix,index,prefix_output_with_timestamp'>
 				<method_info nloc='20' complexity='9' token_count='111' nesting_level='0' start_line='81' end_line='119'></method_info>
 			<added_lines>83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,99,102,106,107,108,112,113,114,115,118,119</added_lines>
 			<deleted_lines>81,82,84,87,88,92,93,99,100,101,102,103,104,105,106,107,108,119</deleted_lines>
 		</method>
 		<method name='prefix_connection.get_context' parameters='rank,prefix'>
 				<method_info nloc='7' complexity='2' token_count='48' nesting_level='1' start_line='97' end_line='103'></method_info>
 			<added_lines>97,99,102</added_lines>
 			<deleted_lines>99,100,101,102,103</deleted_lines>
 		</method>
 		<method name='execute' parameters='command,env,stdout,stderr,index,events,prefix_output_with_timestamp'>
 				<method_info nloc='2' complexity='1' token_count='29' nesting_level='0' start_line='188' end_line='189'></method_info>
 			<added_lines>188,189</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='prefix_connection' parameters='src_connection,dst_stream,prefix,index,prefix_output_with_timestamp'>
 				<method_info nloc='18' complexity='8' token_count='115' nesting_level='0' start_line='83' end_line='145'></method_info>
 			<added_lines>83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,99,102,106,107,108,112,113,114,115,118,119,120,121,122,123,124,125,126,127,128,129,130,137,138,139,140,145</added_lines>
 			<deleted_lines>84,87,88,92,93,99,100,101,102,103,104,105,106,107,108,119,137</deleted_lines>
 		</method>
 		<method name='_exec_middleman' parameters='command,env,exit_event,stdout,stderr,rw'>
 				<method_info nloc='17' complexity='2' token_count='125' nesting_level='0' start_line='148' end_line='179'></method_info>
 			<added_lines>163</added_lines>
 			<deleted_lines>162</deleted_lines>
 		</method>
 		<method name='forward_stream.write' parameters='text'>
 				<method_info nloc='5' complexity='2' token_count='32' nesting_level='1' start_line='91' end_line='95'></method_info>
 			<added_lines>91,92,93,94,95</added_lines>
 			<deleted_lines>92,93</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>16,26,27,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,241,242</added_lines>
 			<deleted_lines>18,198,199</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\single\test_run.py' new_name='test\single\test_run.py'>
 		<file_info nloc='888' complexity='120' token_count='6396'></file_info>
 		<method name='test_prefix_connection_does_stream' parameters='self'>
 				<method_info nloc='24' complexity='1' token_count='182' nesting_level='1' start_line='438' end_line='524'></method_info>
 			<added_lines>438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_with_carriage_return_without_index' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='26' nesting_level='1' start_line='402' end_line='404'></method_info>
 			<added_lines>402,403,404</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='do_test_prefix_connection_with_timestamp.time' parameters='self,seconds'>
 				<method_info nloc='4' complexity='1' token_count='27' nesting_level='3' start_line='429' end_line='432'></method_info>
 			<added_lines>429,430,431,432</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='do_test_prefix_connection' parameters='self,string,prefix,index,expected,timestamp'>
 				<method_info nloc='9' complexity='1' token_count='111' nesting_level='1' start_line='410' end_line='420'></method_info>
 			<added_lines>410,411,412,413,414,415,416,417,418,419,420</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='do_test_prefix_connection_with_timestamp' parameters='self,string_or_connection,prefix,index,expected'>
 				<method_info nloc='7' complexity='1' token_count='52' nesting_level='1' start_line='422' end_line='436'></method_info>
 			<added_lines>422,423,424,425,426,427,428,429,430,431,432,433,434,435,436</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_with_unicode' parameters='self'>
 				<method_info nloc='5' complexity='1' token_count='27' nesting_level='1' start_line='353' end_line='357'></method_info>
 			<added_lines>353,354,355,356,357</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_without_trailing_newline' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='28' nesting_level='1' start_line='380' end_line='385'></method_info>
 			<added_lines>380,381,382,383,384,385</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_with_carriage_return_without_prefix' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='26' nesting_level='1' start_line='406' end_line='408'></method_info>
 			<added_lines>406,407,408</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_without_index' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='26' nesting_level='1' start_line='387' end_line='389'></method_info>
 			<added_lines>387,388,389</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_with_timestamp_without_index' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='26' nesting_level='1' start_line='375' end_line='378'></method_info>
 			<added_lines>375,376,377,378</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_with_timestamp' parameters='self'>
 				<method_info nloc='8' complexity='1' token_count='28' nesting_level='1' start_line='366' end_line='373'></method_info>
 			<added_lines>366,367,368,369,370,371,372,373</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_does_stream.writer' parameters='write_connection'>
 				<method_info nloc='10' complexity='3' token_count='67' nesting_level='2' start_line='449' end_line='481'></method_info>
 			<added_lines>449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_does_stream.reader' parameters='read_connection'>
 				<method_info nloc='19' complexity='6' token_count='93' nesting_level='2' start_line='486' end_line='504'></method_info>
 			<added_lines>486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_does_stream.test_prefix_connection_does_stream.writer.write' parameters='stream,text'>
 				<method_info nloc='19' complexity='5' token_count='100' nesting_level='3' start_line='450' end_line='472'></method_info>
 			<added_lines>450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_without_prefix' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='26' nesting_level='1' start_line='391' end_line='393'></method_info>
 			<added_lines>391,392,393</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='28' nesting_level='1' start_line='346' end_line='351'></method_info>
 			<added_lines>346,347,348,349,350,351</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_with_multibyte_unicode' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='47' nesting_level='1' start_line='359' end_line='364'></method_info>
 			<added_lines>359,360,361,362,363,364</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_prefix_connection_with_carriage_return' parameters='self'>
 				<method_info nloc='6' complexity='1' token_count='28' nesting_level='1' start_line='395' end_line='400'></method_info>
 			<added_lines>395,396,397,398,399,400</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='do_test_prefix_connection_with_timestamp.__init__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='3' start_line='425' end_line='427'></method_info>
 			<added_lines>425,426,427</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>19,20,29,32,36,44,45,46,352,358,365,374,379,386,390,394,401,405,409,421,437,525</added_lines>
 			<deleted_lines>29,30,36,41,42,43,47,48</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
