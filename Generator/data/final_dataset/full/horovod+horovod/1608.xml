<bug_data>
<bug id='1608' author='una-dinosauria' open_date='2019-12-23T20:45:44Z' closed_time='2020-01-09T21:03:49Z'>
 	<summary>RuntimeError: For integral input tensors, argument alpha must not be a floating point number</summary>
 	<description>
 Environment:
 
 Framework: (TensorFlow, Keras, PyTorch, MXNet): Pytorch
 Framework version: 1.3.0
 Horovod version: 0.16.1
 MPI version: 3.1.2 (?)
 CUDA version: 10.2
 NCCL version: ? (probably not important)
 Python version: 2.7
 OS and version: Ubuntu 14.04.4 (probably not important)
 GCC version: 4.8.4 (probably not important)
 
 Bug report:
 Horovod does not like it when my module has integer parameters, even if they do not require a gradient. It says
 Traceback (most recent call last):
   File "/.../mwe.py", line 30, in &lt;module&gt;
     hvd.broadcast_optimizer_state(optimizer, root_rank=0)
   File "/.../python2.7/dist-packages/horovod-0.16.1-py2.7-linux-x86_64.egg/horovod/torch/__init__.py", line 261, in broadcast_optimizer_state
     super(optimizer.__class__, optimizer).step()
   File "/.../torch/optim/sgd.py", line 106, in step
     p.data.add_(-group['lr'], d_p)
 RuntimeError: For integral input tensors, argument alpha must not be a floating point number.
 MWE
 import torch
 import torch.nn as nn
 import torch.optim as optim
 
 import horovod.torch as hvd
 
 
 class A(nn.Module):
 
     def __init__(self, a, b):
         super(A, self).__init__()
         self.a = nn.Parameter(a.int(), requires_grad=False)   # change to a.float() and hvd is happy
         self.b = nn.Parameter(b)
 
     def forward(self, x):
         return torch.index_select(self.b, 0, self.a.long()) * x
 
 
 hvd.init()
 
 a = torch.Tensor([1, 3])
 b = torch.rand(4)
 
 model = A(a, b).cuda()
 
 optimizer = optim.SGD(model.parameters(), lr=0.01)
 optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())
 
 hvd.broadcast_parameters(model.state_dict(), root_rank=0)
 hvd.broadcast_optimizer_state(optimizer, root_rank=0)  # &lt;-- hvd is sad :(
 
 model.train()
 
 for i in range(1000):
 
     x = torch.Tensor(torch.rand(2)).cuda()
 
     optimizer.zero_grad()
 
     loss = torch.mean((model(x) - 0) ** 2)
 
     loss.backward()
     optimizer.step()
 
     if hvd.rank() == 0:
         print(i, loss.detach())
 	</description>
 	<comments>
 		<comment id='1' author='una-dinosauria' date='2019-12-23T23:19:51Z'>
 		Hey &lt;denchmark-link:https://github.com/una-dinosauria&gt;@una-dinosauria&lt;/denchmark-link&gt;
 , thanks for reporting this.  And thanks even more for the reproducible example.  I'll take a look at this today and get back to you as soon as I have a fix.
 		</comment>
 		<comment id='2' author='una-dinosauria' date='2019-12-24T00:35:51Z'>
 		Hey &lt;denchmark-link:https://github.com/una-dinosauria&gt;@una-dinosauria&lt;/denchmark-link&gt;
 , please take a look at &lt;denchmark-link:https://github.com/horovod/horovod/pull/1609&gt;#1609&lt;/denchmark-link&gt;
  if you get a chance, it should address your issue.  Going forward, it will also provide a more generic  function that can be used to broadcast anything, for example, if  doesn't work in the future.
 		</comment>
 		<comment id='3' author='una-dinosauria' date='2019-12-24T05:40:40Z'>
 		Left a comment in the PR. Looks awesome -- thanks.
 		</comment>
 	</comments>
 </bug>
<commit id='b505f149b4cb1c1e8f61e8d3f19aeba15383be71' author='Travis Addair' date='2020-01-09 13:03:48-08:00'>
 	<dmm_unit complexity='0.9791666666666666' interfacing='0.5416666666666666' size='0.6041666666666666'></dmm_unit>
 	<modification change_type='MODIFY' old_name='horovod\torch\__init__.py' new_name='horovod\torch\__init__.py'>
 		<file_info nloc='385' complexity='96' token_count='3007'></file_info>
 		<method name='broadcast_optimizer_state' parameters='optimizer,root_rank'>
 				<method_info nloc='46' complexity='17' token_count='368' nesting_level='0' start_line='478' end_line='595'></method_info>
 			<added_lines>500,501,592</added_lines>
 			<deleted_lines>494,585</deleted_lines>
 		</method>
 		<method name='broadcast_object' parameters='obj,root_rank,name'>
 				<method_info nloc='18' complexity='4' token_count='168' nesting_level='0' start_line='598' end_line='638'></method_info>
 			<added_lines>598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>22,23,26,27,218,389,596,597</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\test_torch.py' new_name='test\test_torch.py'>
 		<file_info nloc='1236' complexity='288' token_count='10441'></file_info>
 		<method name='test_broadcast_state_no_grad.forward' parameters='self,x'>
 				<method_info nloc='2' complexity='1' token_count='28' nesting_level='3' start_line='1117' end_line='1118'></method_info>
 			<added_lines>1117,1118</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_broadcast_state_no_grad' parameters='self'>
 				<method_info nloc='14' complexity='1' token_count='165' nesting_level='1' start_line='1110' end_line='1134'></method_info>
 			<added_lines>1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_broadcast_state_no_grad.__init__' parameters='self,a,b'>
 				<method_info nloc='4' complexity='1' token_count='47' nesting_level='3' start_line='1112' end_line='1115'></method_info>
 			<added_lines>1112,1113,1114,1115</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_broadcast_object' parameters='self'>
 				<method_info nloc='9' complexity='2' token_count='59' nesting_level='1' start_line='1136' end_line='1146'></method_info>
 			<added_lines>1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>22,27,32,33,34,35,36,1108,1109,1135,1147</added_lines>
 			<deleted_lines>25,28,29</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
