<bug_data>
<bug id='11091' author='anishathalye' open_date='2017-06-27T20:57:44Z' closed_time='2017-07-17T16:04:54Z'>
 	<summary>tf.nn.elu: incorrect second derivative</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.2.0
 Bazel version (if compiling from source): N/A
 CUDA/cuDNN version: CUDA 8.0 / cuDNN 6.0
 GPU model and memory: GTX 1080 Ti 11GB
 Exact command to reproduce: see below
 
 tf.nn.elu gives incorrect second derivatives:
 Consider the graph y = 2 * elu(-x).
 x = tf.placeholder(tf.float32, ())
 y = 2 * tf.nn.elu(-x)
 We'll be evaluating at x=1:
 x_ = 1
 We can evaluate first derivatives with automatic differentiation:
 dy, = tf.gradients(y, x)
 dy.eval({x: x_})
 =&gt; -0.7357589
 This lines up with the analytic answer: y' = -2e^(-x)
 However, for the second derivative:
 ddy, = tf.gradients(dy, x)
 ddy.eval({x: x_})
 =&gt; 0.36787945
 Whoops, this doesn't look right! Analytically, the derivative is y'' = 2e^(-x). Evaluated at x=1, this is 0.7357588!
 &lt;denchmark-h:h3&gt;Workaround&lt;/denchmark-h&gt;
 
 Just in case anyone else needs to work around this until it's fixed:
 def elu(x):
     return tf.where(x &gt;= 0.0, x, tf.exp(x) - 1)
 Looks like second derivatives work with that.
 	</description>
 	<comments>
 		<comment id='1' author='anishathalye' date='2017-06-27T20:59:19Z'>
 		Possibly related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7403&gt;#7403&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='anishathalye' date='2017-06-29T18:31:13Z'>
 		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
  do you know why this might be?
 &lt;denchmark-link:https://github.com/anishathalye&gt;@anishathalye&lt;/denchmark-link&gt;
  is it possible to make a simpler repro script that you can share?
 		</comment>
 		<comment id='3' author='anishathalye' date='2017-06-29T18:37:31Z'>
 		Can you use the tensorflow gradient checker to test the gradient of your particular graph? See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradient_checker.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradient_checker.py&lt;/denchmark-link&gt;
  and usages of it in the tensorflow tests.
 		</comment>
 		<comment id='4' author='anishathalye' date='2017-07-05T22:15:30Z'>
 		Okay, so I tried the following:
 I have a network y(x), where y contains no tf.gradient ops, and I checked:
 
 
 tf.check_numerics(tf.gradients(y, x)[0]) -- result is ok
 
 
 tf.test.compute_gradient_error(x, ..., y, ...) -- result is ~ 983 (is that okay? in any case, for training, first derivatives seem to work)
 
 
 tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0]) -- result is ok
 
 
 tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...) - result is ~ 1522423936
 
 
 Finite differences probably isn't producing great results because y is a fairly big graph. But still, having a maximum error of 1e9 seems kind of large.
 What do you suggest looking into next?
 		</comment>
 		<comment id='5' author='anishathalye' date='2017-07-05T22:30:59Z'>
 		This is really hard to debug without having access to the full graph.
 
 What I'd do if I did have access  to the full graph would be bisect it;
 removing chunks of graph at a time until the error in second derivative
 goes down to see if there's some part of the code which is less numerically
 stable than it should be.
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Wed, Jul 5, 2017 at 3:17 PM, Anish Athalye ***@***.***&gt; wrote:
  Okay, so I tried the following:
 
  I have a network y(x), where y contains no tf.gradient ops, and I checked:
 
     1.
 
     tf.check_numerics(tf.gradients(y, x)[0]) -- result is ok
     2.
 
     tf.test.compute_gradient_error(x, ..., y, ...) -- result is ~ 983 (is
     that okay? in any case, for training, first derivatives seem to work)
     3.
 
     tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0]) -- result
     is ok
     4.
 
     tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...) -
     result is ~ 1522423936
 
  Finite differences probably isn't producing great results because y is a
  fairly big graph. But still, having a maximum error of 1e9 seems kind of
  large.
 
  What do you suggest looking into next?
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  &lt;#11091 (comment)&gt;,
  or mute the thread
  &lt;https://github.com/notifications/unsubscribe-auth/AAATxdLiF38xxI13fIJsHsvl7rooT44Cks5sLAuTgaJpZM4OHLlJ&gt;
  .
 
 
 -- 
  - Alex
 
 		</comment>
 		<comment id='6' author='anishathalye' date='2017-07-06T23:42:52Z'>
 		Ok, I have some more evidence indicating that there is in fact a bug. Also, I can share the full graph with you.
 I don't want to post it publicly, so I've sent you an email with this additional information.
 		</comment>
 		<comment id='7' author='anishathalye' date='2017-07-07T21:57:16Z'>
 		I found the bug: I updated the original post.
 		</comment>
 		<comment id='8' author='anishathalye' date='2017-07-17T16:05:06Z'>
 		The commit from last week should have fixed this.
 		</comment>
 	</comments>
 </bug>
<commit id='e121535a7d04cfc7c7dbb09d8694c01eb29da26f' author='Alexandre Passos' date='2017-07-10 12:52:34-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\kernel_tests\relu_op_test.py' new_name='tensorflow\python\kernel_tests\relu_op_test.py'>
 		<file_info nloc='291' complexity='37' token_count='3700'></file_info>
 		<method name='testGradGrad' parameters='self'>
 				<method_info nloc='9' complexity='2' token_count='107' nesting_level='1' start_line='276' end_line='285'></method_info>
 			<added_lines>276,277,278,279,280,281,282,283,284,285</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_elu_grad_grad' parameters='activation'>
 				<method_info nloc='4' complexity='2' token_count='19' nesting_level='0' start_line='36' end_line='39'></method_info>
 			<added_lines>36,37,38,39</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>25,40,41,286</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\ops\nn_grad.py' new_name='tensorflow\python\ops\nn_grad.py'>
 		<file_info nloc='405' complexity='49' token_count='3555'></file_info>
 		<method name='_EluGradGrad' parameters='op,grad'>
 				<method_info nloc='7' complexity='1' token_count='69' nesting_level='0' start_line='329' end_line='335'></method_info>
 			<added_lines>330,332,333,334,335</added_lines>
 			<deleted_lines>330,332,333,334</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
