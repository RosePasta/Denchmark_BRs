<bug_data>
<bug id='31952' author='David-Mao' open_date='2019-08-25T09:51:25Z' closed_time='2019-12-02T23:30:48Z'>
 	<summary>[TF 2.0] tf.gather doesn't work alongside @tf.function</summary>
 	<description>
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Darwin Kernel Version 18.6.0
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 N/A
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 2.0.0-dev20190730
 Python version:
 Python 3.6.8 :: Anaconda, Inc.
 Bazel version (if compiling from source):
 N/A
 GCC/Compiler version (if compiling from source):
 N/A
 CUDA/cuDNN version:
 N/A
 GPU model and memory:
 N/A
 
 Describe the current behavior
 It seems that when tf.gather() is called after a tf.function, the gradient cannot be calculated. The example code blow shows the bug. The code itself raises the following error message:
 
 AssertionError: Expected all args to be Tensors or Variables; but got CompositeTensor
 
 The code will work if we remove the tf.function decorator, or  put the tf.gather line inside the tf.funtion graph.
 Code to reproduce the issue
 &lt;denchmark-code&gt;import numpy as np
 import tensorflow as tf
 
 x = tf.cast(np.random.randn(100, 100), tf.float32)
 z = tf.cast(np.random.randn(1, 100), tf.float32)
 
 layer = tf.keras.layers.Dense(100)
 
 @tf.function  # &lt;- removing this and the code works fine
 def fun(x, layer):
     y = layer(x)
     return y
 
 with tf.GradientTape() as tape:
     y = fun(x, layer)
     y = tf.gather(y, [0])  # if we put this line inside the function it works fine
     loss = tf.norm(y - z)
 
 grads = tape.gradient(loss, layer.trainable_variables)
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='David-Mao' date='2019-08-26T05:58:13Z'>
 		I have tried on Colab with TF version 2.0.0-dev20190730, recent nightly version 2.0.0-dev20190825 and was able to reproduce the issue.Please, find the &lt;denchmark-link:https://colab.research.google.com/drive/1P4tejxmGIxKmXM4TFoesuaXTZUEsPpdx&gt;gist&lt;/denchmark-link&gt;
  here.Thanks!
 		</comment>
 		<comment id='2' author='David-Mao' date='2019-11-27T13:11:30Z'>
 		Hi,
 We encountered the same bug, which currently prevents our migration from TF1 to TF2.
 As David-mao said, it works perfectly well in Eager mode.
 The problem arises only when calling tf.gather on tensors returned from a tf.function, and then calculating its gradients.
 		</comment>
 		<comment id='3' author='David-Mao' date='2019-11-28T15:26:18Z'>
 		&lt;denchmark-link:https://github.com/diNatale&gt;@diNatale&lt;/denchmark-link&gt;
  I found a very ugly workaround for this. You wrap  into a graph function:
 &lt;denchmark-code&gt;
 @tf.function
 def gather(x, ind):
     return tf.gather(x + 0, ind)
 &lt;/denchmark-code&gt;
 
 and use this gather instead of tf.gather in your code.  I know it's absurd (the most absurd part is to have + 0 inside it, which is necessary for reasons unclear to me), but it works in my cases.
 		</comment>
 		<comment id='4' author='David-Mao' date='2019-11-28T15:43:02Z'>
 		Wow!
 "+ 0" - of course, how didn't we guess that :)
 It does work for me if I use both the wrapper and the +0
 Is there any explanation for this behaviour?
 thanks
 		</comment>
 		<comment id='5' author='David-Mao' date='2019-12-02T23:30:49Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='d5ee347de231b55f8ef7c11402db1673ff111d53' author='Alexandre Passos' date='2019-12-02 15:16:16-08:00'>
 	<dmm_unit complexity='0.8461538461538461' interfacing='1.0' size='0.8461538461538461'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\eager\backprop_test.py' new_name='tensorflow\python\eager\backprop_test.py'>
 		<file_info nloc='1410' complexity='246' token_count='13260'></file_info>
 		<method name='testFunctionIndexedSlicesGradient.f' parameters='x'>
 				<method_info nloc='2' complexity='1' token_count='9' nesting_level='2' start_line='313' end_line='314'></method_info>
 			<added_lines>313,314</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testFunctionIndexedSlicesGradient' parameters='self'>
 				<method_info nloc='9' complexity='1' token_count='75' nesting_level='1' start_line='310' end_line='321'></method_info>
 			<added_lines>310,311,312,313,314,315,316,317,318,319,320,321</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>322</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\eager\function.py' new_name='tensorflow\python\eager\function.py'>
 		<file_info nloc='1760' complexity='374' token_count='11281'></file_info>
 		<method name='_wrap_backward_function._backward_function_wrapper' parameters='args'>
 				<method_info nloc='24' complexity='8' token_count='126' nesting_level='2' start_line='1219' end_line='1251'></method_info>
 			<added_lines>1227,1228,1229,1230,1231,1232</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_wrap_backward_function' parameters='self,forward_graph,backward,outputs'>
 				<method_info nloc='39' complexity='15' token_count='255' nesting_level='1' start_line='1177' end_line='1253'></method_info>
 			<added_lines>1227,1228,1229,1230,1231,1232</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
