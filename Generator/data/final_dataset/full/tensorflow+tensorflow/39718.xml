<bug_data>
<bug id='39718' author='wwwind' open_date='2020-05-20T14:15:44Z' closed_time='2020-06-05T19:20:39Z'>
 	<summary>TF Lite nightly: Model with Fully Connected layer can't be converted, fully quantization, int8</summary>
 	<description>
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
 TensorFlow installed from (source or binary): tf-nightly
 TensorFlow version (or github SHA if from source): tf-nightly
 
 Command used to run the converter or code if youâ€™re using the Python API
 If possible, please share a link to Colab/Jupyter/any notebook.
 &lt;denchmark-link:https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing&gt;https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;import numpy as np
 import tensorflow as tf
 
 mnist = tf.keras.datasets.mnist
 train_data, test_data = mnist.load_data()
 
 pre_process = lambda x: x / 255.0
 num_calib = 1000
 calib_data = pre_process(
             train_data[0][0 : num_calib].astype(np.float32)
         )
 
 model = tf.keras.Sequential(
             [
                 tf.keras.layers.InputLayer(input_shape=(28, 28)),
                 tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
                 tf.keras.layers.Conv2D(
                     filters=12, kernel_size=(3, 3), activation=tf.nn.relu
                 ),
                 tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                 tf.keras.layers.Flatten(),
                 tf.keras.layers.Dense(10, activation=tf.nn.softmax),
             ]
         )
 model.summary()
 
 train_images = pre_process(train_data[0])
 train_labels = train_data[1]
 test_images = pre_process(test_data[0])
 test_labels = test_data[1]
 # Train the digit classification model
 model.compile(
   optimizer="adam",
   loss="sparse_categorical_crossentropy",
   metrics=["accuracy"],
 )
 model.fit(
   train_images,
   train_labels,
   epochs=1,
   validation_data=(test_images, test_labels),
 )
 
 def _get_calib_data_func():
   def representative_data_gen():
     for input_value in calib_data:
       input_value = np.expand_dims(input_value, axis=0).astype(np.float32)
       yield [input_value]
 
   return representative_data_gen
 
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 converter.representative_dataset = _get_calib_data_func()
 
 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 tflite_model_INT8 = converter.convert()
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;**RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor sequential_2/reshape_2/Shape
 Empty min/max for tensor sequential_2/reshape_2/Shape**
 &lt;/denchmark-code&gt;
 
 Also, please include a link to the saved model or GraphDef
 &lt;denchmark-code&gt;https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing
 &lt;/denchmark-code&gt;
 
 Failure details
 If the conversion is successful, but the generated model is wrong,
 state what is wrong:
 
 Producing wrong results and/or decrease in accuracy
 Producing correct results, but the model is slower than expected (model generated from old converter)
 
 RNN conversion support
 If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.
 Any other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	</description>
 	<comments>
 		<comment id='1' author='wwwind' date='2020-05-20T15:20:46Z'>
 		&lt;denchmark-link:https://github.com/wwwind&gt;@wwwind&lt;/denchmark-link&gt;
 ,
 I was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/4a4ea0178b34597d6035794ba3eb3e36/39718-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
 . However, the code works with the stable version &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/54d9272a3c32841d1532a994cc5820bb/39718-2-0.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
 . Please check the linked gist.
 Could you please try running the code with TF v2.2 and let us know if it works. Thanks!
 		</comment>
 		<comment id='2' author='wwwind' date='2020-05-20T15:35:07Z'>
 		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
  Yes, I confirm - it works in tensorflow 2.2.0
 		</comment>
 		<comment id='3' author='wwwind' date='2020-05-20T17:41:14Z'>
 		&lt;denchmark-link:https://github.com/wwwind&gt;@wwwind&lt;/denchmark-link&gt;
 ,
 Thank you for the update. Please feel free to close the issue if resolved. Thanks!
 		</comment>
 		<comment id='4' author='wwwind' date='2020-05-25T15:52:25Z'>
 		I get the same error when dealing with tf-nightly. However, I can't go back to 2.2 because a new error suddenly shows up: Tensor 'input_1' has invalid shape '[None, None, None, 3]'. I've seen in other thread it's recommended to use tf-nightly to fix the last error.
 So I can see here kind of deadlock between versions.
 		</comment>
 		<comment id='5' author='wwwind' date='2020-05-27T07:35:05Z'>
 		
 @amahendrakar Yes, I confirm - it works in tensorflow 2.2.0
 
 Closing this issue as resolved.
 		</comment>
 		<comment id='6' author='wwwind' date='2020-05-27T07:36:13Z'>
 		&lt;denchmark-link:https://github.com/sramirez&gt;@sramirez&lt;/denchmark-link&gt;
 ,
 Could you please submit a new issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;this link&lt;/denchmark-link&gt;
  and fill in the template along with the complete code, so that we can track the issue there. Thanks!
 		</comment>
 		<comment id='7' author='wwwind' date='2020-05-27T09:27:07Z'>
 		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
  Could this bug be fixed in tf-nightly ?
 I am working with tf-nightly and I really appreciate if it will be fixed in the current code.
 		</comment>
 		<comment id='8' author='wwwind' date='2020-06-03T16:38:39Z'>
 		Thanks, I will take a look into this.
 		</comment>
 		<comment id='9' author='wwwind' date='2020-06-03T18:39:43Z'>
 		&lt;denchmark-link:https://github.com/wwwind&gt;@wwwind&lt;/denchmark-link&gt;
  I have updated your code and &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/05d9d1bcfe32caead45728ac250e8dbf/39718-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
  is the working version of your code with . Thanks!
 		</comment>
 		<comment id='10' author='wwwind' date='2020-06-03T18:41:23Z'>
 		At a high level there are two ways to fix this issue (one of which &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
  pointed out).
 
 Try setting it in keras and keeping conversion the same way:
 
 &lt;denchmark-code&gt;input = tf.keras.layers.Input(shape=(225, 225, 3), batch_size=1)
 &lt;/denchmark-code&gt;
 
 
 Use set_shape on the model input and fix the batch size to 1 (example here: https://www.tensorflow.org/lite/convert/python_api#examples_).
 
 		</comment>
 		<comment id='11' author='wwwind' date='2020-06-05T19:20:41Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39718&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39718&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='268a0ea1502532e0e127e71d0ad42cc4e8ad81c6' author='Suharsh Sivakumar' date='2020-06-05 12:13:43-07:00'>
 	<dmm_unit complexity='0.16666666666666666' interfacing='0.7692307692307693' size='0.14102564102564102'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\lite\tools\optimize\quantize_model.cc' new_name='tensorflow\lite\tools\optimize\quantize_model.cc'>
 		<file_info nloc='1083' complexity='202' token_count='7881'></file_info>
 		<method name='tflite::optimize::EnsureBiasScaleCompatibility' parameters='model,operator_names,real_value_op_set,error_reporter'>
 				<method_info nloc='122' complexity='20' token_count='766' nesting_level='3' start_line='1110' end_line='1250'></method_info>
 			<added_lines>1112,1119,1120,1121,1122,1123,1124</added_lines>
 			<deleted_lines>1174,1175,1176,1177,1181,1183,1184</deleted_lines>
 		</method>
 		<method name='tflite::optimize::QuantizeWeightsInputOutput' parameters='model,allow_float,operator_names,real_value_op_set,error_reporter'>
 				<method_info nloc='38' complexity='8' token_count='293' nesting_level='3' start_line='883' end_line='925'></method_info>
 			<added_lines>886,895,896,897,898,899,900</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='tflite::optimize::FillQuantizationParams' parameters='model,operator_names,real_value_op_set,error_reporter'>
 				<method_info nloc='92' complexity='21' token_count='629' nesting_level='3' start_line='1001' end_line='1107'></method_info>
 			<added_lines>1003,1010,1011,1012,1013,1014,1015</added_lines>
 			<deleted_lines>1036,1037,1038</deleted_lines>
 		</method>
 		<method name='tflite::optimize::ApplyConstraints' parameters='model,operator_names,error_reporter'>
 				<method_info nloc='60' complexity='11' token_count='462' nesting_level='3' start_line='294' end_line='364'></method_info>
 			<added_lines>364</added_lines>
 			<deleted_lines>294,295,296,303,304,305,306</deleted_lines>
 		</method>
 		<method name='tflite::optimize::QuantizeOpInput' parameters='model,subgraph_idx,op_idx,property,input,error_reporter'>
 				<method_info nloc='162' complexity='25' token_count='1305' nesting_level='3' start_line='481' end_line='661'></method_info>
 			<added_lines>637,638,639,640,641</added_lines>
 			<deleted_lines>565,566,567,568,569</deleted_lines>
 		</method>
 		<method name='tflite::optimize::PopulateRealValueOpSet' parameters='model,operator_names'>
 				<method_info nloc='49' complexity='12' token_count='355' nesting_level='3' start_line='80' end_line='132'></method_info>
 			<added_lines>80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='tflite::optimize::QuantizeBiases' parameters='model,operator_names,error_reporter'>
 				<method_info nloc='50' complexity='10' token_count='382' nesting_level='3' start_line='852' end_line='903'></method_info>
 			<added_lines>886,895,896,897,898,899,900</added_lines>
 			<deleted_lines>862,863,864,865</deleted_lines>
 		</method>
 		<method name='tflite::optimize::FillQuantizationParams' parameters='model,operator_names,error_reporter'>
 				<method_info nloc='88' complexity='20' token_count='599' nesting_level='3' start_line='923' end_line='1025'></method_info>
 			<added_lines>930,939,940,941,942,943,1003,1010,1011,1012,1013,1014,1015</added_lines>
 			<deleted_lines>931,932,933</deleted_lines>
 		</method>
 		<method name='tflite::optimize::EnsureBiasScaleCompatibility' parameters='model,operator_names,error_reporter'>
 				<method_info nloc='118' complexity='19' token_count='736' nesting_level='3' start_line='1028' end_line='1164'></method_info>
 			<added_lines>1112,1119,1120,1121,1122,1123,1124</added_lines>
 			<deleted_lines>1036,1037,1038</deleted_lines>
 		</method>
 		<method name='tflite::optimize::QuantizeBiases' parameters='model,operator_names,real_value_op_set,error_reporter'>
 				<method_info nloc='52' complexity='11' token_count='406' nesting_level='3' start_line='928' end_line='981'></method_info>
 			<added_lines>930,939,940,941,942,943</added_lines>
 			<deleted_lines>931,932,933</deleted_lines>
 		</method>
 		<method name='tflite::optimize::QuantizeModel' parameters='builder,model,input_type,output_type,allow_float,operator_names,error_reporter'>
 				<method_info nloc='26' complexity='1' token_count='193' nesting_level='2' start_line='1255' end_line='1282'></method_info>
 			<added_lines>1260,1261,1262,1263,1264,1268,1269,1270,1272</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='tflite::optimize::ApplyConstraints' parameters='model,operator_names,real_value_op_set,error_reporter'>
 				<method_info nloc='62' complexity='12' token_count='486' nesting_level='3' start_line='364' end_line='436'></method_info>
 			<added_lines>364,365,366,367,374,375,376,377,378</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='tflite::optimize::IsRealValueOp' parameters='real_value_op_set,operator_name'>
 				<method_info nloc='4' complexity='1' token_count='33' nesting_level='3' start_line='73' end_line='76'></method_info>
 			<added_lines>73,74,75,76</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='tflite::optimize::GetOperatorProperty' parameters='operator_names,model,subgraph_index,op_idx,operator_name'>
 				<method_info nloc='17' complexity='4' token_count='131' nesting_level='3' start_line='53' end_line='71'></method_info>
 			<added_lines>58,59</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='tflite::optimize::QuantizeWeightsInputOutput' parameters='model,allow_float,operator_names,error_reporter'>
 				<method_info nloc='34' complexity='7' token_count='263' nesting_level='3' start_line='811' end_line='849'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>822,823,824</deleted_lines>
 		</method>
 		<method name='tflite::optimize::IsFloatTensor' parameters='subgraph,tensor_idx'>
 				<method_info nloc='7' complexity='2' token_count='43' nesting_level='3' start_line='41' end_line='48'></method_info>
 			<added_lines>41,42,43,44,45,46,47,48</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>49,77,78,79,133</added_lines>
 			<deleted_lines>49,50</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
