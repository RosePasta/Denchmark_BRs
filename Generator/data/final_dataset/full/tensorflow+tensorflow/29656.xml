<bug_data>
<bug id='29656' author='Sangwon91' open_date='2019-06-11T17:45:05Z' closed_time='2019-06-13T21:06:11Z'>
 	<summary>Bug on `gather_nd` with gradient.</summary>
 	<description>
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): pip
 TensorFlow version (use command below): tf2-gpu-beta
 Python version: 3.6.8
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 You can collect some of this information using our environment capture
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 A simple test code
 v = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)
 
 with tf.GradientTape() as tape:
     l = tf.gather_nd(v, [[1, 1]])
     l = tf.reduce_sum(l)
     
 grads = tape.gradient(l, v)
 print(grads)
 gives following error message
 &lt;denchmark-code&gt;---------------------------------------------------------------------------
 LookupError                               Traceback (most recent call last)
 &lt;ipython-input-12-28efd3aa3042&gt; in &lt;module&gt;
       5     l = tf.reduce_sum(l)
       6 
 ----&gt; 7 grads = tape.gradient(l, v)
       8 print(grads)
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
    1000         output_gradients=output_gradients,
    1001         sources_raw=flat_sources_raw,
 -&gt; 1002         unconnected_gradients=unconnected_gradients)
    1003 
    1004     if not self._persistent:
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
      74       output_gradients,
      75       sources_raw,
 ---&gt; 76       compat.as_str(unconnected_gradients.value))
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)
     131   """
     132   mock_op = _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)
 --&gt; 133   grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access
     134   if grad_fn is None:
     135     return [None] * num_inputs
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/registry.py in lookup(self, name)
      95     else:
      96       raise LookupError(
 ---&gt; 97           "%s registry has no entry for: %s" % (self._name, name))
 
 LookupError: gradient registry has no entry for: ResourceGatherNd
 &lt;/denchmark-code&gt;
 
 Describe the expected behavior
 the grads should be [[0, 0], [0, 1]] but error occurs.
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	</description>
 	<comments>
 		<comment id='1' author='Sangwon91' date='2019-06-11T17:50:07Z'>
 		I just found very weird behavior.
 If I replace v to v+0 like below,
 v = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)
 
 with tf.GradientTape() as tape:
     l = tf.gather_nd(v+0, [[1, 1]])
     l = tf.reduce_sum(l)
     
 grads = tape.gradient(l, v)
 print(grads)
 it gives expected result...
 &lt;denchmark-code&gt;tf.Tensor(
 [[0. 0.]
  [0. 1.]], shape=(2, 2), dtype=float32)
 &lt;/denchmark-code&gt;
 
 It seems the direct access to variables with gather_nd ruines something unexpected...
 		</comment>
 		<comment id='2' author='Sangwon91' date='2019-06-12T08:35:40Z'>
 		&lt;denchmark-link:https://github.com/Sangwon91&gt;@Sangwon91&lt;/denchmark-link&gt;
  I could able to reproduce the reported issue with Tensorflow 2.0.0.beta0. Thanks!
 		</comment>
 		<comment id='3' author='Sangwon91' date='2019-06-13T21:06:12Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29656&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29656&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='a7ef0da19be94d5f189c8a3af960f1da77e58b41' author='Alexandre Passos' date='2019-06-13 14:03:29-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\kernel_tests\BUILD' new_name='tensorflow\python\kernel_tests\BUILD'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>837</added_lines>
 			<deleted_lines>837</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\kernel_tests\resource_variable_ops_test.py' new_name='tensorflow\python\kernel_tests\resource_variable_ops_test.py'>
 		<file_info nloc='1140' complexity='113' token_count='11685'></file_info>
 		<method name='testGradientGatherNdIndexedSlices' parameters='self'>
 				<method_info nloc='9' complexity='1' token_count='118' nesting_level='1' start_line='280' end_line='290'></method_info>
 			<added_lines>280,281,282,283,284,285,286,287,288,289,290</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testGradientGatherNd' parameters='self'>
 				<method_info nloc='9' complexity='1' token_count='114' nesting_level='1' start_line='267' end_line='277'></method_info>
 			<added_lines>267,268,269,270,271,272,273,274,275,276,277</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>30,266,278,279,291</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\ops\array_grad.py' new_name='tensorflow\python\ops\array_grad.py'>
 		<file_info nloc='672' complexity='106' token_count='5772'></file_info>
 		<method name='_ResourceGatherNdGrad' parameters='op,grad'>
 				<method_info nloc='10' complexity='3' token_count='100' nesting_level='0' start_line='567' end_line='576'></method_info>
 			<added_lines>567,568,569,570,571,572,573,574,575,576</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>34,566,577,578</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
