<bug_data>
<bug id='15034' author='khanrc' open_date='2017-12-01T10:24:45Z' closed_time='2018-01-31T23:02:26Z'>
 	<summary>Optimize graph &amp; graph transform tools do not support NCHW</summary>
 	<description>
 I tried optimizing graph using both &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md&gt;Graph transform tool&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py&gt;Optimize graph for inference&lt;/denchmark-link&gt;
 . Both cases produced the same error because the fused batchnorm used not NCHW, but NHWC. I've got the error like this:
 &lt;denchmark-code&gt;InvalidArgumentError (see above for traceback): Must provide as many biases as the channel dimension of the input tensor: [256] vs. 19 in [1,256,19,19]
 	 [[Node: prefix/convblock/BatchNorm/FusedBatchNorm = BiasAdd[T=DT_FLOAT, data_format="NHWC", _device="/job:localhost/replica:0/task:0/device:GPU:0"](prefix/convblock/Conv2D, prefix/convblock/Conv2D_bn_offset)]
 &lt;/denchmark-code&gt;
 
 Although NCHW is faster than NHWC in GPU environment, why the tools do not support NCHW?
 	</description>
 	<comments>
 		<comment id='1' author='khanrc' date='2017-12-01T17:47:50Z'>
 		Could you provide a reproducible test case of what exactly you tried to do? Generally speaking, I think a lot of the tooling after training requires NHWC, as that was the only format when those were written. If you could provide a reproducible test case, we could work to improve it. &lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;
 , do you have any other comments?
 		</comment>
 		<comment id='2' author='khanrc' date='2017-12-04T03:04:48Z'>
 		I tried:
 &lt;denchmark-code&gt;python tensorflow/python/tools/optimize_for_inference.py \
 --input ./ckpt/frozen_model.pb \
 --output ./ckpt/optimized_model.pb \
 --frozen_graph true \
 --input_names Placeholder \
 --output_names policy_head/softmax,value_head/value/Tanh
 &lt;/denchmark-code&gt;
 
 and
 &lt;denchmark-code&gt;tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
 --in_graph='./ckpt/frozen_model.pb' \
 --out_graph='./ckpt/transformed_model.pb' \
 --inputs='Placeholder' \
 --outputs='policy_head/softmax,value_head/value/Tanh' \
 --transforms='
 fold_constants(ignore_errors=true)
 fold_batch_norms
 fold_old_batch_norms
 fuse_pad_and_conv
 fuse_resize_and_conv
 fuse_resize_pad_and_conv
 '
 &lt;/denchmark-code&gt;
 
 In both cases, the error occurred in fused batchnorm. The frozen model worked well, but the optimized model and transformed model emitted error.
 		</comment>
 		<comment id='3' author='khanrc' date='2017-12-22T07:34:33Z'>
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		</comment>
 		<comment id='4' author='khanrc' date='2018-01-05T19:08:07Z'>
 		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		</comment>
 		<comment id='5' author='khanrc' date='2018-01-24T13:17:05Z'>
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		</comment>
 	</comments>
 </bug>
<commit id='6afe900f543e0005ce69b3152330f1b7b16cb286' author='yegord' date='2018-01-31 15:02:25-08:00'>
 	<dmm_unit complexity='0.8333333333333334' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\tools\optimize_for_inference_lib.py' new_name='tensorflow\python\tools\optimize_for_inference_lib.py'>
 		<file_info nloc='324' complexity='55' token_count='2055'></file_info>
 		<method name='fold_batch_norms' parameters='input_graph_def'>
 				<method_info nloc='133' complexity='19' token_count='1015' nesting_level='0' start_line='201' end_line='365'></method_info>
 			<added_lines>352</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\tools\optimize_for_inference_test.py' new_name='tensorflow\python\tools\optimize_for_inference_test.py'>
 		<file_info nloc='264' complexity='18' token_count='2645'></file_info>
 		<method name='testFoldFusedBatchNorms' parameters='self'>
 				<method_info nloc='45' complexity='4' token_count='472' nesting_level='1' start_line='175' end_line='222'></method_info>
 			<added_lines>176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222</added_lines>
 			<deleted_lines>176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
