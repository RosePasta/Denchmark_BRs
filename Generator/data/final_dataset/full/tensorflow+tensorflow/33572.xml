<bug_data>
<bug id='33572' author='dreamibor' open_date='2019-10-21T13:18:38Z' closed_time='2019-11-22T19:50:09Z'>
 	<summary>[tflite] Support INT8 quantization for PACK with TFLITE_BUILTINS_INT8 OpsSet</summary>
 	<description>
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.14
 Python version: 3.6
 
 
 Similar to the UNPACK node issue in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31902&gt;#31902&lt;/denchmark-link&gt;
 , the new TFLiteConverter post-training quantisation flow, as described in &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations&gt;https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations&lt;/denchmark-link&gt;
 , does not support quantization of PACK/STACK operation when only integer operations are requested in the output model. When such conversion is attempted the following error is reported:
 
 RuntimeError: Quantization not yet supported for op: PACK
 
 Code to reproduce the issue
 For example, the script below:
 import tensorflow as tf
 import numpy as np
 
 def representative_dataset_gen():
 	input_1 = np.ones([1, 10],dtype=np.float32)
 	input_2 = np.ones([1, 10],dtype=np.float32)
 	for _ in range(10):
 		yield [input_1, input_2]
 
 # tf Graph Input
 foo = tf.compat.v1.placeholder("float32", [1, 10])
 bar = tf.compat.v1.placeholder("float32", [1, 10])
 out_stacked = tf.stack([foo, bar], axis=0)
 
 with tf.compat.v1.Session() as sess:
 	tf.io.write_graph(tf.compat.v1.get_default_graph(), '.','pack.pb', as_text=False)
 
 input_name = ["Placeholder", "Placeholder_1"]
 output_name = ["stack"]
 
 tflite_model_name = "int8_pack.tflite"
 converter = tf.lite.TFLiteConverter.from_frozen_graph("pack.pb", input_name, output_name)
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 converter.representative_dataset = representative_dataset_gen
 tflite_model = converter.convert()
 open(tflite_model_name, "wb").write(tflite_model)
 
 # Load TFLite model and allocate tensors.
 interpreter = tf.lite.Interpreter(tflite_model_name)
 interpreter.allocate_tensors()
 
 # Get input and output tensors.
 input_details = interpreter.get_input_details()
 output_details = interpreter.get_output_details()
 
 # Test model on random input data.
 input_shape = input_details[0]['shape']
 input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
 interpreter.set_tensor(input_details[0]['index'], input_data)
 interpreter.invoke()`
 produces errors as follows:
 &lt;denchmark-code&gt;2019-10-21 14:02:33.682706: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2019-10-21 14:02:33.708278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
 2019-10-21 14:02:33.708892: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x33f5a60 executing computations on platform Host. Devices:
 2019-10-21 14:02:33.708924: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
 2019-10-21 14:02:33.717107: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
 2019-10-21 14:02:33.717228: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
 INFO: Initialized TensorFlow Lite runtime.
 Traceback (most recent call last):
   File "pack_example.py", line 26, in &lt;module&gt;
     tflite_model = converter.convert()
   File "/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 908, in convert
     inference_output_type)
   File "/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 200, in _calibrate_quantize_model
     inference_output_type, allow_float)
   File "/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py", line 78, in calibrate_and_quantize
     np.dtype(output_type.as_numpy_dtype()).num, allow_float)
   File "/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py", line 115, in QuantizeModel
     return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
 RuntimeError: Quantization not yet supported for op: PACK
 &lt;/denchmark-code&gt;
 
 Both kTfLiteUInt8 and kTfLiteInt8 version of the PACK operator is already implemented in TFLite (see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/kernels/pack.cc&gt;pack.cc&lt;/denchmark-link&gt;
 ), so it should be straightforward to support PACK as well in the TFLite Converter.
 	</description>
 	<comments>
 		<comment id='1' author='dreamibor' date='2019-11-08T14:16:09Z'>
 		Any updates?
 		</comment>
 		<comment id='2' author='dreamibor' date='2019-11-20T18:26:48Z'>
 		Hi, I have a CL in progress to fix this.
 		</comment>
 		<comment id='3' author='dreamibor' date='2019-11-22T19:50:10Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33572&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33572&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='dreamibor' date='2020-07-31T09:04:38Z'>
 		Hey, I'm facing the same issue when converting  (see &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md&gt;model zoo&lt;/denchmark-link&gt;
 ) to quantized tflite.
 Simple conversion to  tflite works just fine.
 With TF 2 I can't even convert the model to tflite proper.
 Using tensorflow_gpu version 1.5.3 on google Colab, Ubuntu 18.04.
 Any help would be much appreciated üôè
 Conversion script looks like this:
 import tensorflow as tf
 import pathlib
 import os
 import cv2
 from PIL import Image
 import json
 import numpy as np
 
 frozen_model = './mobilenet_varroa_w_inputshape/frozen_inference_graph_00/tflite_graph.pb'
 # TF 1
 input_shapes = {'normalized_input_image_tensor':(1,896,896,3)}
 input_arrays = ['normalized_input_image_tensor']
 output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
 converter = tf.lite.TFLiteConverter.from_frozen_graph(frozen_model, input_arrays, output_arrays, input_shapes)
 
 converter.experimental_new_converter = False
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 
 def representative_dataset_gen():
     directory_images = "./rep_dataset/"
     for img in os.listdir(directory_images):
       tmp_img = Image.open(f"{directory_images}/{img}")
       tmp_img = np.array(tmp_img.getdata()).reshape((807, 807, 3)).astype(np.uint8)
       tmp_img = cv2.resize(tmp_img, (896, 896), interpolation=cv2.INTER_AREA)
       input_data = tmp_img.astype('float32')/255.0
       input_data = tf.expand_dims(input_data, 0)
       input_data = tf.convert_to_tensor(input_data, dtype=tf.float32)
       yield [input_data]
 
 converter.representative_dataset = representative_dataset_gen
 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 converter.allow_custom_ops = True
 converter.inference_input_type = tf.int8 
 converter.inference_output_type = tf.int8 
 tflite_quant_model = converter.convert() # crashes on this line
 open("mobilenet_test.tflite", "wb").write(tflite_quant_model)
 		</comment>
 	</comments>
 </bug>
<commit id='d8668d9e03b65c4a6d9ecb08e74b0b67798fbbab' author='Suharsh Sivakumar' date='2019-11-22 11:49:22-08:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\lite\tools\optimize\operator_property.cc' new_name='tensorflow\lite\tools\optimize\operator_property.cc'>
 		<file_info nloc='357' complexity='51' token_count='2354'></file_info>
 		<method name='tflite::optimize::operator_property::GetOperatorProperty' parameters='model,subgraph_index,op_index'>
 				<method_info nloc='336' complexity='50' token_count='2266' nesting_level='3' start_line='41' end_line='397'></method_info>
 			<added_lines>289,290,291,292,293,294</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
