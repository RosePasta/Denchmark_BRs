<bug_data>
<bug id='3624' author='sbrodeur' open_date='2016-08-03T15:32:28Z' closed_time='2016-10-25T18:04:29Z'>
 	<summary>Basic Element-wise Complex Number Calculations Not Available On GPU</summary>
 	<description>
 Basic element-wise addition, subtraction, multiplication or division for any Tensor of type tf.complex64 is not implemented on GPU.
 &lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;
 
 Operating System: Centos 7,  3.10.0-327.22.2.el7.x86_64
 Installed version of CUDA and cuDNN:  CUDA 7.5 and cuDNN 7.0-v4
 -rw-r--r--. 1 root root 189170 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudadevrt.a
 lrwxrwxrwx. 1 root root     16 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so -&gt; libcudart.so.7.5
 lrwxrwxrwx. 1 root root     19 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18
 -rwxr-xr-x. 1 root root 311596 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18
 -rw-r--r--. 1 root root 558020 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart_static.a
 Tensorflow installed from source:
 
 Commit hash 00700f0
 Bazel information:
 Build label: 0.3.0-2016-07-22 (@ca36b06)
 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
 Build time: Fri Jul 22 19:23:10 2016 (1469215390)
 Build timestamp: 1469215390
 Build timestamp as int: 1469215390
 
 &lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 
 Add, subtract, multiply or divide any Tensor of type tf.complex64. A code example is shown here for element-wise addition:
 
 &lt;denchmark-code&gt;import tensorflow as tf
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         c = a + b
 
         with tf.Session() as sess:
             c = sess.run(c)
 &lt;/denchmark-code&gt;
 
 The code returns the following output if run on GPU (works well on CPU):
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
 name: Tesla K40c
 major: 3 minor: 5 memoryClockRate (GHz) 0.745
 pciBusID 0000:02:00.0
 Total memory: 12.00GiB
 Free memory: 11.90GiB
 W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x5168890
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:
 name: GeForce GT 610
 major: 2 minor: 1 memoryClockRate (GHz) 1.62
 pciBusID 0000:01:00.0
 Total memory: 1023.19MiB
 Free memory: 396.98MiB
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:814] Ignoring gpu device (device: 1, name: GeForce GT 610, pci bus id: 0000:01:00.0) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.5.
 E tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: add = Add[T=DT_COMPLEX64, _device="/device:GPU:0"](Complex, Complex_1)]]
 Traceback (most recent call last):
 File "test_div_gpu_prob.py", line 12, in 
 c = sess.run(c)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 382, in run
 run_metadata_ptr)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 655, in _run
 feed_dict_string, options, run_metadata)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 723, in _do_run
 target_list, options, run_metadata)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 743, in _do_call
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: add = Add[T=DT_COMPLEX64, _device="/device:GPU:0"](Complex, Complex_1)]]
 Caused by op u'add', defined at:
 File "test_div_gpu_prob.py", line 9, in 
 c = a + b
 File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py", line 755, in binary_op_wrapper
 return func(x, y, name=name)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py", line 70, in add
 result = _op_def_lib.apply_op("Add", x=x, y=y, name=name)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 703, in apply_op
 op_def=op_def)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2310, in create_op
 original_op=self._default_original_op, op_def=op_def)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1232, in init
 self._traceback = _extract_stack()
 &lt;denchmark-h:h3&gt;What have you tried?&lt;/denchmark-h&gt;
 
 
 Implementation using builtin Tensorflow functions works, if the real and imaginary parts are separated. See the code below:
 
 &lt;denchmark-code&gt;import numpy as np
 import tensorflow as tf
 
 def complex_add(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr + yr, xi + yi)
 
 def complex_sub(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr - yr, xi - yi)
 
 def complex_mul(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)
 
 def complex_div(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     d = tf.square(yr) + tf.square(yi)
     return tf.complex((xr*yr+xi*yi)/d, (xi*yr-xr*yi)/d)
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
 
         with tf.Session() as sess:
 
             a_, b_, c = sess.run([a,b,complex_add(a,b)])
             assert np.allclose(c, a_ + b_)
 
             a_, b_, c = sess.run([a,b,complex_sub(a,b)])
             assert np.allclose(c, a_ - b_)
 
             a_, b_, c = sess.run([a,b,complex_mul(a,b)])
             assert np.allclose(c, a_ * b_)
 
             a_, b_, c = sess.run([a,b,complex_div(a,b)])
             assert np.allclose(c, a_ / b_)
 &lt;/denchmark-code&gt;
 
 It would be nice to have such functions transparent with the built-in CPU implementations.
 	</description>
 	<comments>
 		<comment id='1' author='sbrodeur' date='2016-08-03T18:17:08Z'>
 		Note: implementations using built-in Tensorflow functions as show above doesn't solve gradient issues caused by the handling of complex numbers:
 &lt;denchmark-code&gt;import tensorflow as tf
 
 def complex_mul(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         c = complex_mul(a, b)
 
         grad = tf.gradients([c], [a])
 
         with tf.Session() as sess:
             grad = sess.run(grad)
 &lt;/denchmark-code&gt;
 
 This code will fail with the following error:
 E tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: gradients/Shape = Shape&lt;denchmark-link:Complex_2&gt;T=DT_COMPLEX64, _device="/device:GPU:0"&lt;/denchmark-link&gt;
 ]]
 Traceback (most recent call last):
 File "test_div_gpu_grad_prob.py", line 19, in 
 grad = sess.run(grad)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 382, in run
 run_metadata_ptr)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 655, in _run
 feed_dict_string, options, run_metadata)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 723, in _do_run
 target_list, options, run_metadata)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 743, in _do_call
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: gradients/Shape = Shape&lt;denchmark-link:Complex_2&gt;T=DT_COMPLEX64, _device="/device:GPU:0"&lt;/denchmark-link&gt;
 ]]
 Caused by op u'gradients/Shape', defined at:
 File "test_div_gpu_grad_prob.py", line 16, in 
 grad = tf.gradients([c], [a])
 File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 367, in gradients
 grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 230, in _DefaultGradYs
 array_ops.shape(y),
 File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py", line 131, in shape
 return gen_array_ops.shape(input, name=name)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1922, in shape
 result = _op_def_lib.apply_op("Shape", input=input, name=name)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 703, in apply_op
 op_def=op_def)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2310, in create_op
 original_op=self._default_original_op, op_def=op_def)
 File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1232, in 
 self._traceback = _extract_stack()
 		</comment>
 		<comment id='2' author='sbrodeur' date='2016-08-03T22:53:42Z'>
 		It seems that support for complex64 types is piecemeal, by op-type and device-type.  Bringing in &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
  for a comment on the policy.
 		</comment>
 		<comment id='3' author='sbrodeur' date='2016-08-03T22:57:51Z'>
 		Yes, this is a good feature request bordering on a bug. Please check the Op registrations of the affected ops, and you'll probably find that the templates of many of them are not specialized for complex data types. It is a relatively simple thing to fix, and I'd love PRs that do it.
 		</comment>
 		<comment id='4' author='sbrodeur' date='2016-08-08T12:56:47Z'>
 		&lt;denchmark-link:https://github.com/sbrodeur&gt;@sbrodeur&lt;/denchmark-link&gt;
 : Are you currently working on this?
 If not, I could go ahead and attempt a fix.
 		</comment>
 		<comment id='5' author='sbrodeur' date='2016-08-08T14:02:21Z'>
 		&lt;denchmark-link:https://github.com/ibab&gt;@ibab&lt;/denchmark-link&gt;
 : I did not yet attempt a fix. I've looked a at little at Eigen:
 ""
 [https://eigen.tuxfamily.org/dox-devel/TopicCustomizingEigen.html]
 Thus, for the simple calculations here, should I expect Eigen to provide compatible functors, e.g. :
 &lt;denchmark-code&gt;template &lt;typename T&gt;
 struct add : base&lt;T, Eigen::internal::scalar_sum_op&lt;T&gt; &gt; {
   static const bool use_bcast_optimization = true;
 };
 &lt;/denchmark-code&gt;
 
 This code is in file cwise_ops.h
 Does this means the fix is similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/2263&gt;#2263&lt;/denchmark-link&gt;
 , i.e. just adding the complex64 type when we register the kernels?
 		</comment>
 		<comment id='6' author='sbrodeur' date='2016-08-08T14:14:47Z'>
 		Yes, you won't have to implement the operations themselves, you just need to enable them.
 For example, you can look at the supported types for the addition op here:
 
 
 
 tensorflow/tensorflow/core/kernels/cwise_op_add.cc
 
 
          Line 22
       in
       32bd3d0
 
 
 
 
 
 
  REGISTER4(BinaryOp, GPU, "Add", functor::add, float, Eigen::half, double, 
 
 
 
 
 
 You would need to add complex64 and complex128 to the macro (and change it into REGISTER6).
 You should make sure that the GPU tests are enabled for complex64 and complex128 for each op that has been extended, for example here: 
 
 
 tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py
 
 
          Line 362
       in
       32bd3d0
 
 
 
 
 
 
  def testComplex64Basic(self): 
 
 
 
 
 .
 		</comment>
 		<comment id='7' author='sbrodeur' date='2016-08-08T14:27:15Z'>
 		Thanks for the information &lt;denchmark-link:https://github.com/ibab&gt;@ibab&lt;/denchmark-link&gt;
 ! I will attempt a fix myself and send a PR soon!
 		</comment>
 		<comment id='8' author='sbrodeur' date='2016-08-12T16:43:15Z'>
 		So far, I can make it work with some operations (add, sub) by simply adding the complex data types when registering the kernels: e.g. 
 
 
 tensorflow/tensorflow/core/kernels/cwise_op_add.cc
 
 
          Line 22
       in
       32bd3d0
 
 
 
 
 
 
  REGISTER4(BinaryOp, GPU, "Add", functor::add, float, Eigen::half, double, 
 
 
 
 
 
 &lt;denchmark-code&gt;DEFINE_BINARY6(add, Eigen::half, float, double, int64, complex64, complex128);
 &lt;/denchmark-code&gt;
 
 Compilation errors however occur for multiplication (and division), as seen below.
 Searching the web, I found here that CUDA may not support std::complex because of STL incompatibilities:
 &lt;denchmark-link:https://forum.kde.org/viewtopic.php?f=74&amp;t=123919&gt;https://forum.kde.org/viewtopic.php?f=74&amp;t=123919&lt;/denchmark-link&gt;
 
 It seems to solve this problem, people have been using reimplementations of the std:complex type (e.g. from thrust, cuda_complex or cusp) so that it can be used in device code:
 &lt;denchmark-link:https://github.com/thrust/thrust/blob/2ef13096187b40a35a71451d09e49b14074b0859/thrust/complex.h&gt;https://github.com/thrust/thrust/blob/2ef13096187b40a35a71451d09e49b14074b0859/thrust/complex.h&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp&gt;https://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/cusplibrary/cusplibrary/blob/master/cusp/complex.h&gt;https://github.com/cusplibrary/cusplibrary/blob/master/cusp/complex.h&lt;/denchmark-link&gt;
 
 Would the Eigen library implementing something similar to what thrust uses solve the issue in Tensorflow?
 &lt;denchmark-h:h3&gt;Compilation output&lt;/denchmark-h&gt;
 
 INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_mul.cu.cc:
 nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
 In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,
 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,
 from :0:
 /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]
 &lt;denchmark-h:h1&gt;warning _FORTIFY_SOURCE requires compiling with optimization (-O)&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;^
 &lt;/denchmark-code&gt;
 
 In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,
 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,
 from :0:
 /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]
 &lt;denchmark-h:h1&gt;warning _FORTIFY_SOURCE requires compiling with optimization (-O)&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;^
 &lt;/denchmark-code&gt;
 
 In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,
 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,
 from :0:
 /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]
 &lt;denchmark-h:h1&gt;warning _FORTIFY_SOURCE requires compiling with optimization (-O)&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;^
 &lt;/denchmark-code&gt;
 
 nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 12 errors detected in the compilation of "/tmp/tmpxft_0000430f_00000000-12_cwise_op_gpu_mul.cu.compute_35.cpp2.i".
 		</comment>
 		<comment id='9' author='sbrodeur' date='2016-08-13T02:37:33Z'>
 		Strange, your errors seem to be caused by the fact that Eigen is trying to assign values from an int Tensor into a complex Tensor.
 I don't think that's supposed to happen usually.
 I've tried enabling complex mul and div just now, and it successfully compiled and ran when I restricted the ops to run on the GPU.
 I'm also using CUDA 7.5, not sure what else could be different between our setups.
 		</comment>
 		<comment id='10' author='sbrodeur' date='2016-08-13T15:40:17Z'>
 		Here is my configuration:
 GPU: Tesla K40c
 Operating System: CentOS Linux 7 (Core)
 Kernel: Linux 3.10.0-327.22.2.el7.x86_64
 Architecture: x86-64
 C Compiler: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)
 CUDA Compiler: Cuda compilation tools, release 7.5, V7.5.17
 I will try with a more recent gcc (e.g. 4.9.2) to see if the compilation problem disappears.
 		</comment>
 		<comment id='11' author='sbrodeur' date='2016-08-13T16:30:20Z'>
 		I'm also using  and.
 Not sure what could be causing this if these two are the same.
 I've uploaded my changes to &lt;denchmark-link:https://github.com/ibab/tensorflow/commit/8c3baae08bd449d25f80e9af8ae4830eb7ae2670&gt;ibab@8c3baae&lt;/denchmark-link&gt;
 , so you might want to compare these with yours.
 If that doesn't help we should try rebasing to the same TensorFlow commit.
 		</comment>
 		<comment id='12' author='sbrodeur' date='2016-08-13T18:30:58Z'>
 		Sadly, I obtain the same errors if I clone and compile the fork &lt;denchmark-link:https://github.com/ibab/tensorflow/commit/8c3baae&gt;ibab/tensorflow@8c3baae&lt;/denchmark-link&gt;
  without any modifications.
 &lt;denchmark-link:https://github.com/ibab&gt;@ibab&lt;/denchmark-link&gt;
  - What is your Linux distribution?
 		</comment>
 		<comment id='13' author='sbrodeur' date='2016-08-13T18:42:39Z'>
 		I'm running Scientific Linux 6, which should be virtually identical to Red Hat 6.
 I get my compiler toolchain from anaconda, though.
 I'll try to make sure it's not something weird on my end.
 		</comment>
 		<comment id='14' author='sbrodeur' date='2016-08-13T18:54:49Z'>
 		On my side, I'll try a build on my laptop which runs the latest Debian 8 (Jessie). I don't have a Nvidia GPU but I should nevertheless be able to compile with CUDA.
 		</comment>
 		<comment id='15' author='sbrodeur' date='2016-08-13T19:11:38Z'>
 		Okay, I've rebuilt tensorflow after a bazel clean --expunge to make extra sure that I'm using the right Eigen version, as I've changed it around a few times previously, but it still built successfully.
 Edit: Btw, do you also get compiler warnings about calling __host__ functions from device code as in the link you posted above?
 		</comment>
 		<comment id='16' author='sbrodeur' date='2016-08-14T14:50:00Z'>
 		I do not get compiler warnings about calling host functions from device code.
 I just tried to build on my laptop (Debian 8, up-to-date) with configuration;
 gcc (Debian 4.9.2-10) 4.9.2
 Cuda compilation tools, release 7.5, V7.5.17
 I obtained the same errors, so it does not seem related to gcc or distribution. I also tried to build with the latest eigen (3782cd1de9c4) on the Centos 7 machine, and that did not help either. I will try building with CUDA 8, after which I will be clueless about those compilation issues.
 Edit:  same errors with CUDA 8.
 		</comment>
 		<comment id='17' author='sbrodeur' date='2016-08-14T16:10:20Z'>
 		I've tried compiling with different compute capabilities, but it still compiled without errors.
 The fact that you reproduced it on two different systems makes me think that it's a problem with my setup, though.
 Unfortunately nvcc doesn't give us a lot of information in the error message (like which template instantiations we are dealing with) :(
 		</comment>
 		<comment id='18' author='sbrodeur' date='2016-08-23T22:17:49Z'>
 		&lt;denchmark-link:https://github.com/ibab&gt;@ibab&lt;/denchmark-link&gt;
  , &lt;denchmark-link:https://github.com/sbrodeur&gt;@sbrodeur&lt;/denchmark-link&gt;
  , thank you so much for working on this. I think it would really speed up one of my projects. Is there any new progress? Are you planning to include this on the next release of TensorFlow? What about basic math functions such as , ?
 Thanks again!
 		</comment>
 		<comment id='19' author='sbrodeur' date='2016-08-24T14:31:48Z'>
 		&lt;denchmark-link:https://github.com/iportillo&gt;@iportillo&lt;/denchmark-link&gt;
  - I will give it another try today. It would also significantly accelerate my experiments, since everything could run on the GPU. I'll try to see if it would be easy to use CUDABlas directly (rather than Eigen) for the basic math functions on complex numbers.
 tf.complex_abs is easy to implement on GPU right now:
 &lt;denchmark-code&gt;def complex_abs(x):
     return tf.sqrt(tf.square(tf.real(x)) + tf.square(tf.imag(x)))
 &lt;/denchmark-code&gt;
 
 By tf.exp(), do you mean converting from the Cartesian to the complex exponential form (angle and norm)? To calculate the angle, this means implementing the atan2 function (for complex x + iy):
 &lt;denchmark-code&gt;def atan2(y, x):
     angle = tf.select(tf.greater(x,0.0), tf.atan(y/x) + np.pi, tf.zeros_like(x))
     angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)
     angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)
     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)
     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)
     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)
     return angle
 
 def complex_arg(x):
     return atan2(tf.imag(x), tf.real(x))
 &lt;/denchmark-code&gt;
 
 It's not optimized but works well on GPU.
 		</comment>
 		<comment id='20' author='sbrodeur' date='2016-08-24T15:12:44Z'>
 		&lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
 : We're having some problems with implementing the product and div ops for  using the Eigen Tensor library.
 Do you see why we would get an error like the following when enabling them in TensorFlow?
 &lt;denchmark-code&gt;external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type "int" cannot be assigned to an entity of type "_ZNSt7complexIfE9_ComplexTE"
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;$ c++filt _ZNSt7complexIfE9_ComplexTE
 std::complex&lt;float&gt;::_ComplexT
 &lt;/denchmark-code&gt;
 
 Maybe we would need to switch to something like  as &lt;denchmark-link:https://github.com/sbrodeur&gt;@sbrodeur&lt;/denchmark-link&gt;
  suggested?
 		</comment>
 		<comment id='21' author='sbrodeur' date='2016-08-24T21:41:53Z'>
 		I made some progress! I can make multiplication and division ops work for complex numbers if I specialized the templates in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops.h#L432&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops.h#L432&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;template &lt;typename T&gt;
 struct mul : base&lt;T, Eigen::internal::scalar_product_op&lt;T&gt; &gt; {};
 
 template &lt;typename T&gt;
 struct multiply_complex {
   typedef std::complex&lt;T&gt; result_type;
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex&lt;T&gt; a,
                                                                std::complex&lt;T&gt; b) const {
     return std::complex&lt;T&gt;(a.real()*b.real() - a.imag()*b.imag(),
                            a.real()*b.imag() + a.imag()*b.real());
   }
 };
 
 template &lt;&gt;
 struct mul&lt;std::complex&lt;float&gt; &gt; : base&lt;std::complex&lt;float&gt;, multiply_complex&lt;float&gt; &gt; {};
 
 template &lt;&gt;
 struct mul&lt;std::complex&lt;double&gt; &gt; : base&lt;std::complex&lt;double&gt;, multiply_complex&lt;double&gt; &gt; {};
 
 &lt;/denchmark-code&gt;
 
 It seems more like a hack, but it doesn't involve changes in Eigen for now.
 Not sure what is wrong with nvcc using scalar_product_op in Eigen for complex numbers:
 &lt;denchmark-link:https://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/functors/BinaryFunctors.h#L76&gt;https://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/functors/BinaryFunctors.h#L76&lt;/denchmark-link&gt;
 
 However, it seems tightly related to using built-in * and / operators for std:complex types.
 For instance, this fails with the same errors as in the previous posts:
 &lt;denchmark-code&gt;template &lt;typename T&gt;
 struct mul : base&lt;T, Eigen::internal::scalar_product_op&lt;T&gt; &gt; {};
 
 template &lt;typename T&gt;
 struct multiply_complex {
   typedef std::complex&lt;T&gt; result_type;
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex&lt;T&gt; a,
                                                                std::complex&lt;T&gt; b) const {
     return a*b;
   }
 };
 
 template &lt;&gt;
 struct mul&lt;std::complex&lt;float&gt; &gt; : base&lt;std::complex&lt;float&gt;, multiply_complex&lt;float&gt; &gt; {};
 
 template &lt;&gt;
 struct mul&lt;std::complex&lt;double&gt; &gt; : base&lt;std::complex&lt;double&gt;, multiply_complex&lt;double&gt; &gt; {};
 
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='22' author='sbrodeur' date='2016-08-24T23:26:17Z'>
 		I can confirm that with the above trick, I can make work a lot of very useful functions for complex numbers on GPU (e.g. square, neg, div, mul, abs) This brings support for complex gradient computation on GPU:
 &lt;denchmark-code&gt;import tensorflow as tf
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         c = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
 
         d = c * tf.neg(tf.square(a + b))
 
         grad = tf.gradients([d], [a])
 
         with tf.Session() as sess:
             grad = sess.run(grad)
 &lt;/denchmark-code&gt;
 
 Should I make a PR or should we investigate further the handling of std::complex by nvcc?
 		</comment>
 		<comment id='23' author='sbrodeur' date='2016-08-29T15:23:06Z'>
 		I'm not a maintainer, but I think a PR would definitely be a good idea 👍
 Maybe you can split it into two PRs, one that requires the extra scalar_prod_op specialization, and one that doesn't?
 In the long term, it would probably be best to get the specialization into Eigen itself, or to find another fix (like switching to thrust::complex).
 		</comment>
 		<comment id='24' author='sbrodeur' date='2016-09-13T17:39:21Z'>
 		I've been privately writing GPU-based complex-valued ops for TF and decided to make my repository public. I think that more general support for computation of complex numbers on the GPU will be valuable to the community. However since my repository is in the early stages and isn't well tested, I think I'd like to develop it as a separate project and then port it as a TF pull request when it's more mature.   Feel free to make contributions and/or suggestions.
 &lt;denchmark-link:https://github.com/woodshop/complex_tf&gt;https://github.com/woodshop/complex_tf&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='25' author='sbrodeur' date='2016-09-14T20:16:29Z'>
 		In C++14,  std::complex methods are marked as constexpr. This will ensure that they can be used inside cuda kernels even though they're not marked as __device__ functions provided that we compile with the --relaxed-constexpr flag (which TensorFlow has been doing for some time now).
 Unfortunately nvcc doesn't yet support c++14, but we can ask nvidia to start adding partial support for it starting with complex numbers.
 		</comment>
 		<comment id='26' author='sbrodeur' date='2016-09-15T19:26:45Z'>
 		&lt;denchmark-link:https://github.com/iportillo&gt;@iportillo&lt;/denchmark-link&gt;
  ComplexAbs (and a few others) added here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/f21642013092b53186491064335053a9e02ce010&gt;f216420&lt;/denchmark-link&gt;
 
 and the corresponding Eigen change:
 &lt;denchmark-link:https://bitbucket.org/eigen/eigen/commits/6d4cd6e5cdd9c750b10cc4c6a374e4c513b267ed&gt;https://bitbucket.org/eigen/eigen/commits/6d4cd6e5cdd9c750b10cc4c6a374e4c513b267ed&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='27' author='sbrodeur' date='2016-09-28T00:58:56Z'>
 		After adding a workaround to Eigen:
 &lt;denchmark-link:https://bitbucket.org/eigen/eigen/commits/27f6140fa81c9fe83167d87e7aeb23031b42f344&gt;https://bitbucket.org/eigen/eigen/commits/27f6140fa81c9fe83167d87e7aeb23031b42f344&lt;/denchmark-link&gt;
 
 We were able to enable addition, subtraction, division, and multiplication kernels for complex types on GPU: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/93f15d4cde2f08057819f1194e5a4771f0d391ff&gt;93f15d4&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='28' author='sbrodeur' date='2016-10-11T15:48:20Z'>
 		&lt;denchmark-link:https://github.com/sbrodeur&gt;@sbrodeur&lt;/denchmark-link&gt;
  Does TensorFlow now support all the operations you need on complex, or are there additional improvements we need to make ?
 		</comment>
 		<comment id='29' author='sbrodeur' date='2016-10-25T16:54:08Z'>
 		&lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
  Tensorflow now supports everything I need for handling complex numbers.
 		</comment>
 		<comment id='30' author='sbrodeur' date='2016-10-25T18:04:29Z'>
 		Thanks, closing the issue.
 		</comment>
 		<comment id='31' author='sbrodeur' date='2017-07-14T13:38:14Z'>
 		Is it possible to calculate a complex number divide a float number without type cast?
 		</comment>
 	</comments>
 </bug>
<commit id='53af29cb8503c7ed55a23d22090dd39ce0056a7a' author='Olivia Nordquist' date='2016-09-14 16:31:25-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\core\kernels\cwise_op_add.cc' new_name='tensorflow\core\kernels\cwise_op_add.cc'>
 		<file_info nloc='14' complexity='0' token_count='116'></file_info>
 		<modified_lines>
 			<added_lines>22,23</added_lines>
 			<deleted_lines>22,23</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\core\kernels\cwise_op_gpu_add.cu.cc' new_name='tensorflow\core\kernels\cwise_op_gpu_add.cu.cc'>
 		<file_info nloc='6' complexity='0' token_count='29'></file_info>
 		<modified_lines>
 			<added_lines>22</added_lines>
 			<deleted_lines>22</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\kernel_tests\cwise_ops_test.py' new_name='tensorflow\python\kernel_tests\cwise_ops_test.py'>
 		<file_info nloc='1643' complexity='315' token_count='19258'></file_info>
 		<method name='testComplex128Basic' parameters='self'>
 				<method_info nloc='14' complexity='1' token_count='225' nesting_level='1' start_line='730' end_line='743'></method_info>
 			<added_lines>743</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testComplex64Basic' parameters='self'>
 				<method_info nloc='14' complexity='1' token_count='225' nesting_level='1' start_line='715' end_line='728'></method_info>
 			<added_lines>728</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
