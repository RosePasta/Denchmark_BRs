<bug_data>
<bug id='33148' author='mimxrt' open_date='2019-10-08T15:25:27Z' closed_time='2020-07-14T16:40:54Z'>
 	<summary>Masking LSTM: OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 Have I written custom code: Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
 TensorFlow installed from (source or binary): Binary, pip
 TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
 Python version: Python 3.7.3
 CUDA/cuDNN version: CUDA=10.0, CUDNN=7.6.2.24-1
 GPU model and memory: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
 
 &lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;
 
 It seems there is an issue with the CuDNN LSTM implementation when using a tf.keras.layers.Masking layer.
 &lt;denchmark-code&gt;batch_size = 256
 num_tsteps = 144
 num_features = 130
 num_units = 88
 model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
     tf.keras.layers.Activation('sigmoid'),
 ])
 &lt;/denchmark-code&gt;
 
 Similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33069&gt;#33069&lt;/denchmark-link&gt;
  I receive this error during training and I have strictly right-padded data (I am doing trimming and right-padding manually). However, in contrast to this issue, I confirmed that I do not have any inputs containing only zeroes via the following snippet:
 &lt;denchmark-code&gt;for i, e in enumerate(ds_train):
     res = []
     f, l = [x.numpy() for x in e]
     for j in range(f.shape[0]):
         if not (f[j] == 0.0).all():
             res.append(1)
         else:
             res.append(0)
     fin = [res[0]]
     for e in res[1:]:
         if e != fin[-1]:
             fin.append(e)
     print("i {}: {}".format(i, fin))
 
 # Result:
 i 0: [1, 0]
 i 1: [1, 0]
 i 2: [1, 0]
 i 3: [1, 0]
 i 4: [1]
 i 5: [1, 0]
 ...
 &lt;/denchmark-code&gt;
 
 If I remove the Masking-layer, the error does not occur. I confirmed this by running a complete epoch (2324 batches), however, the training is probably pretty pointless when including the padded data.
 Is there any other pitfall that I am missing that could cause this issue?
 &lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;
 
 Python output:
 &lt;denchmark-code&gt;Epoch 1/1000
 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: 
 
 
 CancelledErrorTraceback (most recent call last)
 &lt;ipython-input-7-1c503c2dd55c&gt; in &lt;module&gt;
 ----&gt; 1 m.fit(train=True)
 
 /ws/tf/vol_local/_model_lstm.py in fit(self, train, verbose)
     315             ]
     316             self.model.fit(ds_train, epochs=num_epochs, verbose=verbose, shuffle=False,
 --&gt; 317                                 validation_data=ds_val, validation_steps=None, callbacks=cbs)
     318             #self.model.save(sess_hdf5_path)
     319             self.model.save_weights(self.sess_h5_path.as_posix())
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
     726         max_queue_size=max_queue_size,
     727         workers=workers,
 --&gt; 728         use_multiprocessing=use_multiprocessing)
     729 
     730   def evaluate(self,
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
     322                 mode=ModeKeys.TRAIN,
     323                 training_context=training_context,
 --&gt; 324                 total_epochs=epochs)
     325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
     326 
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
     121         step=step, mode=mode, size=current_batch_size) as batch_logs:
     122       try:
 --&gt; 123         batch_outs = execution_function(iterator)
     124       except (StopIteration, errors.OutOfRangeError):
     125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
      84     # `numpy` translates Tensors to values in Eager mode.
      85     return nest.map_structure(_non_none_constant_value,
 ---&gt; 86                               distributed_function(input_fn))
      87 
      88   return execution_function
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
     455 
     456     tracing_count = self._get_tracing_count()
 --&gt; 457     result = self._call(*args, **kwds)
     458     if tracing_count == self._get_tracing_count():
     459       self._call_counter.called_without_tracing()
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
     518         # Lifting succeeded, so variables are initialized and we can run the
     519         # stateless function.
 --&gt; 520         return self._stateless_fn(*args, **kwds)
     521     else:
     522       canon_args, canon_kwds = \
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
    1821     """Calls a graph function specialized to the inputs."""
    1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
 -&gt; 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
    1824 
    1825   @property
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
    1139          if isinstance(t, (ops.Tensor,
    1140                            resource_variable_ops.BaseResourceVariable))),
 -&gt; 1141         self.captured_inputs)
    1142 
    1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
    1222     if executing_eagerly:
    1223       flat_outputs = forward_function.call(
 -&gt; 1224           ctx, args, cancellation_manager=cancellation_manager)
    1225     else:
    1226       gradient_name = self._delayed_rewrite_functions.register()
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
     509               inputs=args,
     510               attrs=("executor_type", executor_type, "config_proto", config),
 --&gt; 511               ctx=ctx)
     512         else:
     513           outputs = execute.execute_with_cancellation(
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
      65     else:
      66       message = e.message
 ---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
      68   except TypeError as e:
      69     keras_symbolic_tensors = [
 
 /ws/miniconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)
 
 CancelledError:  [_Derived_]RecvAsync is cancelled.
 	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
 	 [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]] [Op:__inference_distributed_function_172102]
 
 Function call stack:
 distributed_function
 &lt;/denchmark-code&gt;
 
 Command line log:
 &lt;denchmark-code&gt;2019-10-08 14:38:27.367875: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_169668_171093' and '__inference___backward_cudnn_lstm_with_fallback_169668_171093_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_172102' both implement 'lstm_dce676f4-acdd-4bb5-88d9-e8dd57573aba' but their signatures do not match.
 2019-10-08 14:38:27.536666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
 2019-10-08 14:38:39.982582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
 2019-10-08 14:38:41.215567: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
 2019-10-08 14:38:41.215616: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
 	 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 2019-10-08 14:38:41.215638: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.
 	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
 	 [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]]
 2019-10-08 14:38:41.215693: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.
 	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='mimxrt' date='2019-10-09T09:28:09Z'>
 		&lt;denchmark-link:https://github.com/mimxrt&gt;@mimxrt&lt;/denchmark-link&gt;
  ,
 Thanks for reporting the issue, can you please provide simple and standalone code to reproduce the issue?
 		</comment>
 		<comment id='2' author='mimxrt' date='2019-10-09T14:46:37Z'>
 		I am trying to create a standalone example, however, I am facing some other errors that prevent me from finishing it. I created the following example which can produce the error using one .tfrecord file of my dataset. In the example, I generate a .tfrecord file from random data (for reproducibility) but TensorFlow fails to parse the file afterwards (actually parsing works but I still get the error):
 &lt;denchmark-code&gt;import numpy as np
 import tensorflow as tf
 
 gpus = tf.config.experimental.list_physical_devices('GPU')
 for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)
 assert tf.executing_eagerly()
 
 batch_size = 256
 num_tsteps = 144
 num_features = 130
 num_units = 88
 
 #n_files = 3320
 n_files = 10
 num_epochs = 1000
 
 seq_len_max_trunc = batch_size * num_tsteps
 flen = 3728
 
 ### Create TFRecord
 
 X = np.random.rand(flen + 1, num_features)
 n_label0 = int((flen + 1) * 0.2)
 Y = np.concatenate((
     np.zeros((n_label0, 1)), # label 0
     np.ones((flen - n_label0 + 1, 1)), # label 1
 ), axis=0)
 ds_out = tf.data.Dataset.from_tensor_slices((X, Y))
 ds_ser = ds_out.map(lambda *x: 
    tf.reshape(tf.py_function(lambda *v: 
        tf.train.Example(features=tf.train.Features(feature={
            "features": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),
            "label": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),
        })).SerializeToString(), x, tf.string
    ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE
 )
 for i, e in enumerate(ds_ser):
     ex = tf.train.Example.FromString(e.numpy())
     print("{}: {}".format(i, ex).replace(" ", "").replace("\n", " ")[:100])
 writer = tf.data.experimental.TFRecordWriter("temp.tfrecord")
 writer.write(ds_ser)
 
 ### Read TFRecord and train
 
 files = ["temp.tfrecord"] * n_files
 #files = ["data/myfile.tfrecord"] * n_files
 
 model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
     tf.keras.layers.Activation('sigmoid'),
 ])
 model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])
 
 
 def _prep_ds_file(file):
     _ds = tf.data.TFRecordDataset(file)
     for i, e in enumerate(_ds):
         ex = tf.train.Example.FromString(e.numpy())
         print("{}: {}".format(i, ex).replace(" ", "").replace("\n", " ")[:100])
     print("\n\n\n\n\n\n\n")
     _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {
         "features": tf.io.FixedLenFeature([132], tf.float32),
         "label": tf.io.FixedLenFeature([1], tf.float32),
     }), num_parallel_calls=tf.data.experimental.AUTOTUNE)
 
     _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v["features"][2:], v["label"])))
 
     _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)
     _ds = _ds.take(_trunc)
 
     _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps
     if _c_pad &gt;= 0:
         assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc
         _ds_pad = tf.data.Dataset.from_tensors((
             tf.constant(0.0, shape=[num_features,]),
             tf.constant(0.0, shape=[1,])))
         _ds_pad = _ds_pad.repeat(_c_pad)
         _ds = _ds.concatenate(_ds_pad) # pad to correct size
 
     _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)
     _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))
 
     _ds = _ds.batch(batch_size, drop_remainder=True)
     
     return _ds
 
 
 ds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)
 fs_train = ds_fs.take(int(n_files * 0.7))
 fs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))
 
 ds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]
 for f in fs_train.skip(1):
     ds_train = ds_train.concatenate(_prep_ds_file(f))
 ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 
 ds_val = [_prep_ds_file(f) for f in fs_val.take(1)][0]
 for f in fs_val.skip(1):
     ds_val = ds_val.concatenate(_prep_ds_file(f))
 ds_val = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 
 cbs = [
     tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True),
 ]
 model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,
           validation_data=ds_val, validation_steps=None, callbacks=cbs)
 &lt;/denchmark-code&gt;
 
 For me this code produces the following error:
 &lt;denchmark-code&gt;InvalidArgumentError: 2 root error(s) found.
   (0) Invalid argument:  {{function_node __inference_Dataset_map_&lt;lambda&gt;_32068}} Key: features.  Can't parse serialized Example.
 	 [[{{node ParseSingleExample/ParseSingleExample}}]]
 	 [[IteratorGetNext]]
 	 [[IteratorGetNext/_2]]
   (1) Invalid argument:  {{function_node __inference_Dataset_map_&lt;lambda&gt;_32068}} Key: features.  Can't parse serialized Example.
 	 [[{{node ParseSingleExample/ParseSingleExample}}]]
 	 [[IteratorGetNext]]
 0 successful operations.
 0 derived errors ignored. [Op:__inference_distributed_function_64891]
 
 Function call stack:
 distributed_function -&gt; distributed_function
 &lt;/denchmark-code&gt;
 
 This is weird because it does iterate through the full file and prints the values on the screen (see the dataset iteration both when generating and when parsing the .tfrecord file in _prep_ds_file()). I can see that elements 0 to 3728 are printed. When switching to the file myfile.tfrecord (commented out) I can also see the elements until 3728 (same amount) and finally receive the error mentioned in the issue above.
 Do you have any idea what causes this error? If not I could provile the myfile.tfrecord file but of course that would not be ideal for reproducibility.
 		</comment>
 		<comment id='3' author='mimxrt' date='2019-10-10T05:49:11Z'>
 		Issue replicating with TF-2.0, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/11dcf7d43fb3a2ba2c1d97e22ff145d2/33148.ipynb&gt;gist&lt;/denchmark-link&gt;
  of colab.Thanks!
 		</comment>
 		<comment id='4' author='mimxrt' date='2019-10-10T09:00:05Z'>
 		I found my mistake and updated the code accordingly. Please update the gist to represent the corrected and less verbose version below. The error happens in TF-2.0 and does not happen in TF-1.0 (1.14.0). Please also try to removing the Masking layer to confirm the issue only exists with masking.
 &lt;denchmark-code&gt;import numpy as np
 import tensorflow as tf
 
 # NOTE: Comment the block below for testing with TF-1.0
 gpus = tf.config.experimental.list_physical_devices('GPU')
 for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)
 
 # NOTE: Uncomment the block below for testing with TF-1.0
 # tf.compat.v1.enable_eager_execution()
 # config = tf.compat.v1.ConfigProto()
 # config.gpu_options.allow_growth = True
 # sess = tf.compat.v1.Session(config=config)
 # tf.compat.v1.keras.backend.set_session(sess)
 
 assert tf.executing_eagerly()
 
 batch_size = 256
 num_tsteps = 144
 num_features = 130
 num_units = 88
 
 #n_files = 3320
 n_files = 10
 num_epochs = 1000
 
 seq_len_max_trunc = batch_size * num_tsteps
 flen = 3728
 
 ### Create TFRecord
 
 X = np.random.rand(flen + 1, num_features + 2)
 n_label0 = int((flen + 1) * 0.2)
 Y = np.concatenate((
     np.zeros((n_label0, 1)), # label 0
     np.ones((flen - n_label0 + 1, 1)), # label 1
 ), axis=0)
 ds_out = tf.data.Dataset.from_tensor_slices((X, Y))
 ds_ser = ds_out.map(lambda *x: 
    tf.reshape(tf.py_function(lambda *v: 
        tf.train.Example(features=tf.train.Features(feature={
            "features": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),
            "label": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),
        })).SerializeToString(), x, tf.string
    ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE
 )
 writer = tf.data.experimental.TFRecordWriter("temp.tfrecord")
 writer.write(ds_ser)
 
 ### Read TFRecord and train
 
 files = ["temp.tfrecord"] * n_files
 
 model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
     tf.keras.layers.Activation('sigmoid'),
 ])
 model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])
 
 
 def _prep_ds_file(file):
     _ds = tf.data.TFRecordDataset(file)
     _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {
         "features": tf.io.FixedLenFeature([132], tf.float32),
         "label": tf.io.FixedLenFeature([1], tf.float32),
     }), num_parallel_calls=tf.data.experimental.AUTOTUNE)
         
     _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v["features"][2:], v["label"])))
 
     _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)
     _ds = _ds.take(_trunc)
 
     _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps
     if _c_pad &gt;= 0:
         assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc
         _ds_pad = tf.data.Dataset.from_tensors((
             tf.constant(0.0, shape=[num_features,]),
             tf.constant(0.0, shape=[1,])))
         _ds_pad = _ds_pad.repeat(_c_pad)
         _ds = _ds.concatenate(_ds_pad) # pad to correct size
 
     _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)
     _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))
 
     _ds = _ds.batch(batch_size, drop_remainder=True)
     
     return _ds
 
 
 ds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)
 fs_train = ds_fs.take(int(n_files * 0.7))
 fs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))
 
 ds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]
 for f in fs_train.skip(1):
     ds_train = ds_train.concatenate(_prep_ds_file(f))
 ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 
 cbs = [
     tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True),
 ]
 model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,
           validation_data=None, validation_steps=None, callbacks=cbs)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='mimxrt' date='2019-10-10T16:57:21Z'>
 		&lt;denchmark-link:https://github.com/mimxrt&gt;@mimxrt&lt;/denchmark-link&gt;
  I could reproduce the issue with . However, if I use only cpu then there is no error (with and without masking) as shown in the original post. I think root-cause may be related to . &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/3a768611de0a28330d047f12e120f931/untitled553.ipynb&gt;Here&lt;/denchmark-link&gt;
  is the gist for your reference. Thanks!
 		</comment>
 		<comment id='6' author='mimxrt' date='2019-10-11T07:17:23Z'>
 		Thank you for your reply. I'm not sure what you are saying though: did you try removing tf.config.experimental.set_memory_growth(gpu, True) at all? I tried and for me the same error happens. For me, the only difference is that TF occupies all available GPU memory.
 Could you please try again if the error also happens without memory growth? If so, I guess the issue could be in the GPU LSTM implementation (cuDNN)?
 		</comment>
 		<comment id='7' author='mimxrt' date='2019-10-11T20:14:23Z'>
 		&lt;denchmark-link:https://github.com/mimxrt&gt;@mimxrt&lt;/denchmark-link&gt;
  I mentioned two things.
 
 I could reproduce the issue with tf-nightly-gpu
 Just tried to point some root-cause. I ran your code as it is in a cpu and I don't see any error. so I guess root-cause may be related to tf.config.experimental.set_memory_growth(gpu, True). I may be wrong.
 
 Thanks!
 		</comment>
 		<comment id='8' author='mimxrt' date='2019-10-14T07:12:44Z'>
 		I see, thank you for the clarification. So in summary we can say that this issue does not happen on the CPU and that it is related to the GPU implementation. As I tried running it without tf.config.experimental.set_memory_growth(gpu, True) and the error still occurs, it should probably be save to rule that out as the cause/issue.
 		</comment>
 		<comment id='9' author='mimxrt' date='2019-11-12T01:34:44Z'>
 		same problem here. did you find any solution?
 		</comment>
 		<comment id='10' author='mimxrt' date='2019-11-12T07:33:03Z'>
 		No sorry, still waiting for any help myself.
 		</comment>
 		<comment id='11' author='mimxrt' date='2019-11-12T08:12:30Z'>
 		I changed my model from:
         self.model = Sequential([
             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1],mask_zeros=True),
             LSTM(self.lstm_out),
             Dense(len(self.item_map)-1),
         ])
 to:
         self.model = Sequential([
             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1]),
             Masking(mask_value=0),
             LSTM(self.lstm_out),
             Dense(len(self.item_map)-1),
         ])
 And solved my isssue
 I know &lt;denchmark-link:https://github.com/mimxrt&gt;@mimxrt&lt;/denchmark-link&gt;
 's code has the same model and I dont know why it works for me,
 but im adding this for anyone else comes here with the issue and maybe it can help with debugging
 		</comment>
 		<comment id='12' author='mimxrt' date='2019-11-12T09:35:41Z'>
 		&lt;denchmark-link:https://github.com/ynsgnr&gt;@ynsgnr&lt;/denchmark-link&gt;
  if you are running on CPU then it works properly, the problem is when you run it on GPU. By the way, you do not use the Timedistirbuted layer in your code since this problem shows up when you use Timedistributed with LSTM.
 		</comment>
 		<comment id='13' author='mimxrt' date='2019-11-12T10:16:45Z'>
 		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 , I tried your suggestion but it still produces the same error. I am new to GitHub , how to tell this problem to someone from TensorFlow? do they see our conversation ?
 		</comment>
 		<comment id='14' author='mimxrt' date='2019-11-12T10:20:14Z'>
 		In my experience and example the issue stems from the Masking + LSTM combination, not from the  layer. Please try the example in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342&gt;#33148 (comment)&lt;/denchmark-link&gt;
  without it, i.e.
 &lt;denchmark-code&gt;model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=False, stateful=False),
     tf.keras.layers.Dense(1),
     tf.keras.layers.Activation('sigmoid'),
 ])
 &lt;/denchmark-code&gt;
 
 (change return_sequences to False and remove TimeDistributed).
 You can also try to remove the Masking layer in the example above and the error will go away.
 		</comment>
 		<comment id='15' author='mimxrt' date='2019-11-12T11:40:52Z'>
 		but I have this model:
        model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=True,mask_zero=True) model.add(embeding_layer) model.add(layers.LSTM(50)) model.add(layers.Dropout(0.5)) model.add(layers.Dense(3,activation='softmax')) opt=tf.keras.optimizers.RMSprop(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy']) self.model=model
 which works fine and with no error. As you can see I used masking and LSTM at the same time. so in my experience the problem stem from TimeDistributed and masking.
 		</comment>
 		<comment id='16' author='mimxrt' date='2019-11-12T11:51:23Z'>
 		Interesting. Can you provide a complete example like mine so I can try your code as well?
 		</comment>
 		<comment id='17' author='mimxrt' date='2019-11-12T12:01:51Z'>
 		the above example is a simple classifier, you can make a random dataset and feed the model with it, so you can see that it will work with no problem.
 this is where the model does not work and produce an error while training :
        model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=True) model.add(TimeDistributed(embeding_layer)) model.add(TimeDistributed(tf.keras.layers.LSTM(50))) model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50))) # model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(100))) model.add(layers.Dense(3,activation='softmax')) opt=tf.keras.optimizers.Adam(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])
 in this example, if I set mask_zero=true at the embedding layer then, it crushes at the begging or sometimes at the end of the epoc when evaluating the validation loss. and this is the error message :
  C:\Users\jalil\PycharmProjects\untitled1\venv\Scripts\python.exe C:/Users/jalil/PycharmProjects/untitled1/main_file.py
 2019-11-14 14:11:36.983144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
 2019-11-14 14:11:45.638679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
 2019-11-14 14:11:46.216495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
 name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
 pciBusID: 0000:01:00.0
 2019-11-14 14:11:46.216676: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
 2019-11-14 14:11:46.217282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
 2019-11-14 14:11:50.885396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
 2019-11-14 14:11:51.214275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
 name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
 pciBusID: 0000:01:00.0
 2019-11-14 14:11:51.214484: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
 2019-11-14 14:11:51.218182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
 2019-11-14 14:11:51.905201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
 2019-11-14 14:11:51.905307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] 0
 2019-11-14 14:11:51.905366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N
 2019-11-14 14:11:51.906228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)
 WARNING:tensorflow:From C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.where in 2.0, which has the same broadcast rule as np.where
 Train on 35000 samples, validate on 6447 samples
 Epoch 1/1000
 2019-11-14 14:12:33.178251: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_671418_672877' and '__inference___backward_cudnn_lstm_with_fallback_671418_672877_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_675292' both implement 'lstm_81cdaa4a-fa6f-4675-abbb-02fb4cd0189b' but their signatures do not match.
 2019-11-14 14:12:33.544669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
 2019-11-14 14:12:34.397677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
 32/35000 [..............................] - ETA: 2:03:422019-11-14 14:12:35.151804: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
 2019-11-14 14:12:35.152137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 2019-11-14 14:12:35.152541: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)'
 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]]
 32/35000 [..............................] - ETA: 2:25:41Traceback (most recent call last):
 File "C:/Users/jalil/PycharmProjects/untitled1/main_file.py", line 102, in
 main_model_instance.train_model(train_batch_data,train_batch_labels,test_batch_data,test_batch_labels)
 File "C:\Users\jalil\PycharmProjects\untitled1\main_model.py", line 103, in train_model
 history = self.model.fit(x=np.array(train_batch_data),y=np.array(train_batch_labels),validation_data=(np.array(test_batch_data),np.array(test_batch_labels)),epochs=1000,callbacks=[tensorboard_callback])
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py", line 734, in fit
 use_multiprocessing=use_multiprocessing)
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 324, in fit
 total_epochs=epochs)
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 123, in run_one_epoch
 batch_outs = execution_function(iterator)
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py", line 86, in execution_function
 distributed_function(input_fn))
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 439, in call
 return self._stateless_fn(*args, *kwds)
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 1822, in call
 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 1141, in _filtered_call
 self.captured_inputs)
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 1224, in _call_flat
 ctx, args, cancellation_manager=cancellation_manager)
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py", line 511, in call
 ctx=ctx)
 File "C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\execute.py", line 67, in quick_execute
 six.raise_from(core._status_to_exception(e.code, message), None)
 File "", line 3, in raise_from
 tensorflow.python.framework.errors_impl.UnknownError: [Derived] CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void)&amp;padding_fill)'
 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_675292]
 Function call stack:
 distributed_function -&gt; distributed_function -&gt; distributed_function
 `
 		</comment>
 		<comment id='18' author='mimxrt' date='2020-05-02T18:58:32Z'>
 		import tensorflow as tf
 tf.compat.v1.disable_eager_execution()
 Dissable eager execution and everything is running fine without the fused rnn kernel. Thx for the help guys :)
 		</comment>
 		<comment id='19' author='mimxrt' date='2020-06-01T12:39:43Z'>
 		I have a work around that seems to work:  force TF to use the non CuDNN implementation by selecting a sigmoid activation instead of TANH
 layers.LSTM(...,activation='sigmoid')
 Outputs
 WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
 This forces TF to use a generic GPU kernel in place of CuDNN.  It's slower but a slower implementation is a lot faster than not working at all ;p
 		</comment>
 		<comment id='20' author='mimxrt' date='2020-06-09T07:27:49Z'>
 		If you really want that as a workaround, you can find the requirements for the cuDNN implementation &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM?hl=en#used-in-the-notebooks_1&gt;here&lt;/denchmark-link&gt;
 .
 
 The requirements to use the cuDNN implementation are:
 
 activation == tanh
 recurrent_activation == sigmoid
 recurrent_dropout == 0
 unroll is False
 use_bias is True
 Inputs are not masked or strictly right padded.
 
 
 I guess changing any one of these will result in the CPU non-cuDNN implementation being used.
 		</comment>
 		<comment id='21' author='mimxrt' date='2020-06-12T19:32:36Z'>
 		That's right mimxrt.  I guess the interesting point here is that the inputs to my LSTM are always masked, but I have to force the non-cudnn implementation using the activation function.  Might be a clue for someone who can fix this.
 		</comment>
 		<comment id='22' author='mimxrt' date='2020-06-26T15:47:49Z'>
 		I'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.
 		</comment>
 		<comment id='23' author='mimxrt' date='2020-07-03T10:55:45Z'>
 		Facing this same issue and reported here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40982&gt;#40982&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='24' author='mimxrt' date='2020-07-06T16:47:51Z'>
 		
 I'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.
 
 There is a private field you can set to disable the cudnn kernel once the layer is created.
 &lt;denchmark-code&gt;layer = tf.keras.layers.LSTM(4)
 layer. _could_use_gpu_kernel = False
 &lt;/denchmark-code&gt;
 
 This will focus the lstm layer to use the non-cudnn implementation, even landed on GPU.
 		</comment>
 		<comment id='25' author='mimxrt' date='2020-07-06T16:50:37Z'>
 		&lt;denchmark-link:https://github.com/queirozfcom&gt;@queirozfcom&lt;/denchmark-link&gt;
  what would you suggest to use between this option and disabling eager execution as when eager execution is disabled it does work?
 		</comment>
 		<comment id='26' author='mimxrt' date='2020-07-06T16:57:32Z'>
 		Disable eager will have larger side effects since it change the runtime to execute in graph/session context. We don't recommend user to fallback to graph unless they really need some feature in graph/session.
 		</comment>
 		<comment id='27' author='mimxrt' date='2020-07-06T17:13:03Z'>
 		Thanks :)
 		</comment>
 		<comment id='28' author='mimxrt' date='2020-07-10T18:40:02Z'>
 		I am still facing this issue using TF 2.2.0. I also found the same workaround of forcing the LSTM to not use the cuDNN implementation to work, however it is nearly prohibitively slow. I found the generic GPU implementation took ~30 times longer to train per epoch than the cuDNN version. I hope this can be fixed soon.
 		</comment>
 		<comment id='29' author='mimxrt' date='2020-07-14T16:40:54Z'>
 		I have disable the cudnn code path if there is fully masked data in the batch. It will have some performance dip since it fallback to generic kernel, but won't error out.
 		</comment>
 		<comment id='30' author='mimxrt' date='2020-07-14T16:40:56Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='31' author='mimxrt' date='2020-07-22T00:57:42Z'>
 		facing this issue using TF 2.2.0 while evaluating
 		</comment>
 		<comment id='32' author='mimxrt' date='2020-07-22T02:36:50Z'>
 		Please try with the latest nightly version.
 		</comment>
 		<comment id='33' author='mimxrt' date='2020-08-07T06:42:40Z'>
 		
 @qlzh727 Not yet. We might need to wait for the next major release for the fix.
 
 cudnn 8 is here, is there any info about this issue?
 		</comment>
 		<comment id='34' author='mimxrt' date='2020-09-25T09:44:01Z'>
 		&lt;denchmark-link:https://github.com/geetachavan1&gt;@geetachavan1&lt;/denchmark-link&gt;
  I am using `tensorflow==2.3.0 but for this network:
 def call(self, inputs, training=None, memory_states=None, **kwargs):
 
     x = self.embedding(inputs, training=training)
 
     if memory_states is None:
         memory_states = self.get_initial_state(shape_list(x)[0])
 
     next_memory_states = []
 
     for i, (lstm, norm) in enumerate(self.lstm_stack):
         x = norm(x, training=training)
         outputs = lstm(x, training=training, initial_state=memory_states[i])
         x, state = outputs[0], outputs[1:]
         next_memory_states.append(state)
 
     return x, next_memory_states
 where we have:
 self.embedding = layers.Embedding(vocab_size, embedding_size, mask_zero=True)
 self.lstm_stack = list()
 for _ in range(num_layers):
     lstm = layers.LSTM(
         units=lstm_units,
         return_sequences=True,
         return_state=True,
         dropout=dropout
     )
     norm = layers.LayerNormalization()
     self.lstm_stack.append((lstm, norm))
 I am getting
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py", line 153, in &lt;module&gt;
     main()
   File "/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py", line 85, in main
     y_greedy = model.greedy_decode(encoder_inputs)[0]
   File "/home/sfalk/tmp/speech-v2/asr/model/transducer.py", line 351, in greedy_decode
     init_predict_network_state = PredictNetworkState(*self.predict_network(init_yseq))
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 985, in __call__
     outputs = call_fn(inputs, *args, **kwargs)
   File "/home/sfalk/tmp/speech-v2/asr/model/transducer.py", line 118, in call
     outputs = lstm(x, training=training, initial_state=memory_states[i])
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py", line 720, in __call__
     return super(RNN, self).__call__(inputs, **kwargs)
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 985, in __call__
     outputs = call_fn(inputs, *args, **kwargs)
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py", line 1176, in call
     last_output, outputs, new_h, new_c, runtime = gpu_lstm(
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py", line 1401, in gpu_lstm
     outputs, h, c, _, _ = gen_cudnn_rnn_ops.cudnn_rnnv3(
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py", line 1914, in cudnn_rnnv3
     return cudnn_rnnv3_eager_fallback(
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py", line 2011, in cudnn_rnnv3_eager_fallback
     _result = _execute.execute(b"CudnnRNNV3", 5, inputs=_inputs_flat,
   File "/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
 tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1521): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)' [Op:CudnnRNNV3]
 &lt;/denchmark-code&gt;
 
 Shouldn't this be fixed?
 		</comment>
 		<comment id='35' author='mimxrt' date='2020-10-15T20:28:54Z'>
 		Some update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342&gt;#33148 (comment)&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='36' author='mimxrt' date='2020-11-28T18:48:49Z'>
 		When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.
 		</comment>
 		<comment id='37' author='mimxrt' date='2020-11-30T06:33:20Z'>
 		
 When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.
 
 cudnn 8.0.5 is already out &lt;denchmark-link:https://docs.nvidia.com/deeplearning/cudnn/archives/index.html&gt;https://docs.nvidia.com/deeplearning/cudnn/archives/index.html&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='38' author='mimxrt' date='2020-12-07T11:02:11Z'>
 		
 Some update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by #33148 (comment).
 
 &lt;denchmark-link:https://github.com/kaixih&gt;@kaixih&lt;/denchmark-link&gt;
  where did you read it?
 I'd like to give it a try.
 		</comment>
 	</comments>
 </bug>
<commit id='4d582a660b4e84fb283eba598127ae40fdd8d1ed' author='Scott Zhu' date='2020-07-13 21:27:11-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.16363636363636364'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\keras\layers\gru_v2_test.py' new_name='tensorflow\python\keras\layers\gru_v2_test.py'>
 		<file_info nloc='636' complexity='46' token_count='5284'></file_info>
 		<method name='test_with_fully_masked_inputs' parameters='self'>
 				<method_info nloc='23' complexity='1' token_count='137' nesting_level='1' start_line='615' end_line='641'></method_info>
 			<added_lines>615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>642</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\keras\layers\lstm_v2_test.py' new_name='tensorflow\python\keras\layers\lstm_v2_test.py'>
 		<file_info nloc='898' complexity='72' token_count='7214'></file_info>
 		<method name='test_with_fully_masked_inputs' parameters='self'>
 				<method_info nloc='23' complexity='1' token_count='137' nesting_level='1' start_line='816' end_line='842'></method_info>
 			<added_lines>816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>843</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\keras\layers\recurrent_v2.py' new_name='tensorflow\python\keras\layers\recurrent_v2.py'>
 		<file_info nloc='1174' complexity='69' token_count='4974'></file_info>
 		<method name='call' parameters='self,inputs,mask,training,initial_state'>
 				<method_info nloc='79' complexity='18' token_count='519' nesting_level='1' start_line='1100' end_line='1204'></method_info>
 			<added_lines>1172</added_lines>
 			<deleted_lines>1172</deleted_lines>
 		</method>
 		<method name='cudnn_gru_fn' parameters=''>
 				<method_info nloc='11' complexity='1' token_count='43' nesting_level='2' start_line='738' end_line='748'></method_info>
 			<added_lines>738</added_lines>
 			<deleted_lines>738</deleted_lines>
 		</method>
 		<method name='is_sequence_right_padded' parameters='mask,time_major'>
 				<method_info nloc='8' complexity='2' token_count='77' nesting_level='0' start_line='1564' end_line='1594'></method_info>
 			<added_lines>1564,1581,1593,1594</added_lines>
 			<deleted_lines>1564,1581,1582,1583,1588,1589</deleted_lines>
 		</method>
 		<method name='has_fully_masked_sequence' parameters='mask'>
 				<method_info nloc='5' complexity='1' token_count='26' nesting_level='0' start_line='1593' end_line='1603'></method_info>
 			<added_lines>1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='input_not_right_padded' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='47' nesting_level='2' start_line='750' end_line='761'></method_info>
 			<added_lines>750</added_lines>
 			<deleted_lines>750</deleted_lines>
 		</method>
 		<method name='input_right_padded' parameters=''>
 				<method_info nloc='11' complexity='1' token_count='43' nesting_level='2' start_line='738' end_line='748'></method_info>
 			<added_lines>738</added_lines>
 			<deleted_lines>738</deleted_lines>
 		</method>
 		<method name='cudnn_lstm_fn' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='47' nesting_level='2' start_line='1509' end_line='1520'></method_info>
 			<added_lines>1509</added_lines>
 			<deleted_lines>1509</deleted_lines>
 		</method>
 		<method name='is_sequence_right_padded' parameters='mask'>
 				<method_info nloc='6' complexity='1' token_count='64' nesting_level='0' start_line='1564' end_line='1590'></method_info>
 			<added_lines>1564,1581</added_lines>
 			<deleted_lines>1564,1581,1582,1583,1588,1589</deleted_lines>
 		</method>
 		<method name='is_cudnn_supported_inputs' parameters='mask,time_major'>
 				<method_info nloc='6' complexity='2' token_count='38' nesting_level='0' start_line='1606' end_line='1612'></method_info>
 			<added_lines>1606,1607,1608,1609,1610,1611,1612</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='standard_gru_fn' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='47' nesting_level='2' start_line='750' end_line='761'></method_info>
 			<added_lines>750</added_lines>
 			<deleted_lines>750</deleted_lines>
 		</method>
 		<method name='stardard_lstm_fn' parameters=''>
 				<method_info nloc='13' complexity='1' token_count='51' nesting_level='2' start_line='1522' end_line='1534'></method_info>
 			<added_lines>1522</added_lines>
 			<deleted_lines>1522</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>493,764,765,766,1537,1538,1539,1604,1605,1613,1614</added_lines>
 			<deleted_lines>493,764,765,766,1537,1538,1539</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
