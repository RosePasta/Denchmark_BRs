<bug_data>
<bug id='18769' author='rothn' open_date='2018-04-22T06:03:46Z' closed_time='2019-05-08T02:17:48Z'>
 	<summary>InvalidArgumentError for save/restore of variables (same version, same OS, same directory)</summary>
 	<description>
 I get an InvalidArgumentError with no further information when I try to save and then restore parts of my model later to continue training it (due to needing my laptop for class).
 Initialization:
 saver = tf.train.Saver({"embeddings": embeddings, "weights": nce_weights, "biases": nce_biases})
 Save:
 saver.save(sess, model_checkpoint_path)
 Load:
 saver.restore(sess, model_checkpoint_path)
 &lt;denchmark-code&gt;2018-04-21 22:45:00.143245: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
 Traceback (most recent call last):
   File "/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call
     return fn(*args)
   File "/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_fn
     options, feed_dict, fetch_list, target_list, run_metadata)
   File "/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_tf_sessionrun
     status, run_metadata)
   File "/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in __exit__
     c_api.TF_GetCode(self.status.status))
 tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
 	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 
 ... &lt;contains sensitive info&gt; ...
 
 InvalidArgumentError (see above for traceback): /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
 	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 &lt;/denchmark-code&gt;
 
 
 
 Yes, I modified this code (&lt;denchmark-link:https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py&gt;https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py&lt;/denchmark-link&gt;
 ) to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce_weights and nce_biases so that training may be resumed.
 
 MacOS 10.13.4 (17E199)
 
 pip on VirtualEnv, according to instructions (&lt;denchmark-link:https://www.tensorflow.org/install/install_mac&gt;https://www.tensorflow.org/install/install_mac&lt;/denchmark-link&gt;
 )
 
 1.7
 
 NA
 
 NA
 
 NA
 
 saver = tf.train.Saver({"embeddings": embeddings, "weights": nce_weights, "biases": nce_biases})
 saver.restore(sess, "../trained_model/saved_stuff")
 	</description>
 	<comments>
 		<comment id='1' author='rothn' date='2018-04-22T18:30:04Z'>
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Have I written custom code
 OS Platform and Distribution
 TensorFlow installed from
 TensorFlow version
 Bazel version
 CUDA/cuDNN version
 GPU model and memory
 Exact command to reproduce
 		</comment>
 		<comment id='2' author='rothn' date='2018-04-22T22:26:02Z'>
 		Ticket updated with requested information. Will use template in the future!
 		</comment>
 		<comment id='3' author='rothn' date='2018-04-25T02:39:06Z'>
 		Possibly related to: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18640&gt;#18640&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='rothn' date='2018-04-27T00:36:54Z'>
 		Update: I tried this on Windows as well, and I got rid of the stuff for loading the model graph and used the Saver to explicitly restore my variables. On Windows, it fails for the same reason (because I am loading an embedding larger than 2GB) but with a better error message:
 OutOfRangeError (see above for traceback): Read fewer bytes than requested [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 		</comment>
 		<comment id='5' author='rothn' date='2018-05-13T00:31:41Z'>
 		&lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
  bump :)
 		</comment>
 		<comment id='6' author='rothn' date='2018-06-06T12:36:16Z'>
 		I am using tensorflow-gpu 1.7.0, and meet exactly the same problem when restoring model in distributed tensorflow environment.
 when the model file is larger than 3GB, the error below occurs:
 InvalidArgumentError (see above for traceback): hdfs://xxxx/model.ckpt-5154361.data-00001-of-00008; Invalid argument
 [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device="/job:ps/replica:0/task:0/device:CPU:0"](_recv_save_1/Const_0_S7, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]
 [[Node: save_1/restore_all/NoOp_S10 = _Recv&lt;denchmark-link:&gt;client_terminated=false, recv_device="/job:worker/replica:0/task:0/device:GPU:0", send_device="/job:ps/replica:0/task:0/device:GPU:0", send_device_incarnation=1493071510865629599, tensor_name="edge_147_save_1/restore_all/NoOp", tensor_type=DT_FLOAT, _device="/job:worker/replica:0/task:0/device:GPU:0"&lt;/denchmark-link&gt;
 ]]
 when I decrease the model size, the error changed:
 tensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested
 [[Node: save_1/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT], _device="/job:ps/replica:0/task:1/device:CPU:0"](_recv_save_1/Const_0_S1, save_1/RestoreV2_3/tensor_names, save_1/RestoreV2_3/shape_and_slices)]]
 List the smaller models:
 -rw-r--r--   3 root supergroup   11893084 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00000-of-00008
 -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00001-of-00008
 -rw-r--r--   3 root supergroup  106483864 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00002-of-00008
 -rw-r--r--   3 root supergroup 5130000440 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.data-00003-of-00008
 -rw-r--r--   3 root supergroup     375688 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00004-of-00008
 -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00005-of-00008
 -rw-r--r--   3 root supergroup  232909600 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00006-of-00008
 -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00007-of-00008
 -rw-r--r--   3 root supergroup       1564 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.index
 -rw-r--r--   3 root supergroup     571996 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.meta
 		</comment>
 		<comment id='7' author='rothn' date='2018-06-08T17:18:28Z'>
 		Any updates, &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
  ?
 		</comment>
 		<comment id='8' author='rothn' date='2018-06-11T05:34:43Z'>
 		I think the root cause may be:
 in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/platform/hadoop/hadoop_file_system.cc&gt;https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/platform/hadoop/hadoop_file_system.cc&lt;/denchmark-link&gt;
 
 line 213:
      tSize r = hdfs_-&gt;hdfsPread(fs_, file_, static_cast&lt;tOffset&gt;(offset), dst, static_cast&lt;tSize&gt;(n));
 the last param static_cast(n) means the number of bytes required. ie., the tensor's size.
 in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.7/third_party/hadoop/hdfs.h&gt;https://github.com/tensorflow/tensorflow/blob/r1.7/third_party/hadoop/hdfs.h&lt;/denchmark-link&gt;
 
 line 75,
 typedef int32_t tSize;    /// size of data for read/write io ops
 tSize is int32, but tensor's size is int64, when the tensor is big enough, overflow occurs.
 My solution is to use partitioned variables for large tensor, then problem solved.
 		</comment>
 		<comment id='9' author='rothn' date='2018-06-15T02:06:05Z'>
 		But I'm not using HDFS -- this is all on my local SSD.
 		</comment>
 		<comment id='10' author='rothn' date='2018-07-18T14:14:04Z'>
 		I am having a very similar issue as well. Following &lt;denchmark-link:https://www.tensorflow.org/tutorials/estimators/cnn&gt;this tutorial&lt;/denchmark-link&gt;
 , but working with larger images.
 		</comment>
 		<comment id='11' author='rothn' date='2018-10-13T03:53:12Z'>
 		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
  seems like we're having issues with large restore? I thought the V2 version of the save restore ops was supposed to fix this. Any ideas?
 		</comment>
 		<comment id='12' author='rothn' date='2018-10-15T22:05:30Z'>
 		We can put &gt;2GB in checkpoints now. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/b8c86c3bbd8271ed968087f24e7fb704103bc733#diff-f4340b63dcfb03e060905682f7471faa&gt;b8c86c3#diff-f4340b63dcfb03e060905682f7471faa&lt;/denchmark-link&gt;
  fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From "OP_REQUIRES failed at save_restore_v2_ops.cc:184" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.
 Can someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:
 &lt;denchmark-code&gt;import tensorflow as tf
 from tensorflow.python.ops import io_ops
 
 def main(_):
   tf.enable_eager_execution()
   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))
   path = '/some/network/path'
   io_ops.save_v2([path], ["a"], [""], [large])
   restored = io_ops.restore_v2(path, ["a"], [""], [tf.float32])
   print(tf.reduce_sum(restored))
 
 if __name__ == '__main__':
   tf.app.run()
 
 &lt;/denchmark-code&gt;
 
 Prints:
 tf.Tensor(5e+09, shape=(), dtype=float32)
 		</comment>
 		<comment id='13' author='rothn' date='2018-11-08T13:29:12Z'>
 		
 We can put &gt;2GB in checkpoints now. b8c86c3#diff-f4340b63dcfb03e060905682f7471faa fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From "OP_REQUIRES failed at save_restore_v2_ops.cc:184" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.
 Can someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:
 import tensorflow as tf
 from tensorflow.python.ops import io_ops
 
 def main(_):
   tf.enable_eager_execution()
   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))
   path = '/some/network/path'
   io_ops.save_v2([path], ["a"], [""], [large])
   restored = io_ops.restore_v2(path, ["a"], [""], [tf.float32])
   print(tf.reduce_sum(restored))
 
 if __name__ == '__main__':
   tf.app.run()
 
 Prints:
 tf.Tensor(5e+09, shape=(), dtype=float32)
 
 I've tried this snippet on Mac OS X 10.14.1 + TF 1.12.0
 Got such error:
 
 restored = io_ops.restore_v2(path, ["a"], [""], [tf.float32])
 2018-11-08 17:27:05.767089: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument
 Traceback (most recent call last):
 File "", line 1, in 
 File "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py", line 1486, in restore_v2
 ctx=_ctx)
 File "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py", line 1511, in restore_v2_eager_fallback
 attrs=_attrs, ctx=_ctx, name=name)
 File "/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 66, in quick_execute
 six.raise_from(core._status_to_exception(e.code, message), None)
 File "", line 3, in raise_from
 tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument [Op:RestoreV2]
 
 		</comment>
 		<comment id='14' author='rothn' date='2018-11-08T17:21:01Z'>
 		Interesting, thank you for trying that out. So maybe it is a Mac-specific issue.
 Just to check, do you have &gt;20GB of disk and &gt;20GB of RAM free before running that snippet? This could be a badly reported OOM or out-of-disk error I suppose.
 		</comment>
 		<comment id='15' author='rothn' date='2018-11-09T11:38:08Z'>
 		I have about 400Gb free space on disk and total 16Gb of RAM (swapfile ON).
 Such issue occurs even if try to load 2.7Gb variable file.
 		</comment>
 		<comment id='16' author='rothn' date='2018-11-10T00:32:07Z'>
 		Interesting. I'll find a Mac next week and see if I can reproduce on it.
 		</comment>
 		<comment id='17' author='rothn' date='2018-11-22T04:24:27Z'>
 		Finally i found the way how to use save/load embeddings. Variable partitioning is the answer.
 With tf.variable_axis_size_partitioner and max_shard_bytes=2 ** 30 large embeddings loading works well.
 		</comment>
 		<comment id='18' author='rothn' date='2019-01-21T05:57:20Z'>
 		The issue is still there
 		</comment>
 		<comment id='19' author='rothn' date='2019-03-15T00:29:43Z'>
 		Problem still exist when I restore a 18GB model.
 My OS is Windows Server 2016
 Tensorflow 1.13.0 installed from pip
 512GB memory and hard disk
 Only run on CPU.
 Please advise.
 		</comment>
 		<comment id='20' author='rothn' date='2019-03-16T08:21:04Z'>
 		I also put the below in my code
 tf.variable_axis_size_partitioner(
 max_shard_bytes=2**35,
 axis=0,
 bytes_per_string_element=16,
 max_shards=None
 )
 However it still shows below error when restoring the model:
 2019-03-16 08:12:32.430456: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read fewer bytes than requested
 		</comment>
 		<comment id='21' author='rothn' date='2019-04-05T05:17:47Z'>
 		
 It has been 17 days with no activity and the awaiting response label was assigned. Is this still an issue?
 
 Yes, it is still an issue
 		</comment>
 		<comment id='22' author='rothn' date='2019-05-04T12:41:11Z'>
 		It has been 29 days with no activity and the awaiting response label was assigned. Is this still an issue?
 		</comment>
 		<comment id='23' author='rothn' date='2019-05-04T17:05:31Z'>
 		Yes itâ€™s still an issue
 		</comment>
 		<comment id='24' author='rothn' date='2019-05-08T00:14:27Z'>
 		Finally got a chance to debug this on a mac. Apparently the pread system call, despite taking an eight-byte size_t for its nbytes argument, returns EINVAL if "The sum of the iov_len values in the iov array overflowed a 32-bit integer." And presumably pread is implemented in terms of readv so they have the same limitation.
 I have a change out for review which just limits reads to INT32_MAX on every platform. Seems to work if we do that. I checked that the checkpoints themselves were identical to what gets written on Linux, so existing checkpoints will start working once that change is in.
 		</comment>
 		<comment id='25' author='rothn' date='2019-05-08T02:17:49Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=18769&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=18769&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='26' author='rothn' date='2019-05-22T11:25:04Z'>
 		Uh, this is still an issue? I built the latest version of tensorflow from source and it's still happening! I've tried practically everything. One of the most arcane and infuriating errors I have dealt with...
 		</comment>
 		<comment id='27' author='rothn' date='2019-05-22T16:10:54Z'>
 		@Sam-DevZ operating system and version, TF git version, repro instructions? The &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18769#issuecomment-430030868&gt;repro I posted&lt;/denchmark-link&gt;
  works for me on  (nightly) and macOS 10.14.2.
 		</comment>
 		<comment id='28' author='rothn' date='2019-05-23T11:18:11Z'>
 		Hi! Your snippet doesn't work either. Code:
 import tensorflow as tf
 from tensorflow.python.ops import io_ops
 
 print('Tensorflow version: ' + tf.__version__)
 print('MacOS Version: ' + '10.14.5')
 
 def main(_):
   tf.enable_eager_execution()
   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))
   path = '/Users/clearlycoder/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN'
   io_ops.save_v2([path], ["a"], [""], [large])
   restored = io_ops.restore_v2(path, ["a"], [""], [tf.float32])
   print(tf.reduce_sum(restored))
 
 if __name__ == '__main__':
   tf.app.run()
 Output:
 &lt;denchmark-code&gt;Tensorflow version: 1.12.2
 MacOS Version: 10.14.5
 2019-05-23 07:16:10.724624: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument
 Traceback (most recent call last):
   File "/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py", line 16, in &lt;module&gt;
     tf.app.run()
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 125, in run
     _sys.exit(main(argv))
   File "/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py", line 12, in main
     restored = io_ops.restore_v2(path, ["a"], [""], [tf.float32])
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py", line 1486, in restore_v2
     ctx=_ctx)
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py", line 1511, in restore_v2_eager_fallback
     attrs=_attrs, ctx=_ctx, name=name)
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 66, in quick_execute
     six.raise_from(core._status_to_exception(e.code, message), None)
   File "&lt;string&gt;", line 3, in raise_from
 tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument [Op:RestoreV2]
 [Finished in 80.3s with exit code 1]
 [cmd: ['/Library/Frameworks/Python.framework/Versions/3.6/bin/python3', '-u', '/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py']]
 [dir: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN]
 [path: /usr/bin:/bin:/usr/sbin:/sbin]
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='29' author='rothn' date='2019-05-23T11:18:24Z'>
 		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='30' author='rothn' date='2019-05-23T11:21:02Z'>
 		Oh, er, I'm guessing version 0.12.2 is not recent enough. I need 1.14.1-dev20190522, right? Haha.
 		</comment>
 		<comment id='31' author='rothn' date='2019-05-23T14:55:50Z'>
 		Sorry! It's definitely fixed. I ran:
 &lt;denchmark-code&gt;pip uninstall tensorflow
 pip install tf-nightly
 &lt;/denchmark-code&gt;
 
 And now it works perfectly. Sorry about that!
 		</comment>
 	</comments>
 </bug>
<commit id='f2be10a6d278f9a4546fa9cded94074959e67302' author='Allen Lavoie' date='2019-05-07 19:01:17-07:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\core\platform\posix\posix_file_system.cc' new_name='tensorflow\core\platform\posix\posix_file_system.cc'>
 		<file_info nloc='321' complexity='74' token_count='2045'></file_info>
 		<method name='tensorflow::PosixRandomAccessFile::Read' parameters='offset,n,result,scratch'>
 				<method_info nloc='27' complexity='8' token_count='165' nesting_level='2' start_line='61' end_line='90'></method_info>
 			<added_lines>66,67,68,69,70,71,72,73,74,75</added_lines>
 			<deleted_lines>65</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>19</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
