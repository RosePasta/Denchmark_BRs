<bug_data>
<bug id='9633' author='tpet' open_date='2017-05-03T15:49:22Z' closed_time='2017-05-05T18:20:38Z'>
 	<summary>SIGSEGV with sparse_add and broadcasting</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 yes, enclosed below
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Ubuntu 16.04
 TensorFlow installed from (source or binary):
 binary via pip
 TensorFlow version (use command below):
 ('v1.0.0-65-g4763edf-dirty', '1.0.1')
 Bazel version (if compiling from source):
 N/A, using pip installation
 CUDA/cuDNN version:
 N/A, CPU-only
 GPU model and memory:
 none
 Exact command to reproduce:
 
 &lt;denchmark-code&gt;from __future__ import print_function
 import numpy as np
 import tensorflow as tf
 
 dense_sz = [1, 1000, 1000]
 dense = tf.constant(1.0, shape=dense_sz, dtype=tf.float32)
 
 sparse_sz = [10, 1000, 1000]
 nnz = 100
 nz_ind = np.random.choice(np.prod(sparse_sz), size=nnz, replace=False)
 nz_ind = np.unravel_index(nz_ind, dims=sparse_sz)
 nz_ind = np.array(nz_ind).T
 assert np.all(nz_ind &lt; np.array(sparse_sz)[None, :])
 # Ensure canonical ordering.
 ind = np.lexsort([nz_ind[:, i].flatten() for i in reversed(range(nz_ind.shape[1]))])
 nz_ind = nz_ind[ind, :]
 print('nz_ind\n', nz_ind)
 
 sparse_plc = tf.sparse_placeholder(tf.float32)
 sparse_sum = tf.sparse_add(dense, sparse_plc)
 init = tf.global_variables_initializer()
 
 with tf.Session() as sess:
     sess.run(init)
     print('after init')
     res = sess.run(sparse_sum, feed_dict={sparse_plc: tf.SparseTensorValue(nz_ind, np.ones((nnz,)), sparse_sz)})
     print('sum\n', res)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;
 
 Running the code above results in
 &lt;denchmark-code&gt;[...]
 after init
 
 Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
 &lt;/denchmark-code&gt;
 
 For lower values of nnz, (nnz = 1) it finishes fine quite often.
 &lt;denchmark-code&gt;[...]
 after init
 sum
  [[[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ..., 
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]]
 
 Process finished with exit code 0
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;
 
 See above.
 	</description>
 	<comments>
 		<comment id='1' author='tpet' date='2017-05-03T17:29:05Z'>
 		&lt;denchmark-link:https://github.com/concretevitamin&gt;@concretevitamin&lt;/denchmark-link&gt;
  can you take a look at this?
 Here's the stacktrace:
 &lt;denchmark-code&gt;PC: @     0x7f37662d027d  (unknown)  raise
     @         0x243417e6       1120  FailureSignalHandler()
     @     0x7f37662d03d0       1472  (unknown)
     @     0x7f3764db3191        128  faulthandler_fatal_error
     @     0x7f37662d03d0  (unknown)  (unknown)
     @         0x2217d353       1664  tensorflow::SparseTensorDenseAddOp&lt;&gt;::Compute()
     @         0x22f76586         96  tensorflow::ThreadPoolDevice::Compute()
     @         0x22f0764b       2464  tensorflow::(anonymous namespace)::ExecutorState::Process()
     @         0x22f13cb1        160  std::_Mem_fn&lt;&gt;::operator()&lt;&gt;()
     @         0x22f13bc3         96  std::_Bind&lt;&gt;::__call&lt;&gt;()
     @         0x22f13b06         64  std::_Bind&lt;&gt;::operator()&lt;&gt;()
     @         0x22f136fd         32  std::_Function_handler&lt;&gt;::_M_invoke()
     @         0x13172ede         32  std::function&lt;&gt;::operator()()
     @         0x2325fcc8        128  tensorflow::thread::EigenEnvironment::ExecuteTask()
     @         0x2325f019        208  Eigen::ThreadPoolTempl&lt;&gt;::WorkerLoop()
     @         0x2325ec5e         32  Eigen::ThreadPoolTempl&lt;&gt;::ThreadPoolTempl()::{lambda()#1}::operator()()
     @         0x2325eacd         32  std::_Function_handler&lt;&gt;::_M_invoke()
     @         0x13172ede         32  std::function&lt;&gt;::operator()()
     @         0x2325e934         48  tensorflow::thread::EigenEnvironment::CreateThread()::{lambda()#1}::operator()()
     @         0x2325e76d         32  std::_Function_handler&lt;&gt;::_M_invoke()
     @         0x13172ede         32  std::function&lt;&gt;::operator()()
     @         0x23296f0c         32  tensorflow::(anonymous namespace)::GoogleThread::FuncThread::Run()
     @         0x23d41427        448  Thread::ThreadBody()
     @     0x7f37662c6890        176  start_thread
     @     0x7f3765d2237d  (unknown)  clone
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='tpet' date='2017-05-03T23:10:18Z'>
 		Did you modify anything else in the code? The version shows dirty.
 &lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/av8ramit&gt;@av8ramit&lt;/denchmark-link&gt;
  are the pip installs built from a dirty git repo?
 		</comment>
 		<comment id='3' author='tpet' date='2017-05-04T01:14:38Z'>
 		I'm taking a look.
 		</comment>
 		<comment id='4' author='tpet' date='2017-05-04T10:56:25Z'>
 		The results look the same with tf pip-upgraded to ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0').
 		</comment>
 		<comment id='5' author='tpet' date='2017-05-04T18:59:16Z'>
 		Thanks for the report &lt;denchmark-link:https://github.com/tpet&gt;@tpet&lt;/denchmark-link&gt;
 .
 I am submitting a "fix" that instead of segfaulting, return a proper error status that requires both operands have matching shapes.  This has always been the assumption in the , but was &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_add_op.cc#L56&gt;not enforced&lt;/denchmark-link&gt;
 .  The patch should show up in master within a day or two.
 Do you exactly require the functionality of "sparse + dense -&gt; dense, with dense-to-sparse broadcast"?  If so, I'd like to mark this as contributions welcome (the current kernels do not support this broadcast pattern).
 However, if you can get away with "sparse + dense -&gt; sparse, with dense-to-sparse broadcast", we already have &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_ops.py#L316&gt;sparse_dense_cwise_add()&lt;/denchmark-link&gt;
  that does this.  Let us know, and we can expose this function as a public method.
 		</comment>
 		<comment id='6' author='tpet' date='2017-05-05T10:40:45Z'>
 		&lt;denchmark-link:https://github.com/concretevitamin&gt;@concretevitamin&lt;/denchmark-link&gt;
  I agree raising an exception is much better. The "sparse + dense -&gt; dense, with dense-to-sparse broadcast" really seems not that much useful, compared to "sparse + dense -&gt; sparse, with dense-to-sparse broadcast". Now I actually don't need this particular thing.
 My initial use case was a bit different. In the process of trying to get some reasonable behavior I happened to find the segfault and created this example.
 My use case is this:
 D + reduce_sum(a * S)
 where D and the result is dense [1 n2 n3 n4]
 S is sparse [n1 n2 n3 n4]
 a is dense [n1 1 1 1] and broadcasts to S.
 So far I hasn't been able to get to some reasonable performance with this.
 		</comment>
 		<comment id='7' author='tpet' date='2017-05-05T10:50:52Z'>
 		I'm using a pip installation so it will take some time until it propagates down to me so feel free to close the issue if you think it is resolved. Thanks.
 		</comment>
 		<comment id='8' author='tpet' date='2017-05-05T18:20:38Z'>
 		Okay, closing for now.  For the reduce, take a look at tf.sparse_reduce_sum() and/or tf.sparse_reduce_sum_reduce().
 		</comment>
 	</comments>
 </bug>
<commit id='50b836addfed6b49fc823987e9301f1b6eeef90c' author='Zongheng Yang' date='2017-05-04 12:30:04-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\core\kernels\sparse_tensor_dense_add_op.cc' new_name='tensorflow\core\kernels\sparse_tensor_dense_add_op.cc'>
 		<file_info nloc='85' complexity='7' token_count='632'></file_info>
 		<method name='tensorflow::SparseTensorDenseAddOp::Compute' parameters='ctx'>
 				<method_info nloc='50' complexity='3' token_count='404' nesting_level='2' start_line='37' end_line='102'></method_info>
 			<added_lines>50,51,52,57,58,59,60,61,62,63,64,65,66,67,68,69,95,96,97</added_lines>
 			<deleted_lines>50,51,56,57,58,59,85,86</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\ops\sparse_ops.py' new_name='tensorflow\python\ops\sparse_ops.py'>
 		<file_info nloc='480' complexity='52' token_count='3297'></file_info>
 		<method name='sparse_add' parameters='a,b,thresh'>
 				<method_info nloc='25' complexity='7' token_count='233' nesting_level='0' start_line='236' end_line='315'></method_info>
 			<added_lines>244,245</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
