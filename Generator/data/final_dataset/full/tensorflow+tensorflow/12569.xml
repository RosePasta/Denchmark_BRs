<bug_data>
<bug id='12569' author='malsulaimi' open_date='2017-08-24T21:04:12Z' closed_time='2018-09-09T16:44:11Z'>
 	<summary>missing Documentation of the method AttentionWrapper.zero_state(...)</summary>
 	<description>
 Hello ,
 I have noticed that the method AttentionWrapper.zero_state( batch_size,dtype) does not have any description of its functionality in the  documentation website , below is a reference link :
 &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper&lt;/denchmark-link&gt;
 
 I really hope that this gets fixed , I have spent a couple of days trying to debug a code that I have written until I realized that I was misusing the method .
 thank you
 	</description>
 	<comments>
 		<comment id='1' author='malsulaimi' date='2017-08-24T23:18:58Z'>
 		Can you describe more about how you were misusing the method? (adding &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
  ...)
 		</comment>
 		<comment id='2' author='malsulaimi' date='2017-08-25T11:19:37Z'>
 		I did not know that this would return a AttenionWrapperState , I though that this would return a normal initial state  , and thus I was using it as the below :
 &lt;denchmark-code&gt;def decoding_layer(dec_input, encoder_state,
                    target_sequence_length, max_target_sequence_length,
                    rnn_size,
                    num_layers, target_vocab_to_int, target_vocab_size,
                    batch_size, keep_prob, decoding_embedding_size , encoder_outputs):
     """
     Create decoding layer
     :param dec_input: Decoder input
     :param encoder_state: Encoder state
     :param target_sequence_length: The lengths of each sequence in the target batch
     :param max_target_sequence_length: Maximum length of target sequences
     :param rnn_size: RNN Size
     :param num_layers: Number of layers
     :param target_vocab_to_int: Dictionary to go from the target words to an id
     :param target_vocab_size: Size of target vocabulary
     :param batch_size: The size of the batch
     :param keep_prob: Dropout keep probability
     :param decoding_embedding_size: Decoding embedding size
     :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)
     """
     # 1. Decoder Embedding
     dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))
     dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)
 
     # 2. Construct the decoder cell
     def create_cell(rnn_size):
         lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,
                                             initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))
         drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)
         return drop
 
 
     dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])
     #dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  
 
     #attention details 
         attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) 
 
 attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)
 
 attn_zero = attn_cell.zero_state(batch_size , tf.float32 )
 
 attn_zero = attn_zero.clone(cell_state = encoder_state)
 
 new_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())
 
 """out_cell = tf.contrib.rnn.OutputProjectionWrapper(
             attn_cell, target_vocab_size, reuse=True
         )"""
 
     #end of attention 
 
     output_layer = Dense(target_vocab_size,
                          kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))
 
     with tf.variable_scope("decode"):
         train_decoder_out = decoding_layer_train(new_state, attn_cell, dec_embed_input, 
                          target_sequence_length, max_target_sequence_length, output_layer, keep_prob)
 
     with tf.variable_scope("decode", reuse=True):
         infer_decoder_out = decoding_layer_infer(new_state, attn_cell, dec_embeddings, 
                              target_vocab_to_int['&lt;GO&gt;'], target_vocab_to_int['&lt;EOS&gt;'], max_target_sequence_length, 
                              target_vocab_size, output_layer, batch_size, keep_prob)
 
     return (train_decoder_out, infer_decoder_out)
 
 """
 DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
 """
 #tests.test_decoding_layer(decoding_layer)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='malsulaimi' date='2017-08-25T18:35:53Z'>
 		OK, we'll mark it for adding documentation. FWIW we provide source code, and so you can also see the return type for that function (though its not a substitute for documentation, you can always go read the code as well)...
 		</comment>
 		<comment id='4' author='malsulaimi' date='2017-08-25T19:06:14Z'>
 		Thats great . Thanks
 		</comment>
 		<comment id='5' author='malsulaimi' date='2017-09-29T23:10:33Z'>
 		Added better documentation and an example to the docstrings for both BeamSearchDecoder and AttentionWrapper.__init__ and AttentionWrapper.zero_state.
 		</comment>
 		<comment id='6' author='malsulaimi' date='2017-09-29T23:10:43Z'>
 		Should show up in a day or two.
 		</comment>
 		<comment id='7' author='malsulaimi' date='2018-09-08T18:36:05Z'>
 		Nagging Assignee &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 : It has been 342 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		</comment>
 		<comment id='8' author='malsulaimi' date='2018-09-09T16:44:07Z'>
 		Fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/aae34fa7e35d9c3931cae49bfc20384dd20dffec&gt;aae34fa&lt;/denchmark-link&gt;
 .
 		</comment>
 	</comments>
 </bug>
<commit id='aae34fa7e35d9c3931cae49bfc20384dd20dffec' author='Eugene Brevdo' date='2017-09-29 17:36:01-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py' new_name='tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py'>
 		<file_info nloc='793' complexity='72' token_count='3972'></file_info>
 		<method name='state_size' parameters='self'>
 				<method_info nloc='9' complexity='3' token_count='66' nesting_level='1' start_line='1190' end_line='1203'></method_info>
 			<added_lines>1191,1192,1193,1194,1195</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='zero_state' parameters='self,batch_size,dtype'>
 				<method_info nloc='32' complexity='5' token_count='206' nesting_level='1' start_line='1205' end_line='1255'></method_info>
 			<added_lines>1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py' new_name='tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py'>
 		<file_info nloc='420' complexity='36' token_count='2806'></file_info>
 		<modified_lines>
 			<added_lines>133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,176</added_lines>
 			<deleted_lines>133,144</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
