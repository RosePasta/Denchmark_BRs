<bug_data>
<bug id='27455' author='LynnHo' open_date='2019-04-03T09:07:50Z' closed_time='2019-04-09T21:52:57Z'>
 	<summary>TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model.</summary>
 	<description>
 System information
 
 OS Platform and Distribution: Linux Ubuntu 18.04
 TensorFlow installed from: binary
 TensorFlow version: 2.0.0-alpha0
 Python version: 3.6.8
 
 Describe the current behavior
 I built a keras model with only a tf.nn.relu, but the gradient seems to be None after being decorated by @tf.function
 Code to reproduce the issue
 
 tf.nn.relu + tf.keras.Model + @tf.function (this is the only case that produce None gradient)
 
 import tensorflow as tf
 
 z = tf.keras.Input(())
 h = tf.nn.relu(z)
 m = tf.keras.Model(z, h)
 
 @tf.function
 def f(x):  # with @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 &gt;&gt;&gt; None
 1.2 tf.nn.relu + tf.keras.Model without @tf.function
 def f(x):  # without @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 &gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)
 
 tf.keras.layers.ReLU() + tf.keras.Model + @tf.function
 
 import tensorflow as tf
 
 z = tf.keras.Input(())
 h = tf.keras.layers.ReLU()(z)
 m = tf.keras.Model(z, h)
 
 @tf.function
 def f(x):  # with @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 &gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)
 2.2 tf.keras.layers.ReLU() + tf.keras.Model without @tf.function
 def f(x):  # without @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 &gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)
 
 only tf.nn.relu
 
 import tensorflow as tf
 m = tf.nn.relu
 
 @tf.function
 def f(x):  # with @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 &gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)
 So, I think its the problem between tf.nn.relu and tf.keras.Model? Besides, tf.nn.tanh has the same problem.
 	</description>
 	<comments>
 		<comment id='1' author='LynnHo' date='2019-04-05T21:28:52Z'>
 		&lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
  I think something is broken with the keras graph here since the tape isn't seeing it.
 Can you take a look, or help triage to the right person?
 		</comment>
 		<comment id='2' author='LynnHo' date='2019-04-08T23:10:25Z'>
 		I have a fix for this that will be submitted soon.
 		</comment>
 		<comment id='3' author='LynnHo' date='2019-04-09T21:52:58Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27455&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27455&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='244cb0b925902a29c6a39c62fd1b80cb3797051b' author='A. Unique TensorFlower' date='2019-04-09 14:47:12-07:00'>
 	<dmm_unit complexity='0.10714285714285714' interfacing='1.0' size='0.6785714285714286'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\keras\engine\base_layer.py' new_name='tensorflow\python\keras\engine\base_layer.py'>
 		<file_info nloc='1243' complexity='302' token_count='7302'></file_info>
 		<method name='_make_op' parameters='self,inputs'>
 				<method_info nloc='25' complexity='6' token_count='243' nesting_level='1' start_line='2154' end_line='2184'></method_info>
 			<added_lines>2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>32,59</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\keras\layers\tensorflow_op_layer_test.py' new_name='tensorflow\python\keras\layers\tensorflow_op_layer_test.py'>
 		<file_info nloc='197' complexity='27' token_count='1830'></file_info>
 		<method name='test_gradient_tape_in_function' parameters='self'>
 				<method_info nloc='13' complexity='1' token_count='173' nesting_level='1' start_line='205' end_line='226'></method_info>
 			<added_lines>205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_gradient_tape_in_function.f' parameters='x'>
 				<method_info nloc='6' complexity='1' token_count='40' nesting_level='2' start_line='213' end_line='218'></method_info>
 			<added_lines>213,214,215,216,217,218</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>26,27,28,227</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
