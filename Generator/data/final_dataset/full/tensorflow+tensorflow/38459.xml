<bug_data>
<bug id='38459' author='siavash-khodadadeh' open_date='2020-04-11T16:59:53Z' closed_time='2020-06-08T18:45:06Z'>
 	<summary>Both 'mean' and 'variance' must be None when is_training is True and exponential_avg_factor == 1.0</summary>
 	<description>
 System information
 
 Have I written custom code (as opposed to using a stock
 example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g.,
 Linux Ubuntu 16.04): Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
 the issue happens on mobile device:  No
 TensorFlow installed from (source or
 binary): - TensorFlow version (use command below): 2.2.0-dev20200411
 Python version: 3.6.3
 Bazel
 version (if compiling from source):
 GCC/Compiler version (if compiling from
 source):
 CUDA/cuDNN version: 10.1- GPU model and memory:
 
 You can collect some of this information using our environment capture
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 When instantiating a batch norm layer like this:
 tf.keras.layers.BatchNormalization(momentum=0.0, center=True, scale=False, name='bn1')
 I get the error:
 Both 'mean' and 'variance' must be None when is_training is True and exponential_avg_factor == 1.0
 Describe the expected behavior
 It is not always the expected behavior. Consider meta-learning for example. We are going to see just one batch of training data and we want to adapt all means and variances to this batch, this means the momentum should be zero.
 Then after applying a few training iterations, we evaluate on the same batch norm layer with training=False and that also should work fine.
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 &lt;denchmark-code&gt;import tensorflow as tf
 import numpy as np
 
 inp = tf.keras.layers.Input(shape=(84, 84, 3))
 dense = tf.keras.layers.Conv2D(10, 3, activation=None)(inp)
 bn = tf.keras.layers.BatchNormalization(momentum=0.0, center=True, scale=False, name='bn1')(dense)
 rel = tf.keras.layers.ReLU()(bn)
 flat = tf.keras.layers.Flatten()(rel)
 out = tf.keras.layers.Dense(1, )(flat)
 model = tf.keras.models.Model(inputs=inp, outputs=out)
 
 model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())
 model.fit(x=np.random.uniform(size=(4, 84, 84, 3)), y=np.random.uniform(size=(4, 1)), epochs=1)
 model.evaluate(x=np.random.uniform(size=(3, 84, 84, 3)), y=np.random.uniform(size=(3, 1)))
 model.predict(x=np.random.uniform(size=(1, 84, 84, 3)))
 &lt;/denchmark-code&gt;
 
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	</description>
 	<comments>
 		<comment id='1' author='siavash-khodadadeh' date='2020-04-12T12:15:20Z'>
 		&lt;denchmark-link:https://github.com/siavash-khodadadeh&gt;@siavash-khodadadeh&lt;/denchmark-link&gt;
 , I have tried to reproduce issue. But I have faced no issue in this code. For you reference link of gist is &lt;denchmark-link:https://gist.github.com/khimraj/25f451b7abaade36dd5390fdabb5a935&gt;here&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='2' author='siavash-khodadadeh' date='2020-04-12T16:21:07Z'>
 		&lt;denchmark-link:https://github.com/khimraj&gt;@khimraj&lt;/denchmark-link&gt;
 , I guess you have to update to 2.2.0-dev20200412 (tf-nightly) and then this happens.
 In previous versions, this does not cause an error, and that is why I think it might be a bug.
 		</comment>
 		<comment id='3' author='siavash-khodadadeh' date='2020-04-12T16:34:01Z'>
 		&lt;denchmark-link:https://github.com/siavash-khodadadeh&gt;@siavash-khodadadeh&lt;/denchmark-link&gt;
 , I have used tensorflow version 2.2.0-rc2.
 		</comment>
 		<comment id='4' author='siavash-khodadadeh' date='2020-04-12T16:49:44Z'>
 		&lt;denchmark-link:https://github.com/khimraj&gt;@khimraj&lt;/denchmark-link&gt;
  Sorry, I am not very familiar with versioning. Is rc2 newer than nightly build?
 I mean if this is not resolved, will this be the behavior in TF 2.2.1 or TF 2.3.x?
 		</comment>
 		<comment id='5' author='siavash-khodadadeh' date='2020-04-13T12:34:11Z'>
 		Was able to run the code without any issues on &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/2ad5bccc80083753db0f6657c83ef1ab/38459.ipynb&gt;TF v2.2.0-rc2&lt;/denchmark-link&gt;
 .
 Facing an error stating , while running on &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/e85b2e37339e4c478b666b53edab264f/38459-tf-nightly.ipynb#scrollTo=aDTL7bbKf82y&gt;TF-nightly&lt;/denchmark-link&gt;
 . Please find the attached gist. Thanks!
 		</comment>
 		<comment id='6' author='siavash-khodadadeh' date='2020-04-13T15:00:19Z'>
 		Hi &lt;denchmark-link:https://github.com/siavash-khodadadeh&gt;@siavash-khodadadeh&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/khimraj&gt;@khimraj&lt;/denchmark-link&gt;
   , I think  tf-nightly-2.2.0-dev20200412 is the latest build(12/04/2020) than tf-2.2.0-rc2(28/03/2020) . Generally it should not throw an error since if is_training is true  that implies whitening is done on the same input batch rather than over the moving average statistic of the set.
 		</comment>
 		<comment id='7' author='siavash-khodadadeh' date='2020-04-13T15:14:22Z'>
 		Hello &lt;denchmark-link:https://github.com/abhilash1910&gt;@abhilash1910&lt;/denchmark-link&gt;
 , Sorry, I did not get what you mean by whitening. I checked and when training is False, there is no issue. The only problem is when is_training is True and if I understand you correctly, this is a bug and should be fixed.
 		</comment>
 		<comment id='8' author='siavash-khodadadeh' date='2020-04-13T15:33:22Z'>
 		Hi &lt;denchmark-link:https://github.com/siavash-khodadadeh&gt;@siavash-khodadadeh&lt;/denchmark-link&gt;
  ,by whitening I mean using the mean as 0 and sd/variance as 1 before passing an input vector xi {1,2...m} through an activation unit .I think according to the implementation there is mention of a batch normalised vector value X = x-E[x]/sqrt(var(x) + epsilon) where E[x] is the expectation and epsilon is just added to prevent division by 0. There are other parameters gamma and beta which govern the gradient step (scale and shift rule).During training as false there is a moving average over the input vector of previous layers which is "whitened" by applying the above metric and then passed into the activation units  - relu sigmoid etc of the current layer. Yes you are correct this should not be occurring if is_training is true because in that case there is no need of such moving statistic over previous layers,input  batch of current state is whitened and used.If mean and variance are none in this case(training=true) then it is not possible to determine the normalised value which is to be passed into the activation unit.
 		</comment>
 		<comment id='9' author='siavash-khodadadeh' date='2020-04-13T15:45:12Z'>
 		&lt;denchmark-link:https://github.com/abhilash1910&gt;@abhilash1910&lt;/denchmark-link&gt;
  Thank you for your explanation.
 		</comment>
 		<comment id='10' author='siavash-khodadadeh' date='2020-04-13T15:57:12Z'>
 		Sure &lt;denchmark-link:https://github.com/siavash-khodadadeh&gt;@siavash-khodadadeh&lt;/denchmark-link&gt;
  , glad I could help.
 		</comment>
 		<comment id='11' author='siavash-khodadadeh' date='2020-04-16T13:49:46Z'>
 		Hi &lt;denchmark-link:https://github.com/siavash-khodadadeh&gt;@siavash-khodadadeh&lt;/denchmark-link&gt;
  , &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
  ,&lt;denchmark-link:https://github.com/khimraj&gt;@khimraj&lt;/denchmark-link&gt;
  ,&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
  in the latest nightly release- tf-nightly 2.2.0.dev20200416 the issue is present. I found that according to this commit :&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/84f2ec1d60b5bb14a59ccef8f8fa7eb5a1096e8f#diff-ef8609a43751227afcaacc838670a96f&gt;84f2ec1#diff-ef8609a43751227afcaacc838670a96f&lt;/denchmark-link&gt;
     on  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py&lt;/denchmark-link&gt;
   ,there is mention that if exponential_avg_factor==1.0and is _training ==true ,then the mean and variance should be none. By comparing commits , I found that since is_training  considers current batch inputs,by setting the exponential average to 1 would imply effective mean =0 and variance=0 (mean=(1-exponential_average_factor)exponential_average_factor) and same for variance). However, I think this requires further analysis since  momentum can be 0 on a current input batch and this should not raise exception on mean and variance values since it only belongs to the current batch.
 		</comment>
 		<comment id='12' author='siavash-khodadadeh' date='2020-04-16T14:12:32Z'>
 		&lt;denchmark-link:https://github.com/abhilash1910&gt;@abhilash1910&lt;/denchmark-link&gt;
 
 I wanted to ask a question. When we set the momentum to zero during training=True, we expect the moving mean and moving variance to be updated based on just the latest batch the model sees. Is that correct?
 		</comment>
 		<comment id='13' author='siavash-khodadadeh' date='2020-04-16T14:53:55Z'>
 		Yes &lt;denchmark-link:https://github.com/siavash-khodadadeh&gt;@siavash-khodadadeh&lt;/denchmark-link&gt;
  , that is correct .
 		</comment>
 		<comment id='14' author='siavash-khodadadeh' date='2020-04-16T15:12:43Z'>
 		Thank you for your response, &lt;denchmark-link:https://github.com/abhilash1910&gt;@abhilash1910&lt;/denchmark-link&gt;
 . Just wanted to make sure that I understood it accurately.
 		</comment>
 		<comment id='15' author='siavash-khodadadeh' date='2020-06-08T18:45:07Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38459&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38459&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='3cfba9571bcc4be237bfdfa3498c66073ae59280' author='Scott Zhu' date='2020-06-08 11:36:54-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\ops\nn_impl.py' new_name='tensorflow\python\ops\nn_impl.py'>
 		<file_info nloc='894' complexity='44' token_count='5502'></file_info>
 		<modified_lines>
 			<added_lines>1618,1619,1620,1621,1622</added_lines>
 			<deleted_lines>1618,1619,1620,1621,1622,1623,1624,1625,1626,1627</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
