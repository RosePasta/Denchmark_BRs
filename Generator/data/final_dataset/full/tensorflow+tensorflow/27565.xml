<bug_data>
<bug id='27565' author='llan-ml' open_date='2019-04-06T09:31:57Z' closed_time='2019-05-03T01:11:15Z'>
 	<summary>[TF==2.0.0a0] @tf.function raises ValueError when computing gradients</summary>
 	<description>
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
 TensorFlow version (use command below): pip install tensorflow(-gpu)==2.0.0a0
 Python version: 3.6
 
 Describe the current behavior
 The code executes normally, but raise ValueError when computing gradients (tape.gradient) if I decorate the training function with @tf.function. The traceback is as follows:
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 ~/Workspaces/fgenl/run.py in ()
      80     for batch_id in range(num_batches_each_epoch):
      81         batch_data = data_generator.get_data() # v2
 ---&gt; 82         loss, outputs = train_one_step(batch_data) # v2
      83         # _, loss, outputs, inputs = sess.run([opt_op, loss_, outputs_, batch_data])
      84         if loss_metrics is None:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
     424     # This is the first call of __call__, so we have to initialize.
     425     initializer_map = {}
 --&gt; 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
     427     if self._created_variables:
     428       try:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
     368     self._concrete_stateful_fn = (
     369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
 --&gt; 370             *args, **kwds))
     371
     372     def invalid_creator_scope(*unused_args, **unused_kwds):
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
    1311     if self._input_signature:
    1312       args, kwargs = None, None
 -&gt; 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)
    1314     return graph_function
    1315
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
    1578           or call_context_key not in self._function_cache.missed):
    1579         self._function_cache.missed.add(call_context_key)
 -&gt; 1580         graph_function = self._create_graph_function(args, kwargs)
    1581         self._function_cache.primary[cache_key] = graph_function
    1582         return graph_function, args, kwargs
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
    1510             arg_names=arg_names,
    1511             override_flat_arg_shapes=override_flat_arg_shapes,
 -&gt; 1512             capture_by_value=self._capture_by_value),
    1513         self._function_attributes)
    1514
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
     692                                           converted_func)
     693
 --&gt; 694       func_outputs = python_func(*func_args, **func_kwargs)
     695
     696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
     315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
     316         # the function a weak reference to itself to avoid a reference cycle.
 --&gt; 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
     318     weak_wrapped_fn = weakref.ref(wrapped_fn)
     319
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
     684                   optional_features=autograph_options,
     685                   force_conversion=True,
 --&gt; 686               ), args, kwargs)
     687
     688         # Wrapping around a decorator allows checks like tf_inspect.getargspec
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
     390     return _call_unconverted(f, args, kwargs)
     391
 --&gt; 392   result = converted_f(*effective_args, **kwargs)
     393
     394   # The converted function's closure is simply inserted into the function's
 
 /tmp/tmpx0xgcbu3.py in tf__train_one_step(batch_data)
       6     outputs = ag__.converted_call(model, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (batch_data,), {})
       7     loss, info = ag__.converted_call('calculate_loss', loss_object, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (outputs, batch_data), {})
 ----&gt; 8   gradients = ag__.converted_call('gradient', tape, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (loss, model.trainable_variables), {})
       9   update_list = [(grad, var) for grad, var in ag__.converted_call(zip, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (gradients, model.trainable_variables), {}) if grad is not None]
      10   ag__.converted_call('apply_gradients', optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (update_list,), {})
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
     265
     266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):
 --&gt; 267     return _call_unconverted(f, args, kwargs)
     268
     269   # internal_convert_user_code is for example turned off when issuing a dynamic
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)
     186     return f.__self__.call(args, kwargs)
     187
 --&gt; 188   return f(*args, **kwargs)
     189
     190
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
     954         flat_sources,
     955         output_gradients=output_gradients,
 --&gt; 956         unconnected_gradients=unconnected_gradients)
     957
     958     if not self._persistent:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)
      70       sources,
      71       output_gradients,
 ---&gt; 72       compat.as_str(unconnected_gradients.value))
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _aggregate_grads(gradients)
     565         indexed_slices = ops.IndexedSlices(
     566             grad,
 --&gt; 567             math_ops.range(grad.shape[0]),
     568             constant_op.constant(grad.shape.as_list()))
     569         indexed_slices_list.append(indexed_slices)
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in range(start, limit, delta, dtype, name)
    1258   with ops.name_scope(name, "Range", [start, limit, delta]) as name:
    1259     start = ops.convert_to_tensor(start, dtype=dtype, name="start")
 -&gt; 1260     limit = ops.convert_to_tensor(limit, dtype=dtype, name="limit")
    1261     delta = ops.convert_to_tensor(delta, dtype=dtype, name="delta")
    1262
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
    1048   preferred_dtype = deprecation.deprecated_argument_lookup(
    1049       "dtype_hint", dtype_hint, "preferred_dtype", preferred_dtype)
 -&gt; 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
    1051
    1052
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
    1106       name=name,
    1107       preferred_dtype=dtype_hint,
 -&gt; 1108       as_ref=False)
    1109
    1110
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
    1184
    1185     if ret is None:
 -&gt; 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    1187
    1188     if ret is NotImplemented:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
     302                                          as_ref=False):
     303   _ = as_ref
 --&gt; 304   return constant(v, dtype=dtype, name=name)
     305
     306
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
     243   """
     244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
 --&gt; 245                         allow_broadcast=True)
     246
     247
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
     281       tensor_util.make_tensor_proto(
     282           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
 --&gt; 283           allow_broadcast=allow_broadcast))
     284   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
     285   const_tensor = g.create_op(
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
     453   else:
     454     if values is None:
 --&gt; 455       raise ValueError("None values not supported.")
     456     # if dtype is provided, forces numpy array to be the type
     457     # provided if possible.
 
 ValueError: None values not supported.
 
 Describe the expected behavior
 The code should also execute normally when using @tf.function.
 Code to reproduce the issue
 Sorry, I do not have a simple snippet to reproduce this issue. But could you find something in the traceback? See below please.
 	</description>
 	<comments>
 		<comment id='1' author='llan-ml' date='2019-04-06T12:18:35Z'>
 		I found a simple script to reproduce this issue, and it seems that the op tf.sparse.sparse_dense_matmul causes this issue.
 # -*- coding: utf-8 -*-
 # @Author  : Lin Lan (ryan.linlan@gmail.com)
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import numpy as np
 import scipy as sp
 import tensorflow as tf
 
 
 def sparse_to_tuple(sparse_mx):
     """Convert sparse matrix to tuple representation."""
     def to_tuple(mx):
         if not sp.sparse.isspmatrix_coo(mx):
             mx = mx.tocoo()
         coords = np.vstack((mx.row, mx.col)).transpose()
         values = mx.data
         shape = mx.shape
         return coords, values, shape
 
     if isinstance(sparse_mx, list):
         for i in range(len(sparse_mx)):
             sparse_mx[i] = to_tuple(sparse_mx[i])
     else:
         sparse_mx = to_tuple(sparse_mx)
 
     return sparse_mx
 
 
 def construct_tf_sparse_tensor(sp_sparse_matrix):
     if not sp.sparse.issparse(sp_sparse_matrix):
         raise TypeError
 
     tuple_format = sparse_to_tuple(sp_sparse_matrix)
     tf_sparse_tensor = tf.sparse.SparseTensor(
         indices=tuple_format[0],
         values=tuple_format[1],
         dense_shape=tuple_format[2])
     tf_sparse_tensor = tf.sparse.reorder(tf_sparse_tensor)
     return tf_sparse_tensor
 
 
 weights = tf.Variable(
     tf.random.uniform([512, 128]),
     dtype=tf.float32,
     trainable=True)
 optimizer = tf.optimizers.Adam()
 
 
 @tf.function
 def train(x):
     with tf.GradientTape() as tape:
         embeddings = tf.sparse.sparse_dense_matmul(
             x,
             weights)
         batch_embeddings = tf.nn.embedding_lookup(
             embeddings, [1, 2, 3, 4, 5, 7, 8, 9, 10])
         # embeddings = tf.nn.embedding_lookup(
         #     embeddings, list(range(512)))
         logits = tf.matmul(batch_embeddings, embeddings, transpose_b=True)
         loss = tf.reduce_mean(logits)
     gradients = tape.gradient(loss, [weights])
     optimizer.apply_gradients(zip(gradients, [weights]))
 
 
 random_array = np.random.rand(512, 512)
 sparse_array = sp.sparse.csr_matrix(
     np.asarray(random_array &gt; 0.5, dtype=np.float32))
 sparse_tensor = construct_tf_sparse_tensor(sparse_array)
 train(sparse_tensor)
 
 To let the above code compute gradients normally, one way is to uncomment embeddings = tf.nn.embedding_lookup(embeddings, list(range(512))).
 		</comment>
 		<comment id='2' author='llan-ml' date='2019-04-08T14:55:17Z'>
 		The Tape is unable to see the variables.
 So, use tape.watch(embeddings) after sparse_dense_matmul(). (Sparse Tensors cannot be watched, so watching x is not an option).
 That solves the problem.
 		</comment>
 		<comment id='3' author='llan-ml' date='2019-04-08T17:36:40Z'>
 		&lt;denchmark-link:https://github.com/captain-pool&gt;@captain-pool&lt;/denchmark-link&gt;
   The above code works well in eager mode. It only fails when we use  decoration. So, the tape only cannot see the variable with AutoGraph?
 		</comment>
 		<comment id='4' author='llan-ml' date='2019-04-08T18:01:29Z'>
 		Well, that is exactly what's happening. For Autograph it works only with watch(). I'm still looking through the codebase to find the reason.
 		</comment>
 		<comment id='5' author='llan-ml' date='2019-05-02T01:07:45Z'>
 		There seems to be a slightly more helpful error in tf-nightly, but it looks like it's unrelated to tape.watch or autograph. The shape of embeddings seems to be partially unknown after the sparse_dense_matmul, and this line fixed in my tests:
 &lt;denchmark-code&gt;        embeddings = tf.sparse.sparse_dense_matmul(
             x,
             weights)
         embeddings.set_shape((512, 128))  # This removes the error.
         batch_embeddings = tf.nn.embedding_lookup(
             embeddings, tf.constant([1, 2, 3, 4, 5, 7, 8, 9, 10]))
 &lt;/denchmark-code&gt;
 
 Reassigning to triage the tape error.
 		</comment>
 		<comment id='6' author='llan-ml' date='2019-05-02T18:15:51Z'>
 		There is a bug in the backprop code, where it does math_ops.range(grad.shape[0]) which uses the static shape of the grad tensor, which might be known (a number) or None. To use the dynamic shape we need something like math_ops.range(array_ops.shape(grad)[0]).
 		</comment>
 		<comment id='7' author='llan-ml' date='2019-05-03T01:11:16Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27565&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27565&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='110f0610ed0cf52d256e414906cf91d4e9d657e7' author='Alexandre Passos' date='2019-05-02 17:58:38-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\eager\backprop.py' new_name='tensorflow\python\eager\backprop.py'>
 		<file_info nloc='594' complexity='114' token_count='3320'></file_info>
 		<method name='_aggregate_grads' parameters='gradients'>
 				<method_info nloc='24' complexity='9' token_count='198' nesting_level='0' start_line='546' end_line='583'></method_info>
 			<added_lines>571,572</added_lines>
 			<deleted_lines>571,572</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
