<bug_data>
<bug id='35100' author='olk' open_date='2019-12-13T22:06:00Z' closed_time='2019-12-20T00:22:24Z'>
 	<summary>Error occurred when finalizing GeneratorDataset iterator</summary>
 	<description>
 System information
 
 OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
 TensorFlow installed from: binary
 TensorFlow version: 2.1.0rc0-1
 Keras version: 2.2.4-tf
 Python version: 3.8
 GPU model and memory: 2x GTX 1080 Ti 11GB"`
 
 Describe the current behavior
 executing Tensorflow's MNIST handwriting example produces error:
 the error dissapears if the code doesn't use OneDeviceStrategy or MirroredStrategy
 
 W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 
 Code to reproduce the issue
 &lt;denchmark-code&gt;import tensorflow as tf
  import tensorflow_datasets as tfds
  import time
  
  from tensorflow.keras.optimizers import Adam
  
  def build_model():
      filters = 48
      units = 24
      kernel_size = 7
      learning_rate = 1e-4
      model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
      ])
      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])
      return model
  
  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
  mnist_train, mnist_test = datasets['train'], datasets['test']
  
  num_train_examples = info.splits['train'].num_examples
  num_test_examples = info.splits['test'].num_examples
  
  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')
  
  BUFFER_SIZE = 10000
  BATCH_SIZE = 32
  
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  
  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  
  with strategy.scope():
    model = build_model()
  
  epochs=5
  start = time.perf_counter()
  model.fit(
          train_dataset,
          validation_data=eval_dataset,
          steps_per_epoch=num_train_examples/epochs,
          validation_steps=num_test_examples/epochs,
          epochs=epochs)
  elapsed = time.perf_counter() - start
  print('elapsed: {:0.3f}'.format(elapsed))
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='olk' date='2019-12-16T08:20:34Z'>
 		&lt;denchmark-link:https://github.com/olk&gt;@olk&lt;/denchmark-link&gt;
 , I tried reproducing the reported issue but it worked as expected. Please take a look at the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/0f769c920b16c68d0b2c7d238256e0c9/untitled311.ipynb&gt;gist&lt;/denchmark-link&gt;
 . Thanks!
 		</comment>
 		<comment id='2' author='olk' date='2019-12-16T22:04:34Z'>
 		upgraded to TensorFlow version: 2.1.0-rc1 - still get errors
 please note that I execute the example at real hardware (not colab)
 		</comment>
 		<comment id='3' author='olk' date='2019-12-17T14:33:12Z'>
 		I guess this issue is related to using Tensorflow with Python-3.8.
 		</comment>
 		<comment id='4' author='olk' date='2019-12-17T18:30:53Z'>
 		I've downgraded my system:
 
 Python 3.7.4
 Tensorflow-2.1.0-rc1
 
 Still facing the error:
 
 Train for 30000.0 steps, validate for 5000.0 steps
 Epoch 1/2
 2019-12-17 19:21:54.361240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
 2019-12-17 19:21:55.824790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
 2019-12-17 19:21:56.980785: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
 Relying on driver to perform ptx compilation. This message will be only logged once.
 30000/30000 [==============================] - 115s 4ms/step - loss: 0.0856 - accuracy: 0.9761 - val_loss: 0.0376 - val_accuracy: 0.9879
 Epoch 2/2
 29990/30000 [============================&gt;.] - ETA: 0s - loss: 0.0152 - accuracy: 0.99582019-12-17 19:25:28.372294: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 30000/30000 [==============================] - 111s 4ms/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 0.0375 - val_accuracy: 0.9889
 2019-12-17 19:25:40.010887: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 2019-12-17 19:25:40.031138: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 elapsed: 226.391
 
 seams to be related to tensorflow-2.1.0-rc1
 		</comment>
 		<comment id='5' author='olk' date='2019-12-17T23:18:41Z'>
 		I have the same issue.  Originally I was using:
 tensorflow/tensorflow:nightly-gpu-py3
 which has:
 2.1.0-dev20191106
 Then I tried upgrading tensorflow in the container with:
 &lt;denchmark-link:https://files.pythonhosted.org/packages/a9/fa/8ac34cf1369deb4f523a80eeb86ec0be3dd44139bfb42c45dd3829d6aff5/tf_nightly_gpu-2.1.0.dev20191217-cp36-cp36m-manylinux2010_x86_64.whl&gt;https://files.pythonhosted.org/packages/a9/fa/8ac34cf1369deb4f523a80eeb86ec0be3dd44139bfb42c45dd3829d6aff5/tf_nightly_gpu-2.1.0.dev20191217-cp36-cp36m-manylinux2010_x86_64.whl&lt;/denchmark-link&gt;
 
 I still have the same issue.
 		</comment>
 		<comment id='6' author='olk' date='2019-12-18T00:12:21Z'>
 		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  this seems to be an issue related to tf.distribute + tf.keras. In particular, as far as I can tell, the user code does not use  but the error indicates that GeneratorDataset is used. Could you please triage? Thanks.
 		</comment>
 		<comment id='7' author='olk' date='2019-12-18T00:36:51Z'>
 		The error log suggests that the training completed fine, but something at the end caused this error. Neither the training or validation dataset are using generators, so it does seem weird that there is a generator related error. Also it seems like it's just a warning - since the user's print statement at the end "elapsed.." did get printed as well.
 &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
  is tf.data.Dataset.from_generator the only time generator_dataset_op is used? Or could there be something else that could trigger it?
 &lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;
  could it be something related to any of the fault tolerance callbacks?
 		</comment>
 		<comment id='8' author='olk' date='2019-12-18T15:18:07Z'>
 		I can verify this error with python 3.8 and python-tensorflow-opt-cuda 2.1.0rc1-2 on arch linux.  This error is weirdly not present if you import only the generator from tensorflow, and everything else from Keras.
 		</comment>
 		<comment id='9' author='olk' date='2019-12-19T01:16:10Z'>
 		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
  I realized that generator dataset is used in multi-device iterator. This seems related to newly added support for cancellation in tf.data.
 The good news is that, as you pointed out, the warning is superfluous. The bad news is that, as far as I can tell, this warning will be present for all tf.distribute jobs in TF 2.1 (given how tf.data cancellation is implemented). I will look into having a fix for this cherrypicked into TF 2.1.
 		</comment>
 		<comment id='10' author='olk' date='2019-12-19T01:21:35Z'>
 		Ah, great, thanks &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='11' author='olk' date='2019-12-20T00:22:26Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='12' author='olk' date='2020-02-12T02:54:54Z'>
 		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
  Any update on this? I'm getting this exact message and it looks like my model.fit is not doing it's thing on validation dataset during training.
 		</comment>
 		<comment id='13' author='olk' date='2020-02-13T22:40:16Z'>
 		I'm also experiencing this on the official Google Cloud Platform tf2-gpu.2-1.m42 image with Python 3.5.3.
 		</comment>
 		<comment id='14' author='olk' date='2020-03-05T00:47:14Z'>
 		It might help to point out that this error is being printed out once per active GPU.
 2020-03-05 00:40:55.703069: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled 
 		</comment>
 		<comment id='15' author='olk' date='2020-03-08T18:58:20Z'>
 		Problem description
 I am using TensorFlow 2.1.0 for image classification under Centos Linux. As my image training data set is growing, I have to start using a Generator as I do not have enough RAM to hold all pictures. I have coded the Generator based on this &lt;denchmark-link:https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly&gt;tutorial&lt;/denchmark-link&gt;
 .
 It seems to work fine, until my program all the sudden gets killed without an error message:
 &lt;denchmark-code&gt;Epoch 6/30
 2020-03-08 13:28:11.361785: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 43/43 [==============================] - 54s 1s/step - loss: 5.6839 - accuracy: 0.4669
 Epoch 7/30
 2020-03-08 13:29:05.511813: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
  7/43 [===&gt;..........................] - ETA: 1:04 - loss: 4.3953 - accuracy: 0.5268Killed
 &lt;/denchmark-code&gt;
 
 Looking at the growing memory consumption with linux's top, I suspect a memory leak?
 What I have tried
 
 
 The above suggestion to switch to TF nightly build version. For me it did not help, also downgrading to TF2.0.1 did not help
 
 
 There is a discussion  suggesting that it is important, that 'steps_per_epoch' and 'batch size' correspond (whatever this exactly means) - I played with it without finding any improvement.
 
 
 Trying to narrow down by looking at the size development of all variables in my Generator
 
 
 Relevant code snippets
 &lt;denchmark-code&gt;class DataGenerator(tf.keras.utils.Sequence):
     'Generates data for Keras'
     def __init__(self, list_IDs, labels, dir, n_classes):
         'Initialization'
         config = configparser.ConfigParser()
         config.sections()
         config.read('config.ini')
 
         self.dim = (int(config['Basics']['PicHeight']),int(config['Basics']['PicWidth']))
         self.batch_size = int(config['HyperParameter']['batchsize'])
         self.labels = labels
         self.list_IDs = list_IDs
         self.dir = dir
         self.n_channels = 3
         self.n_classes = n_classes
         self.on_epoch_end()        
 
 
     def __len__(self):
         'Denotes the number of batches per epoch'
         return math.floor(len(self.list_IDs) / self.batch_size)
 
     def __getitem__(self, index):
         'Generate one batch of data'
         # Generate indexes of the batch
         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
 
         # Find list of IDs
         list_IDs_temp = [self.list_IDs[k] for k in indexes]
 
         # Generate data
         X, y = self.__data_generation(list_IDs_temp)
 
         return X, y, [None]
 &lt;/denchmark-code&gt;
 
 being called by
 &lt;denchmark-code&gt;        training_generator = datagenerator.DataGenerator(train_files, labels, dir, len(self.class_names))
         self.model.fit(x=training_generator,
                     use_multiprocessing=False,
                     workers=6, 
                     epochs=self._Epochs, 
                     steps_per_epoch = len(training_generator),
                     callbacks=[LoggingCallback(self.logger.debug)])
 &lt;/denchmark-code&gt;
 
 I have tried running the exact same code under Windows 10, which gives me the following error:
 &lt;denchmark-code&gt;Epoch 9/30
 2020-03-08 20:49:37.555692: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 41/41 [==============================] - 75s 2s/step - loss: 2.0167 - accuracy: 0.3133
 Epoch 10/30
 2020-03-08 20:50:52.986306: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
  1/41 [..............................] - ETA: 2:36 - loss: 1.6237 - accuracy: 0.39062020-03-08 20:50:57.689373: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at matmul_op.cc:480 : Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
 2020-03-08 20:50:57.766163: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
          [[{{node MatMul_6}}]]
 Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 
  2/41 [&gt;.............................] - ETA: 2:02 - loss: 1.6237 - accuracy: 0.3906Traceback (most recent call last):
   File "run.py", line 83, in &lt;module&gt;
     main()
   File "run.py", line 70, in main
     accuracy, num_of_classes = train_Posture(unique_name)
   File "run.py", line 31, in train_Posture
     acc = neuro.train(picdb, train_ids, test_ids, "Posture")
   File "A:\200307 3rd Try\neuro.py", line 161, in train
     callbacks=[LoggingCallback(self.logger.debug)])
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py", line 819, in fit
     use_multiprocessing=use_multiprocessing)
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 342, in fit
     total_epochs=epochs)
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 128, in run_one_epoch
     batch_outs = execution_function(iterator)
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py", line 98, in execution_function
     distributed_function(input_fn))
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 568, in __call__
     result = self._call(*args, **kwds)
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 599, in _call
     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 2363, in __call__
     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 1611, in _filtered_call
     self.captured_inputs)
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 1692, in _call_flat
     ctx, args, cancellation_manager=cancellation_manager))
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 545, in call
     ctx=ctx)
   File "C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\execute.py", line 67, in quick_execute
     six.raise_from(core._status_to_exception(e.code, message), None)
   File "&lt;string&gt;", line 3, in raise_from
 tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
          [[node MatMul_6 (defined at A:\200307 3rd Try\neuro.py:161) ]]
 Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
  [Op:__inference_distributed_function_764]
 
 Function call stack:
 distributed_function
 
 2020-03-08 20:51:00.785175: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='16' author='olk' date='2020-03-09T21:03:42Z'>
 		&lt;denchmark-link:https://github.com/Tuxius&gt;@Tuxius&lt;/denchmark-link&gt;
  I seem have the same issue. Should this be reopened? Why is it closed anyway?
 On my side it seems to happen when early stoppping triggers. So it does not cancel the training.
 And I get no OOM message.
 &lt;denchmark-code&gt;156/156 [==============================] - 86s 550ms/step - loss: 0.0676 - acc: 0.9790 - val_loss: 0.7805 - val_acc: 0.8569
 Epoch 17/1000
 156/156 [==============================] - 86s 550ms/step - loss: 0.0711 - acc: 0.9748 - val_loss: 0.4852 - val_acc: 0.8875
 Epoch 18/1000
 156/156 [==============================] - 86s 550ms/step - loss: 0.0638 - acc: 0.9772 - val_loss: 1.1247 - val_acc: 0.8371
 2020-03-09 20:41:21.425818: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 [0.8876654] [5]
 WARNING:tensorflow:sample_weight modes were coerced from
   {'output': '...'}
     to
   ['...']
 Train for 156 steps, validate on 11715 samples
 Epoch 1/1000
 156/156 [==============================] - 88s 566ms/step - loss: 0.4006 - acc: 0.8377 - val_loss: 1.3430 - val_acc: 0.5214
 Epoch 2/1000
 156/156 [==============================] - 86s 550ms/step - loss: 0.1554 - acc: 0.9437 - val_loss: 0.7877 - val_acc: 0.8219
 Epoch 3/1000
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='17' author='olk' date='2020-03-10T12:35:50Z'>
 		&lt;denchmark-link:https://github.com/Tuxius&gt;@Tuxius&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/PhilipMay&gt;@PhilipMay&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
  the same problem here
 		</comment>
 		<comment id='18' author='olk' date='2020-03-11T20:33:57Z'>
 		Since more and more people (&lt;denchmark-link:https://github.com/PhilipMay&gt;@PhilipMay&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/drsantos89&gt;@drsantos89&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/4doge&gt;@4doge&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/Tuxius&gt;@Tuxius&lt;/denchmark-link&gt;
 , ...) report to also still have the same issue I have reopened it &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37515&gt;#37515&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='19' author='olk' date='2020-03-17T21:38:46Z'>
 		I have the same problem (using fit_generator)
 I'm using windows 10, python 3.6 and tensorflow 2.1 (cuda 10.1, cudnn 7.6.5)
 		</comment>
 		<comment id='20' author='olk' date='2020-03-24T07:57:02Z'>
 		same issue here, with CPU, tf 2.1 and python 3.7
 		</comment>
 		<comment id='21' author='olk' date='2020-03-24T18:10:43Z'>
 		same issue here, scales with validation_freq (occurs on those epochs for which validation is performed.)
 I'm using the tensorflow image tensorflow/tensorflow:2.1.0-gpu-py3 from docker hub: &lt;denchmark-link:https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1&gt;https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1&lt;/denchmark-link&gt;
 
 Within the image, the system is 18.04.3 LTS, cuda 10.1.243, python Python 3.6.9
 		</comment>
 		<comment id='22' author='olk' date='2020-03-25T15:03:18Z'>
 		I'm using keras Sequence and saw this issue too. However it's weird if I only use one worker, it works well. I didn't use multiprocessing tho. 2 worker thread will leave the training hang and data show this error
 		</comment>
 		<comment id='23' author='olk' date='2020-03-25T15:55:33Z'>
 		There was a bug for Keras sequence multi-processing implementation that was fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/e918c6e6fab5d0005fcde83d57e92b70343d3553&gt;e918c6e&lt;/denchmark-link&gt;
 . This will be available in TF 2.2 and should be already available in TF nightly.
 		</comment>
 		<comment id='24' author='olk' date='2020-03-26T03:35:14Z'>
 		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
  thanks for the heads up! I updated to tf-nightly-gpu and the error:
  went away.
 		</comment>
 		<comment id='25' author='olk' date='2020-03-26T15:38:54Z'>
 		Good to know. Note that the warning and the memory leak are unrelated.
 		</comment>
 		<comment id='26' author='olk' date='2020-03-28T19:01:37Z'>
 		
 @jsimsa thanks for the heads up! I updated to tf-nightly-gpu and the error:
 W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled went away.
 
 I updated today. I confirm that.
 		</comment>
 		<comment id='27' author='olk' date='2020-03-28T19:04:26Z'>
 		but the training result   seems not stable.   train/val  loss/accuracy up and downs too much.
 		</comment>
 		<comment id='28' author='olk' date='2020-03-28T23:53:47Z'>
 		For me the results aren't reproducible from run to run either, even with
 tf.random.set_seed(), but I suspect it has to do with multiple workers for
 my image augmentation generator.
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Sat, Mar 28, 2020, 12:04 PM flydragon2018 ***@***.***&gt; wrote:
  but the training result seems not stable. train/val loss/accuracy up and
  downs too much.
 
  —
  You are receiving this because you commented.
  Reply to this email directly, view it on GitHub
  &lt;#35100 (comment)&gt;,
  or unsubscribe
  &lt;https://github.com/notifications/unsubscribe-auth/ABI2DNKEM2P6L6YZLNMF3FLRJZC4XANCNFSM4J2WWO2A&gt;
  .
 
 
 
 		</comment>
 		<comment id='29' author='olk' date='2020-06-12T15:55:38Z'>
 		Had the same problem. Memory leak and crash after some number of epochs. Looks like the ModelCheckpoint callback is a culprit. Removing it solved the issue.
 		</comment>
 		<comment id='30' author='olk' date='2020-07-28T16:21:59Z'>
 		
 I guess this issue is related to using Tensorflow with Python-3.8.
 
 It is not related to Python 3.8. I have the same problem with Python 3.7.4
 		</comment>
 		<comment id='31' author='olk' date='2020-08-01T17:15:53Z'>
 		I found a reason for the problem on my computer - YMMV. I was using the ModelCheckpoint callback to save the best model, and if there was a model with that name already in the folder, I got the error. Removing or renaming the model with that name fixed the issue. Windows 10 system, Python 3.7.4.
 		</comment>
 		<comment id='32' author='olk' date='2020-08-10T18:06:18Z'>
 		Adding this code snippet fixes this issue for me when using RTX GPUs:
 &lt;denchmark-code&gt;devices = tf.config.experimental.list_physical_devices('GPU')
 tf.config.experimental.set_memory_growth(devices[0], True)
 &lt;/denchmark-code&gt;
 
 This is something I have to do in my training scripts as well. Might help someone 👍
 		</comment>
 		<comment id='33' author='olk' date='2020-09-07T01:48:15Z'>
 		&lt;denchmark-link:https://github.com/610265158/face_landmark/issues/34#issuecomment-615152235&gt;610265158/face_landmark#34 (comment)&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='34' author='olk' date='2020-10-03T09:02:54Z'>
 		Ideas from stackoverflow.  I just directly copy the code from deeplearning.ai in colab. A part of it goes like this:
 `train_generator = train_datagen.flow_from_directory(
 'horse-or-human/',  # This is the source directory for training images
 target_size=(300, 300),  # All images will be resized to 300x300
 batch_size=128,
 # Since we use binary_crossentropy loss, we need binary labels
 class_mode='binary')
 history = model.fit(
 train_generator,
 steps_per_epoch=8,
 epochs=15,
 verbose=1)`
 and there are 1027 images. 128*8=1024, less than 1027. I set steps_per_epoch to 9, the error disappear.
 So, for me the .
 At least this is one of the cases for the error.
 Here is the original answer &lt;denchmark-link:url&gt;https://stackoverflow.com/questions/60000573/error-occurred-when-finalizing-generatordataset-iterator-cancelled-operation-w&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='b6edd34c5858ab0ab4380da774e7e2fd81a92da0' author='Jiri Simsa' date='2019-12-19 16:21:22-08:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\core\kernels\data\captured_function.cc' new_name='tensorflow\core\kernels\data\captured_function.cc'>
 		<file_info nloc='755' complexity='140' token_count='5843'></file_info>
 		<method name='tensorflow::data::CapturedFunction::CapturedFunction' parameters='metadata,captured_inputs'>
 				<method_info nloc='4' complexity='1' token_count='39' nesting_level='2' start_line='851' end_line='854'></method_info>
 			<added_lines>852,853</added_lines>
 			<deleted_lines>852,854</deleted_lines>
 		</method>
 		<method name='tensorflow::data::CapturedFunction::Create' parameters='ctx,metadata,argument_name,out_function'>
 				<method_info nloc='10' complexity='1' token_count='89' nesting_level='2' start_line='482' end_line='491'></method_info>
 			<added_lines>484</added_lines>
 			<deleted_lines>484</deleted_lines>
 		</method>
 		<method name='tensorflow::data::CapturedFunction::Create' parameters='ctx,metadata,argument_name,out_function'>
 				<method_info nloc='10' complexity='1' token_count='88' nesting_level='2' start_line='482' end_line='491'></method_info>
 			<added_lines>484</added_lines>
 			<deleted_lines>484</deleted_lines>
 		</method>
 		<method name='tensorflow::data::InstantiatedCapturedFunction::RunInstantiated' parameters='args,rets'>
 				<method_info nloc='34' complexity='2' token_count='247' nesting_level='2' start_line='714' end_line='750'></method_info>
 			<added_lines>729</added_lines>
 			<deleted_lines>731</deleted_lines>
 		</method>
 		<method name='tensorflow::data::InstantiatedCapturedFunction::InstantiatedCapturedFunction' parameters='lib,f_handle,ret_types,runner,cancellation_manager,captured_func'>
 				<method_info nloc='10' complexity='1' token_count='84' nesting_level='2' start_line='620' end_line='629'></method_info>
 			<added_lines>622</added_lines>
 			<deleted_lines>623,628</deleted_lines>
 		</method>
 		<method name='tensorflow::data::CapturedFunction::Create' parameters='ctx,metadata,captured_inputs,out_function'>
 				<method_info nloc='9' complexity='1' token_count='66' nesting_level='2' start_line='494' end_line='502'></method_info>
 			<added_lines>496</added_lines>
 			<deleted_lines>496</deleted_lines>
 		</method>
 		<method name='tensorflow::data::CapturedFunction::Create' parameters='ctx,metadata,captured_inputs,out_function'>
 				<method_info nloc='9' complexity='1' token_count='67' nesting_level='2' start_line='494' end_line='502'></method_info>
 			<added_lines>496</added_lines>
 			<deleted_lines>496</deleted_lines>
 		</method>
 		<method name='tensorflow::data::InstantiatedCapturedFunction::InstantiatedCapturedFunction' parameters='lib,f_handle,ret_types,runner,captured_func'>
 				<method_info nloc='9' complexity='1' token_count='75' nesting_level='2' start_line='619' end_line='627'></method_info>
 			<added_lines>622</added_lines>
 			<deleted_lines>623</deleted_lines>
 		</method>
 		<method name='tensorflow::data::CapturedFunction::CapturedFunction' parameters='metadata,captured_inputs'>
 				<method_info nloc='5' complexity='1' token_count='43' nesting_level='2' start_line='849' end_line='853'></method_info>
 			<added_lines>850,852,853</added_lines>
 			<deleted_lines>852</deleted_lines>
 		</method>
 		<method name='tensorflow::data::CapturedFunction::Instantiate' parameters='ctx,instantiated_captured_function'>
 				<method_info nloc='64' complexity='10' token_count='548' nesting_level='2' start_line='527' end_line='607'></method_info>
 			<added_lines>605</added_lines>
 			<deleted_lines>605,606</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\core\kernels\data\captured_function.h' new_name='tensorflow\core\kernels\data\captured_function.h'>
 		<file_info nloc='138' complexity='13' token_count='861'></file_info>
 		<modified_lines>
 			<added_lines>112,113,193,200,259</added_lines>
 			<deleted_lines>101,113,114,116,195,202,261</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\core\kernels\data\generator_dataset_op.cc' new_name='tensorflow\core\kernels\data\generator_dataset_op.cc'>
 		<file_info nloc='173' complexity='21' token_count='1135'></file_info>
 		<method name='tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::GetNextInternal' parameters='ctx,out_tensors,end_of_sequence'>
 				<method_info nloc='27' complexity='5' token_count='157' nesting_level='4' start_line='120' end_line='153'></method_info>
 			<added_lines>146,148,149</added_lines>
 			<deleted_lines>146,147,149,150</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\core\kernels\data\iterator_ops.h' new_name='tensorflow\core\kernels\data\iterator_ops.h'>
 		<file_info nloc='185' complexity='18' token_count='1165'></file_info>
 		<method name='tensorflow::data::IteratorResource::State::State' parameters='flib_def,pflr,flr,iterator'>
 				<method_info nloc='9' complexity='1' token_count='80' nesting_level='4' start_line='73' end_line='81'></method_info>
 			<added_lines>77,79</added_lines>
 			<deleted_lines>77,79</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
