<bug_data>
<bug id='20516' author='sleighsoft' open_date='2018-07-03T12:25:05Z' closed_time='2018-07-04T00:45:38Z'>
 	<summary>Cannot restore variables with Checkpoint because keys do not align</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): tf-nightly==1.10.0.dev20180609
 Python version: 3.6.5
 Bazel version (if compiling from source): -
 GCC/Compiler version (if compiling from source): -
 CUDA/cuDNN version: -
 GPU model and memory: -
 Exact command to reproduce:
 
 I get the error that is thrown here:
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/util.py#L633&gt;https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/util.py#L633&lt;/denchmark-link&gt;
 
 I cannot provide code to reproduce it. But basically what happens is I have a class that inherits from Checkpointable. It assigns all variables to itself to make them checkpointable. It also assigns an optimizer to itself. I then save the model and restore it. When calling .assert_consumed() on the load_status object of restore(dir, session) it throws an error because some key does not match. The variable it tries to restore is actually in the saved checkpoint it just has a different key then the one it gets from enumerate(self._checkpoint.object_graph_proto.nodes).
 This describes it as good as I can. Sorry for not being able to share the code. I tried to reproduce it but so far I cannot. I believe it is a bug, because I call ckpt.save() and immediately after it ckpt.restore() and I get the exception.
 Output of self._checkpoint.object_by_proto_id.keys() at line 631.
 You can see that some keys are missing (idk why) but theses are the ones I need to restore.
 &lt;denchmark-code&gt;[0, 1, 2, 3, 4, 5, 6, 7, 14, 19, 10, 17, 22, 11, 18, 23, 12, 13]
 &lt;/denchmark-code&gt;
 
 Output of util._serialize_object_graph(self.checkpoint, None) after restore before assert_consumed
 nodes {
   children {
     node_id: 1
     local_name: "model"
   }
   children {
     node_id: 2
     local_name: "save_counter"
   }
 }
 nodes {
   children {
     node_id: 3
     local_name: "_global_step_pretrain"
   }
   children {
     node_id: 4
     local_name: "embedding"
   }
   children {
     node_id: 5
     local_name: "cell"
   }
   children {
     node_id: 6
     local_name: "dense"
   }
   children {
     node_id: 7
     local_name: "_optimizer"
   }
   children {
     local_name: "_checkpoint"
   }
 }
 nodes {
   attributes {
     name: "VARIABLE_VALUE"
     full_name: "initialize_or_restore/save_counter"
     checkpoint_key: "save_counter/.ATTRIBUTES/VARIABLE_VALUE"
   }
 }
 nodes {
   attributes {
     name: "VARIABLE_VALUE"
     full_name: "ConditionedLSTMGenerator2/CONDITIONED_LSTM/pretrain/global_step"
     checkpoint_key: "model/_global_step_pretrain/.ATTRIBUTES/VARIABLE_VALUE"
   }
 }
 nodes {
   attributes {
     name: "VARIABLE_VALUE"
     full_name: "ConditionedLSTMGenerator2/CONDITIONED_LSTM/embedding"
     checkpoint_key: "model/embedding/.ATTRIBUTES/VARIABLE_VALUE"
   }
 }
 nodes {
   children {
     node_id: 8
     local_name: "kernel"
   }
   children {
     node_id: 9
     local_name: "bias"
   }
   attributes {
     name: "OBJECT_CONFIG_JSON"
     checkpoint_key: "model/cell/.ATTRIBUTES/OBJECT_CONFIG_JSON"
   }
 }
 nodes {
   children {
     node_id: 10
     local_name: "kernel"
   }
   children {
     node_id: 11
     local_name: "bias"
   }
   attributes {
     name: "OBJECT_CONFIG_JSON"
     checkpoint_key: "model/dense/.ATTRIBUTES/OBJECT_CONFIG_JSON"
   }
 }
 nodes {
   children {
     node_id: 12
     local_name: "beta1_power"
   }
   children {
     node_id: 13
     local_name: "beta2_power"
   }
   slot_variables {
     original_variable_node_id: 4
     slot_name: "m"
     slot_variable_node_id: 14
   }
   slot_variables {
     original_variable_node_id: 8
     slot_name: "m"
     slot_variable_node_id: 15
   }
   slot_variables {
     original_variable_node_id: 9
     slot_name: "m"
     slot_variable_node_id: 16
   }
   slot_variables {
     original_variable_node_id: 10
     slot_name: "m"
     slot_variable_node_id: 17
   }
   slot_variables {
     original_variable_node_id: 11
     slot_name: "m"
     slot_variable_node_id: 18
   }
   slot_variables {
     original_variable_node_id: 4
     slot_name: "v"
     slot_variable_node_id: 19
   }
   slot_variables {
     original_variable_node_id: 8
     slot_name: "v"
     slot_variable_node_id: 20
   }
   slot_variables {
     original_variable_node_id: 9
     slot_name: "v"
     slot_variable_node_id: 21
   }
   slot_variables {
     original_variable_node_id: 10
     slot_name: "v"
     slot_variable_node_id: 22
   }
   slot_variables {
     original_variable_node_id: 11
     slot_name: "v"
     slot_variable_node_id: 23
   }
 }
 	</description>
 	<comments>
 		<comment id='1' author='sleighsoft' date='2018-07-03T12:43:33Z'>
 		I assume this is something for &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
  ?
 		</comment>
 		<comment id='2' author='sleighsoft' date='2018-07-03T13:15:10Z'>
 		I found the culprit. Double assigning a variable. Here BasicLSTMCell.
 import tensorflow as tf
 from tensorflow.python.training.checkpointable import base as checkpointable
 from tensorflow.python.training.checkpointable import util
 
 class Model(checkpointable.Checkpointable):
 
   def __init__(self):
     self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)
     self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)
     # self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)
     out = self.cell(tf.constant([[1.]]), self.cell.zero_state(1, tf.float32))
     self.optimizer = tf.train.AdamOptimizer()
     self.optimizer.minimize(tf.reduce_sum(out[0]))
     self.session = tf.Session()
     self.checkpoint = tf.train.Checkpoint(model=self)
 
   def init(self):
     print('Init')
     self.session.run(tf.global_variables_initializer())
 
   def save(self):
     print('Save')
     self.checkpoint.save('./tmp/', self.session)
 
   def restore(self):
     print('Restore')
     latest = tf.train.latest_checkpoint('./tmp/')
     load_status = self.checkpoint.restore(latest)
     print(util._serialize_object_graph(self.checkpoint, None))
     load_status.assert_consumed().run_restore_ops(self.session)
 
   def print(self):
     print(self.session.run(self.cell._kernel))
 
 
 m = Model()
 m.init()
 m.print()
 m.save()
 m.restore()
 m.print()
 		</comment>
 		<comment id='3' author='sleighsoft' date='2018-07-03T13:35:35Z'>
 		It works if I double assign a  though. Seems like this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/base.py#L593&gt;https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/base.py#L593&lt;/denchmark-link&gt;
  does not override/clear all previous state.
 Edit: Also does not work for tf.layers.Dense and probably others.
 		</comment>
 		<comment id='4' author='sleighsoft' date='2018-07-03T16:27:24Z'>
 		Huh, good point, looks like the by-name lookup isn't being updated when the dependency is replaced. Thank you for the report!
 		</comment>
 		<comment id='5' author='sleighsoft' date='2018-07-03T17:55:41Z'>
 		Happy to help
 		</comment>
 	</comments>
 </bug>
<commit id='f46627f9ed9cd41b5a1ad9cebbdd4c240846c4e0' author='Allen Lavoie' date='2018-07-03 11:32:16-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\training\checkpointable\base.py' new_name='tensorflow\python\training\checkpointable\base.py'>
 		<file_info nloc='404' complexity='76' token_count='2289'></file_info>
 		<method name='_track_checkpointable' parameters='self,checkpointable,name,overwrite'>
 				<method_info nloc='25' complexity='8' token_count='159' nesting_level='1' start_line='585' end_line='637'></method_info>
 			<added_lines>636</added_lines>
 			<deleted_lines>634</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>504</added_lines>
 			<deleted_lines>504</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\training\checkpointable\tracking_test.py' new_name='tensorflow\python\training\checkpointable\tracking_test.py'>
 		<file_info nloc='137' complexity='9' token_count='1273'></file_info>
 		<method name='testMultipleAssignment' parameters='self'>
 				<method_info nloc='12' complexity='1' token_count='105' nesting_level='1' start_line='36' end_line='49'></method_info>
 			<added_lines>47,48,49</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\training\checkpointable\util_test.py' new_name='tensorflow\python\training\checkpointable\util_test.py'>
 		<file_info nloc='1305' complexity='109' token_count='11281'></file_info>
 		<method name='testAddVariable' parameters='self'>
 				<method_info nloc='50' complexity='3' token_count='372' nesting_level='1' start_line='79' end_line='136'></method_info>
 			<added_lines>105</added_lines>
 			<deleted_lines>105</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
