<bug_data>
<bug id='26048' author='robertnishihara' open_date='2019-02-24T02:14:09Z' closed_time='2019-03-05T17:13:30Z'>
 	<summary>Check failure and silent failures with incorrect usage of tf.custom_gradient (in eager mode).</summary>
 	<description>
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v1.12.0-8779-g2ae06ca491 1.13.0-dev20190223 (as well as 1.12.0)
 Python version: Python 3.6.4 :: Anaconda, Inc.
 
 When tf.custom_gradient is used incorrectly (in this case, the returned grad function returns an empty list, the script segfaults.
 import tensorflow as tf
 
 tf.enable_eager_execution()
 
 @tf.custom_gradient
 def identity(x):
     def grad(dy):
         return []  # This return value is wrong!
     return x, grad
 
 x = tf.Variable(1.0)
 with tf.GradientTape() as t:
     y = identity(x)
 t.gradient(y, [x])
 The t.gradient call fails with
 &lt;denchmark-code&gt;2019-02-23 18:09:14.621207: F ./tensorflow/c/eager/tape.h:642] Check failed: state.op_tape.empty() 
 Abort trap: 6
 &lt;/denchmark-code&gt;
 
 I think it'd be preferable to raise an exception instead of crashing.
 If I instead return too many values from grad, then the script runs, but this is most likely a bug and should probably raise an exception.
 import tensorflow as tf
 
 tf.enable_eager_execution()
 
 @tf.custom_gradient
 def identity(x):
     def grad(dy):
         return 1.0, 2.0  # Too many return values!
     return x, grad
 
 x = tf.Variable(1.0)
 with tf.GradientTape() as t:
     y = identity(x)
 t.gradient(y, [x])
 FYI &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='710b322a8be78b8aff6b148575fcfe5301f42b64' author='Alexandre Passos' date='2019-03-05 09:11:05-08:00'>
 	<dmm_unit complexity='0.8571428571428571' interfacing='0.8571428571428571' size='0.8571428571428571'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\c\eager\tape.h' new_name='tensorflow\c\eager\tape.h'>
 		<file_info nloc='492' complexity='102' token_count='3641'></file_info>
 		<method name='tensorflow::eager::GradientTape&lt;Gradient,BackwardFunction,TapeTensor&gt;::ComputeGradient' parameters='vspace,target_tensor_ids,source_tensor_ids,sources_that_are_targets,output_gradients,result'>
 				<method_info nloc='192' complexity='40' token_count='1459' nesting_level='2' start_line='475' end_line='671'></method_info>
 			<added_lines>642,643,644</added_lines>
 			<deleted_lines>642</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\eager\backprop_test.py' new_name='tensorflow\python\eager\backprop_test.py'>
 		<file_info nloc='1155' complexity='207' token_count='10856'></file_info>
 		<method name='testCustomGradientEmptyError' parameters='self'>
 				<method_info nloc='8' complexity='1' token_count='54' nesting_level='1' start_line='135' end_line='147'></method_info>
 			<added_lines>135,136,137,138,139,140,141,142,143,144,145,146,147</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCustomGradientEmptyError.testCustomGradientEmptyError.identity.grad' parameters='_'>
 				<method_info nloc='2' complexity='1' token_count='8' nesting_level='3' start_line='139' end_line='140'></method_info>
 			<added_lines>139,140</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testCustomGradientEmptyError.identity' parameters='x'>
 				<method_info nloc='3' complexity='1' token_count='11' nesting_level='2' start_line='138' end_line='141'></method_info>
 			<added_lines>138,139,140,141</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>148</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\ops\custom_gradient.py' new_name='tensorflow\python\ops\custom_gradient.py'>
 		<file_info nloc='150' complexity='47' token_count='1118'></file_info>
 		<method name='_eager_mode_decorator' parameters='f,args,kwargs'>
 				<method_info nloc='21' complexity='8' token_count='184' nesting_level='0' start_line='253' end_line='294'></method_info>
 			<added_lines>258,282,283,284,285,286</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_eager_mode_decorator.actual_grad_fn' parameters='result_grads'>
 				<method_info nloc='15' complexity='4' token_count='94' nesting_level='1' start_line='272' end_line='287'></method_info>
 			<added_lines>282,283,284,285,286</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
