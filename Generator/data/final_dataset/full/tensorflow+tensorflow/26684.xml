<bug_data>
<bug id='26684' author='girving' open_date='2019-03-14T05:08:31Z' closed_time='2019-03-31T03:41:34Z'>
 	<summary>Repeatedly allocating a graph and summary writer leaks memory</summary>
 	<description>
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): Source
 TensorFlow version (use command below): v1.12.0-10061-gf3954bf900 1.13.1
 Python version: 3.7.2
 Bazel version (if compiling from source): 0.23.1
 GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 
 Describe the current behavior
 Repeatedly allocating a graph and making a summary writer leaks memory.
 Describe the expected behavior
 Memory should be freed when the graph leaves scope.
 Code to reproduce the issue
 &lt;denchmark-code&gt;#!/usr/bin/env python3
 
 import resource
 import tensorflow as tf
 
 prev = 0
 while True:
     peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
     print(f'peak memory = {peak:,} (+{peak-prev:,})')
     prev = peak
 
     with tf.Graph().as_default(), tf.init_scope():
         tf.contrib.summary.create_file_writer('/tmp/tb')
 &lt;/denchmark-code&gt;
 
 Other info / logs
 Here's what the output looks like:
 &lt;denchmark-code&gt;peak memory = 174,493,696 (+174,493,696)
   
 WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
 For more information, please see:
   * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
   * https://github.com/tensorflow/addons
 If you depend on functionality not listed there, please file an issue.
 
 peak memory = 202,215,424 (+27,721,728)
 peak memory = 202,264,576 (+49,152)
 peak memory = 202,309,632 (+45,056)
 peak memory = 202,358,784 (+49,152)
 peak memory = 202,432,512 (+73,728)
 peak memory = 202,473,472 (+40,960)
 peak memory = 202,522,624 (+49,152)
 peak memory = 202,567,680 (+45,056)
 peak memory = 202,604,544 (+36,864)
 peak memory = 202,641,408 (+36,864)
 peak memory = 202,694,656 (+53,248)
 peak memory = 202,739,712 (+45,056)
 peak memory = 202,784,768 (+45,056)
 peak memory = 202,829,824 (+45,056)
 peak memory = 202,878,976 (+49,152)
 peak memory = 202,919,936 (+40,960)
 peak memory = 202,981,376 (+61,440)
 peak memory = 203,026,432 (+45,056)
 peak memory = 203,067,392 (+40,960)
 ...
 peak memory = 999,665,664 (+49,152)
 peak memory = 999,718,912 (+53,248)
 peak memory = 999,768,064 (+49,152)
 peak memory = 999,817,216 (+49,152)
 peak memory = 999,866,368 (+49,152)
 peak memory = 999,915,520 (+49,152)
 peak memory = 999,964,672 (+49,152)
 peak memory = 1,000,009,728 (+45,056)
 peak memory = 1,000,058,880 (+49,152)
 peak memory = 1,000,108,032 (+49,152)
 peak memory = 1,000,161,280 (+53,248)
 peak memory = 1,000,202,240 (+40,960)
 peak memory = 1,000,255,488 (+53,248)
 ...
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='girving' date='2019-03-14T15:57:28Z'>
 		Forgot that I was using a slightly messy TensorFlow tree.  I've reconfirmed that the bug persists at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/26705&gt;#26705&lt;/denchmark-link&gt;
 , which is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/5b24fba0857394dab67359963726b3bcce071575&gt;5b24fba&lt;/denchmark-link&gt;
  plus a one line header include addition to make it build on my machine.
 		</comment>
 		<comment id='2' author='girving' date='2019-03-15T16:20:00Z'>
 		No more TF bugs, &lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 ? :)
 		</comment>
 		<comment id='3' author='girving' date='2019-03-15T16:42:58Z'>
 		&lt;denchmark-link:https://github.com/nfelt&gt;@nfelt&lt;/denchmark-link&gt;
  are you familiar with summary writers?
 		</comment>
 		<comment id='4' author='girving' date='2019-03-15T16:45:09Z'>
 		&lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
  To clarify what I wrote wasn't intended to ask you to do anything, was just expressing sympathy as a fellow ex-tensorflow person who occasionally gets added to bugs.
 		</comment>
 		<comment id='5' author='girving' date='2019-03-15T17:15:57Z'>
 		Haha no worries &lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 , I just switched teams this week, so still figuring out what to do with all my github issues :)
 		</comment>
 		<comment id='6' author='girving' date='2019-03-15T20:12:37Z'>
 		Thanks for the report.  I can reproduce this against last night's tf-nightly on both macOS and Linux (for anyone else reproducing, maxrss is in bytes on macOS but kb on linux, so the raw numbers are about 1000x smaller).
 I also could still reproduce this even with tf.init_scope() removed.
 cc &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
  if you have any intuition about what might be causing this.
 		</comment>
 		<comment id='7' author='girving' date='2019-03-15T20:21:13Z'>
 		&lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
  I think the issue is that SUMMARY_WRITER_INIT_OP keeps a strong reference to the graph instead of a weak reference, so the graph can never be GC'd.
 		</comment>
 		<comment id='8' author='girving' date='2019-03-20T19:54:16Z'>
 		It looks like there are two problems, one large and one small:
  Every  has a strong reference to the graph it lives inside (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1921&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1921&lt;/denchmark-link&gt;
 ).  This should probably be a weak reference.  This is presumably the cause of the leak, since we store a long lived reference to the op in  at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/summary_ops_v2.py#L221&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/summary_ops_v2.py#L221&lt;/denchmark-link&gt;
 .
   should be something like a &lt;denchmark-link:https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary&gt;https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary&lt;/denchmark-link&gt;
  with keys being the graphs, not keys being strings that live forever.  Even if we fix the large issue the small issue would remain, and thus pretty much any use of  is a bug.
 		</comment>
 		<comment id='9' author='girving' date='2019-03-21T15:42:45Z'>
 		Update: I made a brief attempt at removing that one reference, but it didn't work, so I think there are others.  Probably a more concerted push is required.
 		</comment>
 		<comment id='10' author='girving' date='2019-03-25T17:51:41Z'>
 		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
  Did you get a chance to look at this?  I want to make sure it isn't dropped, since it's blocking my upgrade of TensorFlow.
 		</comment>
 		<comment id='11' author='girving' date='2019-03-25T17:58:57Z'>
 		&lt;denchmark-link:https://github.com/nfelt&gt;@nfelt&lt;/denchmark-link&gt;
  can you look?
 		</comment>
 		<comment id='12' author='girving' date='2019-03-29T17:37:52Z'>
 		What's the likely ETA here?  I'm still blocked from upgrading due to this, so if no one is planning to fix I'd like to know for my own planning purposes.  In particular, if it's going to be weeks more I will have to fix it myself, but then you have to be happy with my weakref design decisions.
 		</comment>
 		<comment id='13' author='girving' date='2019-03-29T17:49:28Z'>
 		I will try to take a closer look today, but if _SUMMARY_WRITER_INIT_OP isn't the main issue then I don't have a good guess at what the leak might be so it may take some time to figure this out.
 Just so I understand, which upgrade path exactly are you blocked on?  Is this upgrading from tf.summary to tf.contrib.summary?  Or did you notice that this leak occurs across a TF version update?
 		</comment>
 		<comment id='14' author='girving' date='2019-03-29T17:52:22Z'>
 		&lt;denchmark-link:https://github.com/nfelt&gt;@nfelt&lt;/denchmark-link&gt;
  Thanks!  The leak appears going from TensorFlow 1.12 to 1.13, so it's blocking the 1.13 upgrade (and therefore also my Python 3.7 upgrade).
 		</comment>
 		<comment id='15' author='girving' date='2019-03-29T18:00:57Z'>
 		Note that it's quite possibly I simply failed to fix the _SUMMARY_WRITER_INIT_OP reference, but as I mentioned it does seem like every use of _graph_key is a bug, and therefore it feels likely that there are other strong references.
 		</comment>
 		<comment id='16' author='girving' date='2019-03-29T20:41:14Z'>
 		Can you check whether using gc frees the graph?
 I.e., is this a simple reference cycle problem, or is a reference to the graph kept hidden someplace?
 		</comment>
 		<comment id='17' author='girving' date='2019-03-29T20:41:49Z'>
 		Ah... Of course as long as the summary init op reference exists, that won't help.
 		</comment>
 		<comment id='18' author='girving' date='2019-03-29T20:51:13Z'>
 		Nope, gc.collect has no effect.
 		</comment>
 		<comment id='19' author='girving' date='2019-03-29T21:00:48Z'>
 		Per discussion with &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
  I think what's easiest here is to revert the part of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/aa8f428a9310b3fd8371bddf612e480b27618b2e&gt;aa8f428&lt;/denchmark-link&gt;
  that changed this from a graph collection to a python dict.  That seems very likely to be the root cause, and it was changed due to deprecation of global collections, but this is a 1.x-only usage anyway, and that seems like the easiest way to fix the regression.
 		</comment>
 		<comment id='20' author='girving' date='2019-03-29T21:04:38Z'>
 		That sounds good.  Arguably global variables with references to graphs should be even more deprecated. :)
 		</comment>
 		<comment id='21' author='girving' date='2019-03-31T03:41:35Z'>
 		Are you satisfied with the resolution of your issue?
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26684&gt;Yes&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26684&gt;No&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='22' author='girving' date='2019-04-01T21:14:46Z'>
 		I've confirmed that this fixes both my minimized test case and my unminimized original code.  Thank you &lt;denchmark-link:https://github.com/nfelt&gt;@nfelt&lt;/denchmark-link&gt;
 !
 		</comment>
 		<comment id='23' author='girving' date='2019-04-01T22:05:24Z'>
 		Glad to hear that :)
 		</comment>
 	</comments>
 </bug>
<commit id='097fc1cdef5c56d4bb239a5a44bf950f0b1c4d37' author='Nick Felt' date='2019-03-30 20:38:06-07:00'>
 	<dmm_unit complexity='None' interfacing='1.0' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\kernel_tests\summary_ops_test.py' new_name='tensorflow\python\kernel_tests\summary_ops_test.py'>
 		<file_info nloc='962' complexity='118' token_count='7824'></file_info>
 		<method name='testNoMemoryLeak_eagerMode' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='36' nesting_level='1' start_line='733' end_line='736'></method_info>
 			<added_lines>733</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='testEagerMemory' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='36' nesting_level='1' start_line='727' end_line='730'></method_info>
 			<added_lines>727,728,729,730</added_lines>
 			<deleted_lines>727</deleted_lines>
 		</method>
 		<method name='testNoMemoryLeak_graphMode' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='35' nesting_level='1' start_line='727' end_line='730'></method_info>
 			<added_lines>727,728,729,730</added_lines>
 			<deleted_lines>727</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>726,731</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tensorflow\python\ops\summary_ops_v2.py' new_name='tensorflow\python\ops\summary_ops_v2.py'>
 		<file_info nloc='559' complexity='137' token_count='3806'></file_info>
 		<method name='__init__' parameters='self,shared_name,init_op_fn,name,v2'>
 				<method_info nloc='12' complexity='2' token_count='95' nesting_level='1' start_line='208' end_line='220'></method_info>
 			<added_lines>220</added_lines>
 			<deleted_lines>219,220</deleted_lines>
 		</method>
 		<method name='summary_writer_initializer_op' parameters=''>
 				<method_info nloc='6' complexity='2' token_count='25' nesting_level='0' start_line='526' end_line='539'></method_info>
 			<added_lines>539</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>55,56,57</added_lines>
 			<deleted_lines>55,56,221,540,541,542</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
