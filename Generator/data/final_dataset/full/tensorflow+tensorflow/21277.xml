<bug_data>
<bug id='21277' author='nfergu' open_date='2018-07-31T13:23:01Z' closed_time='2018-08-09T17:59:36Z'>
 	<summary>Using TensorFlow's Datasets API causes process to hang in session destructor</summary>
 	<description>
 &lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 
 Yes
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 
 MacOS High Sierra (10.13.1), though we've also seen this happen on Linux as well, we believe.
 
 TensorFlow installed from (source or binary):
 
 Source (but happens with the binary version as well)
 
 TensorFlow version (use command below):
 
 v1.8.0-0-g93bc2e2072 1.8.0
 
 Python version:
 
 Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)
 
 Bazel version (if compiling from source):
 
 0.10.1
 
 GCC/Compiler version (if compiling from source):
 
 Apple LLVM version 9.0.0 (clang-900.0.39.2)
 
 CUDA/cuDNN version:
 
 N/A
 
 GPU model and memory:
 
 N/A
 
 Exact command to reproduce:
 
 Unfortunately the issue isn't that easy to reproduce without running our application (I haven't managed to produce a smaller test case).
 &lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;
 
 Summary:
 We are using TensorFlow's Datasets API. More specifically, we're using tf.data.Dataset.from_generator to create a dataset based on a generator function.
 When Python comes to garbage collect our tf.Session object its destructor makes a call into TensorFlow to delete the session (tf_session.TF_DeleteSession). This call hangs because it's trying to execute a tf.py_func function, but cannot acquire Python's global interpreter lock. The function its trying to execute appears to be the "finalize" function from our dataset.
 This looks to me like a bug in TensorFlow, as (my understanding is) that we shouldn't be able to write code that causes this to happen. Although it's clearly a consequence of our specific use of TensorFlow I can't see that we're doing anything in our application that we shouldn't be.
 More Details:
 When our tf.Session object is garbage collected in Python, its destructor (__del__ method) hangs indefinitely. The problem appears to be this call in BaseSession:
 &lt;denchmark-code&gt;tf_session.TF_DeleteSession(self._session)
 &lt;/denchmark-code&gt;
 
 Running lldb shows the following stack trace:
 &lt;denchmark-code&gt;* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
   * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
     frame #3: 0x000000011279a63b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283
     frame #4: 0x0000000112796eb7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423
     frame #5: 0x0000000112797621 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49
     frame #6: 0x00000001090810e3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67
     frame #7: 0x0000000109d4d809 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector&lt;tensorflow::Tensor, std::__1::allocator&lt;tensorflow::Tensor&gt; &gt; const&amp;, std::__1::vector&lt;tensorflow::Tensor, std::__1::allocator&lt;tensorflow::Tensor&gt; &gt;*) + 649
     frame #8: 0x0000000109cffa21 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97
     frame #9: 0x0000000109cffb8e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #10: 0x0000000109cfd669 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105
     frame #11: 0x0000000109cfd6de _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43
     frame #13: 0x0000000109d0a579 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169
     frame #14: 0x0000000109d0a5fe _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14
     frame #15: 0x000000011226db4d libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, unsigned long long, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 301
     frame #16: 0x000000011226dd50 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::type_index, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 192
     frame #17: 0x0000000109d0c558 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104
     frame #18: 0x0000000109d0c71e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14
     frame #19: 0x00000001122670ff libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63
     frame #20: 0x0000000112267ffd libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 205
     frame #21: 0x000000010b880b42 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546
     frame #22: 0x000000010b88108e _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14
     frame #23: 0x000000010935dfd3 _pywrap_tensorflow_internal.so`TF_DeleteSession + 931
     frame #24: 0x0000000109006e5a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122
     frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568
     frame #26: 0x00000001008443e4 Python`call_function + 612
     frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180
     frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121
     frame #33: 0x000000010089b18a Python`collect + 1418
     frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99
     frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119
     frame #36: 0x000000010087b0e0 Python`Py_Exit + 16
     frame #37: 0x000000010087ef4c Python`handle_system_exit + 252
     frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437
     frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125
     frame #40: 0x00000001008992a4 Python`Py_Main + 1812
     frame #41: 0x0000000100000dfe Python
     frame #42: 0x0000000100000c34 Python
 &lt;/denchmark-code&gt;
 
 It appears that the session's destructor is waiting for an op to complete. The culprit seems to be PyFuncOp, which doesn't get past this line:
 &lt;denchmark-code&gt;py_threadstate = PyGILState_Ensure();
 &lt;/denchmark-code&gt;
 
 So it looks like this op is trying to acquire the GIL but can't. My assumption is that this py_func is the "finalize" function for the dataset (from _GeneratorDataset).
 My assumption is that when Python calls tf_session.TF_DeleteSession(self._session) that the GIL should be released, and so PyFuncOp should then be able to acquire it again. Indeed, when I write an isolated test to try and reproduce this I don't see this problem, and the GIL is acquired successfully.
 Unfortunately, as I mention, I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario, but I haven't been able to isolate exactly what it is about this scenario that causes the problem.
 	</description>
 	<comments>
 		<comment id='1' author='nfergu' date='2018-07-31T14:52:46Z'>
 		Thanks for the report and for diving into the details... this definitely sounds like a bug, although my understanding is the same as yours: tf_session.TF_DeleteSession(self._session) should release the GIL because of this SWIG code block here:
 
 
 
 tensorflow/tensorflow/python/client/tf_session.i
 
 
         Lines 106 to 111
       in
       2826d12
 
 
 
 
 
 
  // Release the Python GIL for the duration of most methods. 
 
 
 
  %exception { 
 
 
 
    Py_BEGIN_ALLOW_THREADS; 
 
 
 
    $action 
 
 
 
    Py_END_ALLOW_THREADS; 
 
 
 
  } 
 
 
 
 
 
 In terms of a reproduction, would you be able to capture a dump of all thread stacks when the problem occurs?  One thing I've found useful in debugging this kind of problem is to set config=tf.ConfigProto(inter_op_parallelism=1, intra_op_parallelism=1) when creating the session. This makes the set of threads less unwieldy, and can sometimes tease out deadlock bugs that are less likely to happen with larger threadpools.
 		</comment>
 		<comment id='2' author='nfergu' date='2018-07-31T15:25:24Z'>
 		Sure, I've included a backtrace of all native threads below. This is with inter_op_parallelism_threads = 1 and intra_op_parallelism_threads = 1.
 I've also included a thread dump from Python as well, in case it's interesting. It doesn't look to me like any of the Python threads should be holding the GIL either. The only interesting one looks like 0x00007000025b4000, which is performing an IO operation (consuming from a multiprocessing queue) that I believe should also have caused the GIL to be released.
 Native threads:
 &lt;denchmark-code&gt;(lldb) thread backtrace all
 * thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
   * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
     frame #3: 0x00000001127b665b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283
     frame #4: 0x00000001127b2ed7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423
     frame #5: 0x00000001127b3641 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49
     frame #6: 0x000000010909d4b3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67
     frame #7: 0x0000000109d69869 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector&lt;tensorflow::Tensor, std::__1::allocator&lt;tensorflow::Tensor&gt; &gt; const&amp;, std::__1::vector&lt;tensorflow::Tensor, std::__1::allocator&lt;tensorflow::Tensor&gt; &gt;*) + 649
     frame #8: 0x0000000109d1ba81 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97
     frame #9: 0x0000000109d1bbee _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #10: 0x0000000109d196c9 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105
     frame #11: 0x0000000109d1973e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43
     frame #13: 0x0000000109d265d9 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169
     frame #14: 0x0000000109d2665e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14
     frame #15: 0x0000000112289cad libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, unsigned long long, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 301
     frame #16: 0x0000000112289eb0 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::type_index, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 192
     frame #17: 0x0000000109d285b8 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104
     frame #18: 0x0000000109d2877e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14
     frame #19: 0x000000011228325f libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63
     frame #20: 0x000000011228415d libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 205
     frame #21: 0x000000010b89cba2 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546
     frame #22: 0x000000010b89d0ee _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14
     frame #23: 0x000000010937a0b2 _pywrap_tensorflow_internal.so`TF_DeleteSession + 290
     frame #24: 0x000000010902319a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122
     frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568
     frame #26: 0x00000001008443e4 Python`call_function + 612
     frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180
     frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121
     frame #33: 0x000000010089b18a Python`collect + 1418
     frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99
     frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119
     frame #36: 0x000000010087b0e0 Python`Py_Exit + 16
     frame #37: 0x000000010087ef4c Python`handle_system_exit + 252
     frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437
     frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125
     frame #40: 0x00000001008992a4 Python`Py_Main + 1812
     frame #41: 0x0000000100000dfe Python
     frame #42: 0x0000000100000c34 Python
   thread #2
     frame #0: 0x00000001018566da libsystem_kernel.dylib`__workq_kernreturn + 10
     frame #1: 0x000000010188c06a libsystem_pthread.dylib`_pthread_wqthread + 1035
     frame #2: 0x000000010188bc4d libsystem_pthread.dylib`start_wqthread + 13
   thread #3
     frame #0: 0x0000000000000000
   thread #4
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x0000000100896738 Python`PyThread_acquire_lock_timed + 312
     frame #3: 0x000000010089ca89 Python`acquire_timed + 137
     frame #4: 0x000000010089cbdd Python`lock_PyThread_acquire_lock + 61
     frame #5: 0x00000001007bb545 Python`_PyCFunction_FastCallDict + 245
     frame #6: 0x00000001008443e4 Python`call_function + 612
     frame #7: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #8: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #9: 0x0000000100843fab Python`fast_function + 219
     frame #10: 0x00000001008443cb Python`call_function + 587
     frame #11: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #12: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #13: 0x0000000100843fab Python`fast_function + 219
     frame #14: 0x00000001008443cb Python`call_function + 587
     frame #15: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #16: 0x000000010084412e Python`fast_function + 606
     frame #17: 0x00000001008443cb Python`call_function + 587
     frame #18: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #19: 0x000000010084412e Python`fast_function + 606
     frame #20: 0x00000001008443cb Python`call_function + 587
     frame #21: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #22: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #23: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #24: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #25: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #26: 0x000000010089c587 Python`t_bootstrap + 71
     frame #27: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #28: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #29: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #5
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #6
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #7
     frame #0: 0x0000000101857592 libsystem_kernel.dylib`read + 10
     frame #1: 0x0000000100895402 Python`_Py_read + 82
     frame #2: 0x000000010089f9b9 Python`os_read + 89
     frame #3: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568
     frame #4: 0x00000001008443e4 Python`call_function + 612
     frame #5: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #6: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #7: 0x0000000100843fab Python`fast_function + 219
     frame #8: 0x00000001008443cb Python`call_function + 587
     frame #9: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #10: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #11: 0x0000000100843fab Python`fast_function + 219
     frame #12: 0x00000001008443cb Python`call_function + 587
     frame #13: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #14: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #15: 0x0000000100843fab Python`fast_function + 219
     frame #16: 0x00000001008443cb Python`call_function + 587
     frame #17: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #18: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #19: 0x0000000100843fab Python`fast_function + 219
     frame #20: 0x00000001008443cb Python`call_function + 587
     frame #21: 0x0000000100848aa0 Python`_PyEval_EvalFrameDefault + 17056
     frame #22: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #23: 0x0000000100843fab Python`fast_function + 219
     frame #24: 0x00000001008443cb Python`call_function + 587
     frame #25: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #26: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #27: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95
     frame #28: 0x000000010079344a Python`function_call + 186
     frame #29: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #30: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457
     frame #31: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #32: 0x0000000100843fab Python`fast_function + 219
     frame #33: 0x00000001008443cb Python`call_function + 587
     frame #34: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #35: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #36: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95
     frame #37: 0x000000010079344a Python`function_call + 186
     frame #38: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #39: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457
     frame #40: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #41: 0x0000000100843fab Python`fast_function + 219
     frame #42: 0x00000001008443cb Python`call_function + 587
     frame #43: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #44: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #45: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95
     frame #46: 0x000000010079344a Python`function_call + 186
     frame #47: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #48: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457
     frame #49: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #50: 0x0000000100843fab Python`fast_function + 219
     frame #51: 0x00000001008443cb Python`call_function + 587
     frame #52: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #53: 0x000000010084412e Python`fast_function + 606
     frame #54: 0x00000001008443cb Python`call_function + 587
     frame #55: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #56: 0x000000010084412e Python`fast_function + 606
     frame #57: 0x00000001008443cb Python`call_function + 587
     frame #58: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #59: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #60: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #61: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #62: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #63: 0x000000010089c587 Python`t_bootstrap + 71
     frame #64: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #65: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #66: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #8
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #9
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, std::__1::function&lt;void ()&gt; &gt; &gt;(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
 &lt;/denchmark-code&gt;
 
 Python threads:
 &lt;denchmark-code&gt;Thread 0x0000700002637000 (most recent call first):
 
 Thread 0x00007000025b4000 (most recent call first):
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py", line 379 in _recv
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py", line 407 in _recv_bytes
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py", line 216 in recv_bytes
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py", line 94 in get
 &lt;REDACTED&gt;
 
 
 Thread 0x0000700001fab000 (most recent call first):
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 299 in wait
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 551 in wait
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 1180 in run
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 916 in _bootstrap_inner
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 884 in _bootstrap
 
 Thread 0x0000700001aa8000 (most recent call first):
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 299 in wait
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 551 in wait
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 1180 in run
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 916 in _bootstrap_inner
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 884 in _bootstrap
 
 Thread 0x0000700000f19000 (most recent call first):
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 299 in wait
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/queue.py", line 173 in get
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/watchdog/observers/api.py", line 360 in dispatch_events
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/watchdog/observers/api.py", line 199 in run
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 916 in _bootstrap_inner
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 884 in _bootstrap
 
 Thread 0x0000700000a16000 (most recent call first):
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/selectors.py", line 577 in select
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py", line 1389 in _run_once
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py", line 421 in run_forever
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 864 in run
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 916 in _bootstrap_inner
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py", line 884 in _bootstrap
 
 Current thread 0x0000000101895340 (most recent call first):
   File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 707 in __del__
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='nfergu' date='2018-07-31T15:54:35Z'>
 		Hmm, I think the Python thread in multiprocessing/connection.py (0x00007000025b4000) maps to native thread 7, which is in _Py_read() and that drops the GIL as well.
 As far as I can tell though, none of the native threads is blocked trying to acquire the GIL:
 
 Thread 1 is waiting on a tensorflow::Notification for the end of the finialize function.
 Thread 2 isn't TF-related.
 Thread 3 is ???
 Thread 4 is a Python thread that's waiting on a Python lock. I don't think this is TF-related.
 Threads 5, 6, 8, and 9 are idle TF/Eigen threadpool threads that are waiting for work. I'm a little surprised to see 4 of them rather than inter_op_parallelism + intra_op_parallelism = 2 threads, but they don't seem to be doing anything concerning.
 Thread 7 is (probably) Python thread 0x00007000025b4000, blocked on a multiprocessing queue.
 
 It would be interesting to know if the PyFunc op in finalization actually started and/or finished. One way to do this is to set the environment variable TF_CPP_MIN_VLOG_LEVEL=1 (which triggers very verbose logging, including each op invocation and completion). Could you try that and capture the part of the log that is produced after the tf.Session destructor begins?
 		</comment>
 		<comment id='4' author='nfergu' date='2018-07-31T16:06:53Z'>
 		Yes, I was also confused by the fact that none of the native threads appear to be waiting to acquire the GIL. I haven't tried TF_CPP_MIN_VLOG_LEVEL=1 yet, but if I surround the GIL lock acquisition with a couple of log statements as follows:
 &lt;denchmark-code&gt;neil-mac-2:tensorflow nferguson$ git diff
 diff --git a/tensorflow/python/lib/core/py_func.cc b/tensorflow/python/lib/core/py_func.cc
 index 22317a348c..d52b5e4a00 100644
 --- a/tensorflow/python/lib/core/py_func.cc
 +++ b/tensorflow/python/lib/core/py_func.cc
 @@ -469,7 +469,9 @@ class PyFuncOp : public OpKernel {
      }
 
      PyGILState_STATE py_threadstate;
 +    LOG(ERROR) &lt;&lt; "About to ensure GIL";
      py_threadstate = PyGILState_Ensure();
 +    LOG(ERROR) &lt;&lt; "Got GIL!";
      bool log_on_error;
      Status s = DoCallPyFunc(&amp;call, &amp;log_on_error);
 &lt;/denchmark-code&gt;
 
 I can observe it printing:
 About to ensure GIL
 but not:
 Got GIL!
 Which is why I mentioned that it was getting to PyGILState_Ensure(), but no further.
 Anyway, I will try with TF_CPP_MIN_VLOG_LEVEL=1.
 		</comment>
 		<comment id='5' author='nfergu' date='2018-07-31T16:24:19Z'>
 		I've set TF_CPP_MIN_VLOG_LEVEL=1 and attached the resulting log from after the call to TF_DeleteSession. Note that this includes my added log line "About to ensure GIL" at the end (see my previous comment). And then the program hangs.
 &lt;denchmark-code&gt;2018-07-31 17:22:11.229247: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229282: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229298: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229311: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229329: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229338: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229356: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229375: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229396: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229413: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229426: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229452: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229462: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229474: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229495: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229503: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229515: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229534: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229543: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229917: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229930: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229961: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229970: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229990: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.229998: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230006: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230014: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230022: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230030: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230066: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230074: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230093: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230101: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230109: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230138: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230155: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230219: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230336: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230346: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230354: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230362: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230370: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230384: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.230401: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.231726: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.231838: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
 2018-07-31 17:22:11.232569: I tensorflow/core/common_runtime/function.cc:584] Graph Initial #nodes 5 #edges 6
 2018-07-31 17:22:11.232583: I tensorflow/core/common_runtime/function.cc:584] Graph Before #nodes 5 #edges 6
 2018-07-31 17:22:11.232599: I tensorflow/core/common_runtime/constant_folding.cc:571] No constant foldable nodes found
 2018-07-31 17:22:11.232644: I tensorflow/core/common_runtime/function.cc:584] Graph ReCopy #nodes 5 #edges 7
 2018-07-31 17:22:11.232673: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: _SOURCE = NoOp[]()
 2018-07-31 17:22:11.232701: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: _SINK = NoOp[]()
 2018-07-31 17:22:11.232724: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: arg0 = _Arg[T=DT_INT64, index=0]()
 2018-07-31 17:22:11.232763: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token="pyfunc_2"](arg0)
 2018-07-31 17:22:11.232802: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: pyfunc_RetVal = _Retval[T=DT_INT64, index=0](PyFunc)
 2018-07-31 17:22:11.232861: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 0 step -6231738800936566896 _SOURCE = NoOp[]() is dead: 0
 2018-07-31 17:22:11.232880: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 2 step -6231738800936566896 arg0 = _Arg[T=DT_INT64, index=0]() is dead: 0
 2018-07-31 17:22:11.232906: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -6231738800936566896 kernel_name: "arg0" tensor { dtype: DT_INT64 shape { } allocation_description { requested_bytes: 8 allocator_name: "cpu" } } }
 2018-07-31 17:22:11.232926: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 3 step -6231738800936566896 PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token="pyfunc_2"](arg0) is dead: 0
 2018-07-31 17:22:11.232938: E tensorflow/python/lib/core/py_func.cc:472] About to ensure GIL
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='6' author='nfergu' date='2018-07-31T20:05:46Z'>
 		It definitely looks like it's failing to acquire the lock :). Is there some way you could log the thread ID on which the PyFunc op is trying to acquire the GIL so we can associate it with the stack trace?
 Also, the presence of multiprocessing is slightly suspicious, because depending on when things are forked the TensorFlow runtime might end up in an illegal state (essentially, the process isn't forkable once a tf.Session has been created). Is it possible to reproduce the problem in a process that doesn't use multiprocessing?
 		</comment>
 		<comment id='7' author='nfergu' date='2018-08-01T15:37:48Z'>
 		I've logged the thread that is executing the PyFunc op. Here's the output:
 &lt;denchmark-code&gt;2018-08-01 15:37:06.264162: E tensorflow/python/lib/core/py_func.cc:473] About to ensure GIL in thread: 0x700011a15000
 2018-08-01 15:37:06.264168: E tensorflow/python/lib/core/py_func.cc:476] Native thread ID is : 4923929
 &lt;/denchmark-code&gt;
 
 The native thread ID is the one we're interested in (4923929). This corresponds to 0x4B2219 in hex.
 Here's what lldb tells us about our threads:
 &lt;denchmark-code&gt;(lldb) thread info all
 thread #1: tid = 0x4b1ea2, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
 
 thread #5: tid = 0x4b1f91, 0x00000001018566da libsystem_kernel.dylib`__workq_kernreturn + 10
 
 thread #7: tid = 0x4b21c4, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 
 thread #8: tid = 0x4b21c6, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 
 thread #9: tid = 0x4b2214, 0x0000000101857592 libsystem_kernel.dylib`read + 10
 
 thread #10: tid = 0x4b2218, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 
 thread #12: tid = 0x4b232d, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 &lt;/denchmark-code&gt;
 
 However, 0x4B2219 is not there! However, if we look at our threads just before the session is destroyed we can see that this thread did previously exist:
 thread #11: tid = 0x4b2219, 0x0000000101855e7e libsystem_kernel.dylib__psynch_cvwait + 10`
 As might be expected, this is one of the Eigen threads:
 &lt;denchmark-code&gt;thread #11
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18
     frame #3: 0x0000000111b605f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x0000000111b6026c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x0000000111b5f898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 568
     frame #6: 0x0000000111b5f55f libtensorflow_framework.so`std::__1::__function::__func&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'(), std::__1::allocator&lt;tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function&lt;void ()&gt;)::'lambda'()&gt;, void ()&gt;::operator()() + 47
 &lt;/denchmark-code&gt;
 
 So it looks like this thread has tried to get the GIL, hasn't managed, and then has stopped or been killed.
 Is it possible that the thread encounters an error when getting the GIL (which is not logged for some reason), and then is killed?
 Regarding the multiprocessing stuff, we are using Python's "spawn" multiprocessing context (multiprocessing.get_context('spawn')), so as I understand it the limitations around spawning processes after sessions have been created do not apply (but in any case we don't spawn any processes after session creation anyway). Having said that, both of the scenarios where this happens in our application do use multiprocessing, so I'm not able to rule this out as a cause.
 I was also wondering why there are 4 Eigen threads (in fact there are 6 before session destruction time), so just to double-check our config I dumped it out. Here it is:
 &lt;denchmark-code&gt;intra_op_parallelism_threads: 1
 inter_op_parallelism_threads: 1
 gpu_options {
   per_process_gpu_memory_fraction: 1.0
   allow_growth: true
 }
 allow_soft_placement: true
 
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='8' author='nfergu' date='2018-08-01T17:41:27Z'>
 		It looks like an error may be occurring, which is killing the thread. Stepping through the PyGILState_Ensure function in lldb, I can see the following happening:
 &lt;denchmark-code&gt;Process 71453 stopped
 * thread #11, stop reason = instruction step over
     frame #0: 0x000000010087d794 Python`PyGILState_Ensure + 100
 Python`PyGILState_Ensure:
 -&gt;  0x10087d794 &lt;+100&gt;: movl   $0x0, 0x88(%rbx)
     0x10087d79e &lt;+110&gt;: jmp    0x10087d75a               ; &lt;+42&gt;
     0x10087d7a0 &lt;+112&gt;: leaq   0x8e1d1(%rip), %rdi       ; "Couldn't create thread-state for new thread"
     0x10087d7a7 &lt;+119&gt;: callq  0x10087a510               ; Py_FatalError
 &lt;/denchmark-code&gt;
 
 This looks to me like the Py_FatalError function is being called with "Couldn't create thread-state for new thread". Soon after this happens the thread dies.
 This seems to correspond to these lines from  (looking at the code here: &lt;denchmark-link:https://github.com/python/cpython/blob/e42b705188271da108de42b55d9344642170aa2b/Python/pystate.c&gt;https://github.com/python/cpython/blob/e42b705188271da108de42b55d9344642170aa2b/Python/pystate.c&lt;/denchmark-link&gt;
 ):
 &lt;denchmark-code&gt;tcur = PyThreadState_New(_PyRuntime.gilstate.autoInterpreterState);
         if (tcur == NULL)
             Py_FatalError("Couldn't create thread-state for new thread");
 &lt;/denchmark-code&gt;
 
 But it's not obvious to me why this would be failing.
 		</comment>
 		<comment id='9' author='nfergu' date='2018-08-01T18:53:50Z'>
 		Is the process terminating when you see the hang? Looking the the PyGILState_Ensure() code, as far as I can tell the only situation in which we'd hit this path is if malloc() returned null. I can't think why that might be happening, but I'd be less surprised if we were in some rarely-hit exit path.
 		</comment>
 		<comment id='10' author='nfergu' date='2018-08-01T19:47:58Z'>
 		Yes, the process is terminating when we see the hang. In fact we can see from the original thread dump that the destructor appears to be called as a consequence of a garbage collection which happens as part of shut-down:
 &lt;denchmark-code&gt;frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180
     frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121
     frame #33: 0x000000010089b18a Python`collect + 1418
     frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99
     frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119
     frame #36: 0x000000010087b0e0 Python`Py_Exit + 16
     frame #37: 0x000000010087ef4c Python`handle_system_exit + 252
     frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437
 &lt;/denchmark-code&gt;
 
 The docs for Py_FinalizeEx say "Undo all initializations made by Py_Initialize() and subsequent use of Python/C API functions" so this could well be why PyThreadState_New subsequently fails.
 I think this is may be why my simplified test case does not reproduce the problem: it looks like the destructor is called as part of a "regular" GC in this test case.
 		</comment>
 		<comment id='11' author='nfergu' date='2018-08-01T20:42:08Z'>
 		Adding a call to gc.collect() just before the process starts shutting-down seems to fix the problem for us, so it does look like "normal" garbage collections are OK, but when the Python VM is shutting-down calling BaseSession's destructor is problematic.
 		</comment>
 		<comment id='12' author='nfergu' date='2018-08-01T21:32:55Z'>
 		Aha, that makes sense. Can you try patching the fix in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/8cd2d6fe9389e93a4182ae9287f2f8325913fe6c&gt;8cd2d6f&lt;/denchmark-link&gt;
  and see if that fixes the problem without having to call ?
 		</comment>
 		<comment id='13' author='nfergu' date='2018-08-02T16:04:36Z'>
 		Yes, that fixes the issue, and I get a log message saying:
 &lt;denchmark-code&gt;2018-08-02 16:58:17.950407: W tensorflow/core/kernels/data/generator_dataset_op.cc:129] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
 	 [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token="pyfunc_2"](arg0)]]
 &lt;/denchmark-code&gt;
 
 Thanks very much for your assistance on this.
 		</comment>
 		<comment id='14' author='nfergu' date='2018-08-02T16:12:03Z'>
 		Thank you very much for digging into the details and providing such a useful report!
 		</comment>
 	</comments>
 </bug>
<commit id='8cd2d6fe9389e93a4182ae9287f2f8325913fe6c' author='Derek Murray' date='2018-08-01 14:31:15-07:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tensorflow\python\lib\core\py_func.cc' new_name='tensorflow\python\lib\core\py_func.cc'>
 		<file_info nloc='448' complexity='93' token_count='3029'></file_info>
 		<method name='tensorflow::PyFuncOp::Compute' parameters='ctx'>
 				<method_info nloc='47' complexity='7' token_count='336' nesting_level='2' start_line='484' end_line='540'></method_info>
 			<added_lines>503,504,505,506,507</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
