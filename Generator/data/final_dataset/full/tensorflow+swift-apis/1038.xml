<bug_data>
<bug id='1038' author='tanmayb123' open_date='2020-07-03T17:44:12Z' closed_time='2020-07-22T17:13:08Z'>
 	<summary>sigmoidCrossEntropy is not differentiable</summary>
 	<description>
 I'm working on an example of Swift for TensorFlow and, for some reason, sigmoidCrossEntropy doesn't seem to be differentiable.
 Code:
 &lt;denchmark-code&gt;@differentiable
 func unetWorkerLoss(model: UNet, input: Tensor&lt;Float&gt;, output: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
 }
 &lt;/denchmark-code&gt;
 
 Error:
 &lt;denchmark-code&gt;/home/tanmay/swift-models/UNet/main.swift:77:5: error: expression is not differentiable
     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
     ^
 /home/tanmay/swift-models/UNet/main.swift:77:5: note: cannot differentiate functions that have not been marked '@differentiable' and that are defined in other files
     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='tanmayb123' date='2020-07-03T17:46:11Z'>
 		However, when I do copy in the function directly from swift-apis with no modifications, and change the function names:
 &lt;denchmark-code&gt;@differentiable(wrt: logits)
 public func tbSigmoidCrossEntropy&lt;Scalar: TensorFlowFloatingPoint&gt;(
   logits: Tensor&lt;Scalar&gt;,
   labels: Tensor&lt;Scalar&gt;,
   reduction: @differentiable (Tensor&lt;Scalar&gt;) -&gt; Tensor&lt;Scalar&gt; = _mean
 ) -&gt; Tensor&lt;Scalar&gt; {
   let device = logits.device
   // This numerically stable implementation is based on the TensorFlow Python API.
   let maxLogitsWithZero = max(logits, Tensor(0, on: device))
   let negAbsLogits = max(logits, -logits)  // Custom `abs` to compute gradients at `0`.
   return reduction(maxLogitsWithZero - logits * labels + log1p(exp(-negAbsLogits)))
 }
 
 @differentiable
 func unetWorkerLoss(model: UNet, input: Tensor&lt;Float&gt;, output: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
     tbSigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
 }
 &lt;/denchmark-code&gt;
 
 It works.
 		</comment>
 		<comment id='2' author='tanmayb123' date='2020-07-15T00:38:17Z'>
 		I don't think the original report is a bug though the error message could be improved to say that sigmoidCrossEntropy is only differentiable with respect to logits.   unetWorkerLoss attempts to differentiate with respect to labels also.  Changing @differentiable to @differentiable(wrt: input) in the code above removes the error.  Full working example:
 &lt;denchmark-code&gt;import TensorFlow
 
 struct UNet: Layer {
   var x = Tensor&lt;Float&gt;(1)
   
   func callAsFunction(_ input: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
      x + input
   }
 }
 
 @differentiable(wrt: input)
 func unetWorkerLoss(model: UNet, input: Tensor&lt;Float&gt;, output: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
   sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
 }
 &lt;/denchmark-code&gt;
 
 It is unexpected (and maybe inefficient) that the workaround of rewriting sigmoidCrossEntropy works but it appears @differentiable is not needed (and functions are always differentiable with respect to all inputs) for differentiation within the same file.  For example, this code compiles and executes without error:
 &lt;denchmark-code&gt;import TensorFlow
 
 //@differentiable(wrt: x) // Code still works if this is uncommented.
 func addXY(x: Tensor&lt;Float&gt;, y: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
    x + y
 }
 
 // No need for @differentiable here either.
 func wrapAddXY(x: Tensor&lt;Float&gt;, y: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
    return addXY(x: x, y: y)
 }
 
 let grad = gradient(at: Tensor&lt;Float&gt;(1), Tensor&lt;Float&gt;(2), in: wrapAddXY)
 print(grad) // (1.0, 1.0)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='tanmayb123' date='2020-07-15T16:55:57Z'>
 		
 I don't think the original report is a bug though the error message could be improved to say that sigmoidCrossEntropy is only differentiable with respect to logits.
 
 Yes, this is accurate: the current diagnostic messages are misleading. Instead, they should read something like:
 &lt;denchmark-code&gt;/home/tanmay/swift-models/UNet/main.swift:77:5: error: expression is not differentiable
     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
     ^
 /home/tanmay/swift-models/UNet/main.swift:77:5: note: 'foo' is marked '@differentiable' only with respect to a smaller subset of arguments: '@differentiable(wrt: logits)'
     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://bugs.swift.org/browse/SR-13224&gt;SR-13224&lt;/denchmark-link&gt;
  tracks improving this diagnostic, we'll look into it soon!
 &lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;
 
 As &lt;denchmark-link:https://github.com/mikowals&gt;@mikowals&lt;/denchmark-link&gt;
  noted, changing  to be explicitly  (not with respect to ) fixes the error:
 &lt;denchmark-code&gt;@differentiable(wrt: (model, input))
 func unetWorkerLoss(model: UNet, input: Tensor&lt;Float&gt;, output: Tensor&lt;Float&gt;) -&gt; Tensor&lt;Float&gt; {
   sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))
 }
 &lt;/denchmark-code&gt;
 
 This has the advantage of avoiding work to compute derivatives for the output parameter.
 Besides improving the diagnostic, we can also add @differentiable(wrt: (logits, labels)) attributes to sigmoidCrossEntropy and other loss functions to fix the issue.
 		</comment>
 		<comment id='4' author='tanmayb123' date='2020-07-22T17:13:08Z'>
 		Closing because  is sufficiently differentiable, and I made a comment in &lt;denchmark-link:https://github.com/tensorflow/swift-models/pull/623&gt;tensorflow/swift-models#623&lt;/denchmark-link&gt;
  explaining how to make it work.
 		</comment>
 	</comments>
 </bug>
<commit id='bf942f2634862596a01f8fe5a5a049ecd3b6f2a4' author='Dan Zheng' date='2020-07-15 17:30:43-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='Sources\TensorFlow\BackwardsCompatibility.swift' new_name='Sources\TensorFlow\BackwardsCompatibility.swift'>
 		<file_info nloc='80' complexity='10' token_count='681'></file_info>
 		<modified_lines>
 			<added_lines>27,41,55,69,83,98,112,127,153</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='Sources\TensorFlow\Loss.swift' new_name='Sources\TensorFlow\Loss.swift'>
 		<file_info nloc='201' complexity='22' token_count='1786'></file_info>
 		<modified_lines>
 			<added_lines>25,42,58,73,91,108,125,144,163,184,204,221,315,343</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
