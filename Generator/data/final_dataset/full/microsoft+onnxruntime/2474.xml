<bug_data>
<bug id='2474' author='Mut1nyJD' open_date='2019-11-25T22:40:56Z' closed_time='2019-12-03T06:28:45Z'>
 	<summary>An ONNX model that worked in 0.5.1 now fails in 1.0.0</summary>
 	<description>
 Describe the bug
 I recently upgraded my ONNX Runtime from 0.5.1 to 1.0.0 after changing my C code to adapt to the new API I then found out that my previous ONNX model does no longer run in 1.0.0. My ONNX model uses OpSet-Level 9. It is a PyTorch1.3 ONNX export of a HRNet-backbone landmark detection model.
 When loading the ONNX model in ONNXRuntime 1.0 I'll get an Exception:
 op_kernel.cc:21 onnxruntime::OpKernelContext::OpKernelContext kernel != nullptr was false. OpKernel was null
 it loaded fine in 0.5.1
 	</description>
 	<comments>
 		<comment id='1' author='Mut1nyJD' date='2019-11-26T00:18:17Z'>
 		If I attempt to export that model with opset 9 I see the following error:
 
 python\python37\lib\site-packages\torch\onnx\symbolic_helper.py:198: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.
 ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).
 We recommend using opset 11 and above for models using this operator.
 
 Exporting with opset 11 produces a model that loads successfully. Is that an option? I used the export instructions from this pytorch issue: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/23474&gt;pytorch/pytorch#23474&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='Mut1nyJD' date='2019-11-26T00:31:03Z'>
 		I also exported using opset 9 and the model produced loads successfully. Could you please share your model as well a the requested system info:
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 ONNX Runtime installed from (source or binary):
 ONNX Runtime version:
 Python version:
 Visual Studio version (if applicable):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 		</comment>
 		<comment id='3' author='Mut1nyJD' date='2019-11-26T07:40:49Z'>
 		Sure please find here a download link to the ONNX model
 &lt;denchmark-link:https://mut1nyjd.stackstorage.com/s/ZwctHMbvHJM1bhf&gt;https://mut1nyjd.stackstorage.com/s/ZwctHMbvHJM1bhf&lt;/denchmark-link&gt;
 
 
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 
 
 Windows 10
 
 
 ONNX Runtime installed from (source or binary):
 
 
 Binary NuGet package version
 
 
 ONNX Runtime version:
 
 
 0.51 and 1.0.0
 
 
 Python version:
 
 
 No Python using C Backend
 
 
 Visual Studio version (if applicable):
 
 
 15.9.8
 
 
 CUDA/cuDNN version:
 
 
 10.1/7.1
 
 
 GPU model and memory:
 
 
 GTX1060ti 6GB VRAM
 		</comment>
 		<comment id='4' author='Mut1nyJD' date='2019-11-26T11:33:24Z'>
 		Forgot to say the output I'll get from ONNX Runtime 0.5.1 on this model on a test image is pretty close to what I'll get from the PyTorch model, so the model itself does not seem to be the problem
 		</comment>
 		<comment id='5' author='Mut1nyJD' date='2019-11-27T05:05:28Z'>
 		I have not been able to reproduce on either linux or windows with CUDA 10.1. The cuDNN version I have is a bit newer on each (7.6) though but I don't expect that would matter.
 Can you try completely uninstalling ONNX Runtime and reinstalling? There are only a few places we create OpKernelContext and I can't see how any of those would have a nullptr for the kernel.
 		</comment>
 		<comment id='6' author='Mut1nyJD' date='2019-11-27T09:17:54Z'>
 		Hmm I tried a complete fresh install of ONNXRuntime with a new project also tried it with VS2015.
 I installed latest CuDNN 7.6.5 for 10.1 but still seeing the same error.
 I've installed the MKL-DNN version of ONNXRuntime 1.0.0 that works fine with the model.
 I will try a different GPU and/with other driver and see if that makes a difference.
 		</comment>
 		<comment id='7' author='Mut1nyJD' date='2019-11-27T11:07:02Z'>
 		Could you share the C code you're using? I tested using python so that's one other difference.
 		</comment>
 		<comment id='8' author='Mut1nyJD' date='2019-11-27T11:38:32Z'>
 		Sure no problem, please find here my minimum test code
 // ONNXRuntime
 #include "onnxruntime_c_api.h"
 // CUDA ONNXRuntime
 #include "cuda_provider_factory.h"
 `const OrtApi* g_ort = OrtGetApiBase()-&gt;GetApi(ORT_API_VERSION);`
 void CheckStatus(OrtStatus* status)
 {
 	if (status != NULL) {
 		const char* msg = g_ort-&gt;GetErrorMessage(status);
 		fprintf(stderr, "%s\n", msg);
 		g_ort-&gt;ReleaseStatus(status);
 		exit(1);
 	}
 }
  
 int main()
 {
 	OrtEnv* env = NULL;
 	CheckStatus(g_ort-&gt;CreateEnv(ORT_LOGGING_LEVEL_WARNING, "test", &amp;env));
 	// initialize session options if needed
 	OrtSessionOptions* session_options;
 	g_ort-&gt;CreateSessionOptions(&amp;session_options);
 	// If you have CUDA ONNXRuntime installed otherwise don't use this line
 	OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 0);
 	g_ort-&gt;SetIntraOpNumThreads(session_options, 8);
 	g_ort-&gt;SetInterOpNumThreads(session_options, 8);
 	g_ort-&gt;SetSessionGraphOptimizationLevel(session_options, ORT_ENABLE_BASIC);
 	OrtSession* session = NULL;
 	CheckStatus(g_ort-&gt;CreateSession(env, L"hrnet_w18_landmarks.onnx", session_options, &amp;session));
 	std::cout &lt;&lt; "ONNX model loaded succesfull\n"; 
 }
 		</comment>
 		<comment id='9' author='Mut1nyJD' date='2019-11-27T11:43:13Z'>
 		If you need a full .vcproj + code I can upload that as well
 		</comment>
 		<comment id='10' author='Mut1nyJD' date='2019-11-27T23:13:58Z'>
 		I can reproduce the issue. Should have a fix later today.
 		</comment>
 		<comment id='11' author='Mut1nyJD' date='2019-11-28T07:29:29Z'>
 		&lt;denchmark-link:https://github.com/skottmckay&gt;@skottmckay&lt;/denchmark-link&gt;
 
 Great excellent! Happy you manage to reproduce it and yes sorry I misreported my setup it was CUDA 10.1/CuDNN 7.5 before I have so many CUDA/CuDNN variants installed I loose track from time to time.
 		</comment>
 		<comment id='12' author='Mut1nyJD' date='2019-11-28T21:48:46Z'>
 		Two short term options if needed:
 
 
 disable all optimizations so constant folding doesn't run
 
 simple but has a performance cost
 g_ort-&gt;SetSessionGraphOptimizationLevel(session_options, ORT_DISABLE_ALL);
 
 
 
 run the optimizations with just the CPU execution provider enabled, save the model, and use that model with the CUDA execution provider. this is a one time step.
 
 g_ort-&gt;SetOptimizedModelFilePath(session_options, L"hrnet_w18_landmarks.optimized.onnx");
 comment out the line that adds the CUDA execution provider, set the optimized model file path, create a session and release it (no need to call Run). following that you should be able to use the CUDA execution provider with the optimized model
 
 
 
 		</comment>
 	</comments>
 </bug>
<commit id='e8b327d6573845654700c048c330c9c722f3d3f8' author='Scott McKay' date='2019-12-03 16:28:44+10:00'>
 	<dmm_unit complexity='0.6666666666666666' interfacing='0.6333333333333333' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\optimizer\constant_folding.cc' new_name='onnxruntime\core\optimizer\constant_folding.cc'>
 		<file_info nloc='71' complexity='15' token_count='566'></file_info>
 		<method name='onnxruntime::ConstantFolding::ApplyImpl' parameters='graph,modified,graph_level,logger'>
 				<method_info nloc='63' complexity='15' token_count='545' nesting_level='1' start_line='14' end_line='116'></method_info>
 			<added_lines>28,29,30,31,32,33,34,35,36,37,38,50,51,52,53,54,58,59,60,61,62,86,87</added_lines>
 			<deleted_lines>65,66</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\optimizer\constant_folding.h' new_name='onnxruntime\core\optimizer\constant_folding.h'>
 		<file_info nloc='13' complexity='1' token_count='94'></file_info>
 		<method name='onnxruntime::ConstantFolding::ConstantFolding' parameters='compatible_execution_providers'>
 				<method_info nloc='2' complexity='1' token_count='27' nesting_level='2' start_line='19' end_line='20'></method_info>
 			<added_lines>19,20</added_lines>
 			<deleted_lines>19,20</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>29,30,31,32,33</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\optimizer\optimizer_execution_frame.cc' new_name='onnxruntime\core\optimizer\optimizer_execution_frame.cc'>
 		<file_info nloc='97' complexity='13' token_count='911'></file_info>
 		<method name='onnxruntime::OptimizerExecutionFrame::Info::Info' parameters='nodes,initialized_tensor_set'>
 				<method_info nloc='42' complexity='5' token_count='435' nesting_level='1' start_line='19' end_line='75'></method_info>
 			<added_lines>60,61,70,71,72</added_lines>
 			<deleted_lines>60,61,70,71</deleted_lines>
 		</method>
 		<method name='onnxruntime::OptimizerExecutionFrame::CreateNodeOutputMLValueImpl' parameters='ort_value,ort_value_idx,shape,nnz'>
 				<method_info nloc='28' complexity='4' token_count='303' nesting_level='1' start_line='98' end_line='129'></method_info>
 			<added_lines>122,123</added_lines>
 			<deleted_lines>121,122</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cuda\cuda_execution_provider.cc' new_name='onnxruntime\core\providers\cuda\cuda_execution_provider.cc'>
 		<file_info nloc='1253' complexity='85' token_count='16650'></file_info>
 		<method name='onnxruntime::CUDAExecutionProvider::GetCapability' parameters='graph'>
 				<method_info nloc='78' complexity='25' token_count='718' nesting_level='1' start_line='1259' end_line='1354'></method_info>
 			<added_lines>1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1320,1321,1322,1323,1324,1327</added_lines>
 			<deleted_lines>1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1320,1321,1322,1323,1324,1327</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>10,11</added_lines>
 			<deleted_lines>6</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\test\optimizer\graph_transform_test.cc' new_name='onnxruntime\test\optimizer\graph_transform_test.cc'>
 		<file_info nloc='1027' complexity='80' token_count='11063'></file_info>
 		<method name='onnxruntime::test::TEST' parameters='GraphTransformationTests,ConstantFoldingNodesOnDifferentEP'>
 				<method_info nloc='19' complexity='3' token_count='204' nesting_level='2' start_line='134' end_line='159'></method_info>
 			<added_lines>134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::ValidateAttention' parameters='graph'>
 				<method_info nloc='100' complexity='5' token_count='647' nesting_level='2' start_line='905' end_line='1016'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>1013</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>160</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
