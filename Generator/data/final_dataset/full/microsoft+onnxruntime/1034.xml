<bug_data>
<bug id='1034' author='GuanLuo' open_date='2019-05-15T03:31:36Z' closed_time='2019-06-04T23:32:28Z'>
 	<summary>Can't Create Model Sessions on Different GPU</summary>
 	<description>
 Describe the bug
 I am on a 2 GPU system, I tried to create one session on each GPU (so 2 sessions in total). I get the following error when it's creating the second session (i.e. creating the first session on GPU 0, and the second session on GPU1)
 &lt;denchmark-code&gt;Failed to get allocator for location: OrtAllocatorInfo: [ name:Cuda id:0 mem_type:0 type:1]
 &lt;/denchmark-code&gt;
 
 Changing the creation order (first session on GPU1, second session on GPU0) will gives the same kind of error, but with different id
 &lt;denchmark-code&gt;Failed to get allocator for location: OrtAllocatorInfo: [ name:Cuda id:1 mem_type:0 type:1]
 &lt;/denchmark-code&gt;
 
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
 ONNX Runtime installed from (source or binary): source
 ONNX Runtime version: 0.4.0
 Python version:
 Visual Studio version (if applicable):
 GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
 CUDA/cuDNN version: 10.1.105 / 7.5.0.56
 GPU model and memory: Tesla P100, 16 GB
 
 To Reproduce
 
 Compile the code (modified from c_api_example) below, and run the executable
 the model file "int32_int32_int32.onnx" is just a simple model that takes two inputs, returns two outputs for addition and subtraction. Will provide the generation script if requested, but the model itself seems irrelevant to the issue
 
 &lt;denchmark-code&gt;#include &lt;assert.h&gt;
 #include &lt;core/providers/cuda/cuda_provider_factory.h&gt;
 #include &lt;core/session/onnxruntime_c_api.h&gt;
 #include &lt;stdio.h&gt;
 #include &lt;stdlib.h&gt;
 #include &lt;iostream&gt;
 #include &lt;vector&gt;
 
 //*****************************************************************************
 // helper function to check for status
 #define CHECK_STATUS(expr)                               \
   {                                                      \
     OrtStatus* onnx_status = (expr);                     \
     if (onnx_status != NULL) {                           \
       const char* msg = OrtGetErrorMessage(onnx_status); \
       fprintf(stderr, "%s\n", msg);                      \
       OrtReleaseStatus(onnx_status);                     \
       exit(1);                                           \
     }                                                    \
   }
 
 int
 main(int argc, char* argv[])
 {
   //===================== Env Scope ======================
   // initialize  enviroment...one enviroment per process
   // enviroment maintains thread pools and other state info
   OrtEnv* env = nullptr;
   // [TODO] look into OrtCreateEnvWithCustomLogger()
   CHECK_STATUS(OrtCreateEnv(ORT_LOGGING_LEVEL_WARNING, "test", &amp;env));
 
   //===================== Model Scope ======================
   // session option also specifies GPU device
   // but other session option can be set "globally", just clone the session
   // when setting actual device
   OrtSessionOptions* session_options = OrtCreateSessionOptions();
   OrtSetSessionThreadPoolSize(session_options, 1);
 
   // disable graph optimization
   OrtSetSessionGraphOptimizationLevel(session_options, 0);
 
   //===================== Session Scope ======================
   // create multiple sessions and load models into memory
   std::vector&lt;const char*&gt; model_paths;
   {
     // two instances of the same model
     model_paths.emplace_back();
     model_paths.back() = "int32_int32_int32.onnx";
     model_paths.emplace_back();
     model_paths.back() = "int32_int32_int32.onnx";
   }
 
   std::vector&lt;OrtSession*&gt; sessions(model_paths.size());
   for (size_t idx = 0; idx &lt; model_paths.size(); idx++) {
     // To deploy on different device, need to set provider on cloned option
     // as you can remove a execution provider once it is appended
     OrtSessionOptions* context_options =
         OrtCloneSessionOptions(session_options);
     
     // Use CUDA, device 0. CPU if not set.
     // Use "idx" as we know we have two models and two devices
     OrtSessionOptionsAppendExecutionProvider_CUDA(context_options, idx);
     std::cout &lt;&lt; "here" &lt;&lt; std::endl;
     // [TODO] change session options for different session
     CHECK_STATUS(OrtCreateSession(
         env, model_paths[idx], context_options, &amp;sessions[idx]));
     std::cout &lt;&lt; "after here" &lt;&lt; std::endl;
     // session-wise
     OrtReleaseSessionOptions(context_options);
   }
 }
 &lt;/denchmark-code&gt;
 
 Expected behavior
 Expect to be able to create session on different GPU at the same time. Right now, I can create two sessions on the same GPU (GPU0 only or GPU1 only).
 Screenshots
 If applicable, add screenshots to help explain your problem.
 Additional context
 Add any other context about the problem here.
 	</description>
 	<comments>
 		<comment id='1' author='GuanLuo' date='2019-05-15T17:22:38Z'>
 		Yes, it's a known problem.
 		</comment>
 		<comment id='2' author='GuanLuo' date='2019-05-17T21:11:35Z'>
 		We should fix the following warning:
 &lt;denchmark-code&gt;/home/chasun/src/onnxruntime/onnxruntime/core/providers/cuda/cuda_allocator.h:24:13: warning: private field 'device_id_' is not used [-Wunused-private-field]
   const int device_id_;
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='GuanLuo' date='2019-05-19T00:43:21Z'>
 		I think the problem is in thread_local of per_thread_context in CUDA execution provider. &lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
  can you try create the two sessions in two different threads and see if the problem still exists?
 		</comment>
 		<comment id='4' author='GuanLuo' date='2019-05-20T16:21:01Z'>
 		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
  So I just spawn a thread to execute ? Or the life time of the thread needs to be longer than that? Like once the session created, to run the model, I also need to run it in the same thread.
 Anyway, I will try that.
 		</comment>
 		<comment id='5' author='GuanLuo' date='2019-05-28T18:07:26Z'>
 		&lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
 , yes please try spawn a thread to execute OrtCreateSession for different devices. Your inference code should run on the same thread too. Please let me know what do you find.
 		</comment>
 		<comment id='6' author='GuanLuo' date='2019-05-30T03:05:25Z'>
 		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
  Yes! It works if I spawn a new thread to execute OrtCreateSession, the following is what I changed in my code snap. Notice that I didn't run my inference code on the same thread, and everything seems fine. Is that expected? Or there is pitfall for doing that?
 auto status = std::async(std::launch::async, &amp;OrtCreateSession,
         env, model_paths[idx], context_options, &amp;sessions[idx]);
 CHECK_STATUS(status.get());
 		</comment>
 		<comment id='7' author='GuanLuo' date='2019-05-30T06:07:55Z'>
 		Thanks for the confirmation, will work on a fix to it. Your workaround is OK, as long as the inference threads for different GPUs are separated.
 		</comment>
 		<comment id='8' author='GuanLuo' date='2019-05-30T16:15:28Z'>
 		I just realized I wasn't being clear. I mean my inference code is run on the main thread (sessions are created on different threads via async), and the inference code send one request to each session.
 		</comment>
 		<comment id='9' author='GuanLuo' date='2019-05-31T23:51:38Z'>
 		Thanks &lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
 , I found your &lt;denchmark-link:https://github.com/NVIDIA/tensorrt-inference-server/commit/fbbbeefd24ee537b3074bd055444755fd7889b81&gt;workaround&lt;/denchmark-link&gt;
 . Will try my fix with TRTIS later.
 		</comment>
 		<comment id='10' author='GuanLuo' date='2019-06-04T23:37:54Z'>
 		&lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
  can you try the fix and see if your issue is solved?
 		</comment>
 		<comment id='11' author='GuanLuo' date='2019-06-05T02:25:27Z'>
 		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
  I tried with my sample code and the issue is resolved! I can deploy models on different GPUs without using threading. However, I am not going to try it on our server code until there is new release version for ONNX Runtime, hope you understand that.
 		</comment>
 		<comment id='12' author='GuanLuo' date='2019-06-05T18:42:23Z'>
 		Awesome, thanks for the confirmation.
 		</comment>
 	</comments>
 </bug>
<commit id='7c4494a0bc3baad95a783c6759c753f6b5a51b5e' author='KeDengMS' date='2019-06-04 16:32:27-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cuda\cuda_allocator.cc' new_name='onnxruntime\core\providers\cuda\cuda_allocator.cc'>
 		<file_info nloc='51' complexity='12' token_count='302'></file_info>
 		<method name='onnxruntime::CUDAAllocator::CheckDevice' parameters=''>
 				<method_info nloc='5' complexity='1' token_count='29' nesting_level='1' start_line='17' end_line='25'></method_info>
 			<added_lines>23</added_lines>
 			<deleted_lines>23</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cuda\cuda_allocator.h' new_name='onnxruntime\core\providers\cuda\cuda_allocator.h'>
 		<file_info nloc='24' complexity='1' token_count='158'></file_info>
 		<method name='onnxruntime::CUDAAllocator::CUDAAllocator' parameters='device_id'>
 				<method_info nloc='1' complexity='1' token_count='20' nesting_level='2' start_line='14' end_line='14'></method_info>
 			<added_lines>14</added_lines>
 			<deleted_lines>14</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>24</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cuda\cuda_execution_provider.cc' new_name='onnxruntime\core\providers\cuda\cuda_execution_provider.cc'>
 		<file_info nloc='1002' complexity='93' token_count='12704'></file_info>
 		<method name='onnxruntime::CUDAExecutionProvider::ReleasePerThreadStuffs' parameters=''>
 				<method_info nloc='13' complexity='5' token_count='89' nesting_level='1' start_line='117' end_line='130'></method_info>
 			<added_lines>117,118,119,120,121,122,123,124,125,126,127,128</added_lines>
 			<deleted_lines>117,118,119,120,121,122,123,124,125</deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::PerThreadContext::PerThreadContext' parameters='device_id'>
 				<method_info nloc='9' complexity='1' token_count='85' nesting_level='1' start_line='45' end_line='54'></method_info>
 			<added_lines>49,50,51,52,53</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::OnRunStart' parameters=''>
 				<method_info nloc='23' complexity='5' token_count='169' nesting_level='1' start_line='161' end_line='187'></method_info>
 			<added_lines>182,183</added_lines>
 			<deleted_lines>170,171,172,173,174,175,176,177,178,179,180,187</deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::AddDeferredReleaseCPUPtr' parameters='p'>
 				<method_info nloc='9' complexity='2' token_count='70' nesting_level='1' start_line='148' end_line='159'></method_info>
 			<added_lines>152</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::GetAllocator' parameters='id,mem_type'>
 				<method_info nloc='7' complexity='2' token_count='43' nesting_level='1' start_line='132' end_line='141'></method_info>
 			<added_lines>137</added_lines>
 			<deleted_lines>140</deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::GetPerThreadContext' parameters=''>
 				<method_info nloc='16' complexity='4' token_count='131' nesting_level='1' start_line='99' end_line='115'></method_info>
 			<added_lines>99,100,101,103,104,105,107,108,109,110,111,112,113,114,115</added_lines>
 			<deleted_lines>99,101,103,104,113,114,115</deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::OnRunEnd' parameters=''>
 				<method_info nloc='8' complexity='1' token_count='59' nesting_level='1' start_line='189' end_line='197'></method_info>
 			<added_lines>191</added_lines>
 			<deleted_lines>189</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>43,116</added_lines>
 			<deleted_lines>43,44,95,96,97,98,116</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cuda\cuda_execution_provider.h' new_name='onnxruntime\core\providers\cuda\cuda_execution_provider.h'>
 		<file_info nloc='115' complexity='18' token_count='711'></file_info>
 		<method name='onnxruntime::CUDAExecutionProvider::PerThreadCublasHandle' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='13' nesting_level='2' start_line='45' end_line='47'></method_info>
 			<added_lines>46</added_lines>
 			<deleted_lines>46,47</deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::PerThreadContext::GetAllocator' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='9' nesting_level='3' start_line='129' end_line='131'></method_info>
 			<added_lines>129,130,131</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::GetConstOnes' parameters='count'>
 				<method_info nloc='3' complexity='1' token_count='21' nesting_level='2' start_line='59' end_line='61'></method_info>
 			<added_lines>60</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::CUDAExecutionProvider::PerThreadCudnnHandle' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='14' nesting_level='2' start_line='49' end_line='51'></method_info>
 			<added_lines>50</added_lines>
 			<deleted_lines>49</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>132,145,146,150,151,157</added_lines>
 			<deleted_lines>48,53,54,55,56,57,67,68,69,70,154,155,156,157,158,159,160,161</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
