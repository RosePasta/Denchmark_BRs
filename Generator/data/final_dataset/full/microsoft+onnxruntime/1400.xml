<bug_data>
<bug id='1400' author='xgirones' open_date='2019-07-13T19:15:31Z' closed_time='2019-07-19T19:10:01Z'>
 	<summary>Wrong result when evaluating a batch of padded sequences with varying lengths on GPU</summary>
 	<description>
 Describe the bug
 When supplying the attached model with a batch of padded sequences with varying lengths the results on GPU are incorrect:
 &lt;denchmark-code&gt;Discrepancy between CPU and GPU evaluations.
 Number of sequences in batch: 16
 Sequence lengths: [12 32  4 52  8 64 20 36 48 40 24 60 44 16 28 56]
 Maximum absolute pairwise difference: 0.9621068835258484
 Average absolute pairwise difference: 0.019491384112097738
 &lt;/denchmark-code&gt;
 
 On the other hand, the results on CPU are fine:
 &lt;denchmark-code&gt;Discrepancy between CPU and PyTorch evaluations.
 Number of sequences in batch: 16
 Sequence lengths: [12 32  4 52  8 64 20 36 48 40 24 60 44 16 28 56]
 Maximum absolute pairwise difference: 1.2218952178955078e-06
 Average absolute pairwise difference: 1.330953997859751e-08
 &lt;/denchmark-code&gt;
 
 Urgency
 Stuck on this
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
 ONNX Runtime installed from (source or binary): Source
 ONNX Runtime version: Code pulled from the master branch in 13/7/2019
 Python version: 3.6
 Visual Studio version (if applicable): 2017
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 10.1/7.6
 GPU model and memory: NVIDA Quadro K1200 with 4GB
 
 
 Unzip the attached &lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3389130/discrepancy_CPU_GPU.zip&gt;discrepancy_CPU_GPU.zip&lt;/denchmark-link&gt;
  file and run the model  in the  folder on both CPU and GPU using  and  as inputs (all  and  in the folder  are the same).
 Expected behavior
 Results on GPU and CPU should be the same.
 Screenshots
 If applicable, add screenshots to help explain your problem.
 
 &lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3389130/discrepancy_CPU_GPU.zip&gt;discrepancy_CPU_GPU.zip&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='xgirones' date='2019-07-16T19:09:35Z'>
 		Our cuda implementation for LSTM uses cudnn libray, it has some limitation: the sequences in the mini-batch need to be sorted in descending order according to length. refers to
 &lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNForwardInferenceEx&gt;https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNForwardInferenceEx&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='xgirones' date='2019-07-16T20:49:35Z'>
 		Hi Hector,
 Thanks for your answer. I tried with sorted sequences as you suggested but the problem did not go away:
 &lt;denchmark-code&gt;Discrepancy between CPU and GPU evaluations.
 Number of sequences in batch: 16
 Sequence lengths: [64 60 56 52 48 44 40 36 32 28 24 20 16 12  8  4]
 Total non zero elements: 21216
 Maximum absolute pairwise difference: 0.9727079272270203
 Average absolute pairwise difference: 0.018673043804830376
 &lt;/denchmark-code&gt;
 
 Here you can find the model and the input/output tensors in case you need to take a look at them:
 &lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3399061/discrepancy_CPU_GPU_sorted.zip&gt;discrepancy_CPU_GPU_sorted.zip&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='xgirones' date='2019-07-17T21:35:34Z'>
 		Root cause addressed, will send a PR.
 &lt;denchmark-link:https://github.com/xgirones&gt;@xgirones&lt;/denchmark-link&gt;
  By they way, Is it possible for us to put your model in our build pipeline to make sure no regression in future?
 		</comment>
 		<comment id='4' author='xgirones' date='2019-07-17T22:47:06Z'>
 		This is great news, thanks!
 
 @xgirones By they way, Is it possible for us to put your model in our build pipeline to make sure no regression in future?
 
 Yes, of course. You can use it freely if you find it convenient. It is just a toy model I created to learn more about the best loss functions for sequences.
 		</comment>
 		<comment id='5' author='xgirones' date='2019-07-18T16:28:30Z'>
 		The fix is merged. Also sorted sequence_lengths is not a limitation any more. Your first model also works. I'll add them to our test pipeline.
 Great thanks for your findings!
 		</comment>
 		<comment id='6' author='xgirones' date='2019-07-19T18:10:10Z'>
 		&lt;denchmark-link:https://github.com/xgirones&gt;@xgirones&lt;/denchmark-link&gt;
  Do you mind to close the issue or you still need some verification?
 		</comment>
 		<comment id='7' author='xgirones' date='2019-07-19T19:10:01Z'>
 		Closing it... Thanks for your help!
 		</comment>
 	</comments>
 </bug>
<commit id='1ff957f96e62e6eb4b4439469cd10648be748efc' author='Hector Li' date='2019-07-18 09:17:44-07:00'>
 	<dmm_unit complexity='0.9090909090909091' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cuda\cudnn_common.cc' new_name='onnxruntime\core\providers\cuda\cudnn_common.cc'>
 		<file_info nloc='118' complexity='24' token_count='788'></file_info>
 		<method name='onnxruntime::cuda::CudnnDataTensor::Set' parameters='dataType,max_seq_length,batch_size,data_size,seq_lengths'>
 				<method_info nloc='16' complexity='1' token_count='97' nesting_level='2' start_line='79' end_line='96'></method_info>
 			<added_lines>86,87</added_lines>
 			<deleted_lines>86</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cuda\rnn\cudnn_rnn_base.cc' new_name='onnxruntime\core\providers\cuda\rnn\cudnn_rnn_base.cc'>
 		<file_info nloc='284' complexity='45' token_count='2489'></file_info>
 		<method name='onnxruntime::cuda::CudnnRnnBase&lt;T&gt;::ComputeInternal' parameters='ctx'>
 				<method_info nloc='162' complexity='27' token_count='1456' nesting_level='2' start_line='153' end_line='343'></method_info>
 			<added_lines>236,239,240,241,292,293,294,295</added_lines>
 			<deleted_lines>223,237,238</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\test\providers\cpu\rnn\deep_cpu_lstm_op_test.cc' new_name='onnxruntime\test\providers\cpu\rnn\deep_cpu_lstm_op_test.cc'>
 		<file_info nloc='786' complexity='57' token_count='8034'></file_info>
 		<method name='onnxruntime::test::TEST' parameters='LSTMTest,ONNXRuntime_TestLSTMSequenceLengthShorterThanInputSequenceLengthNoP'>
 				<method_info nloc='20' complexity='1' token_count='226' nesting_level='2' start_line='1094' end_line='1123'></method_info>
 			<added_lines>1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1093</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
