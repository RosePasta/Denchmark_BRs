<bug_data>
<bug id='612' author='xadupre' open_date='2019-03-13T15:33:45Z' closed_time='2019-04-09T17:55:32Z'>
 	<summary>TfidfVectorizer fails with empty strings</summary>
 	<description>
 Describe the bug
 I'm trying to write a converter for TfIdfVectorizer when analyser=='char'. I use the regular expression to split a sentance into characters with '.' (does not work) but '[^\n]' works. With the first one, I get the following error:
 INVALID_ARGUMENT : Input shape must have either [C] or [B,C] dimensions where C &gt; 0 and B &gt; 0
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
 ONNX Runtime installed from (source or binary): master branch from github
 ONNX Runtime version: master branch from github
 Python version: 3.7
 
 To Reproduce
 
 clone https://github.com/xadupre/sklearn-onnx/tree/tfc
 run test test_model_tfidf_vectorizer11_dot in tests/test_SklearnTfidfVectorizerConverterChar.py
 
 Expected behavior
 onnxruntime should return outputs.
 	</description>
 	<comments>
 		<comment id='1' author='xadupre' date='2019-03-13T17:05:28Z'>
 		&lt;denchmark-link:https://github.com/xadupre&gt;@xadupre&lt;/denchmark-link&gt;
  There are few things I want you to note.
 
 
 Current implementation of tokenexp is the opposite of what it should be. I have created a PR below.
 
 
 If you want to split something into individual characters the Tokenizer spec provides for a char tokenization mode which is enabled when you supply a single empty string as separator.
 
 
 The rest below I believe are python related issues.
 
 Per spec Tokenizer accepts UTF-8 strings for both input, output and attributes. This applies as well to StringNormalizer and TfIdfVectorizer.  It looks like pybind11 will automatically convert python strings to utf-8, so it should be good.
 Per spec the regex supported (or intended to be supported) is BRE
 The above error message is a result of TypeAndShapeInference function failure. It expects input as  Tensor of strings with the aforementioned dimensions. Will check what our python expects.
 
 		</comment>
 		<comment id='2' author='xadupre' date='2019-03-13T22:26:15Z'>
 		I have submitted &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/617&gt;#617&lt;/denchmark-link&gt;
  to address this.
 		</comment>
 		<comment id='3' author='xadupre' date='2019-03-14T17:07:30Z'>
 		I tried but that does not fix the issue. StringNormalizer can handle empty strings, Tokenizer too, but not TfidfVectorizer. It should handle empty strings and return an empty results.
 		</comment>
 		<comment id='4' author='xadupre' date='2019-03-14T17:12:58Z'>
 		Another issue maybe introduced with changes &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/617&gt;#617&lt;/denchmark-link&gt;
  is the following.
 &lt;denchmark-code&gt;corpus = numpy.array([
         'This is the first document.',
         'This document is the second document.',
         ]).reshape((2, 1))
 vect = TfidfVectorizer(ngram_range=(1, 1), norm=None,
                        analyzer='word', token_pattern=".{1,2}")
 vect.fit(corpus.ravel())
 pred = vect.transform(corpus.ravel())
 model_onnx = convert_sklearn(vect, 'TfidfVectorizer',
                              [('input', StringTensorType([1, 1]))])
 &lt;/denchmark-code&gt;
 
 The ONNX pipeline includes a couple of nodes which produce the following outputs
 &lt;denchmark-code&gt;-------------- normalized (StringNormalizer)
 ['this is the first document.']
 -------------- tokenized (Tokenizer)
 [['th' 'is' ' i' 's ' 'th' 'e ' 'fi' 'rs' 't ' 'do' 'cu' 'me' 'nt' '.']]
 -------------- flattened (Flatten)
 [['th' 'is' ' i' 's ' 'th' 'e ' 'fi' 'rs' 't ' 'do' 'cu' 'me' 'nt' '.']]
 -------------- tfidfvectorizer (TfidfVectorizer with regex = '.{1,2}'
 2019-03-14 18:08:44.0281985 [E:onnxruntime:InferenceSession, inference_session.cc:541 onnxruntime::InferenceSession::Impl::Initialize] Exception during initialization: onnxruntime\onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc:371 onnxruntime::TfIdfVectorizer::TfIdfVectorizer (before_insert + ngrams) == impl_-&gt;str_set_.size() was false. poll_strings duplicate 1-grams detected
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='xadupre' date='2019-03-15T16:58:04Z'>
 		I found two bugs while converting a TfIdfVectorizer. The first one comes from scikit-learn which creates ambiguities when trying to retrieve n-grams if the tokens contain spaces. The other bug comes from the tokenizer which loses one dimension if the expression to tokenize is empty. The code is the following:
 &lt;denchmark-code&gt;corpus = np.array([
         'This is the first document.',
         'This document is the second document.',
         'And this is the third one.',
         ' ',
         ]).reshape((4, 1))
 vect = TfidfVectorizer(ngram_range=(1, 2), norm=None)
 vect.fit(corpus.ravel())
 pred = vect.transform(corpus.ravel())
 from skl2onnx import convert_sklearn
 from skl2onnx.common.data_types import StringTensorType
 
 model_onnx = convert_sklearn(vect, 'TfidfVectorizer',
                              [('input', StringTensorType([1, 1]))])
 &lt;/denchmark-code&gt;
 
 I then checked the intermediate outputs for two strings, an non-empty string and an empty string (differencs in yellow). When the string is empty, the function return an null dimension.
 &lt;denchmark-code&gt;  if (max_tokens == 0) {
     output_dims.push_back(0);
     TensorShape output_shape(output_dims);
     ctx-&gt;Output(0, output_shape);
     return Status::OK();
   }
 &lt;/denchmark-code&gt;
 
 But if mark_ is true, we would expect to have that string surrounded by start/end markers (this code is after the first one).
 &lt;denchmark-code&gt;  if (mark_) {
     max_tokens += 2;  // Start/end markers as separate tokens
   }
 &lt;/denchmark-code&gt;
 
 I compared the results for the intermediate nodes (graph is below)
 with input: [array(['And this is the third one.'], dtype='&lt;U37')]
 &lt;denchmark-code&gt;-&gt; node 'normalized'
 ['and this is the third one.']
 -&gt; node 'tokenized'
 [['and' 'this' 'is' 'the' 'third' 'one']]
 -&gt; node 'flattened'
 [['and' 'this' 'is' 'the' 'third' 'one']]
 -&gt; node 'variable'
 [[1.9162908 1.9162908 0.        0.        0.        0.        1.2231436
   1.2231436 1.9162908 0.        0.        1.2231436 0.        0.
   1.9162908 1.9162908 1.9162908 1.2231436 0.        1.5108256]]
 &lt;/denchmark-code&gt;
 
 with input: [array([' '], dtype='&lt;U37')]
 &lt;denchmark-code&gt;-&gt; node 'normalized'
 [' ']
 -&gt; node 'tokenized'
 2019-03-15 17:37:20.3345248 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes
 []
 -&gt; node 'flattened'
 2019-03-15 17:37:20.3356089 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes
 2019-03-15 17:37:20.3356310 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes
 []
 -&gt; node 'variable'
 &lt;/denchmark-code&gt;
 
 for the following pipeline...
 &lt;denchmark-link:https://user-images.githubusercontent.com/22452781/54448320-b9647a80-474b-11e9-9242-e3ac0d70ba5f.png&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='xadupre' date='2019-03-22T23:03:00Z'>
 		&lt;denchmark-link:https://github.com/xadupre&gt;@xadupre&lt;/denchmark-link&gt;
  I think we discussed this sufficiently with Wei-Sheng. Pls, talk to me if you feel there still an issue with regards to empty strings.
 		</comment>
 		<comment id='7' author='xadupre' date='2019-03-25T22:17:08Z'>
 		Supporting empty input shape in TfidfVectorizer looks reasonable to me. For example, if we tokenized a document (i.e., a very long string) into a [C]-tensor, but unfortunately remove all the elements in StringNormalizer, and finally TfidfVectorizer gets [0]-tensor. What should be the TFIDF-representation of this document? I'd guess that representation is a [D]-vector where all elements are zeros, because the count of every vocabulary is 0. Note that D is the dictionary's max index in TfidfVectorizer.
 		</comment>
 	</comments>
 </bug>
<commit id='ccd7e801a05d9819ba43386b0623c988f3dd5c72' author='Xavier DuprÃ©' date='2019-04-09 10:55:24-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc' new_name='onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc'>
 		<file_info nloc='471' complexity='89' token_count='3593'></file_info>
 		<method name='onnxruntime::TfIdfVectorizer::ComputeImpl' parameters='ctx'>
 				<method_info nloc='101' complexity='20' token_count='667' nesting_level='1' start_line='437' end_line='563'></method_info>
 			<added_lines>460,462,466,474,475,476,477,478,479,480,481,482,483,484,485</added_lines>
 			<deleted_lines>456,457,458,459,464,466,470,473,474</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\test\providers\cpu\nn\tfidfvectorizer_test.cc' new_name='onnxruntime\test\providers\cpu\nn\tfidfvectorizer_test.cc'>
 		<file_info nloc='500' complexity='29' token_count='5536'></file_info>
 		<method name='onnxruntime::test::TEST' parameters='TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip01_Empty_Dim2'>
 				<method_info nloc='17' complexity='1' token_count='179' nesting_level='2' start_line='139' end_line='159'></method_info>
 			<added_lines>139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim1Fail'>
 				<method_info nloc='17' complexity='1' token_count='164' nesting_level='2' start_line='73' end_line='93'></method_info>
 			<added_lines>73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim2'>
 				<method_info nloc='17' complexity='1' token_count='179' nesting_level='2' start_line='117' end_line='137'></method_info>
 			<added_lines>117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim1Success'>
 				<method_info nloc='17' complexity='1' token_count='177' nesting_level='2' start_line='95' end_line='115'></method_info>
 			<added_lines>95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim2N'>
 				<method_info nloc='18' complexity='1' token_count='195' nesting_level='2' start_line='161' end_line='182'></method_info>
 			<added_lines>161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>94,116,138,160,183</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
