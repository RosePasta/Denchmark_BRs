<bug_data>
<bug id='3417' author='vuvko' open_date='2020-04-03T11:41:07Z' closed_time='2020-04-13T22:02:19Z'>
 	<summary>Segmentation fault on loading quantized model</summary>
 	<description>
 Describe the bug
 Onnxruntime raises SIGSEGV signal on loading the quantized model.
 Urgency
 If there are particular important use cases blocked by this or strict project-related timelines, please share more information and dates. If there are no hard deadlines, please specify none.
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
 ONNX Runtime installed from (source or binary): binary
 ONNX Runtime version: 1.2.0
 Python version: 3.6.9
 Visual Studio version (if applicable): not applicable
 GCC/Compiler version (if compiling from source): not applicable
 CUDA/cuDNN version: not applicable
 GPU model and memory: not applicable
 
 
 Google Colab &lt;denchmark-link:https://colab.research.google.com/drive/1mk9b-_5dlAiSsMYqWvxcYfqxjhi4juRq&gt;link&lt;/denchmark-link&gt;
  for reproduction.
 Github Gist &lt;denchmark-link:https://gist.github.com/vuvko/e5fa172387f9a12b6936e8505cded394&gt;link&lt;/denchmark-link&gt;
  for the reference (&lt;denchmark-link:https://drive.google.com/open?id=1HJP28RcVym3bj70pK_k4LMUQnBj6uoQN&gt;model file&lt;/denchmark-link&gt;
 ).
 Run all cells for the colab notebook, or run the script from Gist with a saved model file as .
 Expected behavior
 Expected to see an input0 to be printed (it's an input layer's name).
 
 The original model is a modified face detector in PyTorch (&lt;denchmark-link:https://drive.google.com/open?id=1Ni8WfR1gdhvz-T2fdfuC5gc_E_7xo6B3&gt;params file&lt;/denchmark-link&gt;
 ). It was converted using  (&lt;denchmark-link:https://drive.google.com/open?id=1lje7hI0Eiatk_SjHGk5lyjADQU-YMVMG&gt;converted model&lt;/denchmark-link&gt;
 ). The converted model can be used as it is but after quantization with &lt;denchmark-link:https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/quantization&gt;quantization tool&lt;/denchmark-link&gt;
  it cannot be loaded with onnxruntime.
 This was tested on google colab, and ArchLinux machine (python 3.8, gcc 9.3, building from source and binary provided by PyPi).
 Running this with gdb shows this stack:
 &lt;denchmark-code&gt;[New Thread 0x7fffe2ffd700 (LWP 669857)]
 [New Thread 0x7fffe27fc700 (LWP 669858)]
 [New Thread 0x7fffe1ffb700 (LWP 669859)]
 [New Thread 0x7fffe17fa700 (LWP 669860)]
 [New Thread 0x7fffe0ff9700 (LWP 669861)]
 
 Thread 1 "python" received signal SIGSEGV, Segmentation fault.
 onnxruntime::DequantizeLinear&lt;unsigned char&gt;::Compute (this=0x55555607f5e0, 
     ctx=0x7fffffffc5b0)
     at /home/andrey/libs/onnxruntime/include/onnxruntime/core/framework/tensor_shape.h:47
 47	    return std::vector&lt;int64_t&gt;::operator[](static_cast&lt;int&gt;(idx));
 
 &lt;/denchmark-code&gt;
 
 I tested loading other quantized models, and everything works with &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/E2E_example_model/resnet50_v1.onnx&gt;resnet50_v1&lt;/denchmark-link&gt;
  model.
 	</description>
 	<comments>
 		<comment id='1' author='vuvko' date='2020-04-03T12:24:27Z'>
 		Apparently, switching the optimization off with GraphOptimizationLevel::ORT_DISABLE_ALL  allow to load the model, but sess.run raises SIGSEGV.
 Code to reproduce:
 &lt;denchmark-code&gt;import numpy as np
 import onnxruntime as rt
 
 
 sess_options = rt.SessionOptions()
 sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_DISABLE_ALL
 sess = rt.InferenceSession('model_q.onnx', sess_options)
 input_name = sess.get_inputs()[0].name
 
 print(input_name)
 
 np_inputs = np.zeros((1, 3, 320, 320), dtype=np.float32)
 
 onnx_res = sess.run(None, {input_name: np_inputs})
 print(np.max(onnx_res[0]))
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='vuvko' date='2020-04-03T17:06:20Z'>
 		&lt;denchmark-link:https://github.com/yufenglee&gt;@yufenglee&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/zhanghuanrong&gt;@zhanghuanrong&lt;/denchmark-link&gt;
  The implementation of DequantizeLinear is overly complicated: it has support for an axis attribute that doesn't exist in the ONNX spec. This is probably based on the old QuantizeLinear implementation in the com.ms namespace that also had an axis attribute. In this case, the input tensor is a scalar and the "broadcastDim=x_shape[axis]" faults. This could be fixed by stripping out all the axis code. Something one of you wants to fix?
 		</comment>
 		<comment id='3' author='vuvko' date='2020-04-13T22:02:19Z'>
 		&lt;denchmark-link:https://github.com/vuvko&gt;@vuvko&lt;/denchmark-link&gt;
  A fix for this has been checked into master. Thanks for reporting the problem!
 A couple comments about your model: you may want to run the model through an ONNX optimizer before quantizing. This will allow the Conv+BatchNormalization nodes to be fused. Also, you may now see better performance using QLinearOps as the quantization mode.
 As things currently stand, the model will be switching between quantized and float formats to do LeakyRelu. The runtime still needs to implement a quantized LeakyRelu to further streamline the model.
 		</comment>
 		<comment id='4' author='vuvko' date='2020-04-22T08:19:37Z'>
 		Thank you for your fix and the comments!
 		</comment>
 	</comments>
 </bug>
<commit id='5aab2671f8346a1489ac8ebf4b364544c6707062' author='Tracy Sharpe' date='2020-04-13 14:52:52-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='onnxruntime\contrib_ops\cpu\quantize_ops.cc' new_name='onnxruntime\contrib_ops\cpu\quantize_ops.cc'>
 		<file_info nloc='30' complexity='0' token_count='150'></file_info>
 		<modified_lines>
 			<added_lines>5,15,16,24,25,33,34,37</added_lines>
 			<deleted_lines>5,6,16,17,18,19,20,28,29,30,31,32,40,41,42,43,44,47</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\core\providers\cpu\tensor\quantize_linear.cc' new_name='onnxruntime\core\providers\cpu\tensor\quantize_linear.cc'>
 		<file_info nloc='120' complexity='16' token_count='1120'></file_info>
 		<method name='onnxruntime::DequantizeLinear&lt;T&gt;::Compute' parameters='ctx'>
 				<method_info nloc='38' complexity='7' token_count='428' nesting_level='1' start_line='28' end_line='75'></method_info>
 			<added_lines>33,35,36,37,40,41,42,43,46,47,49,50,51,52,63,64,65,66,68,69</added_lines>
 			<deleted_lines>28,29,38,41,42,43,46,47,48,51,52,59,60,61,67,68,69,70,71,72,73,75</deleted_lines>
 		</method>
 		<method name='onnxruntime::QuantizeLinear&lt;T&gt;::Compute' parameters='ctx'>
 				<method_info nloc='45' complexity='9' token_count='514' nesting_level='1' start_line='97' end_line='155'></method_info>
 			<added_lines>102,104,105,106,108,109,110,111,112,114,115,116,117,118,119,120,121,122,125,127,128,129,130,132,133,134,135,137,138,139,140,142,143,144,145,147</added_lines>
 			<deleted_lines>102,103,104,113,116,117,119,120,121,122,124,125,126,130,131,132,133,134,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>15,23,82,83,91,92,156</added_lines>
 			<deleted_lines>15,16,17,18,26,27,76,78,79,80,92,93,94,157,158,159,161,162,163,164,165,168,169,170</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onnxruntime\test\providers\cpu\tensor\quantize_linear_test.cc' new_name='onnxruntime\test\providers\cpu\tensor\quantize_linear_test.cc'>
 		<file_info nloc='106' complexity='10' token_count='1256'></file_info>
 		<method name='onnxruntime::test::TEST' parameters='QuantizeLinearOpTest,QuantizeLinear_2D'>
 				<method_info nloc='15' complexity='1' token_count='145' nesting_level='2' start_line='103' end_line='117'></method_info>
 			<added_lines>103</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='DequantizeLinearOpTest,DequantizeLinear_Scalar'>
 				<method_info nloc='8' complexity='1' token_count='95' nesting_level='2' start_line='49' end_line='56'></method_info>
 			<added_lines>49,50,51,52,53,54,55,56</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='DequantizeLinearOpTest,DequantizeLinear_2'>
 				<method_info nloc='15' complexity='1' token_count='147' nesting_level='2' start_line='32' end_line='46'></method_info>
 			<added_lines>32</added_lines>
 			<deleted_lines>32</deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='QuantizeLinearOpTest,QuantizeLinear_1'>
 				<method_info nloc='15' complexity='1' token_count='145' nesting_level='2' start_line='94' end_line='108'></method_info>
 			<added_lines>103</added_lines>
 			<deleted_lines>94</deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='QuantizeLinearOpTest,QuantizeLinear_Scalar'>
 				<method_info nloc='8' complexity='1' token_count='92' nesting_level='2' start_line='120' end_line='127'></method_info>
 			<added_lines>120,121,122,123,124,125,126,127</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='onnxruntime::test::TEST' parameters='DequantizeLinearOpTest,DequantizeLinear_2D'>
 				<method_info nloc='15' complexity='1' token_count='147' nesting_level='2' start_line='32' end_line='46'></method_info>
 			<added_lines>32</added_lines>
 			<deleted_lines>32</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>48,118,119,128</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
