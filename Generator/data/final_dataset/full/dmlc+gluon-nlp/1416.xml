<bug_data>
<bug id='1416' author='preeyank5' open_date='2020-10-29T20:58:56Z' closed_time='2020-10-31T00:33:15Z'>
 	<summary>Issue in formatting the text wikipedia files</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 I tried to run the code block that formats the downloaded the wikipedia text files. Link - &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/datasets/pretrain_corpus/README.md#wikipedia&gt;https://github.com/dmlc/gluon-nlp/blob/master/scripts/datasets/pretrain_corpus/README.md#wikipedia&lt;/denchmark-link&gt;
  , but got an error. Even though the WikiExtractor.py file is present in the mentioned directory.
 &lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;
 
 ImportError: Cannot import WikiExtractor! You can download the "WikiExtractor.py" in &lt;denchmark-link:https://github.com/attardi/wikiextractor&gt;https://github.com/attardi/wikiextractor&lt;/denchmark-link&gt;
  to /home/ec2-user/gluon-nlp/gluon-nlp/scripts/datasets/pretrain_corpus/gluon-nlp/scripts/datasets/pretrain_corpus
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 (If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)
 &lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Create mxnet2.0 python 3.6 environment&lt;/denchmark-h&gt;
 
 conda create -n mxnet2_p36 python=3.6
 source activate mxnet2_p36
 &lt;denchmark-h:h3&gt;Check Cuda version&lt;/denchmark-h&gt;
 
 nvcc --version
 &lt;denchmark-h:h3&gt;Install mxnet-cu100 2.0&lt;/denchmark-h&gt;
 
 python3 -m pip install -U --pre "mxnet-cu100&gt;=2.0.0b20200926" -f &lt;denchmark-link:https://dist.mxnet.io/python&gt;https://dist.mxnet.io/python&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;Git clone from gluon nlp&lt;/denchmark-h&gt;
 
 git clone -b master &lt;denchmark-link:https://github.com/dmlc/gluon-nlp.git&gt;https://github.com/dmlc/gluon-nlp.git&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;cd to gluon-nlp&lt;/denchmark-h&gt;
 
 cd gluon-nlp/
 &lt;denchmark-h:h3&gt;Install gluon-nlp&lt;/denchmark-h&gt;
 
 python3 -m pip install -U -e ."[extras]"
 &lt;denchmark-h:h3&gt;Check nlp_data&lt;/denchmark-h&gt;
 
 nlp_data help
 &lt;denchmark-h:h3&gt;Check nlp_process&lt;/denchmark-h&gt;
 
 nlp_process help
 &lt;denchmark-h:h1&gt;Download Hindi Wikipedia corpus&lt;/denchmark-h&gt;
 
 python3 prepare_wikipedia.py --mode download --lang hi --date latest -o ./
 &lt;denchmark-h:h1&gt;Properly format the text files&lt;/denchmark-h&gt;
 
 python3 prepare_wikipedia.py --mode format -i [path-to-wiki.xml.bz2] -o ./
 Trying on Sagemaker ml.t2.medium instance.
 	</description>
 	<comments>
 		<comment id='1' author='preeyank5' date='2020-10-29T21:05:02Z'>
 		&lt;denchmark-link:https://github.com/ZheyuYe&gt;@ZheyuYe&lt;/denchmark-link&gt;
  Would you have time to take a look?
 		</comment>
 		<comment id='2' author='preeyank5' date='2020-10-31T00:33:15Z'>
 		As confirmed by &lt;denchmark-link:https://github.com/preeyank5&gt;@preeyank5&lt;/denchmark-link&gt;
  , this has been fixed by &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/1417&gt;#1417&lt;/denchmark-link&gt;
  so I've closed the issue.
 		</comment>
 	</comments>
 </bug>
<commit id='e82e0a7c907d19b5642f5425495ae9f5cb57b743' author='Xingjian Shi' date='2020-10-29 21:31:23-07:00'>
 	<dmm_unit complexity='0.7727272727272727' interfacing='0.8636363636363636' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='scripts\datasets\general_nlp_benchmark\prepare_glue.py' new_name='scripts\datasets\general_nlp_benchmark\prepare_glue.py'>
 		<file_info nloc='620' complexity='121' token_count='4754'></file_info>
 		<method name='read_rte' parameters='dir_path'>
 				<method_info nloc='11' complexity='3' token_count='109' nesting_level='0' start_line='233' end_line='243'></method_info>
 			<added_lines>237,242</added_lines>
 			<deleted_lines>237</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\datasets\pretrain_corpus\README.md' new_name='scripts\datasets\pretrain_corpus\README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>30,31,33,34,35,38,39,40,41,42,43,44,45,47,48,49,51</added_lines>
 			<deleted_lines>31,32,35</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\datasets\pretrain_corpus\prepare_wikipedia.py' new_name='scripts\datasets\pretrain_corpus\prepare_wikipedia.py'>
 		<file_info nloc='200' complexity='33' token_count='1837'></file_info>
 		<method name='format_wikicorpus' parameters='input,output,bytes,num_process,num_out_files'>
 				<method_info nloc='38' complexity='9' token_count='356' nesting_level='0' start_line='171' end_line='213'></method_info>
 			<added_lines>171,172,210,211</added_lines>
 			<deleted_lines>171,180,183</deleted_lines>
 		</method>
 		<method name='get_parser' parameters=''>
 				<method_info nloc='32' complexity='1' token_count='194' nesting_level='0' start_line='106' end_line='137'></method_info>
 			<added_lines>135,136</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='main' parameters='args'>
 				<method_info nloc='27' complexity='6' token_count='233' nesting_level='0' start_line='205' end_line='231'></method_info>
 			<added_lines>210,211,214,215</added_lines>
 			<deleted_lines>221,224</deleted_lines>
 		</method>
 		<method name='try_import_wikiextractor' parameters=''>
 				<method_info nloc='17' complexity='3' token_count='68' nesting_level='0' start_line='67' end_line='83'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83</deleted_lines>
 		</method>
 		<method name='format_wikicorpus' parameters='input,output,bytes,num_process,num_out_files,quiet'>
 				<method_info nloc='41' complexity='10' token_count='375' nesting_level='0' start_line='156' end_line='202'></method_info>
 			<added_lines>156,165,166,168,170,171,172</added_lines>
 			<deleted_lines>171,180,183</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>11</added_lines>
 			<deleted_lines>84</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='setup.py' new_name='setup.py'>
 		<file_info nloc='99' complexity='3' token_count='388'></file_info>
 		<modified_lines>
 			<added_lines>88</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonnlp\utils\lazy_imports.py' new_name='src\gluonnlp\utils\lazy_imports.py'>
 		<file_info nloc='137' complexity='28' token_count='350'></file_info>
 		<method name='try_import_wikiextractor' parameters=''>
 				<method_info nloc='10' complexity='2' token_count='22' nesting_level='0' start_line='172' end_line='181'></method_info>
 			<added_lines>172,173,174,175,176,177,178,179,180,181</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>29,30,170,171</added_lines>
 			<deleted_lines>29</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\README.md' new_name='tests\README.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>42,43</added_lines>
 			<deleted_lines>42,43</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\data_cli\test_glue.py'>
 		<file_info nloc='24' complexity='2' token_count='190'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\data_cli\test_wikipedia.py'>
 		<file_info nloc='12' complexity='1' token_count='87'></file_info>
 	</modification>
 </commit>
</bug_data>
