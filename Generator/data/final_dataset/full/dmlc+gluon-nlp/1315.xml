<bug_data>
<bug id='1315' author='ZiyueHuang' open_date='2020-08-25T14:35:13Z' closed_time='2020-08-28T11:53:25Z'>
 	<summary>Bugs for run_electra.py at master branch</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Bug for Horovod support&lt;/denchmark-h&gt;
 
 At &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L418&gt;https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L418&lt;/denchmark-link&gt;
 ,  depends on the data batch, and thus could be different across the horovod processes (note that Horovod will launch a process for each GPU card), meaning that the parameters maintained in each horovod process (since we use ) might diverge.
 We can normalize the loss by batch_size on each worker before trainer.allreduce_grads(), to avoid syncing num_samples_per_update across all workers.
 &lt;denchmark-h:h3&gt;Bug for MLM loss&lt;/denchmark-h&gt;
 
 In the current implementation, the MLM loss actually doesn't take into account masked_weights.
 At &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L383-L384&gt;https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L383-L384&lt;/denchmark-link&gt;
 ,  should be replaced by , please see the doc of  at &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/19010/files&gt;https://github.com/apache/incubator-mxnet/pull/19010/files&lt;/denchmark-link&gt;
  or the example usage in bert at &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/v0.10.x/scripts/bert/pretraining_utils.py#L416-L418&gt;https://github.com/dmlc/gluon-nlp/blob/v0.10.x/scripts/bert/pretraining_utils.py#L416-L418&lt;/denchmark-link&gt;
 
 Note that
 mlm_scores's shape: (batch_size, num_masked_positions, vocab_size)
 unmasked_tokens's shape: (batch_size, num_masked_positions)
 masked_weights's shape: (batch_size, num_masked_positions)
 If we comment out  and add two lines before/after &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/loss.py#L408&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/loss.py#L408&lt;/denchmark-link&gt;
 :  and , running the current script run_electra.py we will get
 &lt;denchmark-code&gt;loss.shape before weighting  (64, 19, 1)  sample_weight.shape  (1216,)
 loss.shape after weighting  (64, 19, 1216)
 &lt;/denchmark-code&gt;
 
 note that 1216 = 64 x 19
 cc &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ZheyuYe&gt;@ZheyuYe&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;
 
 (Paste the complete error message, including stack trace.)
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 (If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)
 &lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 (Paste the commands you ran that produced the error.)
 
 
 
 
 &lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;
 
 
 
 
 
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
 &lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python
 
 # paste outputs here
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='ZiyueHuang' date='2020-08-25T14:58:43Z'>
 		There is indeed a Bug for mlm loss here, thank you very patiently and carefully to revise the training script and find this bug out. As Far as I can see, this MLM bug will lead to a steep increase in calculations which need to be fixed.
 		</comment>
 		<comment id='2' author='ZiyueHuang' date='2020-08-25T15:00:06Z'>
 		&lt;denchmark-link:https://github.com/ZheyuYe&gt;@ZheyuYe&lt;/denchmark-link&gt;
  Thanks for the response, I will work on the fix later.
 		</comment>
 		<comment id='3' author='ZiyueHuang' date='2020-08-25T17:04:53Z'>
 		That might also explain why it's slower. Thanks for the observation! We need to refactor the pretraining script to make it more structured.
 		</comment>
 	</comments>
 </bug>
<commit id='66e5e057347c3710eb8fa27c134a9303309e94c0' author='Ziyue Huang' date='2020-08-28 02:32:22-07:00'>
 	<dmm_unit complexity='0.4666666666666667' interfacing='0.5333333333333333' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='scripts\pretraining\run_electra.py' new_name='scripts\pretraining\run_electra.py'>
 		<file_info nloc='461' complexity='57' token_count='3485'></file_info>
 		<method name='accuracy' parameters='labels,predictions,weights'>
 				<method_info nloc='6' complexity='2' token_count='71' nesting_level='0' start_line='490' end_line='495'></method_info>
 			<added_lines>495</added_lines>
 			<deleted_lines>492,493</deleted_lines>
 		</method>
 		<method name='train' parameters='args'>
 				<method_info nloc='215' complexity='38' token_count='1554' nesting_level='0' start_line='226' end_line='486'></method_info>
 			<added_lines>229,230,231,232,233,234,277,278,280,338,358,383,385,386,387,388,390,398,409,413,415,416,417,418,419,420,421,422,423,424,425,427,441,458,459,460,461,462,463,478,479,482</added_lines>
 			<deleted_lines>271,273,339,340,347,348,349,355,376,381,382,384,386,394,405,409,410,411,412,413,414,416,417,418,420,434,451,452,453,454,455,461,471,472,475,481,482</deleted_lines>
 		</method>
 		<method name='evaluation' parameters='writer,step_num,masked_input,eval_input'>
 				<method_info nloc='33' complexity='2' token_count='247' nesting_level='0' start_line='513' end_line='547'></method_info>
 			<added_lines>533,534,535,536,537,538,539,540,541,542,543,544,545,546,547</added_lines>
 			<deleted_lines>529,530,531,532,533,534,535,536,541,543,544,545,546</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>489</added_lines>
 			<deleted_lines>489</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
