<bug_data>
<bug id='1086' author='liuzh91' open_date='2020-01-03T08:23:43Z' closed_time='2020-01-10T02:46:27Z'>
 	<summary>Conflict between weight tied and weight sharing</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 This bug is the same as &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/17184&gt;apache/incubator-mxnet#17184&lt;/denchmark-link&gt;
  It is an error introduced in gluonnlp models instead of mxnet side, so I move the issue here.
 We experience some weight initialization error when we use weight sharing and weight tied simultaneously. We share weights between model and model_eval. The code is shown below:
 model = nlp.model.train.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,
                                args.tied, args.dropout, args.weight_dropout,
                                args.dropout_h, args.dropout_i, args.dropout_e)
 model_eval = nlp.model.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,
                               args.tied, args.dropout, args.weight_dropout,
                               args.dropout_h, args.dropout_i, args.dropout_e,
                               params=model.collect_params())
 
 model.initialize(mx.init.Xavier(), ctx=context)
 
 model.hybridize(static_alloc=True)
 
 print(model)
 
 def check_initialized(net):
     params = net.collect_params()
     for param in params:
         try:
             params[param].list_ctx()
         except RuntimeError:
             return False
     return True
 
 print(check_initialized(model))
 print(check_initialized(model_eval))
 &lt;denchmark-h:h3&gt;Log Message&lt;/denchmark-h&gt;
 
 If args.tied is set True, we get the following log message:
 True
 False
 If we turn off args.tied, the initialization works correctly.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 (If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)
 The file can be found in (&lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/v0.8.x/scripts/language_model/word_language_model.py&gt;https://github.com/dmlc/gluon-nlp/blob/v0.8.x/scripts/language_model/word_language_model.py&lt;/denchmark-link&gt;
 ). To reproduce the above message, you may need to replace line 153 onward  with the above code snippet. Run the following command:
 &lt;denchmark-code&gt;python -m pdb word_language_model.py --tied --dropout_e=0
 &lt;/denchmark-code&gt;
 
 You will encounter the above error.
 &lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;
 
 The parameter that not initialized properly is the parameter awdrnn0_hybridsequential0_embedding0_bias. It is the weight used in the decoder of AWDRNN.  After some investigation, we found it is the tied weights introducing this error:
     if self._tie_weights:
          output.add(nn.Dense(self._vocab_size, flatten=False,
                              params=self.embedding[0].params))
 I print some debug information which may be helpful here:
 &lt;denchmark-code&gt;(Pdb) model_eval.decoder[0]._params['awdrnn0_hybridsequential0_embedding0_bias'].list_ctx()
 *** RuntimeError: Parameter 'awdrnn0_hybridsequential0_embedding0_bias' has not been initialized
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
 &lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python
 
 # paste outputs here
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='0f29a403068a10ce33faca4c9a4c25879113f5cb' author='liuzh91' date='2020-01-09 12:41:23+01:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\gluonnlp\model\train\language_model.py' new_name='src\gluonnlp\model\train\language_model.py'>
 		<file_info nloc='319' complexity='49' token_count='1926'></file_info>
 		<method name='_get_decoder' parameters='self'>
 				<method_info nloc='16' complexity='3' token_count='130' nesting_level='1' start_line='105' end_line='124'></method_info>
 			<added_lines>109,110,111,112,113,114,115,116,117,118,119,120,121</added_lines>
 			<deleted_lines>106,107</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>21,74,75,76</added_lines>
 			<deleted_lines>21</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
