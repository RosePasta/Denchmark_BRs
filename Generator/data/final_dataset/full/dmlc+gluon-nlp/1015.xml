<bug_data>
<bug id='1015' author='carter54' open_date='2019-11-20T02:31:59Z' closed_time='2019-11-20T08:20:28Z'>
 	<summary>prev_len in gpt.py</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 the gpt code in
 
 
 
 gluon-nlp/scripts/text_generation/model/gpt.py
 
 
          Line 258
       in
       5e11334
 
 
 
 
 
 
  prev_len = states[0].shape[1] 
 
 
 
 
 
 seems to have a mistake.
 I might be wrong, but I think the code prev_len = states[0].shape[1] means to return the length of previous key/value matrix (or the previous input tokens) . However it returns the number of multi-head (12 for the 124M gpt2 model).
 &lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;
 
 NA
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 run scripts/text_generation/sequence_sampling.py script and print the output of prev_len = states[0].shape[1] at line 258 in scripts/text_generation/model/gpt.py.
 &lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;
 
 
 change to prev_len = states[0].shape[2]
 
 	</description>
 	<comments>
 		<comment id='1' author='carter54' date='2019-11-20T04:20:07Z'>
 		Then the positional embedding is calculated for the wrong positions
 
 
 
 gluon-nlp/scripts/text_generation/model/gpt.py
 
 
         Lines 262 to 265
       in
       5e11334
 
 
 
 
 
 
  data_pos = mx.nd.arange(prev_len, prev_len + seq_len, ctx=data.context, dtype=np.float32) 
 
 
 
  data_pos = mx.nd.broadcast_axes(mx.nd.expand_dims(data_pos, axis=0), 
 
 
 
  axis=0, size=batch_size) 
 
 
 
  out = self._embed(data) + self._pos_embed(data_pos) 
 
 
 
 
 
 		</comment>
 		<comment id='2' author='carter54' date='2019-11-20T04:33:44Z'>
 		Added a fix to &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/1010&gt;#1010&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
  please help to review.
 		</comment>
 		<comment id='3' author='carter54' date='2019-11-20T04:47:40Z'>
 		Yes, it's a bug and it should be states[0].shape[2]
 		</comment>
 		<comment id='4' author='carter54' date='2019-11-20T04:47:59Z'>
 		&lt;denchmark-link:https://github.com/carter54&gt;@carter54&lt;/denchmark-link&gt;
  Really appreciate for pointing out this!
 		</comment>
 		<comment id='5' author='carter54' date='2019-11-20T07:42:32Z'>
 		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
  happy to help! May I ask a question not related to this bug. Is there any method to accelerate the inference speed of gpt2 model? Can HybridBlock help? or quantization possibly?
 		</comment>
 		<comment id='6' author='carter54' date='2019-11-20T07:45:08Z'>
 		Hybridization will definitely help. Quantization also helps but Iâ€™m not sure how to do that in MXNet. Another solution is to dump the Json file and use TVM for inference.
 		</comment>
 		<comment id='7' author='carter54' date='2019-11-20T08:17:41Z'>
 		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
  Thanks for your reply~ I read the TVM tutorial, most of the examples are within image processing area. Did you try TVM on gpt2 model before? Is there any examples and what range of the acceleration ratio can be expected? Thank you~
 		</comment>
 	</comments>
 </bug>
<commit id='ebfc920026a2a8a5bbc1cc531eeba75e4e21bff7' author='Leonard Lausen' date='2019-11-20 16:20:27+08:00'>
 	<dmm_unit complexity='0.0' interfacing='0.5454545454545454' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytest.ini' new_name='pytest.ini'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>15,16,17,18</added_lines>
 			<deleted_lines>15</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\tests\test_scripts.py' new_name='scripts\tests\test_scripts.py'>
 		<file_info nloc='289' complexity='32' token_count='2147'></file_info>
 		<method name='test_sampling' parameters='method,lmmodel'>
 				<method_info nloc='16' complexity='5' token_count='108' nesting_level='0' start_line='144' end_line='159'></method_info>
 			<added_lines>144,145,146,147,148,149,150</added_lines>
 			<deleted_lines>144,145</deleted_lines>
 		</method>
 		<method name='test_sampling' parameters='method'>
 				<method_info nloc='11' complexity='3' token_count='92' nesting_level='0' start_line='144' end_line='154'></method_info>
 			<added_lines>144,145,146,147,148,149,150</added_lines>
 			<deleted_lines>144,145</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>143</added_lines>
 			<deleted_lines>138</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\text_generation\model\gpt.py' new_name='scripts\text_generation\model\gpt.py'>
 		<file_info nloc='367' complexity='18' token_count='2158'></file_info>
 		<method name='forward' parameters='self,data,states'>
 				<method_info nloc='38' complexity='4' token_count='587' nesting_level='1' start_line='91' end_line='136'></method_info>
 			<added_lines>94,95,96,97,98,99,100,101,104,105,106,107,108,109,113,116,124,125,126,128,129,131,132,136</added_lines>
 			<deleted_lines>91,92,93,97,100,101,102,103,104,105,109,112,120,121,122,124,125,127,128,132,133</deleted_lines>
 		</method>
 		<method name='hybrid_forward' parameters='self,F,data,states'>
 				<method_info nloc='39' complexity='4' token_count='636' nesting_level='1' start_line='90' end_line='140'></method_info>
 			<added_lines>90,94,95,96,97,98,99,100,101,104,105,106,107,108,109,113,116,124,125,126,128,129,131,132,136,137</added_lines>
 			<deleted_lines>91,92,93,97,100,101,102,103,104,105,109,112,120,121,122,124,125,127,128,132,133</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>25,34,193,230,244,245,246,247,248,249,250,251,252,268,269,270,271,272,274,275,276,277,278,279,280</added_lines>
 			<deleted_lines>25,27,35,189,226,240,241,256,258,260,261,262,263,264</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\text_generation\sequence_sampling.py' new_name='scripts\text_generation\sequence_sampling.py'>
 		<file_info nloc='141' complexity='17' token_count='1208'></file_info>
 		<method name='generate' parameters=''>
 				<method_info nloc='39' complexity='6' token_count='360' nesting_level='0' start_line='144' end_line='186'></method_info>
 			<added_lines>173,174,175</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>64</added_lines>
 			<deleted_lines>64</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
