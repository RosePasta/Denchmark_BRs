<bug_data>
<bug id='905' author='liusy182' open_date='2019-08-28T06:47:59Z' closed_time='2019-09-11T20:52:51Z'>
 	<summary>KeyError: '&amp;lt;pad&amp;gt;' if run /scripts/word_embeddings/evaluate_pretrained.py with flag `analog-max-vocab-size`</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 run /scripts/word_embeddings/evaluate_pretrained.py with flag analog-max-vocab-size throws exception.
 &lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "evaluate_pretrained.py", line 216, in &lt;module&gt;
     vocab.set_embedding(token_embedding_)
   File "/Users/siyuanl/private/gluon-nlp/src/gluonnlp/vocab/vocab.py", line 412, in set_embedding
     new_idx_to_vec[1:, col_start:col_end] = embs[self._idx_to_token[1:]]
   File "/Users/siyuanl/private/gluon-nlp/src/gluonnlp/embedding/token_embedding.py", line 637, in __getitem__
     indices = [self._token_to_idx[token] for token in tokens]
   File "/Users/siyuanl/private/gluon-nlp/src/gluonnlp/embedding/token_embedding.py", line 637, in &lt;listcomp&gt;
     indices = [self._token_to_idx[token] for token in tokens]
 KeyError: '&lt;pad&gt;'
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;cd scripts/word_embeddings
 
 python evaluate_pretrained.py --gpu 0  --embedding-name glove --embedding-source glove.42B.300d --logdir results --analogy-max-vocab-size 300000 --analogy-datasets GoogleAnalogyTestSet BiggerAnalogyTestSet
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;
 
 It looks like the reason is because &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/word_embeddings/evaluate_pretrained.py#L208&gt;enforce_max_size&lt;/denchmark-link&gt;
  trims off certain tokens such as  which results to the error above.
 What should be the correct way to run this script?
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
 &lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python
 
 # paste outputs here
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='liusy182' date='2019-08-28T11:29:01Z'>
 		Thanks for reporting this issue liusy182. The reason was that  function incorrectly overwrote an internal TokenEmbedding datastructure, breaking the use of 's embedding for . When this script was written, modifying TokenEmbedding's internals was required. Based on &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/750&gt;#750&lt;/denchmark-link&gt;
  we can now use proper API to make the required changes.
 In general however, vocab = nlp.Vocab(nlp.data.count_tokens(tokens)) can be replaced with vocab = nlp.Vocab(nlp.data.count_tokens(tokens), unknown_token=token_embedding_.unknown_token, padding_token=None, bos_token=None, eos_token=None). I'm opening a PR for these two changes.
 Unfortunately this error slipped through the tests. I'm also extending the testcase.
 		</comment>
 	</comments>
 </bug>
<commit id='b810d10386fad58bded6b39b233f5009406f6da3' author='Leonard Lausen' date='2019-09-11 13:52:50-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.5'></dmm_unit>
 	<modification change_type='MODIFY' old_name='scripts\tests\test_scripts.py' new_name='scripts\tests\test_scripts.py'>
 		<file_info nloc='360' complexity='31' token_count='2471'></file_info>
 		<method name='test_embedding_evaluate_pretrained' parameters='fasttextloadngrams'>
 				<method_info nloc='12' complexity='2' token_count='61' nesting_level='0' start_line='72' end_line='84'></method_info>
 			<added_lines>72,73,83,84</added_lines>
 			<deleted_lines>72</deleted_lines>
 		</method>
 		<method name='test_embedding_evaluate_from_path' parameters='evaluateanalogies,maxvocabsize'>
 				<method_info nloc='17' complexity='3' token_count='117' nesting_level='0' start_line='96' end_line='112'></method_info>
 			<added_lines>104,108</added_lines>
 			<deleted_lines>101,105</deleted_lines>
 		</method>
 		<method name='test_embedding_evaluate_pretrained' parameters='fasttextloadngrams,maxvocabsize'>
 				<method_info nloc='14' complexity='3' token_count='76' nesting_level='0' start_line='73' end_line='87'></method_info>
 			<added_lines>73,83,84</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='scripts\word_embeddings\evaluate_pretrained.py' new_name='scripts\word_embeddings\evaluate_pretrained.py'>
 		<file_info nloc='177' complexity='18' token_count='1142'></file_info>
 		<method name='load_embedding_from_path' parameters='args'>
 				<method_info nloc='22' complexity='4' token_count='170' nesting_level='0' start_line='134' end_line='165'></method_info>
 			<added_lines>148,149,150,151,152,153,154,157,158,159,160,161</added_lines>
 			<deleted_lines>143,144,145,151,152,153,154,155,156,159</deleted_lines>
 		</method>
 		<method name='enforce_max_size' parameters='token_embedding,size'>
 				<method_info nloc='11' complexity='4' token_count='86' nesting_level='0' start_line='180' end_line='190'></method_info>
 			<added_lines>185,186,187,188,189,190</added_lines>
 			<deleted_lines>183,184,185,186,187,188</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>210,211,212,213,219,220,221,235,236,237</added_lines>
 			<deleted_lines>208,214,228</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonnlp\vocab\vocab.py' new_name='src\gluonnlp\vocab\vocab.py'>
 		<file_info nloc='381' complexity='58' token_count='1720'></file_info>
 		<method name='set_embedding' parameters='self,embeddings'>
 				<method_info nloc='31' complexity='11' token_count='235' nesting_level='1' start_line='373' end_line='421'></method_info>
 			<added_lines>403,404</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
