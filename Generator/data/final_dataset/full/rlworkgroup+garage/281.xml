<bug_data>
<bug id='281' author='ryanjulian' open_date='2018-08-14T01:07:08Z' closed_time='2018-10-30T22:50:14Z'>
 	<summary>policy_ent_coeff does not seem to work in tf/NPO</summary>
 	<description>
 I can choose any value (or GaussianMLPPolicy params) I want, and it doesn't change the policy standard deviation. e.g. -1.0 won't cause the policy stddev to fall, and 1.0 won't cause it to rise. This may be because a gradient path is broken between the reward function and the policy std network.
 	</description>
 	<comments>
 		<comment id='1' author='ryanjulian' date='2018-08-14T20:20:55Z'>
 		What param do you set to -1.0? init_std?
 		</comment>
 		<comment id='2' author='ryanjulian' date='2018-08-14T20:24:17Z'>
 		My assertion was
 if policy_ent_coeff = -1.0, then during training I should see the policy entropy decrease rapidly (if most environment rewards are small)
 likewise, if policy_ent_coeff = 1.0, then I should see policy entropy increase rapidly.
 this is because the optimizer should be able to differentiate through the loss function all the way to the std_network of the policy. the term policy_ent_coeff tunes the magnitude of this gradient.
 		</comment>
 		<comment id='3' author='ryanjulian' date='2018-08-14T20:29:52Z'>
 		Things to check:
 
 Assuming your augmented rewards equation looks like r + (policy_ent_coeff * policy_entropy), ensure that policy_entropy is differentiable (symbolic) all the way back to the std_network of the policy. This means that your loss function retrieves policy_entropy by purely by feeding actions and observations, not by using the recorded mean/std (e.g. agent_infos, dist_infos).
 Make sure that dist_infos are only used for forward pass calculations (e.g. KL divergence and likelihood ratio)
 
 		</comment>
 		<comment id='4' author='ryanjulian' date='2018-10-17T17:50:27Z'>
 		&lt;denchmark-link:https://github.com/CatherineSue&gt;@CatherineSue&lt;/denchmark-link&gt;
  were you able to reproduce this?
 		</comment>
 		<comment id='5' author='ryanjulian' date='2018-10-17T18:05:47Z'>
 		Yes. I reproduced the bug.
 		</comment>
 		<comment id='6' author='ryanjulian' date='2018-10-25T17:51:55Z'>
 		For this issue, let's use
 
 MaxEnt (e.g. augment the reward with an entropy term and a coefficient): \hat r = r + \alpha_1 * H
 log-likelihood estimator: H ~ -logli(action| policy(state))
 Turn off the gradient: H = tf.stop_gradient(H)
 
 		</comment>
 	</comments>
 </bug>
<commit id='26c14a3e943e219b9202ceaacc72b6caffe2b8c8' author='Chang Su' date='2018-10-30 15:44:55-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.8' size='0.13333333333333333'></dmm_unit>
 	<modification change_type='MODIFY' old_name='examples\tf\ppo_pendulum.py' new_name='examples\tf\ppo_pendulum.py'>
 		<file_info nloc='40' complexity='1' token_count='182'></file_info>
 		<method name='run_task' parameters='_'>
 				<method_info nloc='16' complexity='1' token_count='111' nesting_level='0' start_line='21' end_line='45'></method_info>
 			<added_lines>30</added_lines>
 			<deleted_lines>30,31</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>8,9</added_lines>
 			<deleted_lines>8,9</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='garage\tf\algos\npo.py' new_name='garage\tf\algos\npo.py'>
 		<file_info nloc='382' complexity='32' token_count='2371'></file_info>
 		<method name='optimize_policy' parameters='self,itr,samples_data'>
 				<method_info nloc='28' complexity='1' token_count='252' nesting_level='1' start_line='77' end_line='109'></method_info>
 			<added_lines>102,103</added_lines>
 			<deleted_lines>98</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,pg_loss,clip_range,optimizer,optimizer_args,name,policy,policy_ent_coeff,use_softplus_entropy,use_neg_logli_entropy,stop_entropy_gradient,kwargs'>
 				<method_info nloc='12' complexity='1' token_count='54' nesting_level='1' start_line='30' end_line='41'></method_info>
 			<added_lines>37,39,40</added_lines>
 			<deleted_lines>37</deleted_lines>
 		</method>
 		<method name='_build_entropy_term' parameters='self,i'>
 				<method_info nloc='37' complexity='5' token_count='207' nesting_level='1' start_line='359' end_line='403'></method_info>
 			<added_lines>366,367,368,369,375,376,377,378,379,380,381,382,383,384,393,394,395,396</added_lines>
 			<deleted_lines>367,368,376</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,pg_loss,clip_range,optimizer,optimizer_args,name,policy,policy_ent_coeff,use_softplus_entropy,kwargs'>
 				<method_info nloc='10' complexity='1' token_count='46' nesting_level='1' start_line='30' end_line='39'></method_info>
 			<added_lines>37,39</added_lines>
 			<deleted_lines>37</deleted_lines>
 		</method>
 		<method name='_build_policy_loss' parameters='self,i'>
 				<method_info nloc='107' complexity='11' token_count='659' nesting_level='1' start_line='226' end_line='357'></method_info>
 			<added_lines>334</added_lines>
 			<deleted_lines>329</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>45,46</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
