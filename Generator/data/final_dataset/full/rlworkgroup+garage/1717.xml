<bug_data>
<bug id='1717' author='st2yang' open_date='2020-07-07T03:45:15Z' closed_time='2020-07-13T15:41:55Z'>
 	<summary>Failed to reproduce example her_ddpg_fetchreach</summary>
 	<description>
 Hi,
 I was trying to run examples/tf/&lt;denchmark-link:https://github.com/rlworkgroup/garage/blob/master/examples/tf/her_ddpg_fetchreach.py&gt;her_ddpg_fetchreach.py&lt;/denchmark-link&gt;
  but got a much worse performance. I attached the results as follows, and it seems that it's not working at all. Do you have any idea how to make it work? Though the default parameters look reasonable, should I try to tune some parameters?
 Thank you in advance.
 
 AverageSuccessRate                       0
 Epoch                                   49
 Evaluation/AverageDiscountedReturn      -9.94112
 Evaluation/AverageReturn              -999.93
 Evaluation/CompletionRate                0
 Evaluation/Iteration                   980
 Evaluation/MaxReturn                  -998
 Evaluation/MinReturn                 -1000
 Evaluation/NumTrajs                    100
 Evaluation/StdReturn                     0.324191
 Policy/AveragePolicyLoss                 4.17451
 QFunction/AverageAbsQ                    4.19709
 QFunction/AverageAbsY                    4.19412
 QFunction/AverageQ                      -4.16916
 QFunction/AverageQFunctionLoss           0.0232313
 QFunction/AverageY                      -4.16946
 QFunction/MaxQ                           2.87869
 QFunction/MaxY                           2.62668
 TotalEnvSteps                       100000
 
 	</description>
 	<comments>
 		<comment id='1' author='st2yang' date='2020-07-07T18:05:15Z'>
 		&lt;denchmark-link:https://github.com/st2yang&gt;@st2yang&lt;/denchmark-link&gt;
  thanks for the bug report! we will investigate in the next few weeks and get back to you.
 in the meantime, if you find working parameters or the root cause, please let us know!
 		</comment>
 		<comment id='2' author='st2yang' date='2020-07-07T19:41:06Z'>
 		&lt;denchmark-link:https://github.com/ryanjulian&gt;@ryanjulian&lt;/denchmark-link&gt;
  Sure, thanks for your help. I tried to change some parameters and might get a better result. But I'm not sure how good it is. Since I am new to garage, I am wondering if there is a way to evaluate the trained policy by rendering the environment?
 		</comment>
 		<comment id='3' author='st2yang' date='2020-07-07T19:43:59Z'>
 		You can either set  in  or follow &lt;denchmark-link:https://garage--1642.org.readthedocs.build/en/1642/user/reuse_garage_policy.html&gt;this guide&lt;/denchmark-link&gt;
  (it is in a pull request now, but will appear on garage.readthedocs.io in the next day or two)
 		</comment>
 		<comment id='4' author='st2yang' date='2020-07-07T20:13:03Z'>
 		Thanks &lt;denchmark-link:https://github.com/ryanjulian&gt;@ryanjulian&lt;/denchmark-link&gt;
  , I'll try. BTW, when I tried the second option, it has some conflict error regarding tf
 
 data = snapshotter.load('path/to/snapshot/dir')
 File "/homes/yangyang/.local/lib/python3.6/site-packages/garage/experiment/snapshotter.py", line 153, in load
 return joblib.load(file)
 File "/homes/yangyang/.local/lib/python3.6/site-packages/joblib/numpy_pickle.py", line 588, in load
 obj = _unpickle(fobj)
 File "/homes/yangyang/.local/lib/python3.6/site-packages/joblib/numpy_pickle.py", line 526, in _unpickle
 obj = unpickler.load()
 File "/homes/yangyang/miniconda3/envs/mujoco-gym/lib/python3.6/pickle.py", line 1050, in load
 dispatchkey[0]
 File "/homes/yangyang/.local/lib/python3.6/site-packages/joblib/numpy_pickle.py", line 339, in load_build
 Unpickler.load_build(self)
 File "/homes/yangyang/miniconda3/envs/mujoco-gym/lib/python3.6/pickle.py", line 1507, in load_build
 setstate(state)
 File "/homes/yangyang/.local/lib/python3.6/site-packages/garage/tf/policies/continuous_mlp_policy.py", line 209, in setstate
 self._initialize()
 File "/homes/yangyang/.local/lib/python3.6/site-packages/garage/tf/policies/continuous_mlp_policy.py", line 85, in _initialize
 shape=(None, self._obs_dim))
 
 
 File "/homes/yangyang/miniconda3/envs/mujoco-gym/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py", line 3023, in placeholder
 raise RuntimeError("tf.placeholder() is not compatible with "
 RuntimeError: tf.placeholder() is not compatible with eager execution.
 
 		</comment>
 		<comment id='5' author='st2yang' date='2020-07-07T20:20:29Z'>
 		&lt;denchmark-link:https://github.com/st2yang&gt;@st2yang&lt;/denchmark-link&gt;
  can you post the full error message? if it doesn't fit in github, you can use pastebin or similar.
 		</comment>
 		<comment id='6' author='st2yang' date='2020-07-07T20:39:38Z'>
 		&lt;denchmark-link:https://github.com/ryanjulian&gt;@ryanjulian&lt;/denchmark-link&gt;
  Sorry I updated the error message. The error happens when loading tf policy in snapshotter.load('path/to/snapshot/dir').
 		</comment>
 		<comment id='7' author='st2yang' date='2020-07-07T20:41:10Z'>
 		Can you post the full text of the script you're running? It looks like TF2 has been initialized in eager mode (rather than graph mode), which garage doesn't support.
 		</comment>
 		<comment id='8' author='st2yang' date='2020-07-07T20:50:19Z'>
 		&lt;denchmark-link:https://github.com/ryanjulian&gt;@ryanjulian&lt;/denchmark-link&gt;
  I am using the exact example script as you mentioned. Disabling eager by disable_eager_execution() will throw another error. I think it's tf-related issue, I might use pytorch first?
 
 from garage.experiment import Snapshotter
 snapshotter = Snapshotter()
 data = snapshotter.load('path/to/snapshot/dir')
 
 		</comment>
 		<comment id='9' author='st2yang' date='2020-07-07T22:35:37Z'>
 		&lt;denchmark-link:https://github.com/st2yang&gt;@st2yang&lt;/denchmark-link&gt;
  yeah -- if you want an immediate solution, try the PyTorch version. However, I don't think HER is currently implemented in the torch version.
 I don't think it's a difficult modification.
 		</comment>
 		<comment id='10' author='st2yang' date='2020-07-07T22:46:13Z'>
 		&lt;denchmark-link:https://github.com/ryanjulian&gt;@ryanjulian&lt;/denchmark-link&gt;
  I thought HERBuffer is independent of the backend, and I can actually run pytorch code with HER. You mean HERBuffer wouldn't boost policy training in the torch vision? Like HERBuffer would be treated as a regular buffer?
 		</comment>
 		<comment id='11' author='st2yang' date='2020-07-07T22:51:18Z'>
 		&lt;denchmark-link:https://github.com/st2yang&gt;@st2yang&lt;/denchmark-link&gt;
  you're right! we recently made HERReplayBuffer can be used independent of the algorithm.
 the only caveat i'll add is that we haven't tested HERReplayBuffer+torch/DDPG -- let us know how it goes!
 		</comment>
 		<comment id='12' author='st2yang' date='2020-07-07T22:53:51Z'>
 		&lt;denchmark-link:https://github.com/st2yang&gt;@st2yang&lt;/denchmark-link&gt;
  in your original attempt, did you replace  with the actual path to your algorithm's snapshot?
 		</comment>
 		<comment id='13' author='st2yang' date='2020-07-07T23:13:40Z'>
 		&lt;denchmark-link:https://github.com/ryanjulian&gt;@ryanjulian&lt;/denchmark-link&gt;
 
 
 
 snapshot loading
 Yes, I replaced it. And the snapshot works in torch version. So there's some minor bug/conflict in tf version.
 
 
 HER
 HERBuffer + torch/ddpg is at least runnable and give some reasonable results. Since FetchReach is a simple task, I am not sure how much HERBuffer contributes in this case.
 
 
 		</comment>
 		<comment id='14' author='st2yang' date='2020-07-07T23:20:01Z'>
 		Some updates regarding example script her_ddpg_fetchreach:
 After some experiments, I think there're two types of bugs or errors
 (1) the original parameters are not working at all. After I change some parameters, the training is indeed much better and give reasonable performance.
 (2) statistic calculation might be wrong. Under a reasonably good performance (in terms of other statistics and evaluation rendering), AverageSuccessRate and Evaluation/CompletionRate are always 0, which doesn't make sense.
 		</comment>
 		<comment id='15' author='st2yang' date='2020-07-07T23:39:39Z'>
 		&lt;denchmark-link:https://github.com/st2yang&gt;@st2yang&lt;/denchmark-link&gt;
  feel free to send us a PR with the updated params. We would love to have it!
 As for (2), there might be a misunderstanding here around the definition of those metrics:
 
 
 Evaluation/AverageSuccessRate calculates the average number of trajectories which have 'success=True' in their env_info field (see here). This applies only to metaworld environments, as far as I know. If you seeing this metrics in your log while using the fetch environments, that's a bug. To compute this for the gym Fetch environments, you would need to look for the key 'is_success' (see here). Feel free to send us a PR for that.
 
 
 Evaluation/CompletionRate measures the average number of trajectories which end in a termination condition (i.e. done==True in the gym.Env interface). However, not all environments send termination signals, so it's quite common for this to be 0 for a working agent. As far as I can tell, the Fetch environments never send termination signals (see here), so this always being 0 is normal.
 
 
 		</comment>
 		<comment id='16' author='st2yang' date='2020-07-08T00:40:54Z'>
 		&lt;denchmark-link:https://github.com/ryanjulian&gt;@ryanjulian&lt;/denchmark-link&gt;
  I see. Thanks a lot for your detailed explanation! I'll try to look into these and do more experiments, then send a PR about the issues.
 		</comment>
 		<comment id='17' author='st2yang' date='2020-07-08T06:51:21Z'>
 		&lt;denchmark-link:https://github.com/st2yang&gt;@st2yang&lt;/denchmark-link&gt;
  Thanks for bringing this up. For now, could you try entering a  and see if loading tf policy works?
 &lt;denchmark-code&gt;from garage.experiment import Snapshotter
 import tensorflow as tf //import tf
 
 snapshotter = Snapshotter()
 with tf.compat.v1.Session(): //enter a tf session
         data = snapshotter.load('path/to/snapshot/dir')
 	policy = data['algo'].policy
 	env = data['env']
 
 	# See what the trained policy can accomplish
 	from garage.sampler.utils import rollout
 	path = rollout(env, policy, animated=True)
 	print(path)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='18' author='st2yang' date='2020-07-08T14:43:57Z'>
 		&lt;denchmark-link:https://github.com/ahtsan&gt;@ahtsan&lt;/denchmark-link&gt;
  Thanks it works!
 		</comment>
 		<comment id='19' author='st2yang' date='2020-07-08T14:50:17Z'>
 		&lt;denchmark-link:https://github.com/ahtsan&gt;@ahtsan&lt;/denchmark-link&gt;
  can you update the docs page accordingly?
 		</comment>
 		<comment id='20' author='st2yang' date='2020-07-13T15:41:55Z'>
 		close this issue, related to PR &lt;denchmark-link:https://github.com/rlworkgroup/garage/pull/1739&gt;#1739&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='00c92ce90d9209f91b5a9570a03ae2cfd5d405a6' author='Ryan Julian' date='2020-07-09 15:45:54+00:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\user\experiments.rst' new_name='docs\user\experiments.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>115</added_lines>
 			<deleted_lines>115</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\user\save_load_resume_exp.md' new_name='docs\user\save_load_resume_exp.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>185</added_lines>
 			<deleted_lines>185</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\user\training_a_policy.md' new_name='docs\user\training_a_policy.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>183</added_lines>
 			<deleted_lines>183</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='examples\jupyter\custom_env.ipynb' new_name='examples\jupyter\custom_env.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>561,584,649,672</added_lines>
 			<deleted_lines>561,584,649,672</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='examples\jupyter\trpo_gym_tf_cartpole.ipynb' new_name='examples\jupyter\trpo_gym_tf_cartpole.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>411,455,499,543,593,637,681,725,775,819</added_lines>
 			<deleted_lines>411,455,499,543,593,637,681,725,775,819</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\garage\_functions.py' new_name='src\garage\_functions.py'>
 		<file_info nloc='111' complexity='22' token_count='692'></file_info>
 		<method name='log_performance' parameters='itr,batch,discount,prefix'>
 				<method_info nloc='24' complexity='5' token_count='240' nesting_level='0' start_line='122' end_line='161'></method_info>
 			<added_lines>157</added_lines>
 			<deleted_lines>157</deleted_lines>
 		</method>
 		<method name='log_multitask_performance' parameters='itr,batch,discount,name_map'>
 				<method_info nloc='53' complexity='8' token_count='273' nesting_level='0' start_line='66' end_line='119'></method_info>
 			<added_lines>116</added_lines>
 			<deleted_lines>116</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\garage\tf\algos\te_npo.py' new_name='src\garage\tf\algos\te_npo.py'>
 		<file_info nloc='682' complexity='61' token_count='4605'></file_info>
 		<method name='evaluate' parameters='self,policy_opt_input_values,samples_data'>
 				<method_info nloc='64' complexity='8' token_count='625' nesting_level='1' start_line='815' end_line='907'></method_info>
 			<added_lines>903</added_lines>
 			<deleted_lines>903</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\garage\experiment\test_meta_evaluator.py' new_name='tests\garage\experiment\test_meta_evaluator.py'>
 		<file_info nloc='153' complexity='19' token_count='1021'></file_info>
 		<method name='test_meta_evaluator' parameters=''>
 				<method_info nloc='35' complexity='1' token_count='272' nesting_level='0' start_line='68' end_line='102'></method_info>
 			<added_lines>95,96</added_lines>
 			<deleted_lines>95</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\garage\test_functions.py' new_name='tests\garage\test_functions.py'>
 		<file_info nloc='208' complexity='12' token_count='2157'></file_info>
 		<method name='test_log_performance' parameters=''>
 				<method_info nloc='42' complexity='2' token_count='530' nesting_level='0' start_line='28' end_line='70'></method_info>
 			<added_lines>64</added_lines>
 			<deleted_lines>64</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
