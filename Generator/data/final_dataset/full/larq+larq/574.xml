<bug_data>
<bug id='574' author='AlexRux' open_date='2020-09-25T10:01:44Z' closed_time='2020-09-25T14:50:07Z'>
 	<summary>Causal Padding</summary>
 	<description>
 &lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;
 
 "causal" padding with QuantConv1D doesn't work. It reduces the dimension.
 I got this summary:
 &lt;denchmark-code&gt;+model stats--------------------------------------------------------------------------------------+
 | Layer                     Input prec.           Outputs  # 4-bit  # 32-bit  Memory  32-bit MACs |
 |                                 (bit)                        x 1       x 1    (kB)              |
 +-------------------------------------------------------------------------------------------------+
 | input_1                             -  ((None, 29, 1),)        0         0       0            ? |
 | quant_conv1d                        -      (-1, 28, 12)       24         0    0.01            ? |
 | activation                          -      (-1, 28, 12)        0         0       0            ? |
 | quant_conv1d_1                      -      (-1, 26, 16)      384         0    0.19            ? |
 | activation_1                        -      (-1, 26, 16)        0         0       0            ? |
 | quant_conv1d_2                      -      (-1, 22, 20)      640         0    0.31            ? |
 | activation_2                        -      (-1, 22, 20)        0         0       0            ? |
 | quant_conv1d_3                      -      (-1, 21, 24)      960         0    0.47            ? |
 | activation_3                        -      (-1, 21, 24)        0         0       0            ? |
 | global_average_pooling1d            -          (-1, 24)        0         0       0            ? |
 | flatten                             -          (-1, 24)        0         0       0            0 |
 | quant_dense                         -           (-1, 4)       96         4    0.06           96 |
 | activation_4                        -           (-1, 4)        0         0       0            ? |
 +-------------------------------------------------------------------------------------------------+
 | Total                                                       2104         4    1.04           96 |
 +-------------------------------------------------------------------------------------------------+
 &lt;/denchmark-code&gt;
 
 Described by this:
 &lt;denchmark-code&gt;kwargs = dict(
                   kernel_quantizer=kernel_quantizer,
                   kernel_constraint=kernel_constraint,
                   padding="causal",
                   use_bias = False,
     )
 
     x = tf.keras.Input(shape=(Time_series_length, 1))
 
     ConvL1 = lq.layers.QuantConv1D(filters=12, kernel_size=2, dilation_rate=1, **kwargs)(x)
     ConvL1 = tf.keras.layers.Activation(activation)(ConvL1)
 
     ConvL2 = lq.layers.QuantConv1D(filters=16, kernel_size=2, dilation_rate=2, **kwargs)(ConvL1)
     ConvL2 = tf.keras.layers.Activation(activation)(ConvL2)
 
     ConvL3 = lq.layers.QuantConv1D(filters=20, kernel_size=2, dilation_rate=4, **kwargs)(ConvL2)
     ConvL3 = tf.keras.layers.Activation(activation)(ConvL3)
 
     ConvL4 = lq.layers.QuantConv1D(filters=24, kernel_size=2, dilation_rate=1, **kwargs)(ConvL3)
     ConvL4 = tf.keras.layers.Activation(activation)(ConvL4)
 
     final = tf.keras.layers.GlobalAveragePooling1D()(ConvL4)
     final = tf.keras.layers.Flatten()(final)
     final = lq.layers.QuantDense(classi, kernel_quantizer=kernel_quantizer, kernel_constraint=kernel_constraint)(
         final)
     final = tf.keras.layers.Activation("softmax")(final)
 &lt;/denchmark-code&gt;
 
 But using tf.keras.layers.Conv1D instead of lq.layers.QuantConv1D I got this:
 &lt;denchmark-code&gt;+model stats-----------------------------------------------------------------------------+
 | Layer                     Input prec.           Outputs  # 32-bit  Memory  32-bit MACs |
 |                                 (bit)                         x 1    (kB)              |
 +----------------------------------------------------------------------------------------+
 | input_1                             -  ((None, 29, 1),)         0       0            ? |
 | conv1d                              -      (-1, 29, 12)        24    0.09            ? |
 | activation                          -      (-1, 29, 12)         0       0            ? |
 | conv1d_1                            -      (-1, 29, 16)       384    1.50            ? |
 | activation_1                        -      (-1, 29, 16)         0       0            ? |
 | conv1d_2                            -      (-1, 29, 20)       640    2.50            ? |
 | activation_2                        -      (-1, 29, 20)         0       0            ? |
 | conv1d_3                            -      (-1, 29, 24)       960    3.75            ? |
 | activation_3                        -      (-1, 29, 24)         0       0            ? |
 | global_average_pooling1d            -          (-1, 24)         0       0            ? |
 | flatten                             -          (-1, 24)         0       0            0 |
 | quant_dense                         -           (-1, 4)        96    0.38           96 |
 | activation_4                        -           (-1, 4)         0       0            ? |
 +----------------------------------------------------------------------------------------+
 | Total                                                        2104    8.22           96 |
 +----------------------------------------------------------------------------------------+
 &lt;/denchmark-code&gt;
 
 They should be the same. What can I do to fix it? I need quantizations and causal padding.
 TensorFlow version: 2.1
 Larq version: 10.0
 	</description>
 	<comments>
 		<comment id='1' author='AlexRux' date='2020-09-25T12:07:56Z'>
 		I was able to reproduce your issue.
 This is actually caused by the fact that TensorFlow  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/25fba035f3e453d94490932096282c7b0624bbb3/tensorflow/python/keras/layers/convolutional.py#L204&gt;explicitely checks for the class name in the Conv1D layer&lt;/denchmark-link&gt;
  which prevent subclasses of  to use causal padding.
 This bug has been fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/17b63987e58a08ecaa950c2668a5870a399cfaba&gt;tensorflow/tensorflow@17b6398&lt;/denchmark-link&gt;
 , so upgrading to TensorFlow 2.3 should solve your issue.
 I've personally never used causal padding in Keras so please let us know if this fixes your issue.
 		</comment>
 		<comment id='2' author='AlexRux' date='2020-09-25T14:34:02Z'>
 		Fixed, thank you.
 		</comment>
 	</comments>
 </bug>
<commit id='394a9086fea0a3eac982a689c5ca2918dbe1e017' author='Lukas Geiger' date='2020-09-25 13:42:16+01:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='larq\layers_base.py' new_name='larq\layers_base.py'>
 		<file_info nloc='203' complexity='47' token_count='1301'></file_info>
 		<method name='__init__' parameters='self,args,pad_values,kwargs'>
 				<method_info nloc='7' complexity='5' token_count='79' nesting_level='1' start_line='103' end_line='109'></method_info>
 			<added_lines>106,107,108,109</added_lines>
 			<deleted_lines>106,107,108</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='larq\layers_test.py' new_name='larq\layers_test.py'>
 		<file_info nloc='285' complexity='20' token_count='1969'></file_info>
 		<method name='test_conv1d_non_zero_padding_raises' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='38' nesting_level='1' start_line='279' end_line='281'></method_info>
 			<added_lines>279,280,281</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>282</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
