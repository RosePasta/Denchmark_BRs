<bug_data>
<bug id='2631' author='sanvenDev' open_date='2019-09-27T10:45:27Z' closed_time='2020-05-08T01:26:04Z'>
 	<summary>MPC not working with more than or equal to 3 workers</summary>
 	<description>
 I tried training a model using MPC, using 2 workers was successful. (A minor issue I noticed when using refresh() in computing loss, the loss values are good, otherwise it's computes big large values, which is not interpretable, any clue why this happens can be great to my research.)
 Anyways but when using 3 or more workers, I'm facing errors.
 First I got error:
 &lt;denchmark-code&gt;---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
 &lt;ipython-input-7-902fb9b77716&gt; in &lt;module&gt;
      12 x = th.tensor([1.1,2.0,3.2,4.0]).fix_precision().share(*workers, crypto_provider=sw, requires_grad=True)
      13 m=th.nn.Linear(4, 1).fix_precision().share(*workers, crypto_provider=sw, requires_grad=True)
 ---&gt; 14 o=m(x)
 
 /usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     491             result = self._slow_forward(*input, **kwargs)
     492         else:
 --&gt; 493             result = self.forward(*input, **kwargs)
     494         for hook in self._forward_hooks.values():
     495             hook_result = hook(self, input, result)
 
 /usr/local/lib/python3.7/site-packages/torch/nn/modules/linear.py in forward(self, input)
      90     @weak_script_method
      91     def forward(self, input):
 ---&gt; 92         return F.linear(input, self.weight, self.bias)
      93 
      94     def extra_repr(self):
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook.py in overloaded_func(*args, **kwargs)
     745                 handle_func_command = TorchTensor.handle_func_command
     746 
 --&gt; 747             response = handle_func_command(command)
     748 
     749             return response
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py in handle_func_command(cls, command)
     310             new_command = (cmd, None, new_args, new_kwargs)
     311             # Send it to the appropriate class and get the response
 --&gt; 312             response = new_type.handle_func_command(new_command)
     313             # Put back the wrappers where needed
     314             response = syft.frameworks.torch.hook_args.hook_response(
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/autograd.py in handle_func_command(cls, command)
     236             # Try to get recursively the attributes in cmd = "&lt;attr1&gt;.&lt;attr2&gt;.&lt;attr3&gt;..."
     237             cmd = cls.rgetattr(cls, cmd)
 --&gt; 238             return cmd(*args, **kwargs)
     239         except AttributeError:
     240             pass
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/autograd.py in linear(*args)
     201                     Un-hook the function to have its detailed behaviour
     202                     """
 --&gt; 203                     return torch.nn.functional.native_linear(*args)
     204 
     205                 module.linear = linear
 
 /usr/local/lib/python3.7/site-packages/torch/nn/functional.py in linear(input, weight, bias)
    1406         ret = torch.addmm(bias, input, weight.t())
    1407     else:
 -&gt; 1408         output = input.matmul(weight.t())
    1409         if bias is not None:
    1410             output += bias
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/autograd.py in method_with_grad(*args, **kwargs)
     138                 )
     139 
 --&gt; 140                 result = getattr(new_self, name)(*new_args, **new_kwargs)
     141 
     142                 # Put back SyftTensor on the tensors found in the response
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/precision.py in matmul(self, *args, **kwargs)
     414 
     415         # Send it to the appropriate class and get the response
 --&gt; 416         response = getattr(new_self, "matmul")(*new_args, **new_kwargs)
     417 
     418         # Put back SyftTensor on the tensors found in the response
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py in matmul(self, other)
     515             return self._public_mul(other, "matmul")
     516 
 --&gt; 517         return self._private_mul(other, "matmul")
     518 
     519     def mm(self, *args, **kwargs):
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py in _private_mul(self, other, equation)
     413             raise AttributeError("For multiplication a crypto_provider must be passed.")
     414 
 --&gt; 415         shares = spdz.spdz_mul(cmd, self, other, self.crypto_provider, self.field)
     416 
     417         return shares
 
 /usr/local/lib/python3.7/site-packages/syft/frameworks/torch/crypto/spdz.py in spdz_mul(cmd, x_sh, y_sh, crypto_provider, field)
      43         j = sy.MultiPointerTensor(children=[j1, j0])
      44     else:
 ---&gt; 45         j = sy.MultiPointerTensor(children=[j1] + j0.child.values())
      46 
      47     delta_b = cmd(delta, b)
 
 TypeError: can only concatenate list (not "dict_values") to list
 &lt;/denchmark-code&gt;
 
 I tried cloning the repo, fixing this issue by passing  j0.child.values() to list constructor. Then I faced another error:
 &lt;denchmark-code&gt;
 Traceback (most recent call last):
    &lt;ipython-input-7-902fb9b77716&gt; in &lt;module&gt;
      o=m(x)
    File "/Users/sandeep/syft/venv/lib/python3.7/site-packages/torch/nn/modules/module.py", line 493, in __call__
      result = self.forward(*input, **kwargs)
    File "a3.py", line 31, in forward
      x=F.relu(self.fc1(x))
    File "/Users/sandeep/syft/PySyft/syft/generic/frameworks/hook/hook.py", line 413, in overloaded_func
      response = handle_func_command(command)
    File "/Users/sandeep/syft/PySyft/syft/frameworks/torch/tensors/interpreters/native.py", line 297, in handle_func_command
      response = new_type.handle_func_command(new_command)
    File "/Users/sandeep/syft/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py", line 237, in handle_func_command
      return cmd(*args, **kwargs)
   File "/Users/sandeep/syft/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py", line 207, in relu
      return tensor.relu()
    File "/Users/sandeep/syft/PySyft/syft/frameworks/torch/tensors/interpreters/autograd.py", line 141, in method_with_grad
      result = getattr(new_self, name)(*new_args, **new_kwargs)
    File "/Users/sandeep/syft/PySyft/syft/generic/frameworks/hook/hook.py", line 306, in overloaded_syft_method
      response = getattr(new_self, attr)(*new_args, **new_kwargs)
    File "/Users/sandeep/syft/PySyft/syft/frameworks/torch/tensors/interpreters/additive_shared.py", line 793, in relu
      return securenn.relu(self)
    File "/Users/sandeep/syft/PySyft/syft/frameworks/torch/crypto/securenn.py", line 427, in relu
      alice, bob = a_sh.locations
 ValueError: too many values to unpack (expected 2)
 &lt;/denchmark-code&gt;
 
 To Reproduce
 Run this to reproduce:
 &lt;denchmark-code&gt;def connect_to_workers(n_workers):
     return [
         sy.VirtualWorker(hook, id=f"worker{i+1}")
         for i in range(n_workers)
     ]
 def connect_to_crypto_provider():
     return sy.VirtualWorker(hook, id="crypto_provider")
 
 workers = connect_to_workers(n_workers=3)
 sw = connect_to_crypto_provider()
 
 x = th.tensor([1.1,2.0,3.2,4.0]).fix_precision().share(*workers, crypto_provider=sw, requires_grad=True)
 m=th.nn.Linear(4, 1).fix_precision().share(*workers, crypto_provider=sw, requires_grad=True)
 o=m(x)
 &lt;/denchmark-code&gt;
 
 Desktop (please complete the following information):
 
 OS: MacOS
 Version 10.14.6
 
 	</description>
 	<comments>
 		<comment id='1' author='sanvenDev' date='2019-12-04T16:37:23Z'>
 		I'm having the same issue. I know this is a bit old but have you found a solution for this? (Not necessarily using PySyft, but on any Federated Learning solution)
 		</comment>
 	</comments>
 </bug>
<commit id='6a0dc8e4fa9b2dcdb63458df1fdfa3a6f4aa6bbb' author='knexator' date='2020-05-05 14:42:17+02:00'>
 	<dmm_unit complexity='0.9855072463768116' interfacing='0.9855072463768116' size='0.5942028985507246'></dmm_unit>
 	<modification change_type='MODIFY' old_name='syft\frameworks\torch\mpc\securenn.py' new_name='syft\frameworks\torch\mpc\securenn.py'>
 		<file_info nloc='334' complexity='48' token_count='2880'></file_info>
 		<method name='relu' parameters='a_sh'>
 				<method_info nloc='10' complexity='1' token_count='61' nesting_level='0' start_line='515' end_line='537'></method_info>
 			<added_lines>530,535</added_lines>
 			<deleted_lines>521,536,537</deleted_lines>
 		</method>
 		<method name='relu_deriv' parameters='a_sh'>
 				<method_info nloc='17' complexity='2' token_count='119' nesting_level='0' start_line='475' end_line='512'></method_info>
 			<added_lines>491,496,509</added_lines>
 			<deleted_lines>476,497,502</deleted_lines>
 		</method>
 		<method name='select_share' parameters='alpha_sh,x_sh,y_sh'>
 				<method_info nloc='13' complexity='1' token_count='83' nesting_level='0' start_line='136' end_line='168'></method_info>
 			<added_lines>153,157</added_lines>
 			<deleted_lines>151,155</deleted_lines>
 		</method>
 		<method name='maxpool' parameters='x_sh'>
 				<method_info nloc='26' complexity='3' token_count='216' nesting_level='0' start_line='600' end_line='652'></method_info>
 			<added_lines>617,625,626,631,646</added_lines>
 			<deleted_lines>613,636,647,649</deleted_lines>
 		</method>
 		<method name='share_convert' parameters='a_sh'>
 				<method_info nloc='11' complexity='1' token_count='64' nesting_level='0' start_line='368' end_line='472'></method_info>
 			<added_lines>403,404,405,406,407,408,460,461</added_lines>
 			<deleted_lines>427,428,458,463</deleted_lines>
 		</method>
 		<method name='msb' parameters='a_sh'>
 				<method_info nloc='38' complexity='3' token_count='351' nesting_level='0' start_line='268' end_line='340'></method_info>
 			<added_lines>279,291,292,298,301,303,319,324</added_lines>
 			<deleted_lines>277,289,290,296,299,301,317,322</deleted_lines>
 		</method>
 		<method name='division' parameters='x_sh,y_sh,bit_len_max'>
 				<method_info nloc='31' complexity='5' token_count='242' nesting_level='0' start_line='542' end_line='597'></method_info>
 			<added_lines>554,569,570,571</added_lines>
 			<deleted_lines>584,592,593</deleted_lines>
 		</method>
 		<method name='maxpool_deriv' parameters='x_sh'>
 				<method_info nloc='29' complexity='2' token_count='230' nesting_level='0' start_line='655' end_line='706'></method_info>
 			<added_lines>669,677,681,683,690,699</added_lines>
 			<deleted_lines>656,665</deleted_lines>
 		</method>
 		<method name='private_compare' parameters='x_bit_sh,r,beta,L'>
 				<method_info nloc='39' complexity='2' token_count='504' nesting_level='0' start_line='171' end_line='265'></method_info>
 			<added_lines>177,178,190,199,200,201,204,233</added_lines>
 			<deleted_lines>175,176,188,197,198,199,202,231</deleted_lines>
 		</method>
 		<method name='maxpool2d' parameters='a_sh,int,int,int'>
 				<method_info nloc='35' complexity='6' token_count='380' nesting_level='0' start_line='709' end_line='764'></method_info>
 			<added_lines>757,758,759</added_lines>
 			<deleted_lines>723</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5,6,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367</added_lines>
 			<deleted_lines>538,598</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='test\torch\mpc\test_multiparty_nn.py'>
 		<file_info nloc='138' complexity='9' token_count='1170'></file_info>
 	</modification>
 </commit>
</bug_data>
