<bug_data>
<bug id='813' author='sdesrozis' open_date='2020-02-28T08:35:32Z' closed_time='2020-05-29T15:04:33Z'>
 	<summary>CosineAnnealingWarmRestarts is not compatible with LRScheduler.simulate_values()</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug description&lt;/denchmark-h&gt;
 
 CosineAnnealingWarmRestarts is not compatible with LRScheduler.simulate_values(). Precisely, a object of type CosineAnnealingWarmRestarts can't be replicated by LRScheduler._replicate_lr_scheduler(). It works for CosineAnnealingLR.
 For example :
 &lt;denchmark-code&gt;lr_scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=10, eta_min=1e-3)
 lr_values = LRScheduler.simulate_values(num_events=50, lr_scheduler=lr_scheduler)
 &lt;/denchmark-code&gt;
 
 produces the following error
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "tutorials/misc/lr_schedulers.py", line 56, in &lt;module&gt;
     lr_values = LRScheduler.simulate_values(num_events=50, lr_scheduler=lr_scheduler)
   File "/work/desrozis/Softwares/conda/envs/focus-light/lib/python3.7/site-packages/ignite/contrib/handlers/param_scheduler.py", line 606, in simulate_values
     copy_lr_scheduler = LRScheduler._replicate_lr_scheduler(lr_scheduler)
   File "/work/desrozis/Softwares/conda/envs/focus-light/lib/python3.7/site-packages/ignite/contrib/handlers/param_scheduler.py", line 627, in _replicate_lr_scheduler
     copy_lr_scheduler = lr_scheduler_cls(optimizer=dummy_optimizer, **kwargs)
 TypeError: __init__() got an unexpected keyword argument 'T_i'
 &lt;/denchmark-code&gt;
 
 This issue is due to the assumption that lr_scheduler.__state_dict__ contains arguments of method CosineAnnealingWarmRestarts.__init__(). A workaround could be to remove T_i in a similar way to base_lrs and last_epoch but it's not very satisfactory...
 Hope it helps to have a better and stonger ignite.
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 
 PyTorch Version (e.g., 1.4): 1.4
 Ignite Version (e.g., 0.3.0): 0.3.0
 OS (e.g., Linux): Linux
 How you installed Ignite (conda, pip, source): conda
 Python version: 3.7.6
 Any other relevant information:
 
 	</description>
 	<comments>
 		<comment id='1' author='sdesrozis' date='2020-02-28T10:05:02Z'>
 		Thanks for the report &lt;denchmark-link:https://github.com/sdesrozis&gt;@sdesrozis&lt;/denchmark-link&gt;
  ! I'll investigate the issue and push a fix.
 EDIT: Seems like implementation of CosineAnnealingWarmRestarts is a bit different from other LR schedulers, e.g. CosineAnnealingLR. This implies that it wont work even for LR scheduling (not simulation of lr values). The difference is in step and get_lr implementations. In CosineAnnealingWarmRestarts, get_lr does not account on optimizer's lr vs CosineAnnealingLR and others. So, get_lr is stateless but internally in ignite, we use only get_lr without calling step. Finally, this gives a constant learning rate.
 		</comment>
 		<comment id='2' author='sdesrozis' date='2020-04-01T20:03:35Z'>
 		What about copy.deepcopy() ? Does it work on a _LRScheduler ? It could replace the home made replication ?
 		</comment>
 		<comment id='3' author='sdesrozis' date='2020-05-26T12:54:57Z'>
 		The bug is also with MultiplicativeLR. The way to replicate _LRScheduler is really not safe. This scheduler have an argument lr_lambda but it stores in self.lr_lambdas. To replicate, we use self.__state_dict__ and pass it to __init__. It can't works.
 IMO plot values with replication is very tricky.
 		</comment>
 		<comment id='4' author='sdesrozis' date='2020-05-26T12:56:58Z'>
 		
 What about copy.deepcopy() ? Does it work on a _LRScheduler ? It could replace the home made replication ?
 
 copy.deepcopy() will replicate params of the related optimizer, not a good idea.
 		</comment>
 		<comment id='5' author='sdesrozis' date='2020-05-26T12:59:59Z'>
 		&lt;denchmark-link:https://github.com/vfdev-5&gt;@vfdev-5&lt;/denchmark-link&gt;
  maybe we should accept that we can't use a  after ploting values ? (and do not replicate but can plot for every schedulers).
 		</comment>
 		<comment id='6' author='sdesrozis' date='2020-05-26T13:03:00Z'>
 		&lt;denchmark-link:https://github.com/sdesrozis&gt;@sdesrozis&lt;/denchmark-link&gt;
  how about the same approach as used with lr finders etc :
 
 save initial state dicts
 do something
 restore init state
 ?
 
 		</comment>
 		<comment id='7' author='sdesrozis' date='2020-05-26T13:05:22Z'>
 		You save my day üòä I check if it‚Äôs ok.
 		</comment>
 	</comments>
 </bug>
<commit id='b6af7764e5c2dfdb11a722023090aa8bab527094' author='Sylvain Desroziers' date='2020-05-29 17:04:32+02:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='ignite\contrib\handlers\param_scheduler.py' new_name='ignite\contrib\handlers\param_scheduler.py'>
 		<file_info nloc='759' complexity='135' token_count='3874'></file_info>
 		<method name='_replicate_scheduler' parameters='scheduler,save_history'>
 				<method_info nloc='19' complexity='8' token_count='181' nesting_level='0' start_line='994' end_line='1012'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012</deleted_lines>
 		</method>
 		<method name='_replicate_lr_scheduler' parameters='lr_scheduler'>
 				<method_info nloc='13' complexity='6' token_count='121' nesting_level='1' start_line='652' end_line='664'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>652,653,654,655,656,657,658,659,660,661,662,663,664</deleted_lines>
 		</method>
 		<method name='simulate_values' parameters='cls,num_events,schedulers,durations,param_names,kwargs'>
 				<method_info nloc='26' complexity='10' token_count='254' nesting_level='1' start_line='546' end_line='595'></method_info>
 			<added_lines>563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595</added_lines>
 			<deleted_lines>552,553,554,555,557,558,559,560,561,562,563,564,565,566,567,594</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,schedulers,durations,save_history'>
 				<method_info nloc='32' complexity='12' token_count='264' nesting_level='1' start_line='428' end_line='469'></method_info>
 			<added_lines>454,455,456,457,458,459,460,461,462,463</added_lines>
 			<deleted_lines>452</deleted_lines>
 		</method>
 		<method name='simulate_values' parameters='cls,num_events,lr_scheduler,kwargs'>
 				<method_info nloc='23' complexity='4' token_count='180' nesting_level='1' start_line='655' end_line='695'></method_info>
 			<added_lines>666,667,668,669,670,671,672,676,677,678,679,680,681,682,684,685,686,687,688,689,691,692,693,694,695</added_lines>
 			<deleted_lines>655,656,657,658,659,660,661,662,663,664</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,lr_scheduler,save_history,kwds'>
 				<method_info nloc='13' complexity='2' token_count='75' nesting_level='1' start_line='594' end_line='608'></method_info>
 			<added_lines>594,595</added_lines>
 			<deleted_lines>594</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,None,save_history'>
 				<method_info nloc='19' complexity='13' token_count='222' nesting_level='1' start_line='929' end_line='955'></method_info>
 			<added_lines>947,948,951,952,953,954,955</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='save_history' parameters='self,value'>
 				<method_info nloc='3' complexity='2' token_count='19' nesting_level='1' start_line='966' end_line='968'></method_info>
 			<added_lines>966,967,968</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='simulate_values' parameters='cls,num_events,schedulers,kwargs'>
 				<method_info nloc='17' complexity='4' token_count='177' nesting_level='1' start_line='1017' end_line='1052'></method_info>
 			<added_lines>1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052</added_lines>
 			<deleted_lines>1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,lr_scheduler,save_history,kwargs'>
 				<method_info nloc='13' complexity='2' token_count='75' nesting_level='1' start_line='622' end_line='636'></method_info>
 			<added_lines>622</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__call__' parameters='self,engine,name'>
 				<method_info nloc='6' complexity='2' token_count='41' nesting_level='1' start_line='509' end_line='515'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>513</deleted_lines>
 		</method>
 		<method name='_replicate_optimizer' parameters='optimizer'>
 				<method_info nloc='13' complexity='6' token_count='102' nesting_level='0' start_line='1023' end_line='1036'></method_info>
 			<added_lines>1029,1030,1031,1032,1033,1034,1035,1036</added_lines>
 			<deleted_lines>1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036</deleted_lines>
 		</method>
 		<method name='save_history' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='14' nesting_level='1' start_line='962' end_line='963'></method_info>
 			<added_lines>962,963</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>3,8,775,961,964,965,969</added_lines>
 			<deleted_lines>641,642,643,644,645,646,647,649,651,744,916,917,920,984,985,986,987,988,989,990,991</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\ignite\contrib\handlers\test_param_scheduler.py' new_name='tests\ignite\contrib\handlers\test_param_scheduler.py'>
 		<file_info nloc='922' complexity='93' token_count='7925'></file_info>
 		<method name='test_lr_scheduler_asserts' parameters=''>
 				<method_info nloc='5' complexity='1' token_count='32' nesting_level='0' start_line='592' end_line='598'></method_info>
 			<added_lines>597,598</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_param_group_scheduler' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='134' nesting_level='0' start_line='1129' end_line='1167'></method_info>
 			<added_lines>1153,1154,1155,1156</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_concat_scheduler_asserts' parameters=''>
 				<method_info nloc='25' complexity='1' token_count='328' nesting_level='0' start_line='279' end_line='314'></method_info>
 			<added_lines>310,311,312,313,314</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_lr_scheduler' parameters=''>
 				<method_info nloc='5' complexity='1' token_count='43' nesting_level='0' start_line='601' end_line='661'></method_info>
 			<added_lines>659,660,661</added_lines>
 			<deleted_lines>625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641</deleted_lines>
 		</method>
 		<method name='test_param_group_scheduler_asserts' parameters=''>
 				<method_info nloc='35' complexity='1' token_count='438' nesting_level='0' start_line='1081' end_line='1126'></method_info>
 			<added_lines>1113,1114,1115,1121,1122,1123,1124,1125,1126</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_param_group_scheduler._test' parameters='lr_schedulers,optimizer'>
 				<method_info nloc='18' complexity='9' token_count='207' nesting_level='1' start_line='1130' end_line='1155'></method_info>
 			<added_lines>1153,1154,1155</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_concat_scheduler_state_dict' parameters=''>
 				<method_info nloc='23' complexity='2' token_count='251' nesting_level='0' start_line='317' end_line='345'></method_info>
 			<added_lines>344,345</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_param_scheduler_asserts.get_param' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='9' nesting_level='2' start_line='21' end_line='22'></method_info>
 			<added_lines>21,22</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_param_scheduler_asserts' parameters=''>
 				<method_info nloc='13' complexity='1' token_count='138' nesting_level='0' start_line='19' end_line='37'></method_info>
 			<added_lines>19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_piecewiselinear_asserts' parameters=''>
 				<method_info nloc='13' complexity='1' token_count='171' nesting_level='0' start_line='666' end_line='684'></method_info>
 			<added_lines>683,684</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>4,12,38,39,315,346,599,662,663,685,1127</added_lines>
 			<deleted_lines>4</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
