<bug_data>
<bug id='641' author='mbataillou' open_date='2020-02-14T15:00:32Z' closed_time='2020-03-30T18:43:18Z'>
 	<summary>DeepAREstimator can't use DirichletMultinomialOutput anymore due to an assertion error</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 DeepAREstimator can't train DirichletMultinomialOutput due to an assertion error checking for scale to be None while I assume DeepAREstimator is returning a scale.
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import numpy as np
 
 from gluonts.dataset.common import ListDataset
 from gluonts.distribution import DirichletMultinomialOutput
 from gluonts.model.deepar import DeepAREstimator
 from gluonts.trainer import Trainer
 
 train_dataset = ListDataset(
     data_iter=[
         {
             "start": "2019-01-01 00:00:00",
             "target": np.ones(shape=(4, 100)),
         },
     ],
     freq="5min",
     one_dim_target=False,
 )
 
 estimator = DeepAREstimator(
     'H', prediction_length=5, trainer=Trainer(epochs=3, hybridize=False),
     distr_output=DirichletMultinomialOutput(dim=4, n_trials=100),
 )
 
 predictor = estimator.train(train_dataset)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;Error Message&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;---------------------------------------------------------------------------
 AssertionError                            Traceback (most recent call last)
 &lt;ipython-input-8-920902ee83c0&gt; in &lt;module&gt;
      22 )
      23 
 ---&gt; 24 predictor = estimator.train(train_dataset)
 
 /opt/amazon/lib/python3.6/site-packages/gluonts/model/estimator.py in train(self, training_data, validation_data)
     221         self, training_data: Dataset, validation_data: Optional[Dataset] = None
     222     ) -&gt; Predictor:
 --&gt; 223         return self.train_model(training_data, validation_data).predictor
 
 /opt/amazon/lib/python3.6/site-packages/gluonts/model/estimator.py in train_model(self, training_data, validation_data)
     206             input_names=get_hybrid_forward_input_names(trained_net),
     207             train_iter=training_data_loader,
 --&gt; 208             validation_iter=validation_data_loader,
     209         )
     210 
 
 /opt/amazon/lib/python3.6/site-packages/gluonts/trainer/_base.py in __call__(self, net, input_names, train_iter, validation_iter)
     295                     )
     296 
 --&gt; 297                     epoch_loss = loop(epoch_no, train_iter)
     298                     if is_validation_available:
     299                         epoch_loss = loop(
 
 /opt/amazon/lib/python3.6/site-packages/gluonts/trainer/_base.py in loop(epoch_no, batch_iter, is_training)
     235 
     236                             with mx.autograd.record():
 --&gt; 237                                 output = net(*inputs)
     238 
     239                                 # network can returns several outputs, the first being always the loss
 
 /opt/amazon/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)
     546             hook(self, args)
     547 
 --&gt; 548         out = self.forward(*args)
     549 
     550         for hook in self._forward_hooks.values():
 
 /opt/amazon/lib/python3.6/site-packages/mxnet/gluon/block.py in forward(self, x, *args)
     923                     params = {i: j.data(ctx) for i, j in self._reg_params.items()}
     924 
 --&gt; 925                 return self.hybrid_forward(ndarray, x, *args, **params)
     926 
     927         assert isinstance(x, Symbol), \
 
 /opt/amazon/lib/python3.6/site-packages/gluonts/model/deepar/_network.py in hybrid_forward(self, F, feat_static_cat, feat_static_real, past_time_feat, past_target, past_observed_values, future_time_feat, future_target, future_observed_values)
     380             future_time_feat=future_time_feat,
     381             future_target=future_target,
 --&gt; 382             future_observed_values=future_observed_values,
     383         )
     384 
 
 /opt/amazon/lib/python3.6/site-packages/gluonts/model/deepar/_network.py in distribution(self, feat_static_cat, feat_static_real, past_time_feat, past_target, past_observed_values, future_time_feat, future_target, future_observed_values)
     336         distr_args = self.proj_distr_args(rnn_outputs)
     337 
 --&gt; 338         return self.distr_output.distribution(distr_args, scale=scale)
     339 
     340     # noinspection PyMethodOverriding,PyPep8Naming
 
 /opt/amazon/lib/python3.6/site-packages/gluonts/distribution/dirichlet_multinomial.py in distribution(self, distr_args, loc, scale)
     182 
     183     def distribution(self, distr_args, loc=None, scale=None) -&gt; Distribution:
 --&gt; 184         assert loc is None and scale is None
     185         distr = DirichletMultinomial(self.dim, self.n_trials, distr_args)
     186         return distr
 
 AssertionError: 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 
 Operating system: Amazon Linux
 Python version: 3.6
 GluonTS version: a96a0cc4 internal
 
 	</description>
 	<comments>
 		<comment id='1' author='mbataillou' date='2020-02-14T16:16:56Z'>
 		The issue is that some distributions (like Dirichlet) cannot handle translation/scaling.
 I would say that asserting that loc and scale are None is convenient, as not doing so may create confusion. We would need to disable any scaling option in the model: however, setting scaling=False in DeepAREstimator doesn't work because the NOPScaler actually does produce a scale, which happens to be 1.
 I think NOPScaler should return None when called. Also, scalers need not to be of HybridBlock type, they could just be callable objects.
 		</comment>
 		<comment id='2' author='mbataillou' date='2020-02-14T16:31:26Z'>
 		However, we may (and should) have networks that return the distribution's arguments, including loc and scale, in which case having either of them as None will create problems with hybridization.
 So after all, just ignoring loc and scale without any assertion may be the only way to go.
 		</comment>
 	</comments>
 </bug>
<commit id='2e20016827ae7a3708227092d2b2918e595a11d3' author='Lorenzo Stella' date='2020-03-30 20:43:17+02:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\gluonts\distribution\dirichlet.py' new_name='src\gluonts\distribution\dirichlet.py'>
 		<file_info nloc='112' complexity='14' token_count='733'></file_info>
 		<method name='distribution' parameters='self,distr_args,loc,scale'>
 				<method_info nloc='4' complexity='2' token_count='33' nesting_level='1' start_line='150' end_line='153'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>151</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonts\distribution\dirichlet_multinomial.py' new_name='src\gluonts\distribution\dirichlet_multinomial.py'>
 		<file_info nloc='144' complexity='13' token_count='805'></file_info>
 		<method name='distribution' parameters='self,distr_args,loc,scale'>
 				<method_info nloc='4' complexity='2' token_count='41' nesting_level='1' start_line='183' end_line='186'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>184</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonts\distribution\neg_binomial.py' new_name='src\gluonts\distribution\neg_binomial.py'>
 		<file_info nloc='96' complexity='14' token_count='654'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>127</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonts\distribution\poisson.py' new_name='src\gluonts\distribution\poisson.py'>
 		<file_info nloc='73' complexity='14' token_count='436'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>105</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
