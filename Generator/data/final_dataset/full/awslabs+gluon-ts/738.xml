<bug_data>
<bug id='738' author='ehsanmok' open_date='2020-04-02T18:46:16Z' closed_time='2020-04-03T18:25:02Z'>
 	<summary>Multiprocessing broken code</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 The very recently added &lt;denchmark-link:https://github.com/awslabs/gluon-ts/pull/689&gt;parallelized_loaded&lt;/denchmark-link&gt;
  breaks my already working code in SageMaker
 def evaluate(
     model: Predictor, test_data: ListDataset, num_samples: int = 10
 ) -&gt; Tuple[List[Forecast], List[pd.Series], Dict[str, float], pd.DataFrame]:
     forecast_it, ts_it = make_evaluation_predictions(
         dataset=test_data, predictor=model, num_samples=num_samples
     )
     forecasts = list(forecast_it)
     tss = list(ts_it)
     evaluator = MultivariateEvaluator()
     agg_metrics, item_metrics = evaluator(
         iter(tss), iter(forecasts), num_series=len(test_data)
     )
     return forecasts, tss, agg_metrics, item_metrics
 &lt;denchmark-h:h2&gt;Error message or code output&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/usr/local/lib/python3.6/runpy.py", line 193, in _run_module_as_main
     "__main__", mod_spec)
   File "/usr/local/lib/python3.6/runpy.py", line 85, in _run_code
     exec(code, run_globals)
   File "/opt/ml/code/train.py", line 137, in &lt;module&gt;
     forecasts, tss, agg_metrics, item_metrics = evaluate(predictor, dataset.test)
   File "/opt/ml/code/utils.py", line 30, in evaluate
     forecasts = list(forecast_it)
   File "/usr/local/lib/python3.6/site-packages/gluonts/model/predictor.py", line 317, in predict
     num_samples=num_samples,
   File "/usr/local/lib/python3.6/site-packages/gluonts/model/forecast_generator.py", line 195, in __call__
     for batch in inference_data_loader:
   File "/usr/local/lib/python3.6/site-packages/gluonts/dataset/loader.py", line 101, in __iter__
     yield from self.parallel_data_loader
   File "/usr/local/lib/python3.6/site-packages/gluonts/dataset/parallelized_loader.py", line 572, in same_process_iter
     for k, v in batch.items()
   File "/usr/local/lib/python3.6/site-packages/gluonts/dataset/parallelized_loader.py", line 572, in &lt;dictcomp&gt;
     for k, v in batch.items()
   File "/usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py", line 2765, in as_in_context
     return self.copyto(context)
   File "/usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py", line 2632, in copyto
     return _internal._copyto(self, out=hret)
   File "&lt;string&gt;", line 27, in _copyto
   File "/usr/local/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py", line 107, in _imperative_invoke
     ctypes.byref(out_stypes)))
   File "/usr/local/lib/python3.6/site-packages/mxnet/base.py", line 255, in check_call
     raise MXNetError(py_str(_LIB.MXGetLastError()))
 mxnet.base.MXNetError: [18:26:25] src/imperative/./imperative_utils.h:146: Operator _copyto inferring shapes failed.
 input shapes:
 [1,321,-1]
 output shapes:
 [1,321,-1]
 operator attributes:
 
 Stack trace:
   [bt] (0) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x6dfb0b) [0x7f99a9e28b0b]
   [bt] (1) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::imperative::SetShapeType(mxnet::Context const&amp;, nnvm::NodeAttrs const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, mxnet::DispatchMode*)+0x363b) [0x7f99ad0da57b]
   [bt] (2) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&amp;, nnvm::NodeAttrs const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;)+0x1db) [0x7f99ad0e24bb]
   [bt] (3) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x3847f3f) [0x7f99acf90f3f]
   [bt] (4) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(MXImperativeInvokeEx+0x62) [0x7f99acf91502]
   [bt] (5) /usr/local/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7f9a01ccf4e6]
   [bt] (6) /usr/local/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(ffi_call+0x3f1) [0x7f9a01cce241]
   [bt] (7) /usr/local/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2cf) [0x7f9a01cc58ff]
 [2020-04-02 18:26:25.315 ip-10-0-168-31.us-west-2.compute.internal:213 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.
   [bt] (8) /usr/local/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x9829) [0x7f9a01cbc829]
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 
 Operating system: Amazon linux (amzn1.x86_64 SMP Thu Feb 27 23:49:15 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux)
 Python version: 3.6
 GluonTS version: master
 MXNet: 1.6.0
 
 	</description>
 	<comments>
 		<comment id='1' author='ehsanmok' date='2020-04-02T21:44:04Z'>
 		Hi, thanks for the bug report!
 Cold you maybe provide a minimal working example that reproduces the error?
 		</comment>
 		<comment id='2' author='ehsanmok' date='2020-04-03T14:10:00Z'>
 		Hi, I looked into it, and while I was not able so far to recreate the problem, I think I might have found a solution nonetheless.
 Could you please confirm whether the problem was fixed by trying out this version:
 !pip install git+https://github.com/AaronSpieler/gluon-ts.git@mp_data_loader_updates
 		</comment>
 		<comment id='3' author='ehsanmok' date='2020-04-03T16:58:51Z'>
 		Thanks, &lt;denchmark-link:https://github.com/AaronSpieler&gt;@AaronSpieler&lt;/denchmark-link&gt;
 
 I had same problem.
 
 GluonTS version:0.4.3
 MXNet: 1.6.0
 
 But your version works well on my env.
 !pip install git+https://github.com/AaronSpieler/gluon-ts.git@mp_data_loader_updates
 		</comment>
 		<comment id='4' author='ehsanmok' date='2020-04-03T18:12:50Z'>
 		Hi &lt;denchmark-link:https://github.com/AaronSpieler&gt;@AaronSpieler&lt;/denchmark-link&gt;
 , thanks for the fix! It works for me now.
 		</comment>
 		<comment id='5' author='ehsanmok' date='2020-04-03T18:15:38Z'>
 		Thx for the quick feedback and bug report &lt;denchmark-link:https://github.com/ehsanmok&gt;@ehsanmok&lt;/denchmark-link&gt;
  , and thank you too &lt;denchmark-link:https://github.com/AtsunoriFujita&gt;@AtsunoriFujita&lt;/denchmark-link&gt;
  !
 		</comment>
 	</comments>
 </bug>
<commit id='a822353785cb03147e717a2575aeb0ae6509e952' author='Aaron Spieler' date='2020-04-03 20:25:01+02:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\gluonts\dataset\loader.py' new_name='src\gluonts\dataset\loader.py'>
 		<file_info nloc='196' complexity='7' token_count='644'></file_info>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>222,223,224,225,226,227,228,229,230,231</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonts\dataset\parallelized_loader.py' new_name='src\gluonts\dataset\parallelized_loader.py'>
 		<file_info nloc='459' complexity='45' token_count='2311'></file_info>
 		<method name='_worker_initializer' parameters='Dataset,Transformation,int,Queue'>
 				<method_info nloc='5' complexity='1' token_count='18' nesting_level='0' start_line='190' end_line='194'></method_info>
 			<added_lines>193</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__del__' parameters='self'>
 				<method_info nloc='7' complexity='3' token_count='49' nesting_level='1' start_line='414' end_line='423'></method_info>
 			<added_lines>417,418,419,420,421,422,423</added_lines>
 			<deleted_lines>414</deleted_lines>
 		</method>
 		<method name='__next__' parameters='self'>
 				<method_info nloc='39' complexity='7' token_count='174' nesting_level='1' start_line='357' end_line='405'></method_info>
 			<added_lines>392,393,394</added_lines>
 			<deleted_lines>380,381,382,383,384,385,386,387,388,389</deleted_lines>
 		</method>
 		<method name='_as_in_context' parameters='data,ctx'>
 				<method_info nloc='6' complexity='4' token_count='52' nesting_level='0' start_line='136' end_line='142'></method_info>
 			<added_lines>137,138,139,140,141,142</added_lines>
 			<deleted_lines>136,137,138,139,140,141,142</deleted_lines>
 		</method>
 		<method name='default_batchify_fn' parameters='DType,bool,None'>
 				<method_info nloc='5' complexity='1' token_count='28' nesting_level='0' start_line='137' end_line='141'></method_info>
 			<added_lines>137,138,139,140,141</added_lines>
 			<deleted_lines>137,138,139,140,141</deleted_lines>
 		</method>
 		<method name='_worker_initializer' parameters='Dataset,int,int,Transformation,bool,Queue'>
 				<method_info nloc='7' complexity='1' token_count='26' nesting_level='0' start_line='153' end_line='159'></method_info>
 			<added_lines>156,157,158,159</added_lines>
 			<deleted_lines>155,156,158</deleted_lines>
 		</method>
 		<method name='stack' parameters='data,bool,DType,None'>
 				<method_info nloc='5' complexity='1' token_count='23' nesting_level='0' start_line='86' end_line='90'></method_info>
 			<added_lines>86,87,88,89,90</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_as_in_context' parameters='dict,Context'>
 				<method_info nloc='14' complexity='5' token_count='94' nesting_level='0' start_line='156' end_line='171'></method_info>
 			<added_lines>156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171</added_lines>
 			<deleted_lines>156,158,163,164,165,166,167,168,169,170,171</deleted_lines>
 		</method>
 		<method name='sequential_sample_generator' parameters='dataset,transformation,is_train,cyclic'>
 				<method_info nloc='6' complexity='4' token_count='35' nesting_level='0' start_line='188' end_line='194'></method_info>
 			<added_lines>193</added_lines>
 			<deleted_lines>188</deleted_lines>
 		</method>
 		<method name='_sequential_sample_generator' parameters='dataset,transformation,is_train,cyclic'>
 				<method_info nloc='6' complexity='4' token_count='35' nesting_level='0' start_line='211' end_line='217'></method_info>
 			<added_lines>211</added_lines>
 			<deleted_lines>211,212,214,215,216</deleted_lines>
 		</method>
 		<method name='default_batchify_fn' parameters='data,dtype,parallel_processing'>
 				<method_info nloc='9' complexity='3' token_count='50' nesting_level='0' start_line='123' end_line='133'></method_info>
 			<added_lines>123,125,126,127,128</added_lines>
 			<deleted_lines>123,129</deleted_lines>
 		</method>
 		<method name='stack' parameters='data,parallel_processing,dtype'>
 				<method_info nloc='31' complexity='11' token_count='275' nesting_level='0' start_line='85' end_line='119'></method_info>
 			<added_lines>86,87,88,89,90,91,97,110,111,112,113,118</added_lines>
 			<deleted_lines>85,91,104,109,111,113</deleted_lines>
 		</method>
 		<method name='__iter__.same_process_iter' parameters=''>
 				<method_info nloc='14' complexity='3' token_count='60' nesting_level='3' start_line='554' end_line='573'></method_info>
 			<added_lines>568,570</added_lines>
 			<deleted_lines>561,565,566,567,568,569,570,571,572,573</deleted_lines>
 		</method>
 		<method name='__iter__' parameters='self'>
 				<method_info nloc='30' complexity='4' token_count='157' nesting_level='1' start_line='547' end_line='601'></method_info>
 			<added_lines>550,568,570</added_lines>
 			<deleted_lines>561,565,566,567,568,569,570,571,572,573</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>16,26,41,120,121,122,148,150,174,175,176,177,178,179,180,181,182,183,184,185,186,187,198,199,206,207,208,232,233,234,239,240,242,251,256,259,276,277,278,282,283,284,537</added_lines>
 			<deleted_lines>25,42,122,145,146,147,148,149,150,172,173,174,175,176,177,178,185,197,209,210,221,222,224,233,238,241,258,259,260,261,262,263,264,265,266,270,271,272,412,413,527,528,530,543</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonts\dataset\util.py' new_name='src\gluonts\dataset\util.py'>
 		<file_info nloc='113' complexity='19' token_count='531'></file_info>
 		<method name='set_worker_info' parameters='cls,int,int'>
 				<method_info nloc='2' complexity='1' token_count='24' nesting_level='1' start_line='47' end_line='48'></method_info>
 			<added_lines>47,48</added_lines>
 			<deleted_lines>47,48</deleted_lines>
 		</method>
 		<method name='set_worker_info' parameters='cls,int,int,bool'>
 				<method_info nloc='2' complexity='1' token_count='15' nesting_level='1' start_line='45' end_line='46'></method_info>
 			<added_lines>45,46</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>36,40,49,50,51,52</added_lines>
 			<deleted_lines>27,28,38,39</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\dataset\test_multiprocessing_loader.py' new_name='test\dataset\test_multiprocessing_loader.py'>
 		<file_info nloc='265' complexity='19' token_count='1119'></file_info>
 		<method name='test_inference_loader_equivalence' parameters=''>
 				<method_info nloc='43' complexity='1' token_count='151' nesting_level='0' start_line='188' end_line='243'></method_info>
 			<added_lines>188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243</added_lines>
 			<deleted_lines>231</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>27,28,29,30,31,51,187,244,245,294</added_lines>
 			<deleted_lines>27,47</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
