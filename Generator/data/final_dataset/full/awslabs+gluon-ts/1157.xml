<bug_data>
<bug id='1157' author='kaijennissen' open_date='2020-11-19T09:18:33Z' closed_time='2020-11-20T11:01:07Z'>
 	<summary>Multiprocessing hangs when num_workers &amp;gt; len(dataset)</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 I'm trying to serialize a predictor trained on multiple cores. When calling the serialize method nothing happens.
 Running the same code, but without specifying num_workers, it works as expected.
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 from pathlib import Path
 from typing import Optional
 
 from gluonts.dataset.multivariate_grouper import MultivariateGrouper
 from gluonts.dataset.common import TrainDatasets
 from gluonts.model.gpvar import GPVAREstimator
 from gluonts.dataset.repository.datasets import get_dataset
 from gluonts.mx.trainer import Trainer
 
 
 def load_multivariate_dataset(dataset_name: str, target_dim: Optional[int] = None):
     ds = get_dataset(dataset_name)
 
     if target_dim is None:
         target_dim = len(ds.train)
 
     grouper = MultivariateGrouper(max_target_dim=target_dim)
 
     meta = ds.metadata
     meta.feat_static_cat[0].cardinality = target_dim
 
     return (TrainDatasets(
         metadata=meta,
         train=grouper(ds.train),
         test=grouper(ds.test)
     ), target_dim)
 
 
 ds, target_dim = load_multivariate_dataset("exchange_rate")
 metadata = ds.metadata
 
 estimator = GPVAREstimator(
     prediction_length=metadata.prediction_length,
     freq=metadata.freq,
     target_dim=target_dim,
     trainer=Trainer(
         epochs=2,
         num_batches_per_epoch=10,
         batch_size=8,
     ),
 )
 
 predictor = estimator.train(training_data=ds.train, num_workers=2)
 
 predictor.serialize(Path("/tmp"))
 &lt;denchmark-h:h2&gt;Error message or code output&lt;/denchmark-h&gt;
 
 Nothing happens.
 &lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;
 
 
 Operating system: Mac OSX 10.15.7
 Python version: 3.6.12
 GluonTS version: 0.6.0
 MXNet version: 1.7.0post1
 
 	</description>
 	<comments>
 		<comment id='1' author='kaijennissen' date='2020-11-19T10:29:15Z'>
 		&lt;denchmark-link:https://github.com/kaijennissen&gt;@kaijennissen&lt;/denchmark-link&gt;
  I'm not able to reproduce this: the following snippet, adapted from yours, doesn't raise an error
 (I modified the path where the model is serialized, and added a test in the end to check if the reconstructed model is the same)
 Edit: I'm using master with mxnet==1.7.0post1
 from pathlib import Path
 from typing import Optional
 import tempfile
 
 from gluonts.dataset.multivariate_grouper import MultivariateGrouper
 from gluonts.dataset.common import TrainDatasets
 from gluonts.model.predictor import Predictor
 from gluonts.model.gpvar import GPVAREstimator
 from gluonts.dataset.repository.datasets import get_dataset
 from gluonts.mx.trainer import Trainer
 
 
 def load_multivariate_dataset(dataset_name: str, target_dim: Optional[int] = None):
     ds = get_dataset(dataset_name)
 
     if target_dim is None:
         target_dim = len(ds.train)
 
     grouper = MultivariateGrouper(max_target_dim=target_dim)
 
     meta = ds.metadata
     meta.feat_static_cat[0].cardinality = target_dim
 
     return (TrainDatasets(
         metadata=meta,
         train=grouper(ds.train),
         test=grouper(ds.test)
     ), target_dim)
 
 
 ds, target_dim = load_multivariate_dataset("exchange_rate")
 metadata = ds.metadata
 
 estimator = GPVAREstimator(
     prediction_length=metadata.prediction_length,
     freq=metadata.freq,
     target_dim=target_dim,
     trainer=Trainer(
         epochs=2,
         num_batches_per_epoch=10,
         batch_size=8,
     ),
 )
 
 predictor = estimator.train(training_data=ds.train, num_workers=2)
 
 with tempfile.TemporaryDirectory() as path_str:
     print(f"saving model in {path_str}")
     predictor.serialize(Path(path_str))
     predictor_copy = Predictor.deserialize(Path(path_str))
 
 assert predictor == predictor_copy
 		</comment>
 		<comment id='2' author='kaijennissen' date='2020-11-19T10:45:46Z'>
 		I just tried it on gpu with
 import multiprocessing
 multiprocessing.set_start_method("spawn", force=True)
 at the beginning of the script. The training takes approx. 4 minutes to start.
 On CPU it works, but the CPU usage of Python goes up to 99% after the code finishes until I restart the kernel. I think there are some processes which do not get terminated. I think this is related to &lt;denchmark-link:https://github.com/awslabs/gluon-ts/issues/941&gt;#941&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='3' author='kaijennissen' date='2020-11-19T13:11:54Z'>
 		
 On CPU it works, but the CPU usage of Python goes up to 99% after the code finishes until I restart the kernel. I think there are some processes which do not get terminated. I think this is related to #941.
 
 You' re right.
 I've modified the last part and it appears that the problem is not the serialization but that some processes are not terminated.
 It also occurs inside a docker container, with Gitpod (using master) and on a EC2 DLAMI instance (using the mxnet_latest_p37 conda env and pip install gluonts==0.6).
 print("Finished serialization!")
 assert predictor == predictor_copy
 &lt;denchmark-code&gt;learning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.
 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:25&lt;00:00,  2.52s/it, epoch=1/2, avg_epoch_loss=1.31]
 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:18&lt;00:00,  1.90s/it, epoch=2/2, avg_epoch_loss=-.802]
 saving model in /tmp/tmpz684afay
 WARNING:root:Serializing RepresentableBlockPredictor instances does not save the prediction network structure in a backwards-compatible manner. Be careful not to use this method in production.
 Finished serialization!
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='kaijennissen' date='2020-11-19T15:26:10Z'>
 		Looks like the MultivariateGrouper doesn't play well with multiprocessing: the following example works fine with use_grouper = False (where we construct a multivariate dataset manually), but hangs with use_grouper = True (where we group an originally univariate dataset to get a multivariate one).
 import pandas as pd
 import numpy as np
 
 from gluonts.dataset.multivariate_grouper import MultivariateGrouper
 from gluonts.model.gpvar import GPVAREstimator
 from gluonts.mx.trainer import Trainer
 
 use_grouper = False
 
 if use_grouper:
     dataset = [
         {"start": pd.Timestamp("2020-01-01 00:00:00", freq="1H"), "target": np.array([1.0]*1000)}
         for _ in range(8)
     ]
     grouper = MultivariateGrouper(max_target_dim=8)
     multivariate_dataset = grouper(dataset)
 else:
     multivariate_dataset = [
         {"start": pd.Timestamp("2020-01-01 00:00:00", freq="1H"), "target": np.array([[1.0]*1000] * 8)}
     ]
 
 estimator = GPVAREstimator(
     prediction_length=24,
     freq="1H",
     target_dim=8,
     trainer=Trainer(
         epochs=2,
         num_batches_per_epoch=10,
         batch_size=8,
     ),
 )
 
 predictor = estimator.train(training_data=multivariate_dataset, num_workers=2)
 		</comment>
 		<comment id='5' author='kaijennissen' date='2020-11-19T15:47:59Z'>
 		&lt;denchmark-link:https://github.com/kaijennissen&gt;@kaijennissen&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/PascalIversen&gt;@PascalIversen&lt;/denchmark-link&gt;
  the problem appears to be , I reduced the issue to the following minimal example (univariate with DeepAR, but I also checked multivariate with GPVAR):
 from gluonts.dataset.common import ListDataset
 from gluonts.model.deepar import DeepAREstimator
 from gluonts.mx.trainer import Trainer
 
 univariate_dataset = ListDataset(
     data_iter=[
         {"start": "2020-01-01 00:00:00", "target": [1.0] * 1000}
     ],
     freq="1H",
 )
 
 estimator = DeepAREstimator(
     prediction_length=24,
     freq="1H",
     trainer=Trainer(
         epochs=2,
         num_batches_per_epoch=10,
         batch_size=8,
     ),
 )
 
 predictor = estimator.train(training_data=univariate_dataset, num_workers=2)
 		</comment>
 		<comment id='6' author='kaijennissen' date='2020-11-19T16:23:04Z'>
 		&lt;denchmark-link:https://github.com/lostella&gt;@lostella&lt;/denchmark-link&gt;
  Okay. To summarize this.
 For a univariate datasets with multiple entries, one can speed up training by setting .
 The only way to reduce training time in case of a multivariate dataset (which will be of length 1) is to use a .
 Correct?
 		</comment>
 		<comment id='7' author='kaijennissen' date='2020-11-19T16:31:28Z'>
 		This works :
 from gluonts.model.deepar import DeepAREstimator
 from gluonts.mx.trainer import Trainer
 import pandas as pd
 data_iter=[
     {"start": pd.Timestamp("1990-01-01 00:00:00", freq="1H"), "target": [1.0] * 1000}
 ]
 estimator = DeepAREstimator(
     prediction_length=24,
     freq="1H",
     trainer=Trainer(
         epochs=2,
         num_batches_per_epoch=10,
         batch_size=8,
     ),
 )
 
 predictor = estimator.train(training_data=data_iter, num_workers=2)
 Maybe the problem is the ListDataset?
 		</comment>
 		<comment id='8' author='kaijennissen' date='2020-11-19T16:49:33Z'>
 		This works for as-well (only difference is the list() in the last line)
 from pathlib import Path
 from typing import Optional
 
 from gluonts.dataset.multivariate_grouper import MultivariateGrouper
 from gluonts.dataset.common import TrainDatasets
 from gluonts.model.gpvar import GPVAREstimator
 from gluonts.dataset.repository.datasets import get_dataset
 from gluonts.mx.trainer import Trainer
 
 
 def load_multivariate_dataset(dataset_name: str, target_dim: Optional[int] = None):
     ds = get_dataset(dataset_name)
 
     if target_dim is None:
         target_dim = len(ds.train)
 
     grouper = MultivariateGrouper(max_target_dim=target_dim)
 
     meta = ds.metadata
     meta.feat_static_cat[0].cardinality = target_dim
 
     return (TrainDatasets(
         metadata=meta,
         train=grouper(ds.train),
         test=grouper(ds.test)
     ), target_dim)
 
 
 ds, target_dim = load_multivariate_dataset("exchange_rate")
 metadata = ds.metadata
 
 estimator = GPVAREstimator(
     prediction_length=metadata.prediction_length,
     freq=metadata.freq,
     target_dim=target_dim,
     trainer=Trainer(
         epochs=2,
         num_batches_per_epoch=10,
         batch_size=8,
     ),
 )
 
 predictor = estimator.train(training_data=list(ds.train), num_workers=2)
 However the speedup on CPU compared to num_workers=None is negligible. My intuition is that data-loading is not really the bottleneck on CPU.
 		</comment>
 		<comment id='9' author='kaijennissen' date='2020-11-19T16:50:40Z'>
 		
 For a univariate datasets with multiple entries, one can speed up training by setting num_workers&gt;1.
 
 That's correct. Whether you will observe any speedup depends on what the bottleneck is: if data loading is the bottleneck (because the lazy transformations involved there) then multiple workers will help.
 There might be other ways of speeding things up in that case, like caching some of the transformations (instead of doing them again and again) but that's not exposed yet as an option.
 
 The only way to reduce training time in case of a multivariate dataset (which will be of length 1) is to use a GPU.
 
 That's definitely one thing to try, but it may not help with all models. CNN-based models will benefit from it, and also dense layers should operate faster on GPU, but with RNN-based models one may not observe significant speedup. I'm not sure what's the case for GPVAR.
 All of this said, we should keep this issue open and fix the way data loader workers terminate in this (not so) edge case. Thanks for opening this &lt;denchmark-link:https://github.com/kaijennissen&gt;@kaijennissen&lt;/denchmark-link&gt;
 !!
 		</comment>
 		<comment id='10' author='kaijennissen' date='2020-11-19T20:07:03Z'>
 		&lt;denchmark-link:https://github.com/PascalIversen&gt;@PascalIversen&lt;/denchmark-link&gt;
  yes, the issue is in the  (and might be in  too) and concerns what happens if some worker doesn't get assigned any slice of the dataset, which is the case when num_workers &gt; 1 and len(dataset) == 1
 I'm not sure what happens with multiple workers when a list is used directly, but I suspect that all workers just iterate the entire list, so you can have as many workers as you like and they won't be bothered. Edit: of course this is also not the intended behavior, but outside of the scope here.
 		</comment>
 	</comments>
 </bug>
<commit id='86f12318efdd7b41ee744a52a9d3cac1f3fb68ea' author='Lorenzo Stella' date='2020-11-20 12:01:05+01:00'>
 	<dmm_unit complexity='1.0' interfacing='0.5555555555555556' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\gluonts\itertools.py' new_name='src\gluonts\itertools.py'>
 		<file_info nloc='55' complexity='14' token_count='249'></file_info>
 		<method name='cyclic' parameters='it'>
 				<method_info nloc='8' complexity='4' token_count='27' nesting_level='0' start_line='21' end_line='30'></method_info>
 			<added_lines>24,26,27,28,29,30</added_lines>
 			<deleted_lines>25</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='test\test_itertools.py' new_name='test\test_itertools.py'>
 		<file_info nloc='19' complexity='3' token_count='196'></file_info>
 		<method name='test_cyclic' parameters='Iterable,int,List'>
 				<method_info nloc='4' complexity='1' token_count='40' nesting_level='0' start_line='26' end_line='29'></method_info>
 			<added_lines>26,27,28,29</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14,15,20,21,22,23,24,25</added_lines>
 			<deleted_lines>14,19</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
