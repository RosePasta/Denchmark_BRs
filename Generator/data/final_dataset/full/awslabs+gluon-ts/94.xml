<bug_data>
<bug id='94' author='houchangtao' open_date='2019-06-12T00:30:05Z' closed_time='2019-06-20T09:25:59Z'>
 	<summary>Dynamical Feature with Different Length will Throw Exception</summary>
 	<description>
 System: Ubuntu 16.04
 Python: 3.6.4
 mxnet: 1.4.1
 Code to reproduce:
 import pandas as pd
 url = "https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv"
 df = pd.read_csv(url, header=0, index_col=0)
 from gluonts.dataset.common import ListDataset
 training_data = [
     {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-05 00:00:00"], 
      "feat_dynamic_real": pd.to_datetime(df[:"2015-04-05 00:00:00"].index).dayofweek.values},
     {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-10 00:00:00"], 
      "feat_dynamic_real": pd.to_datetime(df[:"2015-04-10 00:00:00"].index).dayofweek.values}
 ]
 from gluonts.model.deepar import DeepAREstimator
 from gluonts.trainer import Trainer
 from gluonts.distribution import NegativeBinomialOutput, StudentTOutput
 estimator = DeepAREstimator(freq="5min", prediction_length=12, distr_output=NegativeBinomialOutput(),
                             trainer=Trainer(epochs=10))
 predictor = estimator.train(training_data=training_data)
 Exceptions:
 &lt;denchmark-code&gt;KeyError                                  Traceback (most recent call last)
 &lt;ipython-input-9-7512b68a283e&gt; in &lt;module&gt;()
       1 estimator = DeepAREstimator(freq="5min", prediction_length=12, distr_output=NegativeBinomialOutput(),
       2                             trainer=Trainer(epochs=10))
 ----&gt; 3 predictor = estimator.train(training_data=training_data)
 
 /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/model/estimator.py in train(self, training_data)
     187     def train(self, training_data: Dataset) -&gt; Predictor:
     188 
 --&gt; 189         training_transformation, trained_net = self.train_model(training_data)
     190 
     191         # ensure that the prediction network is created within the same MXNet
 
 /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/model/estimator.py in train_model(self, training_data)
     180             net=trained_net,
     181             input_names=get_hybrid_forward_input_names(trained_net),
 --&gt; 182             train_iter=training_data_loader,
     183         )
     184 
 
 /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/trainer/_base.py in __call__(self, net, input_names, train_iter)
     249 
     250                     with tqdm(train_iter) as it:
 --&gt; 251                         for batch_no, data_entry in enumerate(it, start=1):
     252                             if self.halt:
     253                                 break
 
 /usr/local/lib/python3.6/site-packages/tqdm/_tqdm.py in __iter__(self)
    1003                 """), fp_write=getattr(self.fp, 'write', sys.stderr.write))
    1004 
 -&gt; 1005             for obj in iterable:
    1006                 yield obj
    1007                 # Update and possibly print the progressbar.
 
 /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in __iter__(self)
     200             ):
     201                 for batch in self._emit_batches_while_buffer_larger_than(
 --&gt; 202                     self.batch_size - 1
     203                 ):
     204                     yield batch
 
 /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in _emit_batches_while_buffer_larger_than(self, thresh)
     167             self._buffer.shuffle()
     168         while len(self._buffer) &gt; thresh:
 --&gt; 169             yield self._buffer.next_batch()
     170 
     171     def _iterate_forever(
 
 /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in next_batch(self)
      58             #print(np.asarray(v[:n]).dtype)
      59             #print(v[:n])
 ---&gt; 60             batch[k] = self.stack(v[:n])
      61         #batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}
      62         for key in self._buffers.keys():
 
 /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in stack(self, xs)
      70             if data.dtype.kind == 'f':
      71                 data = data.astype(self.float_type)
 ---&gt; 72             return mx.nd.array(data, dtype=data.dtype, ctx=self.ctx)
      73         elif isinstance(xs[0], mx.nd.NDArray):
      74             return mx.nd.stack(*xs)
 
 /usr/local/lib/python3.6/site-packages/mxnet/ndarray/utils.py in array(source_array, ctx, dtype)
     144         return _sparse_array(source_array, ctx=ctx, dtype=dtype)
     145     else:
 --&gt; 146         return _array(source_array, ctx=ctx, dtype=dtype)
     147 
     148 
 
 /usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in array(source_array, ctx, dtype)
    2486             except:
    2487                 raise TypeError('source_array must be array like object')
 -&gt; 2488     arr = empty(source_array.shape, ctx, dtype)
    2489     arr[:] = source_array
    2490     return arr
 
 /usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in empty(shape, ctx, dtype)
    3875     if dtype is None:
    3876         dtype = mx_real_t
 -&gt; 3877     return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
    3878 
    3879 
 
 /usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in _new_alloc_handle(shape, ctx, delay_alloc, dtype)
     137         ctypes.c_int(ctx.device_id),
     138         ctypes.c_int(int(delay_alloc)),
 --&gt; 139         ctypes.c_int(int(_DTYPE_NP_TO_MX[np.dtype(dtype).type])),
     140         ctypes.byref(hdl)))
     141     return hdl
 
 KeyError: &lt;class 'numpy.object_'&gt;
 &lt;/denchmark-code&gt;
 
 Error comes from: 
 
 
 gluon-ts/src/gluonts/dataset/loader.py
 
 
          Line 62
       in
       3c6d110
 
 
 
 
 
 
  data = np.asarray(xs) 
 
 
 
 
 
 	</description>
 	<comments>
 		<comment id='1' author='houchangtao' date='2019-06-12T09:52:22Z'>
 		&lt;denchmark-link:https://github.com/houchangtao&gt;@houchangtao&lt;/denchmark-link&gt;
  One issue with your example is that you're feeding a list directly as a dataset for training.
 Using the ListDataset would be appropriate here, but this results in an error as well:
 import pandas as pd
 url = "https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv"
 df = pd.read_csv(url, header=0, index_col=0)
 from gluonts.dataset.common import ListDataset
 training_data = ListDataset(
     data_iter=[
         {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-05 00:00:00"],
          "feat_dynamic_real": pd.to_datetime(df[:"2015-04-05 00:00:00"].index).dayofweek.values},
         {"start": pd.Timestamp(df.index[0], freq='5min'), "target": df.value[:"2015-04-10 00:00:00"],
          "feat_dynamic_real": pd.to_datetime(df[:"2015-04-10 00:00:00"].index).dayofweek.values}
     ],
     freq="5min"
 )
 from gluonts.model.deepar import DeepAREstimator
 from gluonts.trainer import Trainer
 from gluonts.distribution import NegativeBinomialOutput
 estimator = DeepAREstimator(freq="5min", prediction_length=12, distr_output=NegativeBinomialOutput(),
                             trainer=Trainer(epochs=10))
 predictor = estimator.train(training_data=training_data)
 Results in:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/Users/stellalo/gluon-ts/temp/run_issue_94.py", line 19, in &lt;module&gt;
     predictor = estimator.train(training_data=training_data)
   File "/Users/stellalo/gluon-ts/src/gluonts/model/estimator.py", line 189, in train
     training_transformation, trained_net = self.train_model(training_data)
   File "/Users/stellalo/gluon-ts/src/gluonts/model/estimator.py", line 182, in train_model
     train_iter=training_data_loader,
   File "/Users/stellalo/gluon-ts/src/gluonts/trainer/_base.py", line 251, in __call__
     for batch_no, data_entry in enumerate(it, start=1):
   File "/Users/stellalo/.virtualenvs/gluonts/lib/python3.6/site-packages/tqdm/_tqdm.py", line 930, in __iter__
     for obj in iterable:
   File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 195, in __iter__
     self.batch_size - 1
   File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 162, in _emit_batches_while_buffer_larger_than
     yield self._buffer.next_batch()
   File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 54, in next_batch
     batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}
   File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 54, in &lt;dictcomp&gt;
     batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}
   File "/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py", line 62, in stack
     data = np.asarray(xs)
   File "/Users/stellalo/.virtualenvs/gluonts/lib/python3.6/site-packages/numpy/core/numeric.py", line 492, in asarray
     return array(a, dtype, copy=False, order=order)
 ValueError: could not broadcast input array from shape (10684) into shape (1)
 
 Process finished with exit code 1
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='houchangtao' date='2019-06-12T11:17:21Z'>
 		MWE:
 from gluonts.dataset.common import ListDataset
 from gluonts.model.deepar import DeepAREstimator
 from gluonts.trainer import Trainer
 training_data = ListDataset(
     data_iter=[
         {"start": "2019-01-01 00:00:00", "target": [1.0, 2.0, 3.0, 4.0],
          "feat_dynamic_real": [1.0, 2.0, 3.0, 4.0]},
         {"start": "2019-01-01 00:00:00", "target": [1.0, 2.0, 3.0, 4.0, 5.0],
          "feat_dynamic_real": [1.0, 2.0, 3.0, 4.0, 5.0]},
     ],
     freq="5min"
 )
 estimator = DeepAREstimator(freq="5min", prediction_length=2,
                             trainer=Trainer(epochs=10))
 predictor = estimator.train(training_data=training_data)
 		</comment>
 		<comment id='3' author='houchangtao' date='2019-06-12T13:50:26Z'>
 		The problem is that the  field is &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/cd5df5814797e602ec4a525d89f4bc84b6e5e82f/src/gluonts/dataset/common.py#L258&gt;processed&lt;/denchmark-link&gt;
  as part of the , but not stacked together with other time-dependent fields &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/a8c60fac38296268e5b55afb828568021ff23531/src/gluonts/model/deepar/_estimator.py#L189-L192&gt;in the transformation chain&lt;/denchmark-link&gt;
 . Therefore &lt;denchmark-link:https://github.com/awslabs/gluon-ts/blob/cd5df5814797e602ec4a525d89f4bc84b6e5e82f/src/gluonts/dataset/loader.py#L51-L58&gt;when a batch of data is formed&lt;/denchmark-link&gt;
  this field is stacked as-is between different entries in the dataset.
 As far as I can see, solving this issues means either:
 
 (easier) at the beginning of the transformation, explicitly filtering out any fields which will not be consumed
 (harder) completing the transformation chain of DeepAREstimator (and potentially other models), and make it consume all possible fields, possibly defaulting the missing ones (so that no KeyErrors are raised)
 
 		</comment>
 		<comment id='4' author='houchangtao' date='2019-06-20T09:25:54Z'>
 		&lt;denchmark-link:https://github.com/awslabs/gluon-ts/pull/125&gt;#125&lt;/denchmark-link&gt;
  solves this issues: now features must be explicitly enabled, when constructing the estimator, in order for the model to use them.
 		</comment>
 	</comments>
 </bug>
<commit id='6ec5c828dc7f7cc2d25d6d316e84042069938dd0' author='Lorenzo Stella' date='2019-06-20 11:24:32+02:00'>
 	<dmm_unit complexity='0.0' interfacing='0.5384615384615384' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\gluonts\model\deepar\_estimator.py' new_name='src\gluonts\model\deepar\_estimator.py'>
 		<file_info nloc='258' complexity='7' token_count='1005'></file_info>
 		<method name='create_transformation' parameters='self'>
 				<method_info nloc='61' complexity='4' token_count='288' nesting_level='1' start_line='170' end_line='232'></method_info>
 			<added_lines>171,172,173,174,175,176,177,179,180,181,182,183,184,185,186,211,212,213,214,215,216</added_lines>
 			<deleted_lines>191,193,194,195,196</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,str,int,Trainer'>
 				<method_info nloc='19' complexity='1' token_count='129' nesting_level='1' start_line='97' end_line='115'></method_info>
 			<added_lines>108,109,110</added_lines>
 			<deleted_lines>107,108,109</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>28,29,131,132,133,134,135,136,152,153,154</added_lines>
 			<deleted_lines>28,130,131,132,148,166</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonts\model\seq2seq\_transform.py' new_name='src\gluonts\model\seq2seq\_transform.py'>
 		<file_info nloc='83' complexity='4' token_count='555'></file_info>
 		<modified_lines>
 			<added_lines>9,108</added_lines>
 			<deleted_lines>9,108</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\gluonts\transform.py' new_name='src\gluonts\transform.py'>
 		<file_info nloc='1080' complexity='108' token_count='4612'></file_info>
 		<method name='_update_cache' parameters='self,Timestamp,int'>
 				<method_info nloc='24' complexity='6' token_count='173' nesting_level='1' start_line='830' end_line='853'></method_info>
 			<added_lines>831,839,840,841,842</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='transform' parameters='self,DataEntry'>
 				<method_info nloc='5' complexity='3' token_count='34' nesting_level='1' start_line='380' end_line='384'></method_info>
 			<added_lines>380,381,382,383,384</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='shift_timestamp' parameters='Timestamp,int'>
 				<method_info nloc='8' complexity='1' token_count='29' nesting_level='0' start_line='77' end_line='84'></method_info>
 			<added_lines>77,79,80,81,82,84</added_lines>
 			<deleted_lines>77,79,80,82</deleted_lines>
 		</method>
 		<method name='_shift_timestamp_helper' parameters='Timestamp,str,int'>
 				<method_info nloc='2' complexity='1' token_count='15' nesting_level='0' start_line='88' end_line='89'></method_info>
 			<added_lines>88,89</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_compute_date_helper' parameters='ts,freq,offset'>
 				<method_info nloc='6' complexity='2' token_count='52' nesting_level='0' start_line='86' end_line='107'></method_info>
 			<added_lines>88,89,90,99</added_lines>
 			<deleted_lines>86,95,96</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='377' end_line='378'></method_info>
 			<added_lines>377,378</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='compute_date' parameters='Timestamp,int'>
 				<method_info nloc='6' complexity='1' token_count='29' nesting_level='0' start_line='77' end_line='82'></method_info>
 			<added_lines>77,79,80,81,82</added_lines>
 			<deleted_lines>77,79,80,82</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>375,376,379,385,386,503,504,505,535,536,537,538,539,549,550,551,581,582,583,584,585,1071,1072,1073,1201,1241</added_lines>
 			<deleted_lines>488,518,528,558,804,812,814,1042,1170,1210</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='DELETE' old_name='test\model\test_deepar_cat.py' new_name='None'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='RENAME' old_name='test\model\test_deepar.py' new_name='test\model\test_deepar_lags.py'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='test\model\test_deepar_smoke.py'>
 		<file_info nloc='117' complexity='2' token_count='595'></file_info>
 	</modification>
 </commit>
</bug_data>
