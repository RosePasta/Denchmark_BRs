<bug_data>
<bug id='48' author='Z-zhe' open_date='2020-04-08T09:15:23Z' closed_time='2020-04-19T02:10:07Z'>
 	<summary>A little bugÔºÅ</summary>
 	<description>
 Hi, I think there is a little bug at numpy-ml/numpy_ml/neural_nets/activations/activations.py  Line 64.
 your code
 &lt;denchmark-code&gt;fn_x = self.fn_x
 &lt;/denchmark-code&gt;
 
 but
 self.fn_x Never defined
 	</description>
 	<comments>
 		<comment id='1' author='Z-zhe' date='2020-04-09T07:23:52Z'>
 		And numpy-ml/numpy_ml/neural_nets/wrappers/wrappers.py Line 207:
 your code
 &lt;denchmark-code&gt;def backward(self, dLdy, retain_grads):
 """
 retain_grads: Default is True
 """
 &lt;/denchmark-code&gt;
 
 Your code is missing the default value, resulting in an error at  numpy-ml/numpy_ml/neural_nets/layers/layers.py Line 332.
 		</comment>
 		<comment id='2' author='Z-zhe' date='2020-04-09T09:06:04Z'>
 		And numpy-ml/numpy_ml/neural_nets/layers/layers.py Line 2116:
 your code
 &lt;denchmark-code&gt;def backward(self, dLdy):
 """
 retain_grads: Default is True
 """
 &lt;/denchmark-code&gt;
 
 The function in your code is missing an argument retain_grads, resulting in an error at numpy-ml/numpy_ml/neural_nets/wrappers/wrappers.py Line 227.
 		</comment>
 		<comment id='3' author='Z-zhe' date='2020-04-09T09:16:46Z'>
 		Maybe I should fork and create pull request üòÉ
 		</comment>
 		<comment id='4' author='Z-zhe' date='2020-04-09T12:18:33Z'>
 		And numpy-ml/numpy_ml/neural_nets/tests/tests.py line 510:
 your code
 &lt;denchmark-code&gt;from ..activations import Softmax
 &lt;/denchmark-code&gt;
 
 but
 Softmax is not implemented in ..activations, but in ..layers.
 right code
 
 line 510:
 
 &lt;denchmark-code&gt;from ..layers import Softmax
 &lt;/denchmark-code&gt;
 
 
 
 line 527:
 
 &lt;denchmark-code&gt;y_pred = sm.forward(z)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='Z-zhe' date='2020-04-09T12:35:22Z'>
 		And numpy-ml/numpy_ml/neural_nets/tests/tests.py line 771:
 your code
 &lt;denchmark-code&gt;from ..activations import SoftSign
 &lt;/denchmark-code&gt;
 
 but SoftSign is not implemented in ..activations.
 maybe you should delete function test_softsign_grad and test_softsign_activation.
 		</comment>
 		<comment id='6' author='Z-zhe' date='2020-04-09T15:11:55Z'>
 		&lt;denchmark-link:https://github.com/Z-zhe&gt;@Z-zhe&lt;/denchmark-link&gt;
  - Wow, thanks so much for all these! I haven't had a chance to take a look yet, but should have some time this weekend. In the meantime if you feel like submitting a PR with fixes I'd be happy to review it, otherwise I can try to address these shortly.
 		</comment>
 		<comment id='7' author='Z-zhe' date='2020-04-10T09:05:15Z'>
 		PR is complicated, it is easier for you to modify. üòÉ
 		</comment>
 		<comment id='8' author='Z-zhe' date='2020-04-17T09:30:04Z'>
 		numpy-ml/numpy_ml/neural_nets/layers/layers.py line 2341:
 your code
 &lt;denchmark-code&gt;dX = dZ @ W.T
 &lt;/denchmark-code&gt;
 
 I don't think it should be W it should be W_sparse. So I think right code should be:
 &lt;denchmark-code&gt;dX = dZ @ W_sparse.T
 &lt;/denchmark-code&gt;
 
 Please reconsiderÔºåthanks.
 		</comment>
 	</comments>
 </bug>
<commit id='be8ae238d4d5340e7fc7dd27f008275ee03267d5' author='ddbourgin' date='2020-04-18 22:09:52-04:00'>
 	<dmm_unit complexity='1.0' interfacing='0.4270833333333333' size='0.203125'></dmm_unit>
 	<modification change_type='MODIFY' old_name='numpy_ml\neural_nets\activations\activations.py' new_name='numpy_ml\neural_nets\activations\activations.py'>
 		<file_info nloc='495' complexity='57' token_count='1312'></file_info>
 		<method name='__init__' parameters='self,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='16' nesting_level='1' start_line='9' end_line='11'></method_info>
 			<added_lines>10</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,alpha'>
 				<method_info nloc='31' complexity='1' token_count='25' nesting_level='1' start_line='319' end_line='349'></method_info>
 			<added_lines>320,329</added_lines>
 			<deleted_lines>321,340,348</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='1' start_line='31' end_line='33'></method_info>
 			<added_lines>32</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='grad' parameters='self,x,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='1' start_line='25' end_line='27'></method_info>
 			<added_lines>26</added_lines>
 			<deleted_lines>26,27</deleted_lines>
 		</method>
 		<method name='grad' parameters='self,x'>
 				<method_info nloc='10' complexity='1' token_count='25' nesting_level='1' start_line='49' end_line='58'></method_info>
 			<added_lines>50,55</added_lines>
 			<deleted_lines>50,56</deleted_lines>
 		</method>
 		<method name='__call__' parameters='self,z'>
 				<method_info nloc='4' complexity='2' token_count='33' nesting_level='1' start_line='13' end_line='17'></method_info>
 			<added_lines>14</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='grad2' parameters='self,x'>
 				<method_info nloc='11' complexity='1' token_count='33' nesting_level='1' start_line='60' end_line='70'></method_info>
 			<added_lines>61,66,67,69</added_lines>
 			<deleted_lines>61,62,64</deleted_lines>
 		</method>
 		<method name='fn' parameters='self,z'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='20' end_line='22'></method_info>
 			<added_lines>21</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__str__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='8' nesting_level='1' start_line='35' end_line='37'></method_info>
 			<added_lines>36</added_lines>
 			<deleted_lines>35</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,2,40,45,101,105,110,111,112,117,122,123,124,129,130,131,135,166,170,175,176,177,184,190,191,192,199,200,201,205,212,216,220,224,230,235,241,242,265,269,274,279,285,290,296,314,352,356,361,362,363,369,375,376,377,383,389,390,391,399,403,407,408,409,410,411,412,416,422,427,433,439,448,452,453,471,475,480,486,487,488,493,499,500,501,504,508,514,515,516,534,538,543,544,545,546,551,557,558,559,564,570,591,595,600,605,611,617,623,624</added_lines>
 			<deleted_lines>28,40,45,99,104,105,106,111,116,117,118,123,124,128,162,167,168,169,176,182,183,184,191,192,196,203,204,205,212,213,214,218,224,229,235,236,262,267,272,278,283,289,312,353,354,355,361,367,368,369,375,381,382,383,391,392,393,400,404,410,415,421,427,436,440,441,462,467,473,474,475,480,486,487,488,491,495,501,502,503,524,529,530,531,532,537,543,544,545,550,556,580,585,590,596,602,608</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='numpy_ml\neural_nets\layers\layers.py' new_name='numpy_ml\neural_nets\layers\layers.py'>
 		<file_info nloc='2247' complexity='354' token_count='16184'></file_info>
 		<method name='gradients' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='1' start_line='3986' end_line='3991'></method_info>
 			<added_lines>3987,3988,3989,3990</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='freeze' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='18' nesting_level='1' start_line='202' end_line='208'></method_info>
 			<added_lines>203,204,205,206</added_lines>
 			<deleted_lines>208</deleted_lines>
 		</method>
 		<method name='backward' parameters='self,out,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='1' start_line='51' end_line='53'></method_info>
 			<added_lines>52</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,act_fn,optimizer'>
 				<method_info nloc='4' complexity='1' token_count='37' nesting_level='1' start_line='603' end_line='622'></method_info>
 			<added_lines>619</added_lines>
 			<deleted_lines>609</deleted_lines>
 		</method>
 		<method name='backward' parameters='self,dLdA'>
 				<method_info nloc='9' complexity='2' token_count='77' nesting_level='1' start_line='4131' end_line='4154'></method_info>
 			<added_lines>4146</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,keep_dim,optimizer'>
 				<method_info nloc='4' complexity='1' token_count='32' nesting_level='1' start_line='797' end_line='816'></method_info>
 			<added_lines>812</added_lines>
 			<deleted_lines>802</deleted_lines>
 		</method>
 		<method name='update' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='18' nesting_level='1' start_line='4033' end_line='4039'></method_info>
 			<added_lines>4034,4035,4036,4037</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='flush_gradients' parameters='self'>
 				<method_info nloc='9' complexity='3' token_count='82' nesting_level='1' start_line='3551' end_line='3564'></method_info>
 			<added_lines>3552</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,n_out,vocab_size,pool,init,optimizer'>
 				<method_info nloc='2' complexity='1' token_count='20' nesting_level='1' start_line='1680' end_line='1681'></method_info>
 			<added_lines>1681</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_bwd' parameters='self,dLdy,X'>
 				<method_info nloc='13' complexity='7' token_count='125' nesting_level='1' start_line='1847' end_line='1861'></method_info>
 			<added_lines>1855,1858</added_lines>
 			<deleted_lines>1848,1856</deleted_lines>
 		</method>
 		<method name='parameters' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='1' start_line='3994' end_line='3996'></method_info>
 			<added_lines>3995</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,n_out,act_fn,init,optimizer'>
 				<method_info nloc='47' complexity='1' token_count='85' nesting_level='1' start_line='3362' end_line='3409'></method_info>
 			<added_lines>3363,3374,3400</added_lines>
 			<deleted_lines>3364,3390</deleted_lines>
 		</method>
 		<method name='backward' parameters='self,dLdy'>
 				<method_info nloc='10' complexity='4' token_count='79' nesting_level='1' start_line='2116' end_line='2144'></method_info>
 			<added_lines>2126,2143</added_lines>
 			<deleted_lines>2116,2133</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,momentum,epsilon,optimizer'>
 				<method_info nloc='13' complexity='1' token_count='77' nesting_level='1' start_line='894' end_line='959'></method_info>
 			<added_lines>946</added_lines>
 			<deleted_lines>936</deleted_lines>
 		</method>
 		<method name='set_params' parameters='self,summary_dict'>
 				<method_info nloc='3' complexity='1' token_count='27' nesting_level='1' start_line='3998' end_line='4016'></method_info>
 			<added_lines>3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,scale,dropout_p,init,optimizer'>
 				<method_info nloc='38' complexity='1' token_count='51' nesting_level='1' start_line='140' end_line='178'></method_info>
 			<added_lines>141,150,151,152,172</added_lines>
 			<deleted_lines>147,148,149,169</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,epsilon,optimizer'>
 				<method_info nloc='7' complexity='1' token_count='57' nesting_level='1' start_line='1341' end_line='1378'></method_info>
 			<added_lines>1371</added_lines>
 			<deleted_lines>1361</deleted_lines>
 		</method>
 		<method name='unfreeze' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='18' nesting_level='1' start_line='210' end_line='213'></method_info>
 			<added_lines>211</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,n_out,K,init,optimizer'>
 				<method_info nloc='11' complexity='1' token_count='95' nesting_level='1' start_line='349' end_line='379'></method_info>
 			<added_lines>367</added_lines>
 			<deleted_lines>357</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,Q,K,V,retain_derived'>
 				<method_info nloc='70' complexity='2' token_count='61' nesting_level='1' start_line='215' end_line='286'></method_info>
 			<added_lines>216,245,251,252,253,257,261,265,277</added_lines>
 			<deleted_lines>237,243,247,251,255,267</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,dim,optimizer'>
 				<method_info nloc='35' complexity='1' token_count='39' nesting_level='1' start_line='2033' end_line='2068'></method_info>
 			<added_lines>2034,2049,2063</added_lines>
 			<deleted_lines>2039,2053</deleted_lines>
 		</method>
 		<method name='_backward_naive' parameters='self,dLdy,retain_grads'>
 				<method_info nloc='32' complexity='9' token_count='357' nesting_level='1' start_line='2631' end_line='2690'></method_info>
 			<added_lines>2649</added_lines>
 			<deleted_lines>2639</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,kernel_shape,stride,pad,mode,optimizer'>
 				<method_info nloc='9' complexity='1' token_count='67' nesting_level='1' start_line='2967' end_line='2996'></method_info>
 			<added_lines>2987</added_lines>
 			<deleted_lines>2977</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,X,retain_derived'>
 				<method_info nloc='26' complexity='3' token_count='71' nesting_level='1' start_line='639' end_line='664'></method_info>
 			<added_lines>640,655</added_lines>
 			<deleted_lines>645,657,662</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,z,kwargs'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='1' start_line='46' end_line='48'></method_info>
 			<added_lines>47</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='derived_variables' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='1' start_line='3978' end_line='3983'></method_info>
 			<added_lines>3979,3980,3981,3982</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,n_out,act_fn,init,optimizer'>
 				<method_info nloc='37' complexity='1' token_count='72' nesting_level='1' start_line='1865' end_line='1902'></method_info>
 			<added_lines>1866,1894</added_lines>
 			<deleted_lines>1884</deleted_lines>
 		</method>
 		<method name='backward' parameters='self,dLdy,retain_grads'>
 				<method_info nloc='35' complexity='4' token_count='153' nesting_level='1' start_line='296' end_line='333'></method_info>
 			<added_lines>297,302,311,313,315,317</added_lines>
 			<deleted_lines>301,303,305,307</deleted_lines>
 		</method>
 		<method name='backward' parameters='self,dLdY,retain_grads'>
 				<method_info nloc='24' complexity='4' token_count='83' nesting_level='1' start_line='666' end_line='690'></method_info>
 			<added_lines>667,672</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,n_out,vocab_size,pool,init,optimizer'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='1670' end_line='1671'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>1671</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,715,736,751,753,763,768,837,852,864,869,878,880,1038,1085,1180,1436,1471,1546,1713,1792,1836,1988,2188,2221,2325,2351,2468,2578,2745,2809,2859,2917,3039,3092,3180,3242,3288,3621,3858,3893,4019,4020,4021,4022,4026,4030,4070,4158,4159,4160,4161,4166,4167,4168,4169,4174,4178,4179,4180,4181,4185,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4209,4213,4214,4215,4216</added_lines>
 			<deleted_lines>138,287,292,630,705,726,741,743,753,758,827,842,854,859,868,870,1028,1075,1170,1426,1461,1536,1703,1782,1826,1845,1978,2024,2178,2211,2315,2341,2458,2568,2735,2799,2849,2907,3029,3082,3170,3232,3278,3353,3610,3881,4023,4099</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='numpy_ml\neural_nets\tests\tests.py' new_name='numpy_ml\neural_nets\tests\tests.py'>
 		<file_info nloc='1901' complexity='191' token_count='16562'></file_info>
 		<method name='test_softsign_activation' parameters='N'>
 				<method_info nloc='12' complexity='3' token_count='98' nesting_level='0' start_line='627' end_line='641'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>627,628,629,630,631,632,633,634,635,636,637,638,639,640,641</deleted_lines>
 		</method>
 		<method name='test_softsign_grad' parameters='N'>
 				<method_info nloc='13' complexity='3' token_count='104' nesting_level='0' start_line='770' end_line='785'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785</deleted_lines>
 		</method>
 		<method name='test_cross_entropy_grad' parameters='N'>
 				<method_info nloc='17' complexity='3' token_count='130' nesting_level='0' start_line='509' end_line='532'></method_info>
 			<added_lines>511,528</added_lines>
 			<deleted_lines>510,527</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1</added_lines>
 			<deleted_lines>642,643,786,787</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='numpy_ml\neural_nets\wrappers\wrappers.py' new_name='numpy_ml\neural_nets\wrappers\wrappers.py'>
 		<file_info nloc='115' complexity='36' token_count='787'></file_info>
 		<method name='backward' parameters='self,dLdy,retain_grads'>
 				<method_info nloc='4' complexity='1' token_count='43' nesting_level='1' start_line='207' end_line='227'></method_info>
 			<added_lines>212</added_lines>
 			<deleted_lines>207,224</deleted_lines>
 		</method>
 		<method name='backward' parameters='self,dLdy,retain_grads'>
 				<method_info nloc='4' complexity='1' token_count='45' nesting_level='1' start_line='212' end_line='232'></method_info>
 			<added_lines>212,229</added_lines>
 			<deleted_lines>224</deleted_lines>
 		</method>
 		<method name='init_wrappers' parameters='layer,wrappers_list'>
 				<method_info nloc='7' complexity='3' token_count='51' nesting_level='0' start_line='235' end_line='258'></method_info>
 			<added_lines>236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,2,3,4,5</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
