<bug_data>
<bug id='215' author='zkx741481546' open_date='2020-09-12T02:51:26Z' closed_time='2020-09-16T09:43:20Z'>
 	<summary>PER weight does not support customized replay buffer</summary>
 	<description>
 
  I have marked all applicable categories:
 
  exception-raising bug
  RL algorithm bug
  documentation request (i.e. "X is missing from the documentation.")
  new feature request
 
 
  I have visited the source website
  I have searched through the issue tracker for duplicates
  I have mentioned version numbers, operating system and environment, where applicable:
 
 I'm running TD3 with the newest version of tianshou, and when calling td3.learn, there is an exception raised due to td1 is a tensor but weight is a numpy.ndarray, the pytorch automatically turn td1 tensor into numpy.ndarray. But I'm running on gpu, so here comes the exception.
 Here is the code that raised exception:
 def learn(self, batch: Batch, **kwargs) -&gt; Dict[str, float]:
     weight = batch.pop('weight', 1.)
     # critic 1
     current_q1 = self.critic1(batch.obs, batch.act).flatten()
     target_q = batch.returns.flatten()
     td1 = current_q1 - target_q
     critic1_loss = (td1.pow(2) * weight).mean() ------exception
 &lt;denchmark-code&gt;Epoch #1:   0%|          | 0/1 [00:21&lt;?, ?it/s]
 Traceback (most recent call last):
   File "D:/PycharmProjects/Stable-BaselineTrading/Tianshou/TD3.py", line 85, in &lt;module&gt;
     save_fn=lambda p: torch.save(p.state_dict(), save_dir))
   File "D:\PycharmProjects\Stable-BaselineTrading\Tianshou\Trainer\offpolicy.py", line 110, in offpolicy_trainer
     losses = policy.update(batch_size, train_collector.buffer)
   File "C:\Users\zkx74\Anaconda3\envs\RL\lib\site-packages\tianshou\policy\base.py", line 147, in update
     result = self.learn(batch, *args, **kwargs)
   File "C:\Users\zkx74\Anaconda3\envs\RL\lib\site-packages\tianshou\policy\modelfree\td3.py", line 123, in learn
     critic1_loss = (td1.pow(2) * weight).mean()
   File "C:\Users\zkx74\Anaconda3\envs\RL\lib\site-packages\torch\tensor.py", line 480, in __array__
     return self.numpy()
 TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
 &lt;/denchmark-code&gt;
 
 By the way, it's very odd that I can run this code normally on centos gpu server without this error.
 Here is my env version:
 &lt;denchmark-code&gt;numpy 1.19.0
 torch 1.6.0
 cuda 10.1
 tianshou 0.2.7
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='zkx741481546' date='2020-09-12T02:55:36Z'>
 		I think the best way to fix this is detect is the td_error is a tensor on gpu, and convert weight to a tensor saved on the same device
 		</comment>
 		<comment id='2' author='zkx741481546' date='2020-09-12T03:03:32Z'>
 		Could you please provide some other details? In my own Linux computer, it doesn't have such an exception.
 --&gt; Your os platform is win10? Did you use PER in TD3?
 And add one line before    critic1_loss = (td1.pow(2) * weight).mean():
 print(td1.dtype, type(weight), weight.dtype, batch.weight)
 		</comment>
 		<comment id='3' author='zkx741481546' date='2020-09-12T03:30:12Z'>
 		
 Could you please provide some other details? In my own Linux computer, it doesn't have such an exception.
 --&gt; Your os platform is win10? Did you use PER in TD3?
 And add one line before  critic1_loss = (td1.pow(2) * weight).mean():
 print(td1.dtype, type(weight), weight.dtype, batch.weight)
 
 I'm running on win10 with PER in TD3. Oh, I customized my own PER based on last version, maybe it's because I didn't update it to this version, I will check my code and reply later.
 		</comment>
 		<comment id='4' author='zkx741481546' date='2020-09-12T03:32:34Z'>
 		Possibly it is because you create a new class which doesn't inherit the PrioritizedReplayBuffer class, which fails the checking in BasePolicy
 
 
 
 tianshou/tianshou/policy/base.py
 
 
         Lines 246 to 248
       in
       16d8e9b
 
 
 
 
 
 
  # prio buffer update 
 
 
 
  if isinstance(buffer, PrioritizedReplayBuffer): 
 
 
 
  batch.weight = to_torch_as(batch.weight, target_q_torch) 
 
 
 
 
 
 		</comment>
 		<comment id='5' author='zkx741481546' date='2020-09-12T07:42:02Z'>
 		
 Possibly it is because you create a new class which doesn't inherit the PrioritizedReplayBuffer class, which fails the checking in BasePolicy
 
 
 
 tianshou/tianshou/policy/base.py
 
 
         Lines 246 to 248
       in
       16d8e9b
 
 
 
 
 
 
  # prio buffer update 
 
 
 
  if isinstance(buffer, PrioritizedReplayBuffer): 
 
 
 
  batch.weight = to_torch_as(batch.weight, target_q_torch) 
 
 
 
 
 
 
 I customized a StockReplayBuffer class inherit the ReplayBuffer class, and implement a StockPrioritizedReplayBuffer class inherit the StockReplayBuffer class, both of them overwrite the add method.
 		</comment>
 		<comment id='6' author='zkx741481546' date='2020-09-12T07:43:39Z'>
 		But, if you want to use the prioritized experience replay's feature, plz inherit PrioritizedReplayBuffer instead of ReplayBuffer. Or change the line in BasePolicy as listed above to:
     if isinstance(buffer, (PrioritizedReplayBuffer, StockReplayBuffer)):
        batch.weight = to_torch_as(batch.weight, target_q_torch) 
 		</comment>
 		<comment id='7' author='zkx741481546' date='2020-09-12T07:59:02Z'>
 		
 print(td1.dtype, type(weight), weight.dtype, batch.weight)
 
 the output is
 torch.float32
 &lt;class 'numpy.ndarray'&gt;
 float64
 {AttributeError}'Batch' object has no attribute 'weight'
 		</comment>
 		<comment id='8' author='zkx741481546' date='2020-09-12T08:00:16Z'>
 		
 But, if you want to use the prioritized experience replay's feature, plz inherit PrioritizedReplayBuffer instead of ReplayBuffer. Or change the line in BasePolicy as listed above to:
     if isinstance(buffer, (PrioritizedReplayBuffer, StockReplayBuffer)):
        batch.weight = to_torch_as(batch.weight, target_q_torch) 
 
 try to do this first and see if the issue is fixed
 		</comment>
 		<comment id='9' author='zkx741481546' date='2020-09-12T08:01:16Z'>
 		
 But, if you want to use the prioritized experience replay's feature, plz inherit PrioritizedReplayBuffer instead of ReplayBuffer. Or change the line in BasePolicy as listed above to:
     if isinstance(buffer, (PrioritizedReplayBuffer, StockReplayBuffer)):
        batch.weight = to_torch_as(batch.weight, target_q_torch) 
 
 I used a stupid way by copying PrioritizedReplayBuffer code and edit it, I will try to inherit it or use the method you said, thanks for your helping!
 		</comment>
 		<comment id='10' author='zkx741481546' date='2020-09-12T08:50:00Z'>
 		
 
 But, if you want to use the prioritized experience replay's feature, plz inherit PrioritizedReplayBuffer instead of ReplayBuffer. Or change the line in BasePolicy as listed above to:
     if isinstance(buffer, (PrioritizedReplayBuffer, StockReplayBuffer)):
        batch.weight = to_torch_as(batch.weight, target_q_torch) 
 
 try to do this first and see if the issue is fixed
 
 I used multi-inherit, my own PER inherit my replay buffer class and PER from tianshou the same time, now it working normally, thank you very much!
 		</comment>
 		<comment id='11' author='zkx741481546' date='2020-09-12T08:50:38Z'>
 		But my code before working on linux is really weird, it shouldn't.
 		</comment>
 		<comment id='12' author='zkx741481546' date='2020-09-12T08:52:26Z'>
 		Hmm... I think it has a better way to solve this issue: change
 
 
 
 tianshou/tianshou/policy/base.py
 
 
         Lines 246 to 248
       in
       16d8e9b
 
 
 
 
 
 
  # prio buffer update 
 
 
 
  if isinstance(buffer, PrioritizedReplayBuffer): 
 
 
 
  batch.weight = to_torch_as(batch.weight, target_q_torch) 
 
 
 
 
 
 to
 if hasattr(batch, "weight"):
     batch.weight = to_torch_as(batch.weight, target_q_torch)
 and change 
 
 
 tianshou/tianshou/policy/base.py
 
 
         Lines 141 to 143
       in
       c91def6
 
 
 
 
 
 
  if isinstance(buffer, PrioritizedReplayBuffer) and hasattr( 
 
 
 
  batch, "weight" 
 
 
 
  ): 
 
 
 
 
 
 to
 if hasattr(batch, "weight"):
     buffer.update_weight(indice, batch.weight)
 I'll fix it later.
 		</comment>
 		<comment id='13' author='zkx741481546' date='2020-09-12T08:58:38Z'>
 		
 Hmm... I think it has a better way to solve this issue: change
 
 
 
 tianshou/tianshou/policy/base.py
 
 
         Lines 246 to 248
       in
       16d8e9b
 
 
 
 
 
 
  # prio buffer update 
 
 
 
  if isinstance(buffer, PrioritizedReplayBuffer): 
 
 
 
  batch.weight = to_torch_as(batch.weight, target_q_torch) 
 
 
 
 
 
 to
 if hasattr(batch, "weight"):
     batch.weight = to_torch_as(batch.weight, target_q_torch)
 and change
 
 
 
 tianshou/tianshou/policy/base.py
 
 
         Lines 141 to 143
       in
       c91def6
 
 
 
 
 
 
  if isinstance(buffer, PrioritizedReplayBuffer) and hasattr( 
 
 
 
  batch, "weight" 
 
 
 
  ): 
 
 
 
 
 
 to
 if hasattr(batch, "weight"):
     buffer.update_weight(indice, batch.weight)
 I'll fix it later.
 
 Indeed
 		</comment>
 		<comment id='14' author='zkx741481546' date='2020-09-13T09:46:31Z'>
 		Please have a look at &lt;denchmark-link:https://github.com/thu-ml/tianshou/pull/217&gt;#217&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='eec0826fd3a7c3066f68a5cc8b0b7ac145a6f1ac' author='n+e' date='2020-09-16 17:43:19+08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tianshou\policy\base.py' new_name='tianshou\policy\base.py'>
 		<file_info nloc='290' complexity='12' token_count='1162'></file_info>
 		<modified_lines>
 			<added_lines>9,140,253</added_lines>
 			<deleted_lines>9,10,141,142,143,256,257</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tianshou\policy\modelfree\discrete_sac.py' new_name='tianshou\policy\modelfree\discrete_sac.py'>
 		<file_info nloc='129' complexity='6' token_count='860'></file_info>
 		<method name='learn' parameters='self,Batch,Any'>
 				<method_info nloc='46' complexity='3' token_count='432' nesting_level='1' start_line='91' end_line='148'></method_info>
 			<added_lines>130,131</added_lines>
 			<deleted_lines>130,131</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
