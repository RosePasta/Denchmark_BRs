<bug_data>
<bug id='37' author='Trinkle23897' open_date='2020-04-23T14:39:44Z' closed_time='2020-04-26T03:05:00Z'>
 	<summary>compatible with torch 1.5.0</summary>
 	<description>
 Fail in SACPolicy with torch==1.5.0
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "test/continuous/test_sac_with_il.py", line 145, in &lt;module&gt;
     test_sac_with_il()
   File "test/continuous/test_sac_with_il.py", line 106, in test_sac_with_il
     args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)
   File "/home/trinkle/github/tianshou-new/tianshou/trainer/offpolicy.py", line 87, in offpolicy_trainer
     losses = policy.learn(train_collector.sample(batch_size))
   File "/home/trinkle/github/tianshou-new/tianshou/policy/modelfree/sac.py", line 131, in learn
     actor_loss.backward()
   File "/home/trinkle/.local/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
     torch.autograd.backward(self, gradient, retain_graph, create_graph)
   File "/home/trinkle/.local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
     allow_unreachable=True)  # allow_unreachable flag
 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='70290346eaf7aade89a60ff7cfb0f7d11d02d918' author='Trinkle23897' date='2020-04-26 11:04:45+08:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tianshou\policy\modelfree\sac.py' new_name='tianshou\policy\modelfree\sac.py'>
 		<file_info nloc='124' complexity='8' token_count='900'></file_info>
 		<method name='learn' parameters='self,batch,kwargs'>
 				<method_info nloc='40' complexity='1' token_count='360' nesting_level='1' start_line='94' end_line='136'></method_info>
 			<added_lines>110,113,116,119,122,123,124,125,126,127</added_lines>
 			<deleted_lines>109,110,111,112,113,114,115,116,117,118,122,127</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
