<bug_data>
<bug id='167' author='wiljohnhong' open_date='2020-07-26T13:59:29Z' closed_time='2020-07-27T07:55:14Z'>
 	<summary>why the value of `collect_per_step` passed into `n_episode`?</summary>
 	<description>
 
 
 
 tianshou/tianshou/trainer/onpolicy.py
 
 
          Line 87
       in
       bd9c3c7
 
 
 
 
 
 
  result = train_collector.collect(n_episode=collect_per_step, 
 
 
 
 
 
 I see that the parameter collect_per_step in onpolicy_trainer() function refers to "the number of frames the collector would collect before the network update", but its value is finally passed into the n_episode parameter in collect(), I get quite confused about this. Do I misunderstand anything?
 	</description>
 	<comments>
 		<comment id='1' author='wiljohnhong' date='2020-07-27T00:38:33Z'>
 		Sorry. Indeed it should be "the number of episodes the collector would collect before the network update". It's my fault and I'll fix it right now. Thank you for pointing out!
 		</comment>
 		<comment id='2' author='wiljohnhong' date='2020-07-28T02:27:35Z'>
 		thanks, and I recommend that tianshou can allow us to choose whether to collect a fixed num of episodes or a fixed num of timesteps to collect in collect(), since in some environments an episode may consist of a large num of timesteps and we do not want to train after finishing the whole episode, and in some other cases the length of an episode could vary greatly.
 		</comment>
 		<comment id='3' author='wiljohnhong' date='2020-07-28T03:06:04Z'>
 		
 I recommend that tianshou can allow us to choose whether to collect a fixed num of episodes or a fixed num of timesteps to collect in collect(), since in some environments an episode may consist of a large num of timesteps and we do not want to train after collect the whole episode, and in some other cases the length of an episode could vary greatly.
 
 If I have time I'll fix it.
 		</comment>
 		<comment id='4' author='wiljohnhong' date='2020-07-28T08:02:49Z'>
 		
 in some environments an episode may consist of a large num of timesteps and we do not want to train after finishing the whole episode
 
 I think there is an easy workaround without modifying Tianshou: you can truncate the environment to finish after a certain amount of steps. However, in many cases, the reward is very sparse (only available when the episode finishes) and training with partial episodes makes no sense (the rewards are all zero). Therefore, I think only allowing collecting full episodes works in most cases. If one indeed needs to train with partial episodes, they can wrap the environment to finish after a certain amount of steps.
 		</comment>
 	</comments>
 </bug>
<commit id='b7a4015db71b7014cfddeca31eb40829ee4646a9' author='Trinkle23897' date='2020-07-27 16:54:14+08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='examples\pong_a2c.py' new_name='examples\pong_a2c.py'>
 		<file_info nloc='86' complexity='10' token_count='837'></file_info>
 		<method name='get_args' parameters=''>
 				<method_info nloc='26' complexity='2' token_count='322' nesting_level='0' start_line='17' end_line='44'></method_info>
 			<added_lines>26</added_lines>
 			<deleted_lines>26</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='examples\pong_ppo.py' new_name='examples\pong_ppo.py'>
 		<file_info nloc='91' complexity='10' token_count='865'></file_info>
 		<method name='get_args' parameters=''>
 				<method_info nloc='27' complexity='2' token_count='340' nesting_level='0' start_line='17' end_line='44'></method_info>
 			<added_lines>26</added_lines>
 			<deleted_lines>26</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tianshou\data\collector.py' new_name='tianshou\data\collector.py'>
 		<file_info nloc='320' complexity='21' token_count='1780'></file_info>
 		<modified_lines>
 			<added_lines>262,265</added_lines>
 			<deleted_lines>261,265,266,267</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tianshou\policy\base.py' new_name='tianshou\policy\base.py'>
 		<file_info nloc='206' complexity='7' token_count='645'></file_info>
 		<modified_lines>
 			<added_lines>89,90</added_lines>
 			<deleted_lines>89,90,91</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tianshou\trainer\onpolicy.py' new_name='tianshou\trainer\onpolicy.py'>
 		<file_info nloc='138' complexity='1' token_count='676'></file_info>
 		<modified_lines>
 			<added_lines>43,44,45</added_lines>
 			<deleted_lines>43,44,45</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
