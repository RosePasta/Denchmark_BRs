<bug_data>
<bug id='4540' author='zippeurfou' open_date='2020-11-05T23:07:32Z' closed_time='2020-12-05T21:00:32Z'>
 	<summary>Existing logger conflict with lr_finder</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 If you call specific logger function (ie.  on a tensorboard one). The tuner will fails because it overwrite it with a DummyLogger (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/tuner/lr_finder.py#L150&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/tuner/lr_finder.py#L150&lt;/denchmark-link&gt;
 ).
 I would expect to not have to check if the logger I initiated in my trainer is being overwritten or not.
 Additionally, on a minor minor note. I wouldn't expect  to be overwritten as it is &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/tuner/lr_finder.py#L147&gt;here&lt;/denchmark-link&gt;
 .
 &lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Have a smooth way to ignore it.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;* CUDA:
 	- GPU:
 		- Tesla T4
 	- available:         True
 	- version:           10.1
 * Packages:
 	- numpy:             1.18.5
 	- pyTorch_debug:     True
 	- pyTorch_version:   1.7.0+cu101
 	- pytorch-lightning: 0.10.0
 	- tqdm:              4.41.1
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		- 
 	- processor:         x86_64
 	- python:            3.6.9
 	- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020
 &lt;/denchmark-code&gt;
 
 
 PyTorch Version (e.g., 1.0): 1.0
 OS (e.g., Linux): colab
 How you installed PyTorch (conda, pip, source): pip
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 I am putting this as a bug because it is a behavior that required me to check into the source code to figure out where this DummyLogger came from. If the solution is non-trivial, a resolution could just be some warnings in the doc.
 	</description>
 	<comments>
 		<comment id='1' author='zippeurfou' date='2020-11-05T23:08:13Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='zippeurfou' date='2020-11-06T13:58:45Z'>
 		We had this problem before (this issue &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1919&gt;#1919&lt;/denchmark-link&gt;
 ) and it was solved, but after refactors it seems to only be correct if  is called.
 The solution is to rebind the logger after running the a tuner algorithm like here:
 
 
 &lt;denchmark-link:https://github.com/zippeurfou&gt;@zippeurfou&lt;/denchmark-link&gt;
  would you be up for sending a PR? basically the rebind statement just need to be more from  to  and .
 		</comment>
 		<comment id='3' author='zippeurfou' date='2020-11-06T22:32:55Z'>
 		Thanks &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
   I think the issue might be more tricky. It has to go with the fact that I am using an array to define my logger (I believe) and accessing  fails because the  just don't allow that since it is not an array. This example was straightforward forward but in production, I do have multiple loggers, hence the array. If you do remove the array I think it behave as expected.
 In the case of array  also fails. Here is an &lt;denchmark-link:https://colab.research.google.com/drive/13RrnVaPJtoPy5nHaSPbJu7SedkPbpeud?usp=sharing&gt;example&lt;/denchmark-link&gt;
 
 I will read the issue and resolution to see if I can do something along the lines but it might just be some check if the dump logger is an array then create an array one (but that's pretty ugly :D).
 if you're ok with the ugly solution I can write a PR as follow with a one liner change:
 &lt;denchmark-code&gt;  trainer.logger = [DummyLogger()]*len(trainer.__dumped_params.logger.experiment) if isinstance(trainer.__dumped_params.logger.experiment,list) and len(trainer.__dumped_params.logger.experiment)&gt;0 else DummyLogger()
 &lt;/denchmark-code&gt;
 
 From &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/tuner/lr_finder.py#L150&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/tuner/lr_finder.py#L150&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='zippeurfou' date='2020-11-07T10:35:47Z'>
 		&lt;denchmark-link:https://github.com/zippeurfou&gt;@zippeurfou&lt;/denchmark-link&gt;
  what I think is a better solution is for both  and  to support indexing. It would simply be adding this class method to both:
     def __getitem__(self, idx):
         return self
 After adding this, if I run the line that gives problem in your example:
 &lt;denchmark-code&gt;self.logger.experiment[0].add_histogram(f'grad/{name}', param.grad.data, self.global_step)
 &lt;/denchmark-code&gt;
 
 i get no error :]
 Would you be up for sending a PR?
 		</comment>
 	</comments>
 </bug>
<commit id='849737e7ca033b109df00e43a775bb6c96b1873a' author='Marc Ferradou' date='2020-12-05 21:00:31+00:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\loggers\base.py' new_name='pytorch_lightning\loggers\base.py'>
 		<file_info nloc='367' complexity='87' token_count='1871'></file_info>
 		<method name='__getitem__' parameters='self,idx'>
 				<method_info nloc='2' complexity='1' token_count='9' nesting_level='1' start_line='412' end_line='415'></method_info>
 			<added_lines>412,413,414,415</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>416,445,446,447</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\loggers\test_base.py' new_name='tests\loggers\test_base.py'>
 		<file_info nloc='193' complexity='28' token_count='1217'></file_info>
 		<method name='test_dummylogger_support_indexing' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='16' nesting_level='0' start_line='224' end_line='226'></method_info>
 			<added_lines>224,225,226</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_dummyexperiment_support_indexing' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='16' nesting_level='0' start_line='219' end_line='221'></method_info>
 			<added_lines>219,220,221</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>23,222,223,227,228</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
