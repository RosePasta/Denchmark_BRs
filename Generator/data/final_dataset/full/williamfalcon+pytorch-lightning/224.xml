<bug_data>
<bug id='224' author='ananyahjha93' open_date='2019-09-15T19:54:42Z' closed_time='2019-09-16T16:35:30Z'>
 	<summary>set_epoch for DistributedSampler</summary>
 	<description>
 Describe the bug
 PyTorch example suggests the use set_epoch function for DistributedSampler class before each epoch start. I could not find this function call in lightning's trainer module.
 &lt;denchmark-link:https://github.com/pytorch/examples/blob/master/imagenet/main.py&gt;https://github.com/pytorch/examples/blob/master/imagenet/main.py&lt;/denchmark-link&gt;
 
 Line 232-234
 As can be seen from the DistributedSampler class code (&lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py&gt;https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py&lt;/denchmark-link&gt;
 ), the set_epoch function is required to set the seed for each  function call.
 Can you confirm if this function has been called on DistributedSampler (for training dataset) at some point in lightning's trainer module?
 	</description>
 	<comments>
 		<comment id='1' author='ananyahjha93' date='2019-09-15T22:28:13Z'>
 		it's not called which means it is equivalent to shuffle=False (ie: shuffled once but not again every epoch). We can add it though to enable shuffling
 		</comment>
 		<comment id='2' author='ananyahjha93' date='2019-09-15T22:28:21Z'>
 		do you want to submit that PR?
 		</comment>
 		<comment id='3' author='ananyahjha93' date='2019-09-16T07:21:25Z'>
 		Added a PR for this, do you want me to update any docs in my commit related to this issue? Also, I have successfully tested the library with this fix included in my experiments.
 		</comment>
 		<comment id='4' author='ananyahjha93' date='2020-11-02T19:26:27Z'>
 		
 it's not called which means it is equivalent to shuffle=False (ie: shuffled once but not again every epoch). We can add it though to enable shuffling
 
 A short note on this for future readers:
 No setting epochs in DistributedSampler does not equivalent to shuffle=False (for both dataloader and distributed sampler).
 		</comment>
 	</comments>
 </bug>
<commit id='c0f3b6b035f955fc371dec412d3816712f3fc1dd' author='Ananya Harsh Jha' date='2019-09-16 10:21:00-04:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\trainer.py' new_name='pytorch_lightning\trainer\trainer.py'>
 		<file_info nloc='777' complexity='237' token_count='4924'></file_info>
 		<method name='__train' parameters='self'>
 				<method_info nloc='23' complexity='10' token_count='158' nesting_level='1' start_line='929' end_line='970'></method_info>
 			<added_lines>932,933,934,935</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
