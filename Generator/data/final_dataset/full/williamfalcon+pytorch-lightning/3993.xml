<bug_data>
<bug id='3993' author='hbredin' open_date='2020-10-08T15:07:54Z' closed_time='2020-10-09T03:02:24Z'>
 	<summary>Mismatch between docstring and code regarding when `on_load_checkpoint` hook is called</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 The docstring of on_load_checkpoint¬†hook says that it is called before trying to load_state_dict:
 
 
 
 pytorch-lightning/pytorch_lightning/core/saving.py
 
 
         Lines 203 to 206
       in
       cea5f1f
 
 
 
 
 
 
  def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -&gt; None: 
 
 
 
  """ 
 
 
 
          Do something with the checkpoint. 
 
 
 
          Gives model a chance to load something before ``state_dict`` is restored. 
 
 
 
 
 
 However, in LightningModule.load_from_checkpoint, it is called after load_state_dict:
 
 
 
 pytorch-lightning/pytorch_lightning/core/saving.py
 
 
         Lines 195 to 199
       in
       cea5f1f
 
 
 
 
 
 
  # load the state_dict on the model automatically 
 
 
 
  model.load_state_dict(checkpoint['state_dict'], strict=strict) 
 
 
 
  
 
 
 
  # give model a chance to load something 
 
 
 
  model.on_load_checkpoint(checkpoint) 
 
 
 
 
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 Related discussion on Slack: &lt;denchmark-link:https://pytorch-lightning.slack.com/archives/CQXV8BRH9/p1602168345184000&gt;https://pytorch-lightning.slack.com/archives/CQXV8BRH9/p1602168345184000&lt;/denchmark-link&gt;
 
 I think the docstring is correct and the call to on_load_checkpoint¬†should be moved right before load_state_dict¬†to give the model a chance to call setup.
 	</description>
 	<comments>
 		<comment id='1' author='hbredin' date='2020-10-08T15:46:44Z'>
 		&lt;denchmark-link:https://github.com/hbredin&gt;@hbredin&lt;/denchmark-link&gt;
  mind sending a PR to fix it... 
 		</comment>
 		<comment id='2' author='hbredin' date='2020-10-08T15:47:39Z'>
 		I can do that. Should I fix the docstring or the code?
 I'd go with the code.
 		</comment>
 		<comment id='3' author='hbredin' date='2020-10-09T03:03:33Z'>
 		I need this code change as well! I'm doing transfer learning and I want to support both loading the original model with the original weights, and modify it for a new task.
 on_load_checkpoint would allow me to redo the modifications I've done for transfer learning, so the state_dict of the transferred model can be properly restored.
 At present I need to add non-network code to the model to handle this logic, which is ugly and prone to bugs.
 This would allow to have the same model to redo the modifications I've made for transfer learning,
 		</comment>
 	</comments>
 </bug>
<commit id='a8573b005224bde87eb7a81ccdf5f428620c121f' author='Herv√© BREDIN' date='2020-10-08 23:02:23-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\saving.py' new_name='pytorch_lightning\core\saving.py'>
 		<file_info nloc='294' complexity='39' token_count='1305'></file_info>
 		<method name='_load_model_state' parameters='cls,str,bool,cls_kwargs_new'>
 				<method_info nloc='25' complexity='8' token_count='243' nesting_level='1' start_line='157' end_line='201'></method_info>
 			<added_lines>198,199,200</added_lines>
 			<deleted_lines>195,196,197</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
