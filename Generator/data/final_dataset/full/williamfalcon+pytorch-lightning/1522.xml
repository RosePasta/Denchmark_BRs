<bug_data>
<bug id='1522' author='Jonas-Jaeger' open_date='2020-04-17T23:15:19Z' closed_time='2020-04-19T03:07:16Z'>
 	<summary>Performance drop when activating gradient clipping</summary>
 	<description>
 Hello all,
 I experienced a substantial drop in computation time when activating gradient clipping (by passing a non-zero value to the keyword argument gradient_clip_val when initializing the Trainer).
 I noticed that in the current implementation of the clipping_gradient method in pytorch-lightning/trainer/training_tricks.py redundant computations are made by first computing the 2-norm and second squaring this result, which could be shortened by computing the sum of squares directly. This saves one square root and squaring operation per parameter set.
 Best,
 Jonas
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;cuda:
 	GPU:
 	available:           False
 	version:             None
 packages:
 	numpy:               1.18.1
 	pyTorch_debug:       False
 	pyTorch_version:     1.4.0
 	pytorch-lightning:   0.7.4-dev
 	tensorboard:         2.2.1
 	tqdm:                4.45.0
 system:
 	OS:                  Darwin
 	architecture:
 		64bit
 		
 	processor:           i386
 	python:              3.8.2
 	version:             Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 I trained a relatively small (two-layered) MLP on MNIST; perhaps this performance drop does not become that apparent when training on larger network architectures.
 	</description>
 	<comments>
 		<comment id='1' author='Jonas-Jaeger' date='2020-04-17T23:15:56Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='Jonas-Jaeger' date='2020-04-17T23:20:13Z'>
 		Unfortunately, it seems that I do not have push access to this repository to push the proposed fix and create a pull request for this issue.
 		</comment>
 		<comment id='3' author='Jonas-Jaeger' date='2020-04-17T23:42:13Z'>
 		fork the repo
 make the fix
 submit a PR
 :)
 		</comment>
 	</comments>
 </bug>
<commit id='e02146943d3373020b7fa6e8acc31dc18b4201e4' author='Jonas-Jaeger' date='2020-04-18 23:07:15-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_tricks.py' new_name='pytorch_lightning\trainer\training_tricks.py'>
 		<file_info nloc='62' complexity='21' token_count='484'></file_info>
 		<method name='clip_gradients' parameters='self'>
 				<method_info nloc='22' complexity='9' token_count='236' nesting_level='1' start_line='26' end_line='49'></method_info>
 			<added_lines>43</added_lines>
 			<deleted_lines>43</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
