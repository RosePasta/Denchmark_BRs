<bug_data>
<bug id='1815' author='zzzace2000' open_date='2020-05-13T20:31:47Z' closed_time='2020-05-31T19:02:20Z'>
 	<summary>Set precision=16 (using apex) would cause early stopping break</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 The current early stopping monitor initilize by comparing if the function monitor_op is equal to torch.lt.
 self.best = torch_inf if self.monitor_op == torch.lt else -torch_inf
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/early_stopping.py
 
 
          Line 110
       in
       12138ce
 
 
 
 
 
 
  self.best = torch_inf if self.monitor_op == torch.lt else -torch_inf 
 
 
 
 
 
 However when intializing with the apex, it seems that the torch.lt would change and this evaluation would be always false and thus the self.best is intialized to -inf instead of +inf.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import torch
 from pytorch_lightning.callbacks.early_stopping import EarlyStopping
 import apex.amp as amp
 
 es = EarlyStopping()
 es.monitor_op == torch.lt
 Out[6]: True
 
 model = torch.Linear(5, 5).to('cuda')
 optimizers = torch.optim.Adam(model.parameters(), lr=1e-3)
 amp.initialize(model, optimizers)
 
 es.monitor_op == torch.lt
 Out[22]: False
 &lt;/denchmark-code&gt;
 
 And this bug leads to the initialization of self.best to be -inf instead of inf
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 self.best should be initialized to inf instead of -inf.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 CUDA:
 - GPU:
 - TITAN Xp
 - Quadro P400
 - available:         True
 - version:           10.1
 Packages:
 - numpy:             1.18.1
 - pyTorch_debug:     False
 - pyTorch_version:   1.4.0
 - pytorch-lightning: 0.7.5
 - tensorboard:       2.1.1
 - tqdm:              4.43.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.6.10
 - version:           #67-Ubuntu SMP Thu Aug 22 16:55:30 UTC 2019
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 I bump into this bug after downloading from the master branch couple days ago. I would guess the old version is fine but did not test it.
 	</description>
 	<comments>
 		<comment id='1' author='zzzace2000' date='2020-05-13T20:32:30Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 	</comments>
 </bug>
<commit id='bf39cb26c57f1fd5968420e99ba351a9e0df9541' author='authman' date='2020-05-31 15:02:19-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\callbacks\early_stopping.py' new_name='pytorch_lightning\callbacks\early_stopping.py'>
 		<file_info nloc='114' complexity='17' token_count='546'></file_info>
 		<method name='on_train_start' parameters='self,trainer,pl_module'>
 				<method_info nloc='4' complexity='2' token_count='53' nesting_level='1' start_line='106' end_line='110'></method_info>
 			<added_lines>110</added_lines>
 			<deleted_lines>110</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
