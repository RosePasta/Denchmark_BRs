<bug_data>
<bug id='2688' author='thschaaf' open_date='2020-07-24T19:28:26Z' closed_time='2020-07-31T11:53:09Z'>
 	<summary>Training on GPU failed with Torchtext when using include_lengths=True in torchtext.data.Field</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 The issues raises in pytorch_lightning/utilities/apply_func.py which assumes that the attributes of a Batch from trochtext are Tensors, however if torchtext.data.Field is configured to include a length Tensor (include_lengths=True) the field is a tuple.
 A bugfix is prepared and a PR can be submitted soon.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Use Torchtext Field with include_lengths=True on a GPU machine and fit model.
 Training works on CPU but fails on GPU with: TypeError: cannot unpack non-iterable NoneType object
 
 &lt;denchmark-h:h3&gt;Full Error Message&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
  File "debug_torchtext.py", line 105, in &lt;module&gt;
   trainer.fit(model)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1003, in fit
   results = self.single_gpu_train(model)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 186, in single_gpu_train
   results = self.run_pretrain_routine(model)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1213, in run_pretrain_routine
   self.train()
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 370, in train
   self.run_training_epoch()
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 452, in run_training_epoch
   batch_output = self.run_training_batch(batch, batch_idx)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 632, in run_training_batch
   self.hiddens
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 776, in optimizer_closure
   hiddens)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 944, in training_forward
   batch = self.transfer_batch_to_gpu(batch, gpu_id)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 159, in transfer_batch_to_gpu
   return self.__transfer_batch_to_device(batch, device)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 164, in __transfer_batch_to_device
   return model.transfer_batch_to_device(batch, device)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py", line 242, in transfer_batch_to_device
   return move_data_to_device(batch, device)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 128, in move_data_to_device
   return apply_to_collection(batch, dtype=(TransferableDataType, Batch), function=batch_to)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 35, in apply_to_collection
   return function(data, *args, **kwargs)
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 103, in batch_to
   device_field = getattr(data, field).to(device, non_blocking=True)
 AttributeError: 'tuple' object has no attribute 'to'
 Exception ignored in: &lt;function tqdm.__del__ at 0x7fcb5e0b2680&gt;
 Traceback (most recent call last):
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py", line 1086, in __del__
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py", line 1293, in close
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py", line 1471, in display
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py", line 1089, in __repr__
  File "/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py", line 1433, in format_dict
 TypeError: cannot unpack non-iterable NoneType object
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import torch
 from torch import nn, Tensor
 import pytorch_lightning as pl
 from pytorch_lightning import Trainer, seed_everything
 from torchtext import data
 seed_everything(1234)
 def get_debug_data_loader():
     text_field = data.Field(sequential=True, pad_first=False,
                             init_token="&lt;s&gt;", eos_token="&lt;/s&gt;", include_lengths=True)
     example1 = data.example.Example.fromdict({"text": "a b c a c"}, {"text": ("text", text_field)})
     example2 = data.example.Example.fromdict({"text": "b c a a"}, {"text": ("text", text_field)})
     example3 = data.example.Example.fromdict({"text": "c b a"}, {"text": ("text", text_field)})
     dataset = data.Dataset([example1, example2, example3], {"text": text_field})
     text_field.build_vocab(dataset)
     iterator = data.Iterator(dataset, batch_size=3,
                              sort_key=None, device=None, batch_size_fn=None,
                              train=True, repeat=False, shuffle=None, sort=None, sort_within_batch=None)
     return iterator, text_field
 class DebugModel(pl.LightningModule):
     def __init__(self):
         super(DebugModel, self).__init__()
         # setup data loader
         self.debug_data_loader, self.text_field = get_debug_data_loader()
         self.learning_rate = 0.001
         self.hid_dim = 4
         pad_idx = self.text_field.vocab.stoi['&lt;pad&gt;']
         self.criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)
         self.INPUT_DIM = len(self.text_field.vocab)
         self.ENC_EMB_DIM = 4  # keep it small for debugging
         self.embedding = nn.Embedding(self.INPUT_DIM, self.ENC_EMB_DIM)
         self.rnn = nn.GRU(self.ENC_EMB_DIM, self.hid_dim, 1, bidirectional=False)
         self.out = nn.Linear(self.hid_dim, self.embedding.num_embeddings)
         self.OUTPUT_DIM = len(self.text_field.vocab)
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=self.learning_rate)
     def forward(self, input_seq, length):
         embedded: Tensor = self.embedding(input_seq)
         packed_embedded: Tensor = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=False,
                                                                           enforce_sorted=False)
         packed_outputs, hidden = self.rnn(packed_embedded)  # [sent len, batch size, emb dim]
         outputs, length = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs)
         # outputs -&gt; [sent len, batch size, hid dim * n directions]
         # hidden -&gt; [n layers * n directions, batch size, hid dim]
         output = outputs.squeeze(0)
         prediction = self.out(output)
         return prediction
     @staticmethod
     def _parse_batch(batch):
         source = batch.text[0]
         source_length = batch.text[1]
         return source, source_length
     def training_step(self, batch, batch_nb):
         x = self._parse_batch(batch)
         target, target_length = x
         output = self.forward(target, target_length)
         loss = self.criterion(output[:-1].view(-1, output.shape[2]), target[1:].view(-1))
         prefix = 'train'
         tensorboard_logs = {f'{prefix}_loss': loss.item()}
         result = {'loss': loss, 'log': tensorboard_logs}
         return result
     def train_dataloader(self):
         return self.debug_data_loader
 model = DebugModel()
 cuda_device_cnt = torch.cuda.device_count()
 if cuda_device_cnt &gt; 0:
     use_num_cuda_devices = 1
 else:
     use_num_cuda_devices = None
 trainer = Trainer(fast_dev_run=False, max_steps=None,
                   gradient_clip_val=10,
                   weights_summary='full', gpus=use_num_cuda_devices,
                   show_progress_bar=True)
 trainer.fit(model)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Should not raise an error :-)
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt; CUDA:
     - GPU:
         - TITAN X (Pascal)
     - available:     True
     - version:      10.2
 * Packages:
     - numpy:       1.17.3
     - pyTorch_debug:   False
     - pyTorch_version:  1.5.1
     - pytorch-lightning: 0.8.5
     - tensorboard:    2.2.2
     - tqdm:       4.47.0
 * System:
     - OS:        Linux
     - architecture:
         - 64bit
         - 
     - processor:     x86_64
     - python:      3.7.4
     - version:      #1 SMP Tue Mar 17 23:49:17 UTC 2020
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='a6719f09f0a383034f4285d65cba880208a03ae4' author='Thomas Schaaf' date='2020-07-31 07:53:08-04:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.3793103448275862'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>50,51</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\utilities\apply_func.py' new_name='pytorch_lightning\utilities\apply_func.py'>
 		<file_info nloc='72' complexity='17' token_count='369'></file_info>
 		<method name='move_data_to_device' parameters='Any,device'>
 				<method_info nloc='3' complexity='1' token_count='33' nesting_level='0' start_line='79' end_line='110'></method_info>
 			<added_lines>96,104,107,108</added_lines>
 			<deleted_lines>102,103,106,107</deleted_lines>
 		</method>
 		<method name='move_data_to_device.batch_to' parameters='data'>
 				<method_info nloc='8' complexity='4' token_count='62' nesting_level='1' start_line='97' end_line='108'></method_info>
 			<added_lines>104,107,108</added_lines>
 			<deleted_lines>102,103,106,107</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>9</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\utilities\test_apply_func_torchtext.py'>
 		<file_info nloc='39' complexity='4' token_count='376'></file_info>
 	</modification>
 </commit>
</bug_data>
