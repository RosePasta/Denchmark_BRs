<bug_data>
<bug id='2386' author='nischal-sanil' open_date='2020-06-27T16:13:53Z' closed_time='2020-06-29T00:22:04Z'>
 	<summary>An Extra argument passed to the class, loaded from load_from_checkpoint.</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Hello,
 I was facing few issues while using the trainer.test() function, on debugging I found out that the problem was with the _load_model_state class method which is called by load_from_checkpoint.
 &lt;denchmark-h:h4&gt;Code For reference&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;@classmethod
 def _load_model_state(cls, checkpoint: Dict[str, Any], *args, **kwargs):
     # pass in the values we saved automatically
     if cls.CHECKPOINT_HYPER_PARAMS_KEY in checkpoint:
         model_args = {}
 
         # add some back compatibility, the actual one shall be last
         for hparam_key in CHECKPOINT_PAST_HPARAMS_KEYS + (cls.CHECKPOINT_HYPER_PARAMS_KEY,):
             if hparam_key in checkpoint:
                 model_args.update(checkpoint[hparam_key])
 
         if cls.CHECKPOINT_HYPER_PARAMS_TYPE in checkpoint:
             model_args = checkpoint[cls.CHECKPOINT_HYPER_PARAMS_TYPE](model_args)
 
         args_name = checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_NAME)
         init_args_name = inspect.signature(cls).parameters.keys()
 
         if args_name == 'kwargs':
             cls_kwargs = {k: v for k, v in model_args.items() if k in init_args_name}
             kwargs.update(**cls_kwargs)
         elif args_name:
             if args_name in init_args_name:
                 kwargs.update({args_name: model_args})
         else:
             args = (model_args, ) + args
 
     # load the state_dict on the model automatically
     model = cls(*args, **kwargs)
     model.load_state_dict(checkpoint['state_dict'])
 
     # give model a chance to load something
     model.on_load_checkpoint(checkpoint)
 
     return model
 &lt;/denchmark-code&gt;
 
 Consider the case where the  model has no arguments, which corresponds to . Here, the else clause of the if-elif is being executed where the  variable is updated from an empty tuple to a tuple with an empty dictionary  (as ). Therefore, while unpacking the args and kwargs (), There is an extra argument being passed which raises a . &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2364&gt;#2364&lt;/denchmark-link&gt;
 
 In some cases if the model has an argument and the user has forgotten to add it in the load_from_checkpoint, then an empty dictionary will be passed instead and it raises other errors depending on the code. For example, in the issue &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2359&gt;#2359&lt;/denchmark-link&gt;
  an empty dict is passed while loading the model and hence raises .
 I do not fully understand what is happening in the function. It would be great if someone can suggest changes to make in the comments so that I can start working after updating the changes in my forked repo.
 &lt;denchmark-h:h4&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;!pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade
 
 import os
 
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 from torchvision import transforms
 import pytorch_lightning as pl
 
 class MNISTModel(pl.LightningModule):
 
     def __init__(self):
         super(MNISTModel, self).__init__()
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_nb):
         x, y = batch
         loss = F.cross_entropy(self(x), y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def test_step(self, batch, batch_nb):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.02)
 
 
 train_loader = DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
 mnist_model = MNISTModel()
 trainer = pl.Trainer(gpus=1,max_epochs=3)    
 trainer.fit(mnist_model, train_loader)  
 
 test_loader = DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)
 trainer.test(test_dataloaders=test_loader)
 &lt;/denchmark-code&gt;
 
 Which returns:
 &lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
 
 &lt;ipython-input-5-50449ee4f6cc&gt; in &lt;module&gt;()
       1 test_loader = DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)
 ----&gt; 2 trainer.test(test_dataloaders=test_loader)
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in test(self, model, test_dataloaders, ckpt_path)
    1168             if ckpt_path == 'best':
    1169                 ckpt_path = self.checkpoint_callback.best_model_path
 -&gt; 1170             model = self.get_model().load_from_checkpoint(ckpt_path)
    1171 
    1172         self.testing = True
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, tags_csv, *args, **kwargs)
     167         checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)
     168 
 --&gt; 169         model = cls._load_model_state(checkpoint, *args, **kwargs)
     170         return model
     171 
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/saving.py in _load_model_state(cls, checkpoint, *cls_args, **cls_kwargs)
     201 
     202         # load the state_dict on the model automatically
 --&gt; 203         model = cls(*cls_args, **cls_kwargs)
     204         model.load_state_dict(checkpoint['state_dict'])
     205 
 
 TypeError: __init__() takes 1 positional argument but 2 were given
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Start testing
 	</description>
 	<comments>
 		<comment id='1' author='nischal-sanil' date='2020-06-27T16:14:30Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 	</comments>
 </bug>
<commit id='1e16681693e113d25a4207ce2827a39c64b19211' author='Jirka Borovec' date='2020-06-28 20:22:03-04:00'>
 	<dmm_unit complexity='0.975609756097561' interfacing='0.7804878048780488' size='0.975609756097561'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>47,48</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\saving.py' new_name='pytorch_lightning\core\saving.py'>
 		<file_info nloc='312' complexity='42' token_count='1245'></file_info>
 		<method name='_load_model_state' parameters='cls,str,cls_args,cls_kwargs'>
 				<method_info nloc='25' complexity='12' token_count='224' nesting_level='1' start_line='173' end_line='212'></method_info>
 			<added_lines>174,175,198,199,200,201,202,203,204,206</added_lines>
 			<deleted_lines>186,187,188,192,193,201</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_hparams.py' new_name='tests\models\test_hparams.py'>
 		<file_info nloc='289' complexity='50' token_count='2362'></file_info>
 		<method name='test_explicit_args_hparams' parameters='tmpdir'>
 				<method_info nloc='7' complexity='1' token_count='56' nesting_level='0' start_line='102' end_line='120'></method_info>
 			<added_lines>108,113,116,117</added_lines>
 			<deleted_lines>106,111,114,115</deleted_lines>
 		</method>
 		<method name='test_model_nohparams_train_test' parameters='tmpdir,cls'>
 				<method_info nloc='10' complexity='1' token_count='92' nesting_level='0' start_line='465' end_line='478'></method_info>
 			<added_lines>465,466,467,468,469,470,471,472,473,474,475,476,477,478</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_step' parameters='self,batch,batch_nb'>
 				<method_info nloc='4' complexity='1' token_count='41' nesting_level='1' start_line='452' end_line='455'></method_info>
 			<added_lines>452,453,454,455</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_implicit_args_hparams' parameters='tmpdir'>
 				<method_info nloc='7' complexity='1' token_count='56' nesting_level='0' start_line='123' end_line='141'></method_info>
 			<added_lines>129,134,137,138</added_lines>
 			<deleted_lines>127,132,135,136</deleted_lines>
 		</method>
 		<method name='training_step' parameters='self,batch,batch_nb'>
 				<method_info nloc='4' complexity='1' token_count='41' nesting_level='1' start_line='447' end_line='450'></method_info>
 			<added_lines>447,448,449,450</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_model_ignores_non_exist_kwargument' parameters='tmpdir'>
 				<method_info nloc='10' complexity='1' token_count='70' nesting_level='0' start_line='481' end_line='499'></method_info>
 			<added_lines>481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='1' start_line='435' end_line='436'></method_info>
 			<added_lines>435,436</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='configure_optimizers' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='24' nesting_level='1' start_line='457' end_line='458'></method_info>
 			<added_lines>457,458</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_model_ignores_non_exist_kwargument.__init__' parameters='self,batch_size'>
 				<method_info nloc='3' complexity='1' token_count='24' nesting_level='2' start_line='485' end_line='487'></method_info>
 			<added_lines>485,486,487</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_explicit_missing_args_hparams' parameters='tmpdir'>
 				<method_info nloc='15' complexity='1' token_count='119' nesting_level='0' start_line='144' end_line='175'></method_info>
 			<added_lines>150,155,171</added_lines>
 			<deleted_lines>148,153,169</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,x'>
 				<method_info nloc='2' complexity='1' token_count='32' nesting_level='1' start_line='444' end_line='445'></method_info>
 			<added_lines>444,445</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>9,10,15,432,433,434,437,438,439,440,441,442,443,446,451,456,459,460,461,462,463,464,479,480</added_lines>
 			<deleted_lines>13</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
