<bug_data>
<bug id='4156' author='Borda' open_date='2020-10-14T21:59:10Z' closed_time='2020-10-15T14:53:43Z'>
 	<summary>crashing dump for hparams containing fsspec</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 If you pass a model as PL model argument and dum this model, it crashes...
 &lt;denchmark-h:h2&gt;Please reproduce using [the BoringModel and post here]&lt;/denchmark-h&gt;
 
 Failing several models in Bolts
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/runs/1254957648&gt;https://github.com/PyTorchLightning/pytorch-lightning-bolts/runs/1254957648&lt;/denchmark-link&gt;
 
 sample of failing test:
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/ccdc9f9952153abf9b12f00e05294fa332d5f424/tests/models/test_vision.py#L8-L31&gt;https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/ccdc9f9952153abf9b12f00e05294fa332d5f424/tests/models/test_vision.py#L8-L31&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;        cls = type(data)
         if cls in copyreg.dispatch_table:
             reduce = copyreg.dispatch_table[cls](data)
         elif hasattr(data, '__reduce_ex__'):
 &gt;           reduce = data.__reduce_ex__(2)
 E           TypeError: __reduce_ex__() takes exactly one argument (0 given)
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='Borda' date='2020-10-14T22:44:22Z'>
 		seems that there is something special about Bolts's , any idea &lt;denchmark-link:https://github.com/nateraw&gt;@nateraw&lt;/denchmark-link&gt;
 ?
 		</comment>
 		<comment id='2' author='Borda' date='2020-10-14T23:02:49Z'>
 		This is the exact error I ran into in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/pull/264&gt;my PR&lt;/denchmark-link&gt;
  as well. I don't know what's causing it 
 		</comment>
 		<comment id='3' author='Borda' date='2020-10-14T23:03:21Z'>
 		ok, so the more trace is that DM has a pointer to Trainer and somewhere in Trainer there is &lt;fsspec.implementations.local.LocalFileSystem object at 0x13ba296a0&gt; which causes this problem...
 EDIT: MNISTDataModule -&gt; Trainer -&gt; CheckPoint -&gt; fsspec
 		</comment>
 		<comment id='4' author='Borda' date='2020-10-14T23:51:15Z'>
 		&lt;denchmark-code&gt;{'datamodule': &lt;pl_bolts.datamodules.mnist_datamodule.MNISTDataModule object at 0x13b991dd8&gt;, 'embed_dim': 16, 'heads': 2, 'layers': 2, 'pixels': 28, 'vocab_size': 16, 'num_classes': 10, 'classify': False, 'batch_size': 64, 'learning_rate': 0.01, 'steps': 25000, 'data_dir': '.', 'num_workers': 8}
 vvv
 &lt;pl_bolts.datamodules.mnist_datamodule.MNISTDataModule object at 0x13b991dd8&gt;
 vvv
 &lt;pytorch_lightning.trainer.trainer.Trainer object at 0x12ec29780&gt;
 vvv
 [&lt;pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x13ba29630&gt;, &lt;pytorch_lightning.callbacks.progress.ProgressBar object at 0x13ba29668&gt;]
 vvv
 &lt;pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x13ba29630&gt;
 vvv
 &lt;fsspec.implementations.local.LocalFileSystem object at 0x13ba296a0&gt;
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='Borda' date='2020-10-15T02:56:19Z'>
 		should hparams be a more formal concept in the lightning module? given how complex objects can be passed to the module or the trainer, should users explicitly save set the hyperparameters their module uses? and should lightning skip saving them if they're not set? i wonder how the frame capture logic for arguments is going to hold up over time.
 		</comment>
 		<comment id='6' author='Borda' date='2020-10-15T07:08:48Z'>
 		
 should hparams be a more formal concept in the lightning module? given how complex objects can be passed to the module or the trainer, should users explicitly save set the hyperparameters their module uses? and should lightning skip saving them if they're not set? i wonder how the frame capture logic for arguments is going to hold up over time.
 
 good questions, we do not want to limit users much, I made the two following fixes:
 
 save only parameters which can be saved and some is invalid skip it and warn user, this is just preventing to crash in runtime, #4158
 the problem came from runtimeTTrainer linking to used instances so it is impossible to do any static check or other checks before training starts, also this runtime changes are not needed for Init state reconstruction, see we save just the initial hparams, #4163
 
 note that each of this PR solves just a part of the issue üê∞
 		</comment>
 	</comments>
 </bug>
<commit id='4204ef7b533be7fe64e753372147aa204290540b' author='Jirka Borovec' date='2020-10-15 16:53:42+02:00'>
 	<dmm_unit complexity='0.6086956521739131' interfacing='0.8695652173913043' size='0.6086956521739131'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>26</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\saving.py' new_name='pytorch_lightning\core\saving.py'>
 		<file_info nloc='304' complexity='41' token_count='1361'></file_info>
 		<method name='save_hparams_to_yaml' parameters='config_yaml,dict'>
 				<method_info nloc='35' complexity='10' token_count='250' nesting_level='0' start_line='348' end_line='390'></method_info>
 			<added_lines>377,378,379,380,381,382,383,384,385,386,387,388,390</added_lines>
 			<deleted_lines>375,378</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>21</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\models\test_hparams.py' new_name='tests\models\test_hparams.py'>
 		<file_info nloc='354' complexity='59' token_count='2808'></file_info>
 		<method name='test_model_with_fsspec_as_parameter' parameters='tmpdir'>
 				<method_info nloc='11' complexity='1' token_count='50' nesting_level='0' start_line='591' end_line='601'></method_info>
 			<added_lines>591,592,593,594,595,596,597,598,599,600,601</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,my_path,any_param'>
 				<method_info nloc='3' complexity='1' token_count='23' nesting_level='1' start_line='586' end_line='588'></method_info>
 			<added_lines>586,587,588</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>21,583,584,585,589,590</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
