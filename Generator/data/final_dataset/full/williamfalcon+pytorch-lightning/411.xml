<bug_data>
<bug id='411' author='kossnick' open_date='2019-10-22T17:58:31Z' closed_time='2019-10-23T10:11:18Z'>
 	<summary>Increment `global_step` at end of iter?</summary>
 	<description>
  currently increments at the start of a batch: &lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/f18aee30a530149524b7004b6215a08110dca653/pytorch_lightning/trainer/train_loop_mixin.py#L77&gt;https://github.com/williamFalcon/pytorch-lightning/blob/f18aee30a530149524b7004b6215a08110dca653/pytorch_lightning/trainer/train_loop_mixin.py#L77&lt;/denchmark-link&gt;
 
 That means at batch_nb = 0, global_step = 1. Shouldn't global_step update at the end of the iter, so batch_nb = 0 when global_step = 0?
 	</description>
 	<comments>
 		<comment id='1' author='kossnick' date='2019-10-22T18:10:59Z'>
 		fair point. feel free to make a PR!
 		</comment>
 		<comment id='2' author='kossnick' date='2019-10-22T18:40:32Z'>
 		Here you go: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/412&gt;#412&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='56fa2075a528a7a8744f4f803df902f02ef36372' author='David Kossnick' date='2019-10-23 06:11:18-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\train_loop_mixin.py' new_name='pytorch_lightning\trainer\train_loop_mixin.py'>
 		<file_info nloc='145' complexity='60' token_count='1080'></file_info>
 		<method name='run_training_epoch' parameters='self'>
 				<method_info nloc='33' complexity='19' token_count='255' nesting_level='1' start_line='68' end_line='130'></method_info>
 			<added_lines>86,87,113,114,115,117,118,122,123,124,125,126</added_lines>
 			<deleted_lines>77,82,83,84,85,86,87,88</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
