<bug_data>
<bug id='2678' author='andrewredd' open_date='2020-07-23T12:44:45Z' closed_time='2020-10-05T11:33:47Z'>
 	<summary>training_epoch_end seems to fail when returning nothing</summary>
 	<description>
 I'm trying to log weight histograms to tensorboard at the end of each training epoch. I have the following code:
 &lt;denchmark-code&gt;    def training_epoch_end(self, outputs):
         self.log_hists()
 &lt;/denchmark-code&gt;
 
 This is in line with the documentation. "If you don't need to display anything, Don't return anything"
 However when this function runs I get the following error:
 &lt;denchmark-code&gt;...
 File "/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 499, in run_training_epoch
 │    self.run_training_epoch_end(epoch_output)
 │  File "/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 533, in run_training_epoch_end
 │    _processed_outputs = self.process_output(epoch_output)
 │  File "/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/logging.py", line 106, in process_output
 │    for k, v in output.items():
 │AttributeError: 'NoneType' object has no attribute 'items'
 │Exception ignored in: &lt;object repr() failed&gt;
 │Traceback (most recent call last):
 │  File "/venv/lib/python3.6/site-packages/tqdm/std.py", line 1086, in __del__
 │  File "/venv/lib/python3.6/site-packages/tqdm/std.py", line 1293, in close
 │  File "/venv/lib/python3.6/site-packages/tqdm/std.py", line 1471, in display
 │  File "/venv/lib/python3.6/site-packages/tqdm/std.py", line 1089, in __repr__
 │  File "/venv/lib/python3.6/site-packages/tqdm/std.py", line 1433, in format_dict
 │TypeError: 'NoneType' object is not iterable```
 
 The fix that I've found is to return a dictionary according to the expected typed return, as follows:
 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;def training_epoch_end(self, outputs):
     self.log_hists()
     return {'dummy': torch.Tensor()}
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;
 This seems like a bandaid rather than a true fix since I don't need to return anything.
 
 1) Is there a better way to log the histograms to tensorboard?
 2) Should I be doing something different in this function?
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='andrewredd' date='2020-07-23T12:45:34Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='andrewredd' date='2020-07-24T16:59:03Z'>
 		training_epoch_end by default doesn't log histogram.
 If you want to plot histogram on tensorboard, try self.logger.experiment.add_histogram().
 It's &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/experiment_logging.html#tensorboard&gt;the docs&lt;/denchmark-link&gt;
 .
 PL supports every method of  class with .
 		</comment>
 		<comment id='3' author='andrewredd' date='2020-07-24T18:09:50Z'>
 		Thanks Jeff!
 
 The log_hists function that I have in the example is logging the entire the
 model weights, the biases and the gradients and it does use the referenced
 method.  Is the fact that if I don't return something from the function a
 bug or user error?
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 On Fri, Jul 24, 2020 at 12:59 PM Jeff Yang ***@***.***&gt; wrote:
  training_epoch_end by default doesn't log histogram.
  If you want to plot histogram on tensorboard, try
  self.logger.experiment.add_histogram().
 
  It's the docs
  &lt;https://pytorch-lightning.readthedocs.io/en/latest/experiment_logging.html#tensorboard&gt;
  .
  PL supports every method of SummaryWriter class with
  self.logger.experiment.summary_writer_methods.
 
  —
  You are receiving this because you authored the thread.
  Reply to this email directly, view it on GitHub
  &lt;#2678 (comment)&gt;,
  or unsubscribe
  &lt;https://github.com/notifications/unsubscribe-auth/ALQO4BRY7WOVCZWYLZDUSPLR5G4WPANCNFSM4PFV6LKA&gt;
  .
 
 
 
 		</comment>
 		<comment id='4' author='andrewredd' date='2020-07-27T09:23:04Z'>
 		&lt;denchmark-link:https://github.com/andrewredd&gt;@andrewredd&lt;/denchmark-link&gt;
  sorry for the delayed reply, currently we need to return empty dict in  to make it work. But, Lightning will be supporting  in the future.
 		</comment>
 		<comment id='5' author='andrewredd' date='2020-08-26T19:10:32Z'>
 		Running into the same issue
 		</comment>
 		<comment id='6' author='andrewredd' date='2020-09-22T16:11:59Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  are we going to support return None?
 		</comment>
 	</comments>
 </bug>
<commit id='b014223f72ee457285fa3eb336d1d4039cedb651' author='William Falcon' date='2020-10-05 07:33:46-04:00'>
 	<dmm_unit complexity='0.8235294117647058' interfacing='0.5588235294117647' size='0.35294117647058826'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\training_loop.py' new_name='pytorch_lightning\trainer\training_loop.py'>
 		<file_info nloc='479' complexity='146' token_count='3746'></file_info>
 		<method name='_process_training_step_output' parameters='self,training_step_output,split_batch'>
 				<method_info nloc='24' complexity='7' token_count='163' nesting_level='1' start_line='336' end_line='378'></method_info>
 			<added_lines>339,340,341,342</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='run_training_batch' parameters='self,batch,batch_idx,dataloader_idx'>
 				<method_info nloc='68' complexity='21' token_count='548' nesting_level='1' start_line='608' end_line='735'></method_info>
 			<added_lines>666,667,668</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='training_step_and_backward' parameters='self,split_batch,batch_idx,opt_idx,optimizer,hiddens'>
 				<method_info nloc='8' complexity='2' token_count='71' nesting_level='1' start_line='737' end_line='754'></method_info>
 			<added_lines>744,745,746,747</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='process_train_step_outputs' parameters='self,all_train_step_outputs,early_stopping_accumulator,checkpoint_accumulator'>
 				<method_info nloc='14' complexity='10' token_count='117' nesting_level='1' start_line='816' end_line='846'></method_info>
 			<added_lines>827,828,829</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='training_step' parameters='self,split_batch,batch_idx,opt_idx,hiddens'>
 				<method_info nloc='29' complexity='3' token_count='175' nesting_level='1' start_line='294' end_line='334'></method_info>
 			<added_lines>311,312,313</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,trainer'>
 				<method_info nloc='8' complexity='1' token_count='49' nesting_level='1' start_line='39' end_line='46'></method_info>
 			<added_lines>44</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>34</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\data_flow\test_train_loop_flow_scalar_1_0.py' new_name='tests\trainer\data_flow\test_train_loop_flow_scalar_1_0.py'>
 		<file_info nloc='150' complexity='21' token_count='839'></file_info>
 		<method name='test_train_step_no_return.training_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='4' complexity='1' token_count='41' nesting_level='2' start_line='196' end_line='199'></method_info>
 			<added_lines>196,197,198,199</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_train_step_no_return.training_epoch_end' parameters='self,outputs'>
 				<method_info nloc='2' complexity='1' token_count='16' nesting_level='2' start_line='201' end_line='202'></method_info>
 			<added_lines>201,202</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_train_step_no_return' parameters='tmpdir'>
 				<method_info nloc='16' complexity='1' token_count='74' nesting_level='0' start_line='191' end_line='217'></method_info>
 			<added_lines>191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>4,7,189,190</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
