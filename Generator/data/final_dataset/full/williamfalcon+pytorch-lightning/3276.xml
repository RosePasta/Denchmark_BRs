<bug_data>
<bug id='3276' author='s-rog' open_date='2020-08-31T01:33:53Z' closed_time='2020-10-13T10:42:12Z'>
 	<summary>Logging non-tensor scalar with result breaks subsequent epoch aggregation</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Logging non-tensor scalar with result breaks subsequent epoch/tbptt aggregation
 (on both 0.9 and master)
 &lt;denchmark-code&gt;-- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 20, in _wrap
     fn(i, *args)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py", line 165, in ddp_train
     results = self.trainer.run_pretrain_routine(model)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in run_pretrain_routine
     self.train()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 396, in train
     self.run_training_epoch()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 543, in run_training_epoch
     self.run_training_epoch_end(epoch_output, checkpoint_accumulator, early_stopping_accumulator, num_optimizers)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 672, in run_training_epoch_end
     epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 696, in __auto_reduce_results_on_epoch_end
     tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py", line 392, in reduce_across_time
     result[k] = tbptt_reduce_fx(value)
 TypeError: mean(): argument 'input' (position 1) must be Tensor, not list
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
         x, y = batch[0], batch[1]
         x = self.forward(x)
         loss = self.loss(x, y)
         result = pl.TrainResult(loss)
         result.log("non tensor scalar", 1.0)
         result.log("loss", loss, on_step=False, on_epoch=True)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;To Fix&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;result.log("non tensor scalar", torch.tensor(1.0))
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 In log() of result objects, value should accept non tensor values as value: Any and not cause issues with other metrics to be logged
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 log() can be changed to only accept tensors, or have a built-in conversion, will update as I investigate further
 	</description>
 	<comments>
 		<comment id='1' author='s-rog' date='2020-09-01T14:51:45Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='s-rog' date='2020-09-15T16:52:44Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  is it still there? &lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
  mind check if it is still on master? or better add test for such case...
 		</comment>
 		<comment id='3' author='s-rog' date='2020-09-16T01:26:44Z'>
 		I'll take a look again when I get a chance, haven't probed much due to the refactors... Are they mostly done?
 		</comment>
 		<comment id='4' author='s-rog' date='2020-09-16T02:02:51Z'>
 		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  the bug is still here, the following template tests for this issue as well as &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3278&gt;#3278&lt;/denchmark-link&gt;
 
 For this problem the easiest fix would be to force type to tensors. Though that's probably just a bandaid solution, thoughts?
 
 Test template for reference
 #!/opt/conda/bin/python
 """
 Runs a model on a single node across multiple gpus.
 """
 import os
 from argparse import ArgumentParser
 
 import torch.nn.functional as F
 
 import pytorch_lightning as pl
 from pl_examples.models.lightning_template import LightningTemplateModel
 from pytorch_lightning import Trainer, seed_everything
 
 seed_everything(234)
 
 class custom_template(LightningTemplateModel):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
     
     def on_epoch_start(self):
         print("on_epoch_start")
 
     def on_fit_start(self):
         print("on_fit_start")
         
     def training_step(self, batch, batch_idx):
         """
         Lightning calls this inside the training loop with the data from the training dataloader
         passed in as `batch`.
         """
         # forward pass
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         result = pl.TrainResult(loss)
         result.log("non tensor scalar", 1.0)
         result.log("loss", loss, on_step=False, on_epoch=True)
         return result
         
 
 def main(args):
     """ Main training routine specific for this project. """
     # ------------------------
     # 1 INIT LIGHTNING MODEL
     # ------------------------
     model = custom_template(**vars(args))
 
     # ------------------------
     # 2 INIT TRAINER
     # ------------------------
     trainer = Trainer.from_argparse_args(args)
 
     # ------------------------
     # 3 START TRAINING
     # ------------------------
     trainer.fit(model)
 
 
 def run_cli():
     # ------------------------
     # TRAINING ARGUMENTS
     # ------------------------
     # these are project-wide arguments
     root_dir = os.path.dirname(os.path.realpath(__file__))
     parent_parser = ArgumentParser(add_help=False)
 
     # each LightningModule defines arguments relevant to it
     parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)
     parser = Trainer.add_argparse_args(parser)
     parser.set_defaults(gpus=1, distributed_backend=None)
     args = parser.parse_args()
 
     # ---------------------
     # RUN TRAINING
     # ---------------------
     main(args)
 
 
 if __name__ == '__main__':
     run_cli()
 
 
 		</comment>
 		<comment id='5' author='s-rog' date='2020-09-17T01:01:44Z'>
 		I looked into it a bit and reduce_across_time is getting called on all metrics if one metric in results is logged with on_epoch=True which makes on_epoch=True only compatible with tensor scalars since the default tbptt fn is torch.mean
 This is probably not intended behavior?
 		</comment>
 		<comment id='6' author='s-rog' date='2020-09-17T15:24:59Z'>
 		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='7' author='s-rog' date='2020-09-17T15:57:06Z'>
 		&lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
  this is not related to metrics.
 &lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
  I guess a simple type check that converts scalars to scalars tensor should do the trick? If so, could you open a PR with this fix?
 		</comment>
 		<comment id='8' author='s-rog' date='2020-10-05T07:45:50Z'>
 		fixed by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3855&gt;#3855&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='9' author='s-rog' date='2020-10-13T08:16:31Z'>
 		&lt;denchmark-code&gt;Traceback (most recent call last):
   File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
     fn(i, *args)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_accelerator.py", line 152, in ddp_train
     results = self.train_or_test()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 53, in train_or_test
     results = self.trainer.train()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 483, in train
     self.train_loop.run_training_epoch()
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 598, in run_training_epoch
     self.num_optimizers
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py", line 339, in log_train_epoch_end_metrics
     epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py", line 449, in __auto_reduce_results_on_epoch_end
     tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)
   File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py", line 483, in reduce_across_time
     result[k] = tbptt_reduce_fx(value.float())
 AttributeError: 'list' object has no attribute 'float'
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  I don't think the issue was fixed completely, this is on rc5 (using self.log)
 		</comment>
 		<comment id='10' author='s-rog' date='2020-10-13T09:49:37Z'>
 		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
  mind add a test for this case?
 		</comment>
 		<comment id='11' author='s-rog' date='2020-10-13T10:43:22Z'>
 		william beat me to it :]
 		</comment>
 	</comments>
 </bug>
<commit id='2d5a7f5e7dc686cfc8172101a81505bf421468af' author='William Falcon' date='2020-10-13 06:42:11-04:00'>
 	<dmm_unit complexity='0.5' interfacing='0.75' size='0.25'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\step_result.py' new_name='pytorch_lightning\core\step_result.py'>
 		<file_info nloc='615' complexity='173' token_count='4066'></file_info>
 		<method name='reduce_across_time' parameters='cls,time_outputs'>
 				<method_info nloc='22' complexity='9' token_count='163' nesting_level='1' start_line='457' end_line='489'></method_info>
 			<added_lines>479,480,481</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\logging\test_eval_loop_logging_1_0.py' new_name='tests\trainer\logging\test_eval_loop_logging_1_0.py'>
 		<file_info nloc='186' complexity='15' token_count='1103'></file_info>
 		<method name='test__validation_step__log.training_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='7' complexity='1' token_count='55' nesting_level='2' start_line='20' end_line='27'></method_info>
 			<added_lines>24,25</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test__validation_step__log' parameters='tmpdir'>
 				<method_info nloc='35' complexity='1' token_count='151' nesting_level='0' start_line='13' end_line='72'></method_info>
 			<added_lines>24,25,55,71</added_lines>
 			<deleted_lines>68</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
