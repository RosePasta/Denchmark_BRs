<bug_data>
<bug id='4319' author='SeanNaren' open_date='2020-10-23T10:13:33Z' closed_time='2020-10-24T20:55:50Z'>
 	<summary>After DDP train processes have different best val paths</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Tied to &lt;denchmark-link:https://github.com/huggingface/transformers/pull/7852&gt;huggingface/transformers#7852&lt;/denchmark-link&gt;
 
 There is no synchronisation/communication to ensure the model has finished saving before loading. If you look at ddp_spawn/ddp_cpu there is communication to ensure that each process has the same best_val_path stored in the model after save.
 Run below on multi-gpu:
 # Copyright The PyTorch Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # --------------------------------------------
 # --------------------------------------------
 # --------------------------------------------
 # USE THIS MODEL TO REPRODUCE A BUG YOU REPORT
 # --------------------------------------------
 # --------------------------------------------
 # --------------------------------------------
 import glob
 import os
 from tempfile import TemporaryDirectory
 
 import torch
 from torch.utils.data import Dataset
 
 from pytorch_lightning import Trainer, LightningModule
 from pytorch_lightning.callbacks import ModelCheckpoint
 
 
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return self.data[index]
 
     def __len__(self):
         return self.len
 
 
 class BoringModel(LightningModule):
 
     def __init__(self):
         """
         Testing PL Module
 
         Use as follows:
         - subclass
         - modify the behavior for what you want
 
         class TestModel(BaseTestModel):
             def training_step(...):
                 # do your own thing
 
         or:
 
         model = BaseTestModel()
         model.training_epoch_end = None
 
         """
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
 
     def forward(self, x):
         return self.layer(x)
 
     def loss(self, batch, prediction):
         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))
 
     def step(self, x):
         x = self.layer(x)
         out = torch.nn.functional.mse_loss(x, torch.ones_like(x))
         return out
 
     def training_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log('loss', loss)
         return {"loss": loss}
 
     def training_step_end(self, training_step_outputs):
         return training_step_outputs
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log('x', loss)
 
     def test_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log('y', loss)
 
     def configure_optimizers(self):
         optimizer = torch.optim.AdamW(self.layer.parameters(), lr=0.1)
         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
         return [optimizer], [lr_scheduler]
 
 
 def run_test():
     class TestModel(BoringModel):
         def validation_step(self, batch, batch_idx):
             output = self.layer(batch)
             loss = self.loss(batch, output)
             self.log('x', loss)
 
     # fake data
     train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))
     val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))
 
     # model
     model = TestModel()
     tmp_dir = 'temp/'
     if os.path.exists(tmp_dir):
         os.rmdir(tmp_dir)
 
     trainer = Trainer(
         default_root_dir=os.getcwd(),
         max_epochs=2,
         accelerator='ddp',
         gpus=2,
         checkpoint_callback=ModelCheckpoint(
             dirpath=tmp_dir,
             monitor='x',
             mode='min',
             save_top_k=1
         )
     )
     trainer.fit(model, train_data, val_data)
     checkpoints = list(sorted(glob.glob(os.path.join(tmp_dir, "*.ckpt"), recursive=True)))
     print("checkpoints", checkpoints)
     print(trainer.checkpoint_callback.best_model_path)
     assert os.path.exists(
         trainer.checkpoint_callback.best_model_path), f'Could not find checkpoint at rank {trainer.global_rank}'
 
 
 if __name__ == '__main__':
     run_test()
 Output:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "/home/jovyan/transformers/reproduce.py", line 139, in &lt;module&gt;
     run_test()
   File "/home/jovyan/transformers/reproduce.py", line 135, in run_test
     trainer.checkpoint_callback.best_model_path), f'Could not find checkpoint at rank {trainer.global_rank}'
 AssertionError: Could not find checkpoint at rank 1
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Assertion does not fail
 	</description>
 	<comments>
 		<comment id='1' author='SeanNaren' date='2020-10-23T10:14:08Z'>
 		cc &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='SeanNaren' date='2020-10-23T12:25:12Z'>
 		Yes, this makes sense.
 Only the process 0 is saving checkpoints, so one solution is to send the best model path of rank 0 to all other processes.
 The checkpoint directory based on logger name and version is one example where we do this already:
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/model_checkpoint.py
 
 
          Line 445
       in
       3abfec8
 
 
 
 
 
 
  version, name = trainer.accelerator_backend.broadcast((version, trainer.logger.name)) 
 
 
 
 
 
 so it could be done in a similar way.
 		</comment>
 		<comment id='3' author='SeanNaren' date='2020-10-23T13:03:13Z'>
 		A workaround for you until this issue is fix is to check the rank before you access the path, i.e.
 if trainer.global_rank == 0:
     print(trainer.checkpoint_callback.best_model_path)  # do whatever with path
 		</comment>
 	</comments>
 </bug>
<commit id='5641b266d56d2324e1d9cb3ba12a60b47ab10558' author='Sean Naren' date='2020-10-24 16:55:49-04:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='CHANGELOG.md' new_name='CHANGELOG.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>34</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\accelerators\accelerator.py' new_name='pytorch_lightning\accelerators\accelerator.py'>
 		<file_info nloc='170' complexity='41' token_count='1208'></file_info>
 		<method name='teardown' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='54' end_line='56'></method_info>
 			<added_lines>55,56</added_lines>
 			<deleted_lines>55</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\accelerators\dp_accelerator.py' new_name='pytorch_lightning\accelerators\dp_accelerator.py'>
 		<file_info nloc='102' complexity='33' token_count='705'></file_info>
 		<method name='teardown' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='21' nesting_level='1' start_line='101' end_line='104'></method_info>
 			<added_lines>104</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\accelerators\horovod_accelerator.py' new_name='pytorch_lightning\accelerators\horovod_accelerator.py'>
 		<file_info nloc='99' complexity='34' token_count='876'></file_info>
 		<method name='teardown' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='6' nesting_level='1' start_line='110' end_line='111'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>110,111</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>112</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
