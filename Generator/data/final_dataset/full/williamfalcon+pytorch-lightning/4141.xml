<bug_data>
<bug id='4141' author='ChanKaHou' open_date='2020-10-14T09:57:48Z' closed_time='2020-10-15T13:12:05Z'>
 	<summary>the self.log problem in validation_step()</summary>
 	<description>
 as doc say we should use self.log in last version,
 but the loged data are strange if we change EvalResult() to self.log(on_epoch=True)
 Then we check the data in tensorboard, the self.log() will only log the result of last batch each epoch, instead of the mean of them.
 That is quite unreliable about this issue, it must be turned back to EvalResult() for correct experiments.
 	</description>
 	<comments>
 		<comment id='1' author='ChanKaHou' date='2020-10-14T09:59:45Z'>
 		Hi! thanks for your contribution!, great first issue!
 		</comment>
 		<comment id='2' author='ChanKaHou' date='2020-10-14T10:38:11Z'>
 		I am having the same issue... Using PL 1.0.0. The progress bar does get the correct values for validation loss, on the other hand.
 		</comment>
 		<comment id='3' author='ChanKaHou' date='2020-10-14T11:18:05Z'>
 		I found something wrong when saving the model. This is a serious bug.
 		</comment>
 		<comment id='4' author='ChanKaHou' date='2020-10-14T12:20:00Z'>
 		To speed things up: here is a &lt;denchmark-link:https://colab.research.google.com/drive/149CNaNZPHWzIBI2Ea0k6PSzVePsygYue?usp=sharing&gt;boring model&lt;/denchmark-link&gt;
  demonstrating the error. In the TB output, observe that:
 
 the value for val_loss and val_loss_epoch are simply equal to the value of val_loss_step for the last step (same as val_loss_step in progress bar).
 the values for val_loss and val_loss_epoch in TB are different from their equivalents in the progress bar.
 
 So what's happening is that when logging to loggers, reduction is not applied.
 This occurs in version 0.10.0 as well as 1.0.0
 I don't understand how a bug this serious could've made it through all the tests from 0.10.0, through all the 1.0 rcs and all they way to the final version...
 		</comment>
 		<comment id='5' author='ChanKaHou' date='2020-10-14T13:05:28Z'>
 		The Issue is still present in version 1.0.1.
 I also found out that mean function used to calculate value at the and of the epoch (the value that is present in the progress bar) does not return the correct average value.
 &lt;denchmark-code&gt;def validation_step(self, batch, batch_idx):
     loss = self.step(batch)
     self.log('batch_idx', batch_idx, prog_bar=True)
     self.batch_idxs.append(batch_idx)
     return loss
 
 def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
     super().validation_epoch_end(outputs)
     print("batch_idxs mean", sum(self.batch_idxs) / len(self.batch_idxs))
     print("batch_idxs torch mean", torch.mean(torch.tensor(self.batch_idxs, dtype=torch.float)))
     self.batch_idxs = []
 &lt;/denchmark-code&gt;
 
 logged value = 31
 progress bar value = 15.1
 mean value = 15.5
 torch mean value = 15.5
 		</comment>
 		<comment id='6' author='ChanKaHou' date='2020-10-14T15:44:42Z'>
 		Here 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
          Line 199
       in
       09c2020
 
 
 
 
 
 
  epoch_logs = self.trainer.get_model()._results 
 
 
 
 
  this private variable (_results) keeps last step_log. Maybe this is the problem, but I donâ€™t know how to fix it yet.
 		</comment>
 		<comment id='7' author='ChanKaHou' date='2020-10-15T08:59:47Z'>
 		I also find this problem, then I log it in validation_epoch_end and it seems ok
 &lt;denchmark-code&gt;     def validation_step(self,...):
         return {'val_loss': loss}
 
     def validation_epoch_end(self, outputs):
         val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
 
 
         dist.all_reduce(val_loss, op=dist.ReduceOp.SUM)
         val_loss = val_loss / self.trainer.world_size
         self.log('val_loss', val_loss, on_epoch=True, sync_dist=True)
 
         return {'val_loss': val_loss,}
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='8' author='ChanKaHou' date='2020-10-15T10:41:36Z'>
 		ok. good catch. fixing this now!
 		</comment>
 		<comment id='9' author='ChanKaHou' date='2020-10-15T10:53:47Z'>
 		Running into the same problem with the logger only reporting the last value for the epoch rather than the average across the epoch. I was wondering why I was getting funky test scores ðŸ˜… .
 		</comment>
 		<comment id='10' author='ChanKaHou' date='2020-10-15T11:16:03Z'>
 		Is this what you mean?
 &lt;denchmark-link:https://user-images.githubusercontent.com/3640001/96116314-46d2f800-0eb6-11eb-8819-4f3a9ba63e80.png&gt;&lt;/denchmark-link&gt;
 
 val_loss_epoch and val_loss are the same? but instead, val_loss should be the same as val_loss_step?
 Just wrote a test and it looks like everything is correct, except that the val_loss gets overwritten by the val_loss_epoch by mistake. So:
 
 val_loss_step is correct
 val_loss_epoch is correct
 val_loss is incorrect
 
 		</comment>
 		<comment id='11' author='ChanKaHou' date='2020-10-15T11:19:37Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  Yes, that's correct. Maybe even val_loss_epoch is incorrect, per &lt;denchmark-link:https://github.com/Alek96&gt;@Alek96&lt;/denchmark-link&gt;
 's comment. Although the bigger issue is wrong values being sent to the (tensorboard of anything non pbar) logger.
 		</comment>
 		<comment id='12' author='ChanKaHou' date='2020-10-15T11:22:19Z'>
 		
 def validation_step(self, batch, batch_idx):
 loss = self.step(batch)
 self.log('batch_idx', batch_idx, prog_bar=True)
 self.batch_idxs.append(batch_idx)
 return loss
 def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
 super().validation_epoch_end(outputs)
 print("batch_idxs mean", sum(self.batch_idxs) / len(self.batch_idxs))
 print("batch_idxs torch mean", torch.mean(torch.tensor(self.batch_idxs, dtype=torch.float)))
 self.batch_idxs = []
 
 Well... there is a bug in this code:
 def validation_step(self, batch, batch_idx):
     loss = self.step(batch)
     self.log('batch_idx', batch_idx, prog_bar=True)
     self.batch_idxs.append(batch_idx)
     return loss
 
 def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
     super().validation_epoch_end(outputs)
     print("batch_idxs mean", sum(self.batch_idxs) / len(self.batch_idxs))
     print("batch_idxs torch mean", torch.mean(torch.tensor(self.batch_idxs, dtype=torch.float)))
     self.batch_idxs = []
 You return the loss but then compare the batch_idxs...
 outputs has losses, not batch_idxs.
 ie:
 def validation_step(self, batch, batch_idx):
     loss = self.step(batch)
     self.log('batch_idx', batch_idx, prog_bar=True)
     self.batch_idxs.append(batch_idx)
     return loss # &lt;---------- causes outputs to be losses not indexes
 
 def validation_epoch_end(self, outputs: List[Any]) -&gt; None:
     outputs = outputs # &lt;------------- losses not indexes!
 		</comment>
 		<comment id='13' author='ChanKaHou' date='2020-10-15T11:31:56Z'>
 		ok, i think i found it. posting a fix now. Looks like the calculations are correct, but the wrong value got logged
 		</comment>
 		<comment id='14' author='ChanKaHou' date='2020-10-15T13:12:19Z'>
 		Thanks for the fast fix.
 To explain my intention:
 
 You return the loss but then compare the batch_idxs...
 outputs has losses, not batch_idxs.
 
 In my understanding it should not matter if I returned loss value and then compared the batch_idxs. self.log method should work the same regardless of the value you pass in. That is way I called a method with a value that easily shows if the calculated mean is correct.
 self.log('batch_idx', batch_idx, prog_bar=True)
 The "bug" was not in the code, but in my understating of mean function. PyTorch lightning is using weighted_mean that is also taking in the account the size of each batch.
 
 
 
 pytorch-lightning/pytorch_lightning/core/step_result.py
 
 
         Lines 446 to 456
       in
       f064682
 
 
 
 
 
 
  if option['on_epoch']: 
 
 
 
  fx = option['reduce_fx'] 
 
 
 
  if fx == torch.mean: 
 
 
 
  try: 
 
 
 
  reduced_val = weighted_mean(result[k], batch_sizes) 
 
 
 
  except Exception as e: 
 
 
 
  reduced_val = torch.mean(result[k]) 
 
 
 
  else: 
 
 
 
  reduced_val = fx(result[k]) 
 
 
 
  
 
 
 
  result[k] = reduced_val 
 
 
 
 
 
 &lt;denchmark-link:https://colab.research.google.com/drive/1oa6TKF59k744HTX19597QHGY9gJV0A8I?usp=sharing&gt;Boring_model&lt;/denchmark-link&gt;
  that shows the behavior.
 		</comment>
 		<comment id='15' author='ChanKaHou' date='2020-10-15T13:14:16Z'>
 		all good!
 Please try on master now. Let me know if the errors are fixed.
 For reference, here is the new test for this:
 
 
 
 pytorch-lightning/tests/trainer/logging/test_eval_loop_logging_1_0.py
 
 
         Lines 247 to 313
       in
       45d05ff
 
 
 
 
 
 
  def test_eval_logging_auto_reduce(tmpdir): 
 
 
 
  """ 
 
 
 
      Tests that only training_step can be used 
 
 
 
      """ 
 
 
 
  seed_everything(1234) 
 
 
 
  
 
 
 
  os.environ['PL_DEV_DEBUG'] = '1' 
 
 
 
  
 
 
 
  class TestModel(BoringModel): 
 
 
 
  def on_pretrain_routine_end(self) -&gt; None: 
 
 
 
  self.seen_vals = [] 
 
 
 
  self.manual_epoch_end_mean = None 
 
 
 
  
 
 
 
  def on_validation_epoch_start(self) -&gt; None: 
 
 
 
  self.seen_vals = [] 
 
 
 
  
 
 
 
  def validation_step(self, batch, batch_idx): 
 
 
 
  output = self.layer(batch) 
 
 
 
  loss = self.loss(batch, output) 
 
 
 
  self.seen_vals.append(loss) 
 
 
 
  self.log('val_loss', loss, on_epoch=True, on_step=True, prog_bar=True) 
 
 
 
  return {"x": loss} 
 
 
 
  
 
 
 
  def validation_epoch_end(self, outputs) -&gt; None: 
 
 
 
  for passed_in, manually_tracked in zip(outputs, self.seen_vals): 
 
 
 
  assert passed_in['x'] == manually_tracked 
 
 
 
  self.manual_epoch_end_mean = torch.stack([x['x'] for x in outputs]).mean() 
 
 
 
  
 
 
 
  model = TestModel() 
 
 
 
  
 
 
 
  trainer = Trainer( 
 
 
 
  default_root_dir=tmpdir, 
 
 
 
  limit_train_batches=3, 
 
 
 
  limit_val_batches=3, 
 
 
 
  max_epochs=1, 
 
 
 
  log_every_n_steps=1, 
 
 
 
  weights_summary=None, 
 
 
 
  checkpoint_callback=callbacks.ModelCheckpoint('val_loss') 
 
 
 
      ) 
 
 
 
  trainer.fit(model) 
 
 
 
  
 
 
 
  # make sure all the metrics are available for callbacks 
 
 
 
  manual_mean = model.manual_epoch_end_mean 
 
 
 
  callback_metrics = set(trainer.callback_metrics.keys()) 
 
 
 
  assert callback_metrics == {'debug_epoch', 'val_loss', 'val_loss_epoch'} 
 
 
 
  
 
 
 
  # make sure values are correct 
 
 
 
  assert trainer.logged_metrics['val_loss_epoch'] == manual_mean 
 
 
 
  assert trainer.callback_metrics['val_loss'] == trainer.logged_metrics['val_loss_step/epoch_0'] 
 
 
 
  
 
 
 
  # make sure correct values were logged 
 
 
 
  logged_val = trainer.dev_debugger.logged_metrics 
 
 
 
  
 
 
 
  # sanity check 
 
 
 
  assert logged_val[0]['global_step'] == 0 
 
 
 
  assert logged_val[1]['global_step'] == 0 
 
 
 
  
 
 
 
  # 3 val batches 
 
 
 
  assert logged_val[2]['val_loss_step/epoch_0'] == model.seen_vals[0] 
 
 
 
  assert logged_val[3]['val_loss_step/epoch_0'] == model.seen_vals[1] 
 
 
 
  assert logged_val[4]['val_loss_step/epoch_0'] == model.seen_vals[2] 
 
 
 
  
 
 
 
  # epoch mean 
 
 
 
  assert logged_val[5]['val_loss_epoch'] == model.manual_epoch_end_mean 
 
 
 
  
 
 
 
  # only those logged 
 
 
 
  assert len(logged_val) == 6 
 
 
 
 
 
 		</comment>
 		<comment id='16' author='ChanKaHou' date='2020-10-15T13:30:38Z'>
 		as a bonus, got rid of the duplicate metric, metric_step chart without losing support for callbacks
 &lt;denchmark-link:https://user-images.githubusercontent.com/3640001/96133953-1183d580-0ec9-11eb-8850-dc50f5b78fea.png&gt;&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
   FYI
 		</comment>
 		<comment id='17' author='ChanKaHou' date='2020-10-15T14:49:35Z'>
 		ok, mind verifying that this worked for you guys?
 this is a critical bug, so, we'll do a minor release now to fix it for everyone.
 Here's the new colab with master showing it's fixed:
 &lt;denchmark-link:https://colab.research.google.com/drive/1lEZms9QjRZ7kPosu_m7Gbr_sdBZ7exzg?usp=sharing&gt;https://colab.research.google.com/drive/1lEZms9QjRZ7kPosu_m7Gbr_sdBZ7exzg?usp=sharing&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='18' author='ChanKaHou' date='2020-10-15T14:57:05Z'>
 		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  it works now, thanks a lot.
 		</comment>
 		<comment id='19' author='ChanKaHou' date='2020-10-15T14:58:38Z'>
 		Works here too :)
 		</comment>
 		<comment id='20' author='ChanKaHou' date='2020-10-15T16:46:44Z'>
 		Just testing and it works for me too ðŸ’ª
 		</comment>
 	</comments>
 </bug>
<commit id='45d05ff68dbf3db300a782af97ea54cab70a3ff9' author='William Falcon' date='2020-10-15 09:12:05-04:00'>
 	<dmm_unit complexity='0.8529411764705882' interfacing='0.8970588235294118' size='0.23529411764705882'></dmm_unit>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\core\step_result.py' new_name='pytorch_lightning\core\step_result.py'>
 		<file_info nloc='641' complexity='186' token_count='4230'></file_info>
 		<method name='get_forked_metrics' parameters='self'>
 				<method_info nloc='9' complexity='4' token_count='50' nesting_level='1' start_line='320' end_line='334'></method_info>
 			<added_lines>320,321,322,323,324,325,326,327,328,329,330,331,332,333,334</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='reduce_on_epoch_end' parameters='cls,outputs'>
 				<method_info nloc='30' complexity='9' token_count='196' nesting_level='1' start_line='450' end_line='488'></method_info>
 			<added_lines>468,469,470,471,472</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_epoch_log_metrics' parameters='self'>
 				<method_info nloc='19' complexity='10' token_count='119' nesting_level='1' start_line='268' end_line='292'></method_info>
 			<added_lines>279,280,281</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='choose_last' parameters='x'>
 				<method_info nloc='6' complexity='4' token_count='57' nesting_level='0' start_line='561' end_line='566'></method_info>
 			<added_lines>561,562,563,564,565,566</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_epoch_pbar_metrics' parameters='self'>
 				<method_info nloc='16' complexity='10' token_count='117' nesting_level='1' start_line='294' end_line='318'></method_info>
 			<added_lines>305,306,307</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>335,567,568</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\connectors\logger_connector.py' new_name='pytorch_lightning\trainer\connectors\logger_connector.py'>
 		<file_info nloc='311' complexity='86' token_count='2185'></file_info>
 		<method name='_log_on_evaluation_epoch_end_metrics' parameters='self,epoch_logs'>
 				<method_info nloc='34' complexity='7' token_count='267' nesting_level='1' start_line='137' end_line='206'></method_info>
 			<added_lines>192,193,194,195,200,201</added_lines>
 			<deleted_lines>196,197</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\trainer\evaluation_loop.py' new_name='pytorch_lightning\trainer\evaluation_loop.py'>
 		<file_info nloc='242' complexity='88' token_count='1820'></file_info>
 		<method name='__run_eval_epoch_end' parameters='self,num_dataloaders,using_eval_result'>
 				<method_info nloc='32' complexity='13' token_count='190' nesting_level='1' start_line='218' end_line='263'></method_info>
 			<added_lines>221,222,223</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\core\test_metric_result_integration.py' new_name='tests\core\test_metric_result_integration.py'>
 		<file_info nloc='87' complexity='15' token_count='717'></file_info>
 		<method name='test_result_metric_integration' parameters=''>
 				<method_info nloc='28' complexity='5' token_count='253' nesting_level='0' start_line='104' end_line='142'></method_info>
 			<added_lines>138</added_lines>
 			<deleted_lines>139</deleted_lines>
 		</method>
 		<method name='_ddp_test_fn' parameters='rank,worldsize'>
 				<method_info nloc='34' complexity='5' token_count='284' nesting_level='0' start_line='46' end_line='92'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>86</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\logging\test_eval_loop_logging_1_0.py' new_name='tests\trainer\logging\test_eval_loop_logging_1_0.py'>
 		<file_info nloc='226' complexity='22' token_count='1462'></file_info>
 		<method name='test__validation_step__step_end__epoch_end__log' parameters='tmpdir'>
 				<method_info nloc='45' complexity='1' token_count='190' nesting_level='0' start_line='88' end_line='166'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>145,150</deleted_lines>
 		</method>
 		<method name='test_eval_logging_auto_reduce.validation_step' parameters='self,batch,batch_idx'>
 				<method_info nloc='6' complexity='1' token_count='61' nesting_level='2' start_line='263' end_line='268'></method_info>
 			<added_lines>263,264,265,266,267,268</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_eval_logging_auto_reduce' parameters='tmpdir'>
 				<method_info nloc='32' complexity='1' token_count='221' nesting_level='0' start_line='247' end_line='313'></method_info>
 			<added_lines>247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test__validation_step__log' parameters='tmpdir'>
 				<method_info nloc='35' complexity='1' token_count='151' nesting_level='0' start_line='26' end_line='85'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>71</deleted_lines>
 		</method>
 		<method name='test_eval_logging_auto_reduce.on_pretrain_routine_end' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='18' nesting_level='2' start_line='256' end_line='258'></method_info>
 			<added_lines>256,257,258</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_eval_logging_auto_reduce.on_validation_epoch_start' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='13' nesting_level='2' start_line='260' end_line='261'></method_info>
 			<added_lines>260,261</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_eval_logging_auto_reduce.validation_epoch_end' parameters='self,outputs'>
 				<method_info nloc='4' complexity='3' token_count='53' nesting_level='2' start_line='270' end_line='273'></method_info>
 			<added_lines>270,271,272,273</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>18,314,315</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\trainer\logging\test_train_loop_logging_1_0.py' new_name='tests\trainer\logging\test_train_loop_logging_1_0.py'>
 		<file_info nloc='343' complexity='38' token_count='2530'></file_info>
 		<method name='test_different_batch_types_for_sizing' parameters='tmpdir'>
 				<method_info nloc='22' complexity='1' token_count='80' nesting_level='0' start_line='362' end_line='403'></method_info>
 			<added_lines>398,399</added_lines>
 			<deleted_lines>398,399</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
