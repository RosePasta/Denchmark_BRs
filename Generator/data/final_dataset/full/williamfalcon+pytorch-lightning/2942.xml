<bug_data>
<bug id='2942' author='ananthsub' open_date='2020-08-13T04:47:43Z' closed_time='2020-08-14T09:37:22Z'>
 	<summary>ddp_backend in 0.9.0rc12 fails if no CUDA_VISIBLE_DEVICES found</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 In ddp_backend, training immediately fails if the environment variable CUDA_VISIBLE_DEVICES isn't set. This line should handle the None case gracefully: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/accelerators/ddp_backend.py#L90&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/accelerators/ddp_backend.py#L90&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Start a run using ddp on CPU. This was discovered using torchelastic to launch
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 This shouldn't crash if the environment variable isn't set. We could default to num_gpus = 0 in this case.
 Replacing the line above with something like this could work:
 num_gpus = os.environ.get('CUDA_VISIBLE_DEVICES', []).split(',').__len__() 
 	</description>
 	<comments>
 		<comment id='1' author='ananthsub' date='2020-08-13T05:52:19Z'>
 		This is a bug in our usage of distributed backend, not an issue with lightning
 		</comment>
 		<comment id='2' author='ananthsub' date='2020-08-13T06:26:22Z'>
 		&lt;denchmark-link:https://github.com/ananthsub&gt;@ananthsub&lt;/denchmark-link&gt;
  good catch, mind send a PR?
 		</comment>
 		<comment id='3' author='ananthsub' date='2020-08-13T17:30:00Z'>
 		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
   reading more about the accelerator changes, I am confused about the split between ddp and ddp_cpu backends. For example, if the user wants to launch training with torchelastic or a non-slurm launcher, then we use the ddp backend. This will fail if the training is on CPUs. The docs aren't updated with the latest changes either: &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-modes&gt;https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-modes&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='ananthsub' date='2020-08-13T21:04:16Z'>
 		the ddp is GPU based compare to ddp_cpu is CPU only...
 		</comment>
 		<comment id='5' author='ananthsub' date='2020-08-13T23:21:35Z'>
 		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  in our test case, we launch with these trainer parameters:
 
 and this is the stacktrace we run into:
 &lt;denchmark-code&gt;    trainer.fit(self)
   File "pytorch_lightning/trainer/states.py", line 34, in wrapped_fn
     result = fn(self, *args, **kwargs)
   File "pytorch_lightning/trainer/trainer.py", line 1030, in fit
     results = self.accelerator_backend.spawn_ddp_children(model)
   File "pytorch_lightning/accelerators/ddp_backend.py", line 90, in spawn_ddp_children
     num_gpus = os.environ['CUDA_VISIBLE_DEVICES'].split(',').__len__()
   File "/usr/lib/python3.7/os.py", line 679, in __getitem__
     raise KeyError(key) from None
 KeyError: 'CUDA_VISIBLE_DEVICES'
 &lt;/denchmark-code&gt;
 
 Should this be detected by the config validator earlier?
 		</comment>
 	</comments>
 </bug>
<commit id='0c264689cb566582ac47333d8b7192d656e19440' author='William Falcon' date='2020-08-13 21:54:57-04:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\source\debugging.rst' new_name='docs\source\debugging.rst'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>4,5</added_lines>
 			<deleted_lines>4,5</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='pytorch_lightning\accelerators\ddp_backend.py' new_name='pytorch_lightning\accelerators\ddp_backend.py'>
 		<file_info nloc='123' complexity='27' token_count='995'></file_info>
 		<method name='spawn_ddp_children' parameters='self,model'>
 				<method_info nloc='41' complexity='10' token_count='345' nesting_level='1' start_line='58' end_line='121'></method_info>
 			<added_lines>90</added_lines>
 			<deleted_lines>90</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
