<bug_data>
<bug id='221' author='monikatomar92' open_date='2018-09-25T23:36:36Z' closed_time='2018-10-22T13:37:22Z'>
 	<summary>Dataloader load from disk by batches</summary>
 	<description>
 &lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;
 
 After the first model training, in the second model, I am getting the Memory error. I have also added the latest release as in another issue it was mentioned that memory error bug has been fixed in the latest issue. The error looks as follows:
 Traceback (most recent call last):
 File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py", line 885, in del
 self.close()
 File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py", line 1090, in close
 self._decr_instances(self)
 File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py", line 454, in _decr_instances
 cls.monitor.exit()
 File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_monitor.py", line 52, in exit
 self.join()
 File "/usr/lib/python3.6/threading.py", line 1053, in join
 raise RuntimeError("cannot join current thread")
 RuntimeError: cannot join current thread
 /usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
 len(cache))
 Traceback (most recent call last):
 File "mnist.py", line 21, in 
 clf.fit(x_train, y_train, time_limit=12 * 60 * 60)
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/image_supervised.py", line 239, in fit
 run_searcher_once(train_data, test_data, self.path, int(time_remain))
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/image_supervised.py", line 40, in run_searcher_once
 searcher.search(train_data, test_data, timeout)
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/search.py", line 190, in search
 timeout)
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/bayesian.py", line 257, in optimize_acq
 for temp_graph in transform(elem.graph):
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/net_transformer.py", line 83, in transform
 temp_graph = to_deeper_graph(deepcopy(graph))
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/net_transformer.py", line 63, in to_deeper_graph
 graph.to_conv_deeper_model(layer_id, 3)
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/graph.py", line 313, in to_conv_deeper_model
 new_layers = deeper_conv_block(target, kernel_size, self.weighted)
 File "/home/monika/.local/lib/python3.6/site-packages/autokeras/layer_transformer.py", line 11, in deeper_conv_block
 weight = np.zeros((n_filters, n_filters) + filter_shape)
 MemoryError
 If I run the code with the mnist classification example, I get the following error after the 8th model üëç
 Process SpawnPoolWorker-9:
 Traceback (most recent call last):
 File "/usr/lib/python3.6/multiprocessing/pool.py", line 125, in worker
 put((job, i, result))
 File "/usr/lib/python3.6/multiprocessing/queues.py", line 347, in put
 self._writer.send_bytes(obj)
 File "/usr/lib/python3.6/multiprocessing/connection.py", line 200, in send_bytes
 self._send_bytes(m[offset:offset + size])
 File "/usr/lib/python3.6/multiprocessing/connection.py", line 397, in _send_bytes
 self._send(header)
 File "/usr/lib/python3.6/multiprocessing/connection.py", line 368, in _send
 n = write(self._handle, buf)
 BrokenPipeError: [Errno 32] Broken pipe
 During handling of the above exception, another exception occurred:
 Traceback (most recent call last):
 File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
 self.run()
 File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
 self._target(*self._args, **self._kwargs)
 File "/usr/lib/python3.6/multiprocessing/pool.py", line 130, in worker
 put((job, i, (False, wrapped)))
 File "/usr/lib/python3.6/multiprocessing/queues.py", line 347, in put
 self._writer.send_bytes(obj)
 File "/usr/lib/python3.6/multiprocessing/connection.py", line 200, in send_bytes
 self._send_bytes(m[offset:offset + size])
 File "/usr/lib/python3.6/multiprocessing/connection.py", line 404, in _send_bytes
 self._send(header + buf)
 File "/usr/lib/python3.6/multiprocessing/connection.py", line 368, in _send
 n = write(self._handle, buf)
 BrokenPipeError: [Errno 32] Broken pipe
 /usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
 len(cache))
 /usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 4 leaked semaphores to clean up at shutdown
 len(cache))
 &lt;denchmark-h:h3&gt;Reproducing Steps&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior:
 
 Step 1: ... use the mnist.py from examples.
 Step 2: ... run the script with either the custom dataset for classification or the default example
 
 &lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;
 
 Include the details about the versions of:
 
 OS type and version: Ubuntu 18.04
 Python: 3.6.5
 autokeras: 0.2.14
 scikit-learn:0.19.1
 numpy:1.15.2
 keras:2.2.2
 scipy:1.1.0
 tensorflow:1.10.1
 pytorch:0.4.1
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 My data is of the following dimension :
 Training Data : (42532, 128,128), training label :  ( 42532,24)
 Testing Data : ( 1325,128,128) , testing label : (1325,24)
 Running the code on GTX 1080TI 11GB and 32GB RAM
 	</description>
 	<comments>
 		<comment id='1' author='monikatomar92' date='2018-09-27T12:27:55Z'>
 		I met the same error
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "ak_aoi_img.py", line 23, in &lt;module&gt;
     clf.fit(x_train, y_train, time_limit=2 * 60 * 60)
   File "/home/adt/wen/ak_aoi/autokeras/image_supervised.py", line 239, in fit
     run_searcher_once(train_data, test_data, self.path, int(time_remain))
   File "/home/adt/wen/ak_aoi/autokeras/image_supervised.py", line 40, in run_searcher_once
     searcher.search(train_data, test_data, timeout)
   File "/home/adt/wen/ak_aoi/autokeras/search.py", line 193, in search
     remaining_time)
   File "/home/adt/wen/ak_aoi/autokeras/bayesian.py", line 256, in optimize_acq
     for temp_graph in transform(elem.graph):
   File "/home/adt/wen/ak_aoi/autokeras/net_transformer.py", line 83, in transform
     temp_graph = to_deeper_graph(deepcopy(graph))
   File "/home/adt/wen/ak_aoi/autokeras/net_transformer.py", line 63, in to_deeper_graph
     graph.to_conv_deeper_model(layer_id, 3)
   File "/home/adt/wen/ak_aoi/autokeras/graph.py", line 313, in to_conv_deeper_model
     new_layers = deeper_conv_block(target, kernel_size, self.weighted)
   File "/home/adt/wen/ak_aoi/autokeras/layer_transformer.py", line 11, in deeper_conv_block
     weight = np.zeros((n_filters, n_filters) + filter_shape)
 MemoryError
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='2' author='monikatomar92' date='2018-09-29T20:51:31Z'>
 		The solution would be to load the data from disk batch by batch, which can be done by pytorch dataloader.
 		</comment>
 	</comments>
 </bug>
<commit id='e36d9378305235df61fe4ccd8f33322e315dd8be' author='Haifeng Jin' date='2018-10-22 08:37:21-05:00'>
 	<dmm_unit complexity='1.0' interfacing='0.7714285714285715' size='0.5142857142857142'></dmm_unit>
 	<modification change_type='MODIFY' old_name='autokeras\image\image_supervised.py' new_name='autokeras\image\image_supervised.py'>
 		<file_info nloc='195' complexity='48' token_count='1381'></file_info>
 		<method name='read_csv_file' parameters='csv_file_path'>
 				<method_info nloc='10' complexity='2' token_count='70' nesting_level='0' start_line='21' end_line='39'></method_info>
 			<added_lines>33</added_lines>
 			<deleted_lines>21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39</deleted_lines>
 		</method>
 		<method name='read_images' parameters='img_file_names,images_dir_path'>
 				<method_info nloc='15' complexity='5' token_count='104' nesting_level='0' start_line='20' end_line='41'></method_info>
 			<added_lines>33</added_lines>
 			<deleted_lines>20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>16,17</added_lines>
 			<deleted_lines>1,9,18,19,55</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='autokeras\preprocessor.py' new_name='autokeras\preprocessor.py'>
 		<file_info nloc='133' complexity='29' token_count='1154'></file_info>
 		<method name='__len__' parameters='self'>
 				<method_info nloc='2' complexity='1' token_count='12' nesting_level='1' start_line='173' end_line='174'></method_info>
 			<added_lines>173,174</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,csv_file_path,image_path,has_target'>
 				<method_info nloc='8' complexity='1' token_count='81' nesting_level='1' start_line='153' end_line='162'></method_info>
 			<added_lines>153,154,155,156,157,158,159,160,161,162</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__getitem__' parameters='self,index'>
 				<method_info nloc='8' complexity='3' token_count='71' nesting_level='1' start_line='164' end_line='171'></method_info>
 			<added_lines>164,165,166,167,168,169,170,171</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,8,89,150,151,152,163,172</added_lines>
 			<deleted_lines>87</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='autokeras\utils.py' new_name='autokeras\utils.py'>
 		<file_info nloc='134' complexity='44' token_count='1035'></file_info>
 		<method name='read_csv_file' parameters='csv_file_path'>
 				<method_info nloc='10' complexity='2' token_count='70' nesting_level='0' start_line='176' end_line='194'></method_info>
 			<added_lines>176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='read_image' parameters='img_path'>
 				<method_info nloc='3' complexity='1' token_count='17' nesting_level='0' start_line='197' end_line='199'></method_info>
 			<added_lines>197,198,199</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1,10,174,175,195,196</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\common.py' new_name='tests\common.py'>
 		<file_info nloc='190' complexity='22' token_count='2001'></file_info>
 		<method name='mock_train' parameters='kwargs'>
 				<method_info nloc='3' complexity='1' token_count='14' nesting_level='0' start_line='267' end_line='269'></method_info>
 			<added_lines>267,268,269</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>265,266</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\image\test_image_supervised.py' new_name='tests\image\test_image_supervised.py'>
 		<file_info nloc='215' complexity='11' token_count='1845'></file_info>
 		<method name='mock_train' parameters='kwargs'>
 				<method_info nloc='3' complexity='1' token_count='14' nesting_level='0' start_line='9' end_line='11'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>9,10,11</deleted_lines>
 		</method>
 		<method name='test_init_image_classifier_with_none_path' parameters='_'>
 				<method_info nloc='3' complexity='1' token_count='16' nesting_level='0' start_line='179' end_line='181'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>179,180,181</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6</added_lines>
 			<deleted_lines>6,7,8,178,182,183</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\test_preprocessor.py'>
 		<file_info nloc='25' complexity='1' token_count='221'></file_info>
 	</modification>
 </commit>
</bug_data>
