<bug_data>
<bug id='1238' author='FontTian' open_date='2020-07-16T03:06:09Z' closed_time='2020-08-21T17:11:23Z'>
 	<summary>How use multiple gpu?</summary>
 	<description>
 &lt;denchmark-h:h3&gt;Feature Description&lt;/denchmark-h&gt;
 
 I want to use a single machine with multiple gpu for training, but it seems to have no actual effect### Code Example
 with strategy.scope():
 &lt;denchmark-h:h3&gt;Reason&lt;/denchmark-h&gt;
 
 Speed up the calculation of toxins
 &lt;denchmark-h:h3&gt;Solution&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='FontTian' date='2020-07-20T03:58:25Z'>
 		I think this is supported in Keras-Tuner's Tuner class. AutoKeras supports all args in Tuner class.
 You can pass  to any model you use in AutoKeras.
 The details you can find on this page: &lt;denchmark-link:https://keras-team.github.io/keras-tuner/documentation/tuners/&gt;https://keras-team.github.io/keras-tuner/documentation/tuners/&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='FontTian' date='2020-07-21T13:14:20Z'>
 		emmm,I find it
 
 distribution_strategy: Optional. A TensorFlow tf.distribute DistributionStrategy instance. If specified, each trial will run under this scope. For example, tf.distribute.MirroredStrategy(['/gpu:0, /'gpu:1]) will run each trial on two GPUs. Currently only single-worker strategies are supported.
 
 and I ran the following code, but an error appeared.
 strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])
 # tuner.Tuner(distribution_strategy=strategy)
 import autokeras as ak
 clf = ak.ImageClassifier(
     overwrite=True,
     max_trials=1,distribution_strategy=strategy)
 
 ...
 RuntimeError: Too many failed attempts to build model.
 		</comment>
 		<comment id='3' author='FontTian' date='2020-07-21T20:33:19Z'>
 		We didn't fully test this feature with AutoKeras yet. You may try pass one more arg tuner='random'. If the error still exists, you may try a basic example with KerasTuner to see if it is a KerasTuner issue. Thanks.
 		</comment>
 		<comment id='4' author='FontTian' date='2020-07-31T16:54:57Z'>
 		Hello,
 got the same issue. I am specifying 4 GPUs (out of 8) to train the current model in a distributed fashion, using tf.distribute.MirroredStrategy( ) since tf.keras.utils.multi_gpu_model( ) is deprecated and removed since april 2020.
 When doing:
 &lt;denchmark-code&gt;def make_model(ckpt_path, max_try = 1):
     
     callbacks = [
     keras.callbacks.ModelCheckpoint(
         filepath=ckpt_path + '/bestMod.hdf5', verbose = 1, monitor="val_accuracy",
                            mode='max', save_best_only = True), 
     keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,  mode='auto')]
     
     
     AutoClassifier = ak.ImageClassifier(directory= "fruits/", 
                    project_name = "fruits_AutoKeras",
                    max_trials = max_try, 
                    metrics = ['accuracy', 'top_k_categorical_accuracy'], 
                    seed = 1245,
                    overwrite = True)     
     
     model = AutoClassifier.fit(
         data_train,
         callbacks=callbacks,
         validation_data=vali_data,
         verbose=2)
     
     return model
 
 
 def run_search(ckpt_path, max_try = 1):
 
     strat = tf.distribute.MirroredStrategy()
     with strat.scope():
         model = make_model(ckpt_path, max_try)
 
     return model
 
 run_search(checkpoint, max_try = 3)
 
 &lt;/denchmark-code&gt;
 
 only one single GPU is doing all the computations, the other three remain idle. When following &lt;denchmark-link:https://github.com/FontTian&gt;@FontTian&lt;/denchmark-link&gt;
  and inserting  into the initialisation of the image classifier, the same error  occurs. Same happens when adding  to ak.ImageClassifier.
 As suggested by &lt;denchmark-link:https://github.com/haifeng-jin&gt;@haifeng-jin&lt;/denchmark-link&gt;
 , I ran a basic KerasTuner example on 4 GPUs which worked just fine. Furthermore, in &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/440#issuecomment-592160313&gt;#440 (comment)&lt;/denchmark-link&gt;
  I read that the clear_session() before every run might wipe out the gpu configuration. Removing this line from the code did not change anything with respect to the errors/problems stated above.
 Thanks in advance
 		</comment>
 		<comment id='5' author='FontTian' date='2020-08-04T22:34:58Z'>
 		&lt;denchmark-link:https://github.com/AnSmithD&gt;@AnSmithD&lt;/denchmark-link&gt;
  Thank you for your report. I believe this is a very important bug we need to track down. Would you like to setup a call with us to talk about it? I believe this would be the fastest way to solve it. Please reach me on Slack if you would like to do so. Thank you!
 		</comment>
 		<comment id='6' author='FontTian' date='2020-08-14T15:35:23Z'>
 		Hi, is there any update on this topic? Is there currently a workaround to make use of multiple GPUs?
 		</comment>
 		<comment id='7' author='FontTian' date='2020-08-15T07:48:01Z'>
 		&lt;denchmark-link:https://github.com/ywtaccelerator&gt;@ywtaccelerator&lt;/denchmark-link&gt;
   I am working on another issue related to the tuner right now. Will get to this one right afterwards. Thanks
 		</comment>
 		<comment id='8' author='FontTian' date='2020-08-15T20:21:12Z'>
 		&lt;denchmark-link:https://github.com/haifeng-jin&gt;@haifeng-jin&lt;/denchmark-link&gt;
  Thank you! Looking forward to the solving of this issue, I really appreciate your work in AutoKeras.
 		</comment>
 		<comment id='9' author='FontTian' date='2020-08-17T23:41:17Z'>
 		&lt;denchmark-link:https://github.com/FontTian&gt;@FontTian&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ywtaccelerator&gt;@ywtaccelerator&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/AnSmithD&gt;@AnSmithD&lt;/denchmark-link&gt;
  This bug is fixed in master branch. Please refer to &lt;denchmark-link:https://github.com/keras-team/autokeras/pull/1285&gt;#1285&lt;/denchmark-link&gt;
  for fixing details. Thanks.
 		</comment>
 		<comment id='10' author='FontTian' date='2020-08-20T09:46:51Z'>
 		&lt;denchmark-link:https://github.com/haifeng-jin&gt;@haifeng-jin&lt;/denchmark-link&gt;
  Thank you very much for your effort and fixing it quickly, but unfortunately it still does not work for me. I have installed AutoKeras from the master branch and ak. shows its version is 1.0.7, but neither running the Simple Example &lt;denchmark-link:https://autokeras.com/tutorial/image_classification/&gt;here&lt;/denchmark-link&gt;
  directly, nor trying the ways mentioned above in this page, could make use of multiple GPUs...
 Could you double check if it functions well, or briefly describe how to make it work? Thanks!
 		</comment>
 		<comment id='11' author='FontTian' date='2020-08-20T12:25:38Z'>
 		&lt;denchmark-link:https://github.com/ywtaccelerator&gt;@ywtaccelerator&lt;/denchmark-link&gt;
  Have you tried uninstalling autokeras and keras-tuner first? And then reinstalling keras-tuner directly via  and  installing autokeras from the master branch?
 &lt;denchmark-link:https://github.com/haifeng-jin&gt;@haifeng-jin&lt;/denchmark-link&gt;
  's fix is working for me (after above described procedure) but unfortunately, it does not seem to result in any performance gains in terms of computing time ...
 		</comment>
 		<comment id='12' author='FontTian' date='2020-08-20T17:55:59Z'>
 		The solution is also added to the FAQ here:
 &lt;denchmark-link:http://autokeras.com/tutorial/faq/#how-to-use-multiple-gpus&gt;http://autokeras.com/tutorial/faq/#how-to-use-multiple-gpus&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='13' author='FontTian' date='2020-08-21T17:14:28Z'>
 		&lt;denchmark-link:https://github.com/ywtaccelerator&gt;@ywtaccelerator&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/AnSmithD&gt;@AnSmithD&lt;/denchmark-link&gt;
  I have had another fix for the bug merged. I tested it with the &lt;denchmark-link:https://github.com/keras-team/autokeras/blob/master/examples/mnist.py&gt;mnist example&lt;/denchmark-link&gt;
  by adding the distribution_strategy argument to it. It runs fine now.
 Before this fix it also runs fine for the fit function but not for the predict and evaluate function.
 I cannot reproduce the bug of failing on the second trail.
 Please paste the entire stack trace if it still doesn't work.
 Also please try if the mnist example works.
 Thank you!
 		</comment>
 	</comments>
 </bug>
<commit id='ede75e52d17a85a58f96df125295bc59b966863e' author='Haifeng Jin' date='2020-08-17 18:40:10-05:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='autokeras\graph.py' new_name='autokeras\graph.py'>
 		<file_info nloc='239' complexity='84' token_count='1639'></file_info>
 		<method name='build' parameters='self,hp'>
 				<method_info nloc='25' complexity='7' token_count='182' nesting_level='1' start_line='250' end_line='276'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>252</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\autokeras\engine\tuner_test.py' new_name='tests\autokeras\engine\tuner_test.py'>
 		<file_info nloc='111' complexity='12' token_count='911'></file_info>
 		<method name='test_tuner_does_not_crash_with_distribution_strategy' parameters='tmp_path'>
 				<method_info nloc='7' complexity='1' token_count='46' nesting_level='0' start_line='123' end_line='129'></method_info>
 			<added_lines>123,124,125,126,127,128,129</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>130,131</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\autokeras\tuners\task_specific_test.py' new_name='tests\autokeras\tuners\task_specific_test.py'>
 		<file_info nloc='56' complexity='6' token_count='577'></file_info>
 		<method name='clear_session' parameters=''>
 				<method_info nloc='4' complexity='1' token_count='23' nesting_level='0' start_line='26' end_line='29'></method_info>
 			<added_lines>27</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
