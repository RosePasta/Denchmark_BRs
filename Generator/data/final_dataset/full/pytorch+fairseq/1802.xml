<bug_data>
<bug id='1802' author='avi-jit' open_date='2020-03-08T21:16:49Z' closed_time='2020-03-09T13:02:41Z'>
 	<summary>AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'</summary>
 	<description>
 &lt;denchmark-h:h4&gt;AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'&lt;/denchmark-h&gt;
 
 I'm trying to finetune roberta by registering a new classification head, as explained here: &lt;denchmark-link:github.com/pytorch/fairseq/blob/master/examples/roberta/README.custom_classification.md&gt;github.com/pytorch/fairseq/blob/master/examples/roberta/README.custom_classification.md&lt;/denchmark-link&gt;
 . I've tokenized and preprocessed my data as described but the training phase is where I'm stuck.
 &lt;denchmark-h:h4&gt;Code &amp; Output&lt;/denchmark-h&gt;
 
 Here's the execution script and arguments:
 &lt;denchmark-code&gt;WARMUP_UPDATES=479
 TOTAL_NUM_UPDATES=7812
 LR=1e-05
 HEAD_NAME=head_name
 NUM_CLASSES=1014
 MAX_SENTENCES=2
 ROBERTA_PATH=roberta.large/model.pt 
 CUDA_VISIBLE_DEVICES=0
 DATA_DIR=../data/
 MAX_TOKENS=4
 
 python train.py $DATA_DIR \
 --restore-file $ROBERTA_PATH --max-positions 512 --max-sentences $MAX_SENTENCES \
 --max-tokens $MAX_TOKENS --task sentence_prediction --reset-optimizer \
 --reset-dataloader --reset-meters --required-batch-size-multiple 1 --init-token 0 \
 --separator-token 2 --arch roberta_large --criterion sentence_prediction \
 --classification-head-name $HEAD_NAME --num-classes $NUM_CLASSES --dropout 0.1 \
 --attention-dropout 0.1 --weight-decay 0.1 --optimizer adam \
 --adam-betas "(0.9, 0.98)" --adam-eps 1e-06 --clip-norm 0.0 \
 --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES \
 --warmup-updates $WARMUP_UPDATES --fp16 --fp16-init-scale 4 \
 --threshold-loss-scale 1 --fp16-scale-window 128 --max-epoch 10 \
 --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \
 --truncate-sequence --find-unused-parameters --update-freq 4 
 &lt;/denchmark-code&gt;
 
 And here's the error trace:
 &lt;denchmark-code&gt;2020-03-08 13:40:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe=None, broadcast_buffers=False, bucket_cap_mb=25, classification_head_name='head_name', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='../data/', dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=100, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_positions=512, max_sentences=2, max_sentences_valid=2, max_tokens=4, max_tokens_valid=4, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_shuffle=False, num_classes=1014, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='roberta.large/model.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, skip_invalid_size_inputs_valid_test=False, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=1.0, tokenizer=None, total_num_update=7812, train_subset='train', truncate_sequence=True, update_freq=[4], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=479, weight_decay=0.1)
 2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types
 2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 1009 types
 2020-03-08 13:40:04 | INFO | fairseq.data.data_utils | loaded 40151 examples from: ../data/input0/valid
 2020-03-08 13:40:04 | INFO | fairseq.data.data_utils | loaded 40151 examples from: ../data/label/valid
 2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 40151
 2020-03-08 13:40:11 | INFO | fairseq_cli.train | RobertaModel 
 &lt;/denchmark-code&gt;
 
 ... I'm removing the model specs (layer wise) since it's too long ...
 &lt;denchmark-code&gt;2020-03-08 13:40:11 | INFO | fairseq_cli.train | model roberta_large, criterion SentencePredictionCriterion
 2020-03-08 13:40:11 | INFO | fairseq_cli.train | num. model params: 357499983 (num. trained: 357499983)
 2020-03-08 13:40:11 | INFO | fairseq_cli.train | training on 1 GPUs
 2020-03-08 13:40:11 | INFO | fairseq_cli.train | max tokens per GPU = 4 and max sentences per GPU = 2
 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.dense.weight
 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.dense.bias
 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.out_proj.weight
 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.out_proj.bias
 2020-03-08 13:40:11 | INFO | fairseq.trainer | loaded checkpoint roberta.large/model.pt (epoch 1 @ 0 updates)
 2020-03-08 13:40:11 | INFO | fairseq.trainer | loading train data for epoch 1
 2020-03-08 13:40:11 | INFO | fairseq.data.data_utils | loaded 160600 examples from: ../data/input0/train
 2020-03-08 13:40:11 | INFO | fairseq.data.data_utils | loaded 160600 examples from: ../data/label/train
 2020-03-08 13:40:11 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 160600
 2020-03-08 13:40:11 | WARNING | fairseq.data.data_utils | 160580 samples have invalid sizes and will be skipped, max_positions=4, first few sample ids=[141335, 12811, 122058, 104432, 1925, 141500, 65517, 143950, 59283, 155828]
 Traceback (most recent call last):
   File "train.py", line 11, in &lt;module&gt;
     cli_main()
   File "/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py", line 322, in cli_main
     main(args)
   File "/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py", line 100, in main
     train(args, trainer, task, epoch_itr)
   File "/nas/home/thawani/anaconda3/envs/env/lib/python3.7/contextlib.py", line 74, in inner
     return func(*args, **kwds)
   File "/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py", line 177, in train
     log_output = trainer.train_step(samples)
   File "/nas/home/thawani/anaconda3/envs/env/lib/python3.7/contextlib.py", line 74, in inner
     return func(*args, **kwds)
   File "/nas/home/thawani/MCS/fairseq/fairseq/trainer.py", line 319, in train_step
     ignore_grad=is_dummy_batch,
   File "/nas/home/thawani/MCS/fairseq/fairseq/tasks/fairseq_task.py", line 337, in train_step
     loss, sample_size, logging_output = criterion(model, sample)
   File "/nas/home/thawani/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
     result = self.forward(*input, **kwargs)
   File "/nas/home/thawani/MCS/fairseq/fairseq/criterions/sentence_prediction.py", line 40, in forward
     and self.args.classification_head_name in model.classification_heads
   File "/nas/home/thawani/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 576, in __getattr__
     type(self).__name__, name))
 AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'
 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;What have you tried?&lt;/denchmark-h&gt;
 
 
 creating new environment and trying again from scratch: same issue.
 tracing the error through the code: I can't make heads or tails out of it.
 
 &lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;
 
 
 fairseq Version: 0.9.0
 PyTorch Version: 1.4.0
 OS: CentOS Linux release 7.7.1908
 How you installed fairseq: source
 Build command used: pip install --editable .
 Python version: 3.7.6
 CUDA/cuDNN version: 10.2
 GPU models and configuration: NVIDIA TU102
 Any other relevant information: I'm using a conda enviornment
 
 	</description>
 	<comments>
 		<comment id='1' author='avi-jit' date='2020-03-08T22:42:05Z'>
 		Thanks, this is a bug introduced in our recent refactoring of criterions to not depend on args: &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/46b773a393c423f653887c382e4d55e69627454d&gt;46b773a&lt;/denchmark-link&gt;
 . I'll submit a fix
 		</comment>
 		<comment id='2' author='avi-jit' date='2020-03-08T22:47:00Z'>
 		Looks like a quick fix - &lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
  want me to take it?
 		</comment>
 	</comments>
 </bug>
<commit id='9e1fb47035e87df5d1b7fd86dc152b666dca9217' author='Elijah Rippeth' date='2020-03-09 06:02:31-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fairseq\criterions\sentence_prediction.py' new_name='fairseq\criterions\sentence_prediction.py'>
 		<file_info nloc='73' complexity='16' token_count='523'></file_info>
 		<method name='forward' parameters='self,model,sample,reduce'>
 				<method_info nloc='36' complexity='4' token_count='219' nesting_level='1' start_line='31' end_line='77'></method_info>
 			<added_lines>41,47,52,73</added_lines>
 			<deleted_lines>40,46,51,72</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,task,classification_head_name'>
 				<method_info nloc='3' complexity='1' token_count='22' nesting_level='1' start_line='18' end_line='20'></method_info>
 			<added_lines>18</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,task,classification_head_name,regression_target'>
 				<method_info nloc='4' complexity='1' token_count='29' nesting_level='1' start_line='18' end_line='21'></method_info>
 			<added_lines>18,21</added_lines>
 			<deleted_lines>18</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
