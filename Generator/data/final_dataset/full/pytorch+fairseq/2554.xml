<bug_data>
<bug id='2554' author='vadimkantorov' open_date='2020-09-01T13:17:58Z' closed_time='2020-10-13T15:11:18Z'>
 	<summary>get_normalized_probs_scriptable seems broken</summary>
 	<description>
 &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/2887663/fairseq/models/fairseq_model.py#L59&gt;https://github.com/pytorch/fairseq/blob/2887663/fairseq/models/fairseq_model.py#L59&lt;/denchmark-link&gt;
 :
   def get_normalized_probs_scriptable(
         self,
         net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],
         log_probs: bool,
         sample: Optional[Dict[str, Tensor]] = None,
     ):
         """Scriptable helper function for get_normalized_probs in ~BaseFairseqModel"""
         if hasattr(self, "decoder"):
             return self.decoder.get_normalized_probs(net_output, log_probs, sample)
         elif torch.is_tensor(net_output):
             logits = net_output.float()
             if log_probs:
                 return F.log_softmax(logits, dim=-1)
             else:
                 return F.softmax(logits, dim=-1)
         raise NotImplementedError
 It accepts net_output as tuple and later checks it to be a tensor. Is it okay?
 	</description>
 	<comments>
 		<comment id='1' author='vadimkantorov' date='2020-09-03T11:06:36Z'>
 		What error message are you experiencing? are you on the latest version of fairseq?
 		</comment>
 		<comment id='2' author='vadimkantorov' date='2020-09-03T11:15:12Z'>
 		It was making the NotImplementedError. I'm no longer using this code path, so my problem is solved, so unfortunately cannot provide any more details. It was a recent master branch. But I thought this might be a bug (either with typing or with if check), so I reported it.
 		</comment>
 		<comment id='3' author='vadimkantorov' date='2020-09-04T00:50:56Z'>
 		Yeah this looks off. Probably didn't get fully updated when we added types for torchscript.
 		</comment>
 		<comment id='4' author='vadimkantorov' date='2020-10-29T11:53:17Z'>
 		&lt;denchmark-link:https://github.com/vadimkantorov&gt;@vadimkantorov&lt;/denchmark-link&gt;
   I am also facing the same problem.
 How did you solve this issue? Which code path did you use for inferencing ?
 Okay the new update from yesterday solved this issue.
 		</comment>
 	</comments>
 </bug>
<commit id='c005362349075fb5952ece139481232cc49e2286' author='Myle Ott' date='2020-10-13 08:10:58-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fairseq\distributed_utils.py' new_name='fairseq\distributed_utils.py'>
 		<file_info nloc='308' complexity='72' token_count='2126'></file_info>
 		<method name='_all_reduce_dict' parameters='OrderedDict'>
 				<method_info nloc='8' complexity='5' token_count='119' nesting_level='1' start_line='384' end_line='391'></method_info>
 			<added_lines>387,389,390,391</added_lines>
 			<deleted_lines>387,389</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fairseq\models\fairseq_model.py' new_name='fairseq\models\fairseq_model.py'>
 		<file_info nloc='314' complexity='86' token_count='2036'></file_info>
 		<modified_lines>
 			<added_lines>72,73</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
