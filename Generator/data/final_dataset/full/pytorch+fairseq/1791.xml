<bug_data>
<bug id='1791' author='mgaido91' open_date='2020-03-06T16:27:03Z' closed_time='2020-03-10T18:51:18Z'>
 	<summary>Sequence generation fails if the encoder outputs a sequence with lenght different from input</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 The problem is present when there is a model in which the encoder outputs a sequence having a length different from the input one. This is very common in speech processing models which contain convolutions reducing the size of the input.
 With such a model, the following command fails:
 &lt;denchmark-code&gt;python generate.py  $DATA_DIR \
     --gen-subset test \
     --path $MY_MODEL_PATH \
     --max-source-positions 2000 --max-target-positions 1000 \
     --task speech_recognition --user-dir examples/speech_recognition \
     --skip-invalid-size-inputs-valid-test
 &lt;/denchmark-code&gt;
 
 with the following stacktrace:
 &lt;denchmark-code&gt;  File ".../fairseq/tasks/fairseq_task.py", line 317, in inference_step
     return generator.generate(models, sample, prefix_tokens=prefix_tokens)
   File ".../python3.7/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
     return func(*args, **kwargs)
   File ".../fairseq/sequence_generator.py", line 92, in generate
     return self._generate(model, sample, **kwargs)
   File ".../python3.7/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
     return func(*args, **kwargs)
   File .../fairseq/sequence_generator.py", line 335, in _generate
     attn[:, :, step + 1].copy_(avg_attn_scores)
 RuntimeError: The size of tensor a (1069) must match the size of tensor b (268) at non-singleton dimension 1
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 A very simple reproducer is provided as unit test in the PR related to this issue.
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The expected behavior is that the generation works fine without failures.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 fairseq Version (e.g., 1.0 or master): master
 PyTorch Version (e.g., 1.0): 1.4
 OS (e.g., Linux): Linux
 How you installed fairseq (pip, source): source
 Build command you used (if compiling from source): pip install -e .
 Python version: 3.7
 CUDA/cuDNN version: 10.0
 GPU models and configuration: K80
 Any other relevant information:
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 In the failure described in the command above I am using an implementation of &lt;denchmark-link:https://www.aclweb.org/anthology/W19-6603.pdf&gt;https://www.aclweb.org/anthology/W19-6603.pdf&lt;/denchmark-link&gt;
 .
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='431d604f696a15c06fceab56b4ace271bb85e74b' author='Marco Gaido' date='2020-03-10 11:51:08-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.32142857142857145' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fairseq\sequence_generator.py' new_name='fairseq\sequence_generator.py'>
 		<file_info nloc='489' complexity='66' token_count='4151'></file_info>
 		<modified_lines>
 			<added_lines>334</added_lines>
 			<deleted_lines>334</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\test_sequence_generator.py' new_name='tests\test_sequence_generator.py'>
 		<file_info nloc='306' complexity='38' token_count='4230'></file_info>
 		<method name='test_encoder_with_different_output_len' parameters='self'>
 				<method_info nloc='9' complexity='3' token_count='106' nesting_level='1' start_line='141' end_line='149'></method_info>
 			<added_lines>141,142,143,144,145,146,147,148,149</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>150</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\utils.py' new_name='tests\utils.py'>
 		<file_info nloc='183' complexity='35' token_count='1473'></file_info>
 		<method name='reorder_encoder_out' parameters='self,encoder_out,new_order'>
 				<method_info nloc='2' complexity='1' token_count='18' nesting_level='1' start_line='252' end_line='253'></method_info>
 			<added_lines>252,253</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,args,dictionary'>
 				<method_info nloc='3' complexity='1' token_count='22' nesting_level='1' start_line='239' end_line='241'></method_info>
 			<added_lines>239,240,241</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='build_model' parameters='cls,args,task'>
 				<method_info nloc='4' complexity='1' token_count='36' nesting_level='1' start_line='261' end_line='264'></method_info>
 			<added_lines>261,262,263,264</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,encoder,decoder'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='257' end_line='258'></method_info>
 			<added_lines>257,258</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='forward' parameters='self,src_tokens,src_lengths,kwargs'>
 				<method_info nloc='8' complexity='2' token_count='65' nesting_level='1' start_line='243' end_line='250'></method_info>
 			<added_lines>243,244,245,246,247,248,249,250</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>8,236,237,238,242,251,254,255,256,259,260</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
