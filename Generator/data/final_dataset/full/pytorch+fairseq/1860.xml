<bug_data>
<bug id='1860' author='loopdigga96' open_date='2020-03-18T07:22:18Z' closed_time='2020-03-21T18:16:32Z'>
 	<summary>wmt19 model cannot run on gpu except #0.</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 I am running &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/wmt19&gt;tutorial&lt;/denchmark-link&gt;
 . I successfully loaded model from hub and tried to run it on second gpu (id=1). Which raised an exception that data and model are stored on different GPUs. With gpu(id=0) works fine.
 &lt;denchmark-code&gt;Using cache found in /home/vlad/.cache/torch/hub/pytorch_fairseq_master
 Loading codes from /home/vlad/.cache/torch/pytorch_fairseq/0695ef328ddefcb8cbcfabc3196182f59c0e41e0468b10cc0db2ae9c91881fcc.bb1be17de4233e13870bd7d6065bfdb03fca0a51dd0f5d0b7edf5c188eda71f1/bpecodes ...
 Read 30000 codes from the codes file.
 Traceback (most recent call last):
   File "/home/vlad/Documents/coding/experiments/paper-analyzer-Tretyak_Internship/papers/experiments/scientific_generation/scripts/paraphrase.py", line 46, in &lt;module&gt;
     en2de.translate(['hello'])
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py", line 126, in translate
     return self.sample(sentences, beam, verbose, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py", line 132, in sample
     batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py", line 165, in generate
     translations = self.task.inference_step(generator, self.models, batch)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/fairseq_task.py", line 351, in inference_step
     return generator.generate(models, sample, prefix_tokens=prefix_tokens)
   File "/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
     return func(*args, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py", line 93, in generate
     return self._generate(model, sample, **kwargs)
   File "/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
     return func(*args, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py", line 276, in _generate
     tokens[:, :step + 1], encoder_outs, temperature=self.temperature,
   File "/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 49, in decorate_no_grad
     return func(*args, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py", line 549, in forward_decoder
     temperature=temperature,
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py", line 568, in _decode_one
     tokens, encoder_out=encoder_out, incremental_state=self.incremental_states[model],
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/fairseq_model.py", line 274, in forward_decoder
     return self.decoder(prev_output_tokens, **kwargs)
   File "/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
     result = self.forward(*input, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py", line 704, in forward
     alignment_heads=alignment_heads,
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py", line 807, in extract_features
     need_head_weights=bool((idx == alignment_layer)),
   File "/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
     result = self.forward(*input, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/transformer_layer.py", line 297, in forward
     need_head_weights=need_head_weights,
   File "/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
     result = self.forward(*input, **kwargs)
   File "/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/multihead_attention.py", line 325, in forward
     key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float("-inf")
 RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorMasked.cu:28
 
 Process finished with exit code 1
 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import torch
 
 en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de',
                        checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt',
                        tokenizer='moses', bpe='fastbpe').to(torch.device('cuda:1'))
 
 result = en2de.translate(['hello'])
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 Run without errors
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 fairseq Version==0.9.0
 PyTorch Version ==1.4.0
 OS (e.g., Linux): Ubuntu 18.04
 How you installed fairseq (pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.6
 CUDA/cuDNN version: 10.2
 GPU models and configuration: RTX 2080 x 2
 Any other relevant information:
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='loopdigga96' date='2020-10-11T03:59:14Z'>
 		I still have this question when using gpu other than gpu: 0, have the problem been solved ?
 		</comment>
 	</comments>
 </bug>
<commit id='bee6d71646c22eb552ec7d78d439729b38dfa55b' author='Myle Ott' date='2020-03-21 11:16:24-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fairseq\modules\layer_norm.py' new_name='fairseq\modules\layer_norm.py'>
 		<file_info nloc='29' complexity='9' token_count='234'></file_info>
 		<method name='forward' parameters='self,x'>
 				<method_info nloc='3' complexity='1' token_count='28' nesting_level='2' start_line='18' end_line='20'></method_info>
 			<added_lines>19,20</added_lines>
 			<deleted_lines>19</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fairseq\modules\multihead_attention.py' new_name='fairseq\modules\multihead_attention.py'>
 		<file_info nloc='393' complexity='20' token_count='2805'></file_info>
 		<modified_lines>
 			<added_lines>381,382,383,384,389,390,391,392</added_lines>
 			<deleted_lines>381,382,383,384,389,390,391</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
