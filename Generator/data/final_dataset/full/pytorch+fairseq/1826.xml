<bug_data>
<bug id='1826' author='freewym' open_date='2020-03-12T05:04:42Z' closed_time='2020-03-21T18:15:44Z'>
 	<summary>CUDA error when training with multiple GPUs and criterion's logging_outputs_can_be_summed() is False</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 Currently almost all subclass of FairseqCriterion 's logging_outputs_can_be_summed() returns True, and it trains OK with multiple GPUs. But if I change, for example, CrossEntropyCriterion's  logging_outputs_can_be_summed () to return False and train with 2 GPUs , it gives error:
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 118, in join
 raise Exception(msg)
 Exception:
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
 fn(i, *args)
 File "/home/ywang/fairseq6/train.py", line 286, in distributed_main
 main(args, init_distributed=True)
 File "/home/ywang/fairseq6/train.py", line 96, in main
 train(args, trainer, task, epoch_itr)
 File "/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py", line 74, in inner
 return func(*args, **kwds)
 File "/home/ywang/fairseq6/train.py", line 176, in train
 log_output = trainer.train_step(samples)
 File "/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py", line 74, in inner
 return func(*args, **kwds)
 File "/home/ywang/fairseq6/fairseq/trainer.py", line 347, in train_step
 logging_outputs, sample_size, ooms, ignore=is_dummy_batch,
 File "/home/ywang/fairseq6/fairseq/trainer.py", line 616, in _aggregate_logging_outputs
 logging_outputs, *extra_stats_to_sum, ignore=ignore
 File "/home/ywang/fairseq6/fairseq/trainer.py", line 634, in _all_gather_list_sync
 max_size=getattr(self.args, 'all_gather_list_size', 16384),
 File "/home/ywang/fairseq6/fairseq/distributed_utils.py", line 176, in all_gather_list
 result.append(pickle.loads(bytes(out_buffer[header_size:header_size + enc_size].tolist())))
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/storage.py", line 134, in _load_from_bytes
 return torch.load(io.BytesIO(b))
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 529, in load
 return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 702, in _legacy_load
 result = unpickler.load()
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 665, in persistent_load
 deserialized_objects[root_key] = restore_location(obj, location)
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 156, in default_restore_location
 result = fn(storage, location)
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py", line 136, in _cuda_deserialize
 return storage_type(obj.size())
 File "/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/cuda/init.py", line 480, in _lazy_new
 return super(_CudaBase, cls).new(cls, *args, **kwargs)
 RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable
 The reason why I want it to return False is, I have some stats in logging_output which is a tensor not scalar, to be aggregated in reduce_matrices().
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Steps to reproduce the behavior (always include the command you ran):
 
 Just have CrossEntropyCriterion.logging_outputs_can_be_summed() to return False and train with multi-GPUs
 See error
 
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 fairseq Version (e.g., 1.0 or master): master
 PyTorch Version (e.g., 1.0): 1.4
 OS (e.g., Linux): Linux
 How you installed fairseq (pip, source): source
 Build command you used (if compiling from source):
 Python version: 3.7
 CUDA/cuDNN version: 10.2
 GPU models and configuration:  GeForce GTX 1080
 Any other relevant information:
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='freewym' date='2020-03-20T21:29:40Z'>
 		Merging the above fix, please reopen if this is still an issue after this is merged!
 		</comment>
 	</comments>
 </bug>
<commit id='aed8cf4c38db088c2f640fa941a7d2c462d72b16' author='Myle Ott' date='2020-03-21 11:16:20-07:00'>
 	<dmm_unit complexity='0.8333333333333334' interfacing='0.8333333333333334' size='0.8333333333333334'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fairseq\benchmark\dummy_lm.py' new_name='fairseq\benchmark\dummy_lm.py'>
 		<file_info nloc='75' complexity='18' token_count='534'></file_info>
 		<method name='setup_task' parameters='cls,args,kwargs'>
 				<method_info nloc='6' complexity='2' token_count='58' nesting_level='1' start_line='41' end_line='47'></method_info>
 			<added_lines>46</added_lines>
 			<deleted_lines>41,42</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6,7,15,16,17</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fairseq\benchmark\dummy_masked_lm.py' new_name='fairseq\benchmark\dummy_masked_lm.py'>
 		<file_info nloc='84' complexity='18' token_count='590'></file_info>
 		<method name='setup_task' parameters='cls,args,kwargs'>
 				<method_info nloc='6' complexity='2' token_count='58' nesting_level='1' start_line='52' end_line='58'></method_info>
 			<added_lines>57</added_lines>
 			<deleted_lines>52,53</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6,7,15,16,17</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fairseq\distributed_utils.py' new_name='fairseq\distributed_utils.py'>
 		<file_info nloc='180' complexity='36' token_count='1190'></file_info>
 		<method name='all_gather_list' parameters='data,group,max_size'>
 				<method_info nloc='42' complexity='7' token_count='301' nesting_level='0' start_line='131' end_line='188'></method_info>
 			<added_lines>155</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fairseq\utils.py' new_name='fairseq\utils.py'>
 		<file_info nloc='344' complexity='111' token_count='2552'></file_info>
 		<method name='move_to_cpu._move_to_cpu' parameters='tensor'>
 				<method_info nloc='2' complexity='1' token_count='11' nesting_level='1' start_line='71' end_line='72'></method_info>
 			<added_lines>71,72</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='move_to_cpu' parameters='sample'>
 				<method_info nloc='3' complexity='1' token_count='14' nesting_level='0' start_line='70' end_line='74'></method_info>
 			<added_lines>70,71,72,73,74</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>75,76</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
