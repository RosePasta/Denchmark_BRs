<bug_data>
<bug id='2574' author='Alvant' open_date='2020-09-04T12:35:32Z' closed_time='2020-10-14T16:32:36Z'>
 	<summary>Signature of apply_sparse_mask method in MultiheadAttention</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;
 
 In MultiheadAttention module,  method signature
 &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L445&gt;https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L445&lt;/denchmark-link&gt;
 
 is invalid (but it still works). It is written like a function, not a class method.
 I suggest fixing the signature from
 def apply_sparse_mask(attn_weights, ...
 to
 def apply_sparse_mask(self, attn_weights, ...
 and the line &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L320&gt;https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L320&lt;/denchmark-link&gt;
  from
 attn_weights = MultiheadAttention.apply_sparse_mask(...)
 to
 attn_weights = self.apply_sparse_mask(...)
 Why am I suggesting using an instance method instead of a class- or staticmethod?
 I noticed that SparseMultiHeadAttention redefines this  method &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sparse_multihead_attention.py#L101&gt;https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sparse_multihead_attention.py#L101&lt;/denchmark-link&gt;
 .
 So it seems that  signature would be more appropriate to use in MultiheadAttention.
 &lt;denchmark-h:h3&gt;P.S.&lt;/denchmark-h&gt;
 
 It is not quite a bug, but it is something not quite right.
 &lt;denchmark-h:h3&gt;P.P.S.&lt;/denchmark-h&gt;
 
 If the idea is OK, I could make a pullrequest.
 	</description>
 	<comments>
 		<comment id='1' author='Alvant' date='2020-09-08T14:38:00Z'>
 		Nice catch!  Definitely agree with your proposed fix, feel free to submit a PR, thanks!
 		</comment>
 		<comment id='2' author='Alvant' date='2020-09-08T20:44:28Z'>
 		Ok! Here &lt;denchmark-link:https://github.com/pytorch/fairseq/pull/2587&gt;#2587&lt;/denchmark-link&gt;
 
 but for some reason the tests did not run after submitting the pull request...
 		</comment>
 		<comment id='3' author='Alvant' date='2020-09-09T20:48:16Z'>
 		I think it might be because there was a lint error that got merged somehow.  It should be fixed now if you want to rebase.
 		</comment>
 		<comment id='4' author='Alvant' date='2020-10-15T16:27:52Z'>
 		üéà üòÉ
 		</comment>
 	</comments>
 </bug>
<commit id='5e831033069b52b09905e0bf8ba104d016e04efd' author='Vasiliy Alekseev' date='2020-10-14 09:32:23-07:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='examples\speech_recognition\models\vggtransformer.py' new_name='examples\speech_recognition\models\vggtransformer.py'>
 		<file_info nloc='725' complexity='86' token_count='3951'></file_info>
 		<method name='parse_transformer_sampling' parameters='self,transformer_sampling,num_layers'>
 				<method_info nloc='21' complexity='7' token_count='102' nesting_level='1' start_line='437' end_line='473'></method_info>
 			<added_lines>462</added_lines>
 			<deleted_lines>462</deleted_lines>
 		</method>
 		<method name='validate_transformer_config' parameters='self,transformer_config'>
 				<method_info nloc='10' complexity='3' token_count='57' nesting_level='1' start_line='392' end_line='401'></method_info>
 			<added_lines>397,399</added_lines>
 			<deleted_lines>397,399</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fairseq\modules\multihead_attention.py' new_name='fairseq\modules\multihead_attention.py'>
 		<file_info nloc='400' complexity='21' token_count='2907'></file_info>
 		<method name='apply_sparse_mask' parameters='self,attn_weights,int,int,int'>
 				<method_info nloc='2' complexity='1' token_count='21' nesting_level='1' start_line='445' end_line='446'></method_info>
 			<added_lines>445</added_lines>
 			<deleted_lines>445</deleted_lines>
 		</method>
 		<method name='apply_sparse_mask' parameters='attn_weights,int,int,int'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='445' end_line='446'></method_info>
 			<added_lines>445</added_lines>
 			<deleted_lines>445</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>320</added_lines>
 			<deleted_lines>320</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
