<bug_data>
<bug id='1983' author='noe' open_date='2020-04-08T19:15:44Z' closed_time='2020-04-14T17:28:32Z'>
 	<summary>Sampling from GeneratorHubInterface does not generate anything</summary>
 	<description>
 &lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;
 
 When calling function  from , as specified in the &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/language_model&gt;language modeling readme&lt;/denchmark-link&gt;
 , the result is always the same prefix passed as argument to the function, with no extra tokens sampled from the LM.
 Internally, the problem is that, when the sentence is converted to token IDs, the EOS token ID is added. When the model is presented with a prefix that "has already ended", it just generates another EOS and finishes.
 &lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;
 
 Just follow the code to sample from an LM from the &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/language_model&gt;language modeling readme&lt;/denchmark-link&gt;
 .
 &lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;from fairseq.models.transformer_lm import TransformerLanguageModel
 custom_lm = TransformerLanguageModel.from_pretrained('/path/to/model/dir', 'checkpoint100.pt')
 custom_lm.sample('Barack Obama', beam=1)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;
 
 The returned sampled sentences have more tokens apart from the ones already supplied as argument to sample.
 &lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;
 
 
 fairseq Version: master:
 PyTorch Version: 1.4.0
 OS: Ubuntu 16.04.2 LTS
 How you installed fairseq: from source
 Python version: 3.7.4
 CUDA/cuDNN version: none
 GPU models and configuration: none
 
 	</description>
 	<comments>
 		<comment id='1' author='noe' date='2020-04-08T19:17:10Z'>
 		I would be happy to provide a PR.
 		</comment>
 		<comment id='2' author='noe' date='2020-04-09T02:41:21Z'>
 		same issue
 		</comment>
 		<comment id='3' author='noe' date='2020-04-09T19:07:56Z'>
 		I got a similar issue here, I copied the code from template: &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/language_model&gt;https://github.com/pytorch/fairseq/tree/master/examples/language_model&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;lm.sample('Barack Obama', sampling=True, sampling_topk=10, temperature=0.9)
 &gt;&gt;&gt; "Barack Obama"
 &lt;/denchmark-code&gt;
 
 It won't generate any longer sentence except 'Barack Obama'.
 		</comment>
 		<comment id='4' author='noe' date='2020-04-13T22:14:24Z'>
 		Ah, thanks for flagging. I mistakenly broke this in &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/7c0ab23d14882d77ae5017ee71085925c5c03373&gt;7c0ab23&lt;/denchmark-link&gt;
 .
 Will submit a fix shortly.
 		</comment>
 	</comments>
 </bug>
<commit id='88daeb748b31ad27de6c34630968e0fc191e4326' author='Myle Ott' date='2020-04-14 10:28:15-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.5714285714285714' size='0.5714285714285714'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fairseq\data\strip_token_dataset.py' new_name='fairseq\data\strip_token_dataset.py'>
 		<file_info nloc='12' complexity='6' token_count='101'></file_info>
 		<method name='__getitem__' parameters='self,index'>
 				<method_info nloc='7' complexity='5' token_count='67' nesting_level='1' start_line='15' end_line='21'></method_info>
 			<added_lines>17,18,19,20,21</added_lines>
 			<deleted_lines>17</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fairseq\tasks\language_modeling.py' new_name='fairseq\tasks\language_modeling.py'>
 		<file_info nloc='210' complexity='28' token_count='1132'></file_info>
 		<method name='build_dataset_for_inference' parameters='self,src_tokens,src_lengths,kwargs'>
 				<method_info nloc='35' complexity='2' token_count='198' nesting_level='1' start_line='201' end_line='241'></method_info>
 			<added_lines>207,208,209,210,211,212,213,214,215,216,217,227,228,229,230</added_lines>
 			<deleted_lines>206,207,208,209,210,211,212,213,214,215,216</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>23</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
