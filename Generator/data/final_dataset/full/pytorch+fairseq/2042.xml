<bug_data>
<bug id='2042' author='mohammadKhalifa' open_date='2020-04-22T08:11:49Z' closed_time='2020-05-01T11:05:58Z'>
 	<summary>Masked LM training with BERT fails</summary>
 	<description>
 Training with a masked lm objective as follows:
 fairseq-train $DATA_DIR \
 		--task masked_lm --criterion masked_lm\
 		--arch bert_base \
 		--optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 1.0 \
 		--lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \
 		--dropout 0.1 --weight-decay 0.01 \
 		--max-tokens $TOKENS_PER_SAMPLE --update-freq $UPDATE_FREQ \
 		--max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --save-interval-updates 1000\
 		--valid-subset valid \
 		--mask-prob 0.25\
 		--random-token-prob 0.2\
 		--skip-invalid-size-inputs-valid-test \
 Gives me the following exception:
 &lt;denchmark-code&gt;File "/home/mkhalifa/py36/bin/fairseq-train", line 11, in &lt;module&gt;
     load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()
   File "/home/mkhalifa/fairseq/fairseq_cli/train.py", line 321, in cli_main
     main(args)
   File "/home/mkhalifa/fairseq/fairseq_cli/train.py", line 96, in main
     train(args, trainer, task, epoch_itr)
   File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py", line 52, in inner
     return func(*args, **kwds)
   File "/home/mkhalifa/fairseq/fairseq_cli/train.py", line 176, in train
     log_output = trainer.train_step(samples)
   File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py", line 52, in inner
     return func(*args, **kwds)
   File "/home/mkhalifa/fairseq/fairseq/trainer.py", line 319, in train_step
     ignore_grad=is_dummy_batch,
   File "/home/mkhalifa/fairseq/fairseq/tasks/fairseq_task.py", line 337, in train_step
     loss, sample_size, logging_output = criterion(model, sample)
   File "/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
     result = self.forward(*input, **kwargs)
   File "/home/mkhalifa/fairseq/fairseq/criterions/masked_lm.py", line 52, in forward
     ignore_index=self.padding_idx,
   File "/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/functional.py", line 1836, in nll_loss
     .format(input.size(0), target.size(0)))
 ValueError: Expected input batch_size (892) to match target batch_size (223).
 
 &lt;/denchmark-code&gt;
 
 The same error occurs when using other masked_lm architectures such as xlm_base. Roberta architectures work fine.
 	</description>
 	<comments>
 		<comment id='1' author='mohammadKhalifa' date='2020-04-22T14:43:33Z'>
 		Confirmed that I can reproduce this on my end.  CC &lt;denchmark-link:https://github.com/ngoyal2707&gt;@ngoyal2707&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
 , it seems that &lt;denchmark-link:https://github.com/pytorch/fairseq/commit/718677ebb044e27aaf1a30640c2f7ab6b8fa8509&gt;718677e&lt;/denchmark-link&gt;
  breaks .  Specifically,  doesn't do anything with the  parameter in its  method.
 		</comment>
 		<comment id='2' author='mohammadKhalifa' date='2020-04-23T09:52:14Z'>
 		&lt;denchmark-link:https://github.com/lematt1991&gt;@lematt1991&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ngoyal2707&gt;@ngoyal2707&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/myleott&gt;@myleott&lt;/denchmark-link&gt;
 
 I created a pull request &lt;denchmark-link:https://github.com/pytorch/fairseq/pull/2050&gt;#2050&lt;/denchmark-link&gt;
  that fixed the issue
 		</comment>
 	</comments>
 </bug>
<commit id='1f2bf68f95f2676a600cabb549180c974f8c3a56' author='Muhammad' date='2020-05-01 04:06:02-07:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fairseq\models\masked_lm.py' new_name='fairseq\models\masked_lm.py'>
 		<file_info nloc='247' complexity='30' token_count='1776'></file_info>
 		<method name='forward' parameters='self,src_tokens,segment_labels,unused'>
 				<method_info nloc='23' complexity='6' token_count='171' nesting_level='1' start_line='191' end_line='242'></method_info>
 			<added_lines>191,221,222,223</added_lines>
 			<deleted_lines>191,231</deleted_lines>
 		</method>
 		<method name='forward' parameters='self,src_tokens,segment_labels,masked_tokens,unused'>
 				<method_info nloc='25' complexity='7' token_count='189' nesting_level='1' start_line='191' end_line='244'></method_info>
 			<added_lines>191,221,222,223</added_lines>
 			<deleted_lines>191,231</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
