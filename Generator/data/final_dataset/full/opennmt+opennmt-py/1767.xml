<bug_data>
<bug id='1767' author='ziyanyang' open_date='2020-04-01T03:51:18Z' closed_time='2020-04-03T07:53:21Z'>
 	<summary>'Field' object has no attribute 'vocab'</summary>
 	<description>
 Hi,
 I'm trying to do domain adaptation as described here(&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/768&gt;#768&lt;/denchmark-link&gt;
 ). I want to finetune a pre-trained model (from &lt;denchmark-link:https://opennmt.net/Models-py/&gt;https://opennmt.net/Models-py/&lt;/denchmark-link&gt;
 ) using multi30k and follow the instructions to pre-process the data here(&lt;denchmark-link:https://opennmt.net/OpenNMT-py/extended.html&gt;https://opennmt.net/OpenNMT-py/extended.html&lt;/denchmark-link&gt;
 ). However, when I retrain the model using:
 CUDA_VISIBLE_DEVICES=1,2 python train.py -world_size 2 -gpu_ranks 0 1 -batch_size 64 -encoder_type brnn -rnn_size 500 -save_model available_models/multi30k_finetune -data data/multi30k.atok.low -reset_optim keep_states -train_from available_models/iwslt-brnn2.s131_acc_62.71_ppl_7.74_e20.pt -learning_rate 0.1
 I get the error as:
 [2020-03-31 23:34:16,775 INFO] Loading dataset from data/multi30k.atok.low.train.0.pt
 [2020-03-31 23:34:17,091 INFO] number of examples: 29000
 [2020-03-31 23:34:17,474 INFO] Loading checkpoint from available_models/iwslt-brnn2.s131_acc_62.71_ppl_7.74_e20.pt
 [2020-03-31 23:34:17,771 INFO] Loading vocab from checkpoint at available_models/iwslt-brnn2.s131_acc_62.71_ppl_7.74_e20.pt.
 [2020-03-31 23:34:17,771 INFO]  * src vocab size = 35444
 [2020-03-31 23:34:17,771 INFO]  * tgt vocab size = 24725
 [2020-03-31 23:34:17,771 INFO] Building model...
 Process SpawnProcess-3:
 Traceback (most recent call last):
 File "/zf18/zy3cx/ENTER/envs/rnn_language_model/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
 self.run()
 File "/zf18/zy3cx/ENTER/envs/rnn_language_model/lib/python3.7/multiprocessing/process.py", line 99, in run
 self._target(*self._args, **self._kwargs)
 File "/net/zf18/zy3cx/OpenNMT-py/onmt/bin/train.py", line 115, in batch_producer
 b = next_batch(0)
 File "/net/zf18/zy3cx/OpenNMT-py/onmt/bin/train.py", line 111, in next_batch
 new_batch = next(generator_to_serve)
 File "/net/zf18/zy3cx/OpenNMT-py/onmt/inputters/inputter.py", line 822, in iter
 for batch in self._iter_dataset(path):
 File "/net/zf18/zy3cx/OpenNMT-py/onmt/inputters/inputter.py", line 804, in _iter_dataset
 for batch in cur_iter:
 File "/net/zf18/zy3cx/OpenNMT-py/onmt/inputters/inputter.py", line 695, in iter
 self.device)
 File "/zf18/zy3cx/ENTER/envs/rnn_language_model/lib/python3.7/site-packages/torchtext/data/batch.py", line 34, in init
 setattr(self, name, field.process(batch, device=device))
 File "/zf18/zy3cx/ENTER/envs/rnn_language_model/lib/python3.7/site-packages/torchtext/data/field.py", line 237, in process
 tensor = self.numericalize(padded, device=device)
 File "/zf18/zy3cx/ENTER/envs/rnn_language_model/lib/python3.7/site-packages/torchtext/data/field.py", line 338, in numericalize
 arr = [self.vocab.stoi[x] for x in arr]
 File "/zf18/zy3cx/ENTER/envs/rnn_language_model/lib/python3.7/site-packages/torchtext/data/field.py", line 338, in 
 arr = [self.vocab.stoi[x] for x in arr]
 AttributeError: 'Field' object has no attribute 'vocab'
 Does anyone meet similar problem? The vocab in iwslt-brnn2.s131_acc_62.71_ppl_7.74_e20.pt is an 'old style vocab', and I'm not sure if it makes the error.
 	</description>
 	<comments>
 		<comment id='1' author='ziyanyang' date='2020-04-01T15:53:01Z'>
 		Hmm not sure if this is linked but you would need to preprocess with the same vocab as the one of the model if you want to -train_from it.
 You can retrieve it from the checkpoint:
 &lt;denchmark-code&gt;import torch
 checkpoint = torch.load(&lt;checkpoint.pt&gt;)
 torch.save(checkpoint['vocab'], "vocab.pt")
 &lt;/denchmark-code&gt;
 
 And then preprocess using -src_vocab "vocab.pt".
 (Only -src_vocabis necessary here, as both src and tgt vocabs are stored in the .pt file.
 		</comment>
 		<comment id='2' author='ziyanyang' date='2020-04-01T20:00:24Z'>
 		
 Hmm not sure if this is linked but you would need to preprocess with the same vocab as the one of the model if you want to -train_from it.
 You can retrieve it from the checkpoint:
 import torch
 checkpoint = torch.load(&lt;checkpoint.pt&gt;)
 torch.save(checkpoint['vocab'], "vocab.pt")
 
 And then preprocess using -src_vocab "vocab.pt".
 (Only -src_vocabis necessary here, as both src and tgt vocabs are stored in the .pt file.
 
 Thank you for the suggestion. I tried it, but the error still exists.
 I found the pre-trained model's vocab has only [('src', &lt;torchtext.vocab.Vocab object at 0x7f2968601750&gt;), ('tgt', &lt;torchtext.vocab.Vocab object at 0x7f28f3d45d10&gt;)] two parts. However, in onmt/inputters/inputter.py line 692: yield torchtext.data.Batch(minibatch, self.dataset, self.device), the self.dataset will have dict_keys(['src', 'tgt', 'indices', 'corpus_id']) four fields. The last field 'corpus_id' is not built in the pre-trained model's vocab. The error AttributeError: 'Field' object has no attribute 'vocab' indicates this problem.
 		</comment>
 		<comment id='3' author='ziyanyang' date='2020-04-01T20:53:39Z'>
 		Oh yes, this field was added in &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/1732&gt;#1732&lt;/denchmark-link&gt;
 .
 We probably need to add a patch for such a case.
 		</comment>
 		<comment id='4' author='ziyanyang' date='2020-04-02T14:25:45Z'>
 		Hey &lt;denchmark-link:https://github.com/ziyanyang&gt;@ziyanyang&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/1769&gt;#1769&lt;/denchmark-link&gt;
  should fix this.
 Would you mind checking it's all good on your end before I merge?
 		</comment>
 		<comment id='5' author='ziyanyang' date='2020-04-02T19:31:09Z'>
 		
 Hey @ziyanyang
 #1769 should fix this.
 Would you mind checking it's all good on your end before I merge?
 
 Hi, in train.py the function patch_fields(opt, fields) will get the error as AttributeError: 'list' object has no attribute 'get'.
 This is because in function patch_fields: dvocab = torch.load(opt.data + '.vocab.pt') will try to load the vocab of the new data, but actually if I process the new data with old vocab, the new data's vocab will be the same as the old data's vocab. Therefore, this step will load the same vocab which is a list instead of a dictionary and still do not have 'corpus_id'.
 Is 'corpus_id' the same for most the text data? It includes {'': 0, '': 1, 'train': 2} in multi30k(using its own generated vocab). Is it only used to indicate the type of data?
 		</comment>
 		<comment id='6' author='ziyanyang' date='2020-04-02T19:51:31Z'>
 		
 Is 'corpus_id' the same for most the text data? It includes {'': 0, '': 1, 'train': 2} in multi30k(using its own generated vocab). Is it only used to indicate the type of data?
 
 It's for when we use multiple datasets (-data_ids / -train_ids). The corpus_id field was added in &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/1732&gt;#1732&lt;/denchmark-link&gt;
  to track from which corpus each example orgiginate, and apply noise on only examples from some of those datasets. It will probably also be useful in the future to apply different treatment to different datasets.
 		</comment>
 		<comment id='7' author='ziyanyang' date='2020-04-02T20:08:40Z'>
 		Ok, I just updated the PR. In preprocess, we will now add the corpus_id field to the existing vocab. And it will also update it to the 'new' dict format.
 Let me know if that works for you!
 		</comment>
 		<comment id='8' author='ziyanyang' date='2020-04-03T04:13:03Z'>
 		
 Ok, I just updated the PR. In preprocess, we will now add the corpus_id field to the existing vocab. And it will also update it to the 'new' dict format.
 Let me know if that works for you!
 
 It works fine now. Thank you so much!
 		</comment>
 	</comments>
 </bug>
<commit id='c20dbeac02688918607637f5f30ec73c0f17d817' author='FranÃ§ois Hernandez' date='2020-04-03 09:53:21+02:00'>
 	<dmm_unit complexity='0.7142857142857143' interfacing='0.8571428571428571' size='0.7142857142857143'></dmm_unit>
 	<modification change_type='MODIFY' old_name='onmt\bin\preprocess.py' new_name='onmt\bin\preprocess.py'>
 		<file_info nloc='253' complexity='38' token_count='1693'></file_info>
 		<modified_lines>
 			<added_lines>18,19,20,212,213,214,215,216,217,218,220,223,224,225,226,227,228,229,230,231,232</added_lines>
 			<deleted_lines>18,211,212,213,214,215,216,217</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onmt\bin\train.py' new_name='onmt\bin\train.py'>
 		<file_info nloc='156' complexity='33' token_count='1163'></file_info>
 		<method name='train' parameters='opt'>
 				<method_info nloc='59' complexity='10' token_count='429' nesting_level='0' start_line='20' end_line='93'></method_info>
 			<added_lines>45,46,47</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>14</added_lines>
 			<deleted_lines>14</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onmt\inputters\inputter.py' new_name='onmt\inputters\inputter.py'>
 		<file_info nloc='627' complexity='128' token_count='3958'></file_info>
 		<method name='patch_fields' parameters='opt,fields'>
 				<method_info nloc='5' complexity='2' token_count='45' nesting_level='0' start_line='192' end_line='196'></method_info>
 			<added_lines>192,193,194,195,196</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>197,198</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onmt\train_single.py' new_name='onmt\train_single.py'>
 		<file_info nloc='113' complexity='24' token_count='761'></file_info>
 		<method name='main' parameters='opt,device_id,batch_queue,semaphore'>
 				<method_info nloc='79' complexity='15' token_count='538' nesting_level='0' start_line='42' end_line='149'></method_info>
 			<added_lines>72,73,74</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>7</added_lines>
 			<deleted_lines>7</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
