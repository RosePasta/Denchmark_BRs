<bug_data>
<bug id='950' author='murthyrudra' open_date='2018-09-06T16:15:20Z' closed_time='2019-01-27T07:49:39Z'>
 	<summary>"-fix_word_vecs_enc" triggers an error when reloading an existing model</summary>
 	<description>
 Hi, I was training the Transformer model with the options as specified in &lt;denchmark-link:link&gt;http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model&lt;/denchmark-link&gt;
 . The training works fine, but the problem arises when i stop the training and resume the training using the option . I get the following error:
 &lt;denchmark-code&gt;    Traceback (most recent call last):
       File "train.py", line 40, in &lt;module&gt;
         main(opt)
       File "train.py", line 27, in main
         single_main(opt)
       File "/home/rudra/OpenNMT-py/onmt/train_single.py", line 113, in main
         optim = build_optim(model, opt, checkpoint)
       File "/home/rudra/OpenNMT-py/onmt/utils/optimizers.py", line 62, in build_optim
         optim.optimizer.load_state_dict(saved_optimizer_state_dict)
       File "/home/rudra/miniconda3/envs/py36/lib/python3.6/site-packages/torch/optim/optimizer.py", line         107, in load_state_dict
         raise ValueError("loaded state dict contains a parameter group "
     ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
 &lt;/denchmark-code&gt;
 
 The error occurs when the model tries to load the optimizer state in the File utils/optimizers.py line 62. Could anyone please help me in debugging this error?
 	</description>
 	<comments>
 		<comment id='1' author='murthyrudra' date='2018-09-06T16:45:53Z'>
 		Give your command lines, both the first initial training and the train_from
 		</comment>
 		<comment id='2' author='murthyrudra' date='2018-09-06T16:57:10Z'>
 		Hi, the initial command I ran was
 &lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES=1 python train.py -data data/demo -save_model model-transformer-baseline 
 -pre_word_vecs_enc data/embeddings.enc.pt -word_vec_size 300 -gpuid 1 -encoder_type transformer 
 -decoder_type transformer -layers 5 -rnn_size 300 -transformer_ff 500 -heads 5 -fix_word_vecs_enc 
 -max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens -normalization tokens 
 -accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 
 -learning_rate 1 -max_grad_norm 0 -param_init 0  -param_init_glorot -label_smoothing 0.1 
 -valid_steps 5000 -save_checkpoint_steps 5000
 &lt;/denchmark-code&gt;
 
 I stopped the training after 5000 steps and ran the following command
 &lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES=1 python train.py -data data/demo -save_model model-transformer-baseline 
 -pre_word_vecs_enc data/embeddings.enc.pt -word_vec_size 300 -gpuid 1 -encoder_type transformer 
 -decoder_type transformer -layers 5 -rnn_size 300 -transformer_ff 500 -heads 5 -fix_word_vecs_enc 
 -max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens -normalization tokens 
 -accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 
 -learning_rate 1 -max_grad_norm 0 -param_init 0  -param_init_glorot -label_smoothing 0.1 
 -valid_steps 5000 -save_checkpoint_steps 5000 -train_from model-transformer-baseline_step_5000.pt
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='murthyrudra' date='2018-09-06T18:05:13Z'>
 		it might not be theproblem but you are not on master. line 62 of optimizer.py is not the same.
 git pull and repost the error.
 		</comment>
 		<comment id='4' author='murthyrudra' date='2018-09-07T03:34:59Z'>
 		Hi, I cloned the master branch and ran the model using the same command as earlier. I am getting the same error. I am using PyTorch 0.4.0 and Cuda 9.0
 		</comment>
 		<comment id='5' author='murthyrudra' date='2018-09-07T05:33:48Z'>
 		I need the error trace to see the lines number
 		</comment>
 		<comment id='6' author='murthyrudra' date='2018-09-07T05:36:11Z'>
 		Hi, the error occurs when the optimizer tries to load the saved state, specifically in the code optim/optimizer.py in &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py&gt;https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py&lt;/denchmark-link&gt;
  line 107. I tried printing the contents of  and  and the  has length of 218 whereas  has length of 217 and the individual contents are in the attached file
 &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/files/2359639/error.txt&gt;error.txt&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='7' author='murthyrudra' date='2018-09-07T05:40:39Z'>
 		Hi, Please find the error traceback:
 &lt;denchmark-code&gt;traceback (most recent call last):
   File "train.py", line 40, in &lt;module&gt;
     main(opt)
   File "train.py", line 27, in main
     single_main(opt)
   File "/home/rudra/NMT/OpenNMT/OpenNMT-py/onmt/train_single.py", line 112, in main
     optim = build_optim(model, opt, checkpoint)
   File "/home/rudra/NMT/OpenNMT/OpenNMT-py/onmt/utils/optimizers.py", line 51, in build_optim
     optim.optimizer.load_state_dict(saved_optimizer_state_dict)
   File "/home/rudra/miniconda3/envs/py36/lib/python3.6/site-packages/torch/optim/optimizer.py", line 107, in load_state_dict
 raise ValueError("loaded state dict contains a parameter group "
 ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='8' author='murthyrudra' date='2018-09-07T06:34:05Z'>
 		can  you remove this -pre_word_vecs_enc data/embeddings.enc.pt (and maybe the -fix_word_vecs_enc) from the second command line ?
 		</comment>
 		<comment id='9' author='murthyrudra' date='2018-09-07T06:54:43Z'>
 		Hi, I am getting the same error. The error occurs when it tries to load the previously stored Optimizer's state. I did try bypassing the loading of Optimizer's state, but that results in perplexity being Nan values.
 		</comment>
 		<comment id='10' author='murthyrudra' date='2018-09-07T07:05:31Z'>
 		Just as a sanity check can you please retrain from scratch without -pre_word_vecs_enc -fix_word_vecs_enc
 save after afew steps, and then do a train_from ?
 thanks
 		</comment>
 		<comment id='11' author='murthyrudra' date='2018-09-07T08:26:20Z'>
 		Hi, If I train without specifying pre-trained word embeddings using pre_word_vecs_enc and the -fix_word_vecs_enc option and retraining using -train_from, the model works well. There is no error and the perplexity values seems good.
 Now, I tried specifying only pre-trained word embeddings using pre_word_vecs_enc and retraining after some steps, the model works well, with no error. The problem arises only when I specify the argument fix_word_vecs_enc.
 Thanks
 		</comment>
 		<comment id='12' author='murthyrudra' date='2019-01-27T03:19:34Z'>
 		The problem has beed solved by pull request &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/1211&gt;#1211&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='6a8b4b1b4cc1877e6267893e8f065a7126196045' author='Zhenxin Fu' date='2019-01-24 11:37:35+01:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='onmt\model_builder.py' new_name='onmt\model_builder.py'>
 		<file_info nloc='233' complexity='45' token_count='1455'></file_info>
 		<method name='build_base_model' parameters='model_opt,fields,gpu,checkpoint'>
 				<method_info nloc='88' complexity='25' token_count='678' nesting_level='0' start_line='174' end_line='303'></method_info>
 			<added_lines>295,298</added_lines>
 			<deleted_lines>291,294</deleted_lines>
 		</method>
 		<method name='build_embeddings' parameters='opt,word_field,feat_fields,for_encoder'>
 				<method_info nloc='23' complexity='5' token_count='157' nesting_level='0' start_line='28' end_line='61'></method_info>
 			<added_lines>44,45,46,58,59</added_lines>
 			<deleted_lines>55</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='onmt\modules\embeddings.py' new_name='onmt\modules\embeddings.py'>
 		<file_info nloc='154' complexity='14' token_count='814'></file_info>
 		<method name='__init__' parameters='self,word_vec_size,word_vocab_size,word_padding_idx,position_encoding,feat_merge,feat_vec_exponent,feat_vec_size,feat_padding_idx,feat_vocab_sizes,dropout,sparse'>
 				<method_info nloc='10' complexity='1' token_count='48' nesting_level='1' start_line='89' end_line='98'></method_info>
 			<added_lines>98</added_lines>
 			<deleted_lines>98</deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,word_vec_size,word_vocab_size,word_padding_idx,position_encoding,feat_merge,feat_vec_exponent,feat_vec_size,feat_padding_idx,feat_vocab_sizes,dropout,sparse,fix_word_vecs'>
 				<method_info nloc='11' complexity='1' token_count='52' nesting_level='1' start_line='89' end_line='99'></method_info>
 			<added_lines>98,99</added_lines>
 			<deleted_lines>98</deleted_lines>
 		</method>
 		<method name='load_pretrained_vectors' parameters='self,emb_file'>
 				<method_info nloc='11' complexity='4' token_count='91' nesting_level='1' start_line='173' end_line='189'></method_info>
 			<added_lines>173</added_lines>
 			<deleted_lines>186,187</deleted_lines>
 		</method>
 		<method name='load_pretrained_vectors' parameters='self,emb_file,fixed'>
 				<method_info nloc='13' complexity='5' token_count='105' nesting_level='1' start_line='169' end_line='187'></method_info>
 			<added_lines>173</added_lines>
 			<deleted_lines>169,186,187</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>160,161,162</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
