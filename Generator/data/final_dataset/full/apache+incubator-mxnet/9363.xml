<bug_data>
<bug id='9363' author='eric-haibin-lin' open_date='2018-01-09T23:05:47Z' closed_time='2018-03-12T19:33:39Z'>
 	<summary>Incorrect weight_decay implementation in AdaGrad</summary>
 	<description>
 In Adagrad, weight_decay should be applied to grad before clipping the gradient/updating the state/weight.
 &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L856-L858&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L856-L858&lt;/denchmark-link&gt;
 
 Also adadelta: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L982&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L982&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='eric-haibin-lin' date='2018-01-17T15:01:12Z'>
 		wd term is also not merged into grad in SGD. Should we fix them? Will it cause some problems for backward compatibility?
 cc &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='eric-haibin-lin' date='2018-01-17T15:45:22Z'>
 		For SGD, not merging in the wd term will linearly scale the “real” wd and the impact is smaller compared to Adagrad. However, the change may break lots of our current examples and I’m not sure whether to revise it or not. We need to investigate how the other packages implement the WD term.
 
 Get Outlook for iOS&lt;&lt;denchmark-link:https://aka.ms/o0ukef&gt;https://aka.ms/o0ukef&lt;/denchmark-link&gt;
 &gt;
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 ________________________________
 From: Ziyue Huang &lt;notifications@github.com&gt;
 Sent: Wednesday, January 17, 2018 7:01:29 AM
 To: apache/incubator-mxnet
 Cc: Xingjian SHI; Mention
 Subject: Re: [apache/incubator-mxnet] Incorrect weight_decay implementation in AdaGrad (#9363)
 
 
 wd term is also not merged into grad in SGD. Should we fix them? Will it cause some problems for backward compatibility?
 cc @sxjscience&lt;https://github.com/sxjscience&gt;
 
 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub&lt;#9363 (comment)&gt;, or mute the thread&lt;https://github.com/notifications/unsubscribe-auth/AE8D7tu5nAPXmIkEMh35dR1rS6yrQ5odks5tLgtJgaJpZM4RYksA&gt;.
 
 		</comment>
 		<comment id='3' author='eric-haibin-lin' date='2018-03-12T19:33:39Z'>
 		Closed by the PR. Is the AdaDelta solved?
 		</comment>
 		<comment id='4' author='eric-haibin-lin' date='2018-03-15T03:45:36Z'>
 		No, the change was reverted in the PR. My PR only contains a sparse op with Wd == 0
 		</comment>
 		<comment id='5' author='eric-haibin-lin' date='2018-03-15T04:17:26Z'>
 		OK, we need to reopen it then.
 
 Get Outlook for iOS&lt;&lt;denchmark-link:https://aka.ms/o0ukef&gt;https://aka.ms/o0ukef&lt;/denchmark-link&gt;
 &gt;
 &lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;
 
 
 ________________________________
 From: Haibin Lin &lt;notifications@github.com&gt;
 Sent: Wednesday, March 14, 2018 8:45:48 PM
 To: apache/incubator-mxnet
 Cc: Xingjian SHI; State change
 Subject: Re: [apache/incubator-mxnet] Incorrect weight_decay implementation in AdaGrad (#9363)
 
 
 No, the change was reverted in the PR. My PR only contains a sparse op with Wd == 0
 
 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub&lt;#9363 (comment)&gt;, or mute the thread&lt;https://github.com/notifications/unsubscribe-auth/AE8D7h68o3ArEYcJ96deH8NaLvVaOhW2ks5teePrgaJpZM4RYksA&gt;.
 
 		</comment>
 	</comments>
 </bug>
<commit id='fc9e70bf2d349ad4c6cb65ff3f0958e23a7410bf' author='Haibin Lin' date='2018-03-03 14:12:23+08:00'>
 	<dmm_unit complexity='1.0' interfacing='0.27702702702702703' size='0.20945945945945946'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\api\python\ndarray\sparse.md' new_name='docs\api\python\ndarray\sparse.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>487</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\mxnet\optimizer.py' new_name='python\mxnet\optimizer.py'>
 		<file_info nloc='1116' complexity='180' token_count='6443'></file_info>
 		<method name='update' parameters='self,index,weight,grad,state'>
 				<method_info nloc='21' complexity='5' token_count='193' nesting_level='1' start_line='1091' end_line='1113'></method_info>
 			<added_lines>1098,1102,1103,1104,1105,1106,1108,1109,1110</added_lines>
 			<deleted_lines>1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1111</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1074,1075,1076,1077</added_lines>
 			<deleted_lines>30,31,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\optimizer_op-inl.h' new_name='src\operator\optimizer_op-inl.h'>
 		<file_info nloc='1473' complexity='133' token_count='13343'></file_info>
 		<method name='mxnet::op::AdagradUpdateEx' parameters='attrs,ctx,inputs,req,outputs'>
 				<method_info nloc='15' complexity='3' token_count='146' nesting_level='2' start_line='1618' end_line='1632'></method_info>
 			<added_lines>1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='mxnet::op::AdagradStorageType' parameters='attrs,dev_mask,dispatch_mode,in_attrs,out_attrs'>
 				<method_info nloc='17' complexity='5' token_count='134' nesting_level='2' start_line='1512' end_line='1529'></method_info>
 			<added_lines>1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='mxnet::op::AdagradUpdateDnsRspDnsImpl' parameters='param,ctx,weight,grad,state,req,out'>
 				<method_info nloc='33' complexity='3' token_count='335' nesting_level='2' start_line='1559' end_line='1591'></method_info>
 			<added_lines>1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='mxnet::op::AdagradParam::DMLC_DECLARE_PARAMETER' parameters='AdagradParam'>
 				<method_info nloc='18' complexity='1' token_count='89' nesting_level='3' start_line='1492' end_line='1509'></method_info>
 			<added_lines>1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='mxnet::op::AdagradUpdateRspRspRspImpl' parameters='param,ctx,weight,grad,state,req,out'>
 				<method_info nloc='20' complexity='2' token_count='137' nesting_level='2' start_line='1594' end_line='1615'></method_info>
 			<added_lines>1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='mxnet::op::AdagradDnsRspDnsKernel::Map' parameters='i,row_length,out_data,state_data,weight_data,grad_idx,grad_data,clip_gradient,epsilon,lr,rescale_grad'>
 				<method_info nloc='21' complexity='3' token_count='187' nesting_level='3' start_line='1534' end_line='1555'></method_info>
 			<added_lines>1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1486,1487,1488,1489,1490,1491,1510,1511,1530,1531,1532,1533,1556,1558,1592,1593,1616,1617</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\optimizer_op.cc' new_name='src\operator\optimizer_op.cc'>
 		<file_info nloc='518' complexity='8' token_count='3051'></file_info>
 		<modified_lines>
 			<added_lines>41,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\optimizer_op.cu' new_name='src\operator\optimizer_op.cu'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>203,204,205</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_optimizer.py' new_name='tests\python\unittest\test_optimizer.py'>
 		<file_info nloc='793' complexity='171' token_count='7682'></file_info>
 		<method name='test_adagrad' parameters=''>
 				<method_info nloc='23' complexity='7' token_count='220' nesting_level='0' start_line='1009' end_line='1031'></method_info>
 			<added_lines>1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='create_state' parameters='self,index,weight'>
 				<method_info nloc='2' complexity='1' token_count='30' nesting_level='1' start_line='993' end_line='994'></method_info>
 			<added_lines>993,994</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,eps,kwargs'>
 				<method_info nloc='3' complexity='1' token_count='31' nesting_level='1' start_line='989' end_line='991'></method_info>
 			<added_lines>989,990,991</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='update' parameters='self,index,weight,grad,state'>
 				<method_info nloc='11' complexity='2' token_count='116' nesting_level='1' start_line='996' end_line='1007'></method_info>
 			<added_lines>996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,992,995,1008,1032,1033</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
