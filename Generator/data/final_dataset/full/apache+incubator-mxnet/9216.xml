<bug_data>
<bug id='9216' author='wkcn' open_date='2017-12-28T07:54:53Z' closed_time='2018-03-10T01:36:59Z'>
 	<summary>Loss of Precision in BatchNorm and output_var may be wrong</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 
 BatchNorm loses a little precision
 the output_var in BatchNorm may be wrong
 
 &lt;denchmark-h:h2&gt;Environment Info&lt;/denchmark-h&gt;
 
 OS: Arch Linux 4.14.8
 MXNet: 1.0.0 and 1.0.1 (the latest version, CPU version)
 &lt;denchmark-h:h2&gt;Build Config&lt;/denchmark-h&gt;
 
 make -j8 USE_OPENCV=1 USE_BLAS=openblas
 &lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;
 
 Hi, there.
 I converted &lt;denchmark-link:https://github.com/KaimingHe/deep-residual-networks&gt;ResNet Model on Caffe&lt;/denchmark-link&gt;
  to &lt;denchmark-link:https://github.com/wkcn/resnet-v1-mx&gt;ResNet model on MXNet&lt;/denchmark-link&gt;
 .
 And I found that the output results between Caffe and MXNet are different.
 The reason is that the computations of Caffe and MXNet are different.
 For the BatchNorm in Caffe, the output is (x - mean(x)) / sqrt(var(x) + eps).
 For the BatchNorm in MXNet, the output is (x - mean(x)) * factor, and factor = 1.0 / sqrt(var(x) + eps).
 I think the method in MXNet will lose a little precision but bring the higher performance (Reduce the times of division).
 At the same time, I found that the output_var in BatchNorm may be wrong.
 The output_var is invstd, namely the multiplicative inverse of the standard deviation. I think it should be the variance.
 &lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;
 
 Here is &lt;denchmark-link:https://github.com/wkcn/test_mxnet_bn&gt;my testing code&lt;/denchmark-link&gt;
 .
 I compare three outputs:
 
 numpy (compute manally)
 caffe
 mxnet
 
 &lt;denchmark-code&gt;caffe and numpy 0.0 0.0
 caffe and mx 16.0 2.36527e-07
 numpy and mx 16.0 2.36527e-07
 &lt;/denchmark-code&gt;
 
 The first column is the maxmimum absolute error, and the second column is the maxmimum relative error.
 &lt;denchmark-h:h1&gt;What I have tried to solve it&lt;/denchmark-h&gt;
 
 I change the BatchNorm implement in MXNet, and the output is below:
 &lt;denchmark-code&gt;caffe and numpy 0.0 0.0
 caffe and mx 0.0 0.0
 numpy and mx 0.0 0.0
 &lt;/denchmark-code&gt;
 
 The modified BatchNorm(cpu) code is &lt;denchmark-link:https://github.com/wkcn/incubator-mxnet/commit/5ecd4882bc043cf059e962f7ce488270bafa07c7&gt;here&lt;/denchmark-link&gt;
 .
 	</description>
 	<comments>
 		<comment id='1' author='wkcn' date='2017-12-30T00:18:24Z'>
 		What's the result if we use this line instead: &lt;denchmark-link:https://github.com/wkcn/test_mxnet_bn/blob/master/compute.py#L15&gt;https://github.com/wkcn/test_mxnet_bn/blob/master/compute.py#L15&lt;/denchmark-link&gt;
 . I think it may be the same as the MXNet version. Also, the rel_error is relatively small and should not be a problem.
 		</comment>
 		<comment id='2' author='wkcn' date='2017-12-30T05:34:59Z'>
 		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
  If using the line 15th, the result will be the same as that of MXNet.
 The rel_error is relatively small.
 However, there may be a bug.
 The &lt;denchmark-link:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html?highlight=batchnorm#mxnet.symbol.BatchNorm&gt;documents&lt;/denchmark-link&gt;
  said that
 output_mean_var (boolean, optional, default=0) â€“ Output All,normal mean and var
 &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147&lt;/denchmark-link&gt;
 
 The output_var is  now, I think it should be .
 		</comment>
 		<comment id='3' author='wkcn' date='2017-12-30T05:53:37Z'>
 		Looks like a bug, I'm not sure. &lt;denchmark-link:https://github.com/piiswrong&gt;@piiswrong&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/cjolivier01&gt;@cjolivier01&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='wkcn' date='2018-01-02T01:20:21Z'>
 		Caffe and MXNet will get the same BN output as I've tested before.
 Please be caution that CUDNN cannot handle BN when eps &lt;= 1e-5. In this situation, you can set cudnn_off=true in BN of MXNet.
 &lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
  ,Models of &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/9215&gt;#9215&lt;/denchmark-link&gt;
  get a very lower error rate.
 		</comment>
 		<comment id='5' author='wkcn' date='2018-01-02T03:58:10Z'>
 		&lt;denchmark-link:https://github.com/chinakook&gt;@chinakook&lt;/denchmark-link&gt;
  Thank you!
 I wrote &lt;denchmark-link:https://github.com/wkcn/test_mxnet_bn/tree/ndarray&gt;a new code&lt;/denchmark-link&gt;
  to test.
 And I found that using CUDNN and not using CUDNN on Caffe get the same BN output.
 But there is a little difference between the CPU result and the GPU result on Caffe.
 &lt;denchmark-code&gt;max absolute error, max relative error, mean error
 8.0, 3.4666334e-07, 0.024640804
 &lt;/denchmark-code&gt;
 
 The comparison between MXNet and Caffe is showed below.
 Using the GPU result of Caffe.
 &lt;denchmark-code&gt;('context: ', cpu(0))
 ('cudnn_off: ', True)
 ndarray: y = gamma * [(x - mean) / sqrt(var + eps)] + beta
 
                 max absolute error, max relative error, mean error
 ('caffe and ndarray', 8.0, 3.4666334e-07, 0.024640804)
 ('caffe and mx', 16.0, 3.4666334e-07, 0.057109512)
 ('ndarray and mx', 16.0, 2.3652711e-07, 0.038025968)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;('context: ', gpu(0))
 ('cudnn_off: ', True)
 ndarray: y = gamma * [(x - mean) / sqrt(var + eps)] + beta
 
                 max absolute error, max relative error, mean error
 ('caffe and ndarray', 8.0, 3.4666334e-07, 0.024640804)
 ('caffe and mx', 16.0, 2.3644267e-07, 0.052587245)
 ('ndarray and mx', 16.0, 2.4033051e-07, 0.054628547)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;('context: ', gpu(0))
 ('cudnn_off: ', False)
 ndarray: y = gamma * [(x - mean) / sqrt(var + eps)] + beta
 
                 max absolute error, max relative error, mean error
 ('caffe and ndarray', 8.0, 3.4666334e-07, 0.024640804)
 ('caffe and mx', 16.0, 2.3644267e-07, 0.052587245)
 ('ndarray and mx', 16.0, 2.4033051e-07, 0.054628547)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='6' author='wkcn' date='2018-01-02T04:05:00Z'>
 		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
  Can you confirm that the output is invstd instead of var?
 		</comment>
 		<comment id='7' author='wkcn' date='2018-01-02T04:06:58Z'>
 		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
  These errors are too small and will cause no problem. However, there is certainly a bug if the output is invstd instead of var.
 		</comment>
 		<comment id='8' author='wkcn' date='2018-01-02T04:44:44Z'>
 		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 
 Thank you!
 I'm sure that the output_var is invstd instead of variance.
 &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L161&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L161&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='9' author='wkcn' date='2018-01-02T21:18:56Z'>
 		the documentation should be fixed
 		</comment>
 		<comment id='10' author='wkcn' date='2018-01-05T18:31:07Z'>
 		I find it will be improper to output the invstd as the value will be undefined when batch_size=1. We should consider to change it back to output the variance.
 		</comment>
 		<comment id='11' author='wkcn' date='2018-01-05T18:41:42Z'>
 		Just FYI, CUDNN stores invstd for moving_variance rather than actual variance, if I remember correctly.
 This can be seen in the unit test not checking for those being the same when CUDNN is enabled: 
 
 
 incubator-mxnet/tests/cpp/operator/batchnorm_test.cc
 
 
          Line 409
       in
       f9fd88b
 
 
 
 
 
 
  #if MXNET_USE_CUDNN != 1 /* CUDNN takes a different approach here on first pass */ 
 
 
 
 
 
 		</comment>
 		<comment id='12' author='wkcn' date='2018-01-05T18:52:35Z'>
 		&lt;denchmark-link:https://github.com/cjolivier01&gt;@cjolivier01&lt;/denchmark-link&gt;
  I see, we need to test the correctness of the mean and "var" as we can fetch these two values by turning on . &lt;denchmark-link:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.BatchNorm&gt;https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.BatchNorm&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='13' author='wkcn' date='2018-01-05T18:55:47Z'>
 		I believe that it was stored as invstd in order to avoid calculating it back and forth between forward and backward pass, but I am open to it being adjusted as long as everything in tests/cpp/batchnorm_test.cc still passes.
 		</comment>
 		<comment id='14' author='wkcn' date='2018-01-05T19:05:02Z'>
 		&lt;denchmark-link:https://github.com/cjolivier01&gt;@cjolivier01&lt;/denchmark-link&gt;
  In that case, it should be okay to keep the invstd format. We need to test the case when  is turned on.
 		</comment>
 	</comments>
 </bug>
<commit id='279ccb1c77d7fdfb2652a4f6574466ea1ecb3a09' author='Xingjian Shi' date='2018-03-09 17:36:58-08:00'>
 	<dmm_unit complexity='0.3250883392226148' interfacing='0.09540636042402827' size='0.15547703180212014'></dmm_unit>
 	<modification change_type='MODIFY' old_name='docs\api\python\gluon\nn.md' new_name='docs\api\python\gluon\nn.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>23</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\api\python\ndarray\ndarray.md' new_name='docs\api\python\ndarray\ndarray.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>643</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='docs\api\python\symbol\symbol.md' new_name='docs\api\python\symbol\symbol.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>644</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\mxnet\gluon\nn\basic_layers.py' new_name='python\mxnet\gluon\nn\basic_layers.py'>
 		<file_info nloc='595' complexity='66' token_count='2467'></file_info>
 		<method name='__repr__' parameters='self'>
 				<method_info nloc='8' complexity='2' token_count='78' nesting_level='1' start_line='584' end_line='591'></method_info>
 			<added_lines>584,585,586,587,588,589,590,591</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__init__' parameters='self,axis,epsilon,center,scale,beta_initializer,gamma_initializer,in_channels,prefix,params'>
 				<method_info nloc='3' complexity='1' token_count='44' nesting_level='1' start_line='564' end_line='566'></method_info>
 			<added_lines>564,565,566</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='hybrid_forward' parameters='self,F,data,gamma,beta'>
 				<method_info nloc='3' complexity='1' token_count='43' nesting_level='1' start_line='580' end_line='582'></method_info>
 			<added_lines>580,581,582</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>22,422,423,424,425,430,432,433,482,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,567,568,569,570,571,572,573,574,575,576,577,578,579,583,592,593</added_lines>
 			<deleted_lines>22,422,427,429,478</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\nn\batch_norm-inl.h' new_name='src\operator\nn\batch_norm-inl.h'>
 		<file_info nloc='287' complexity='37' token_count='2298'></file_info>
 		<method name='mxnet::op::BatchNormParam::DMLC_DECLARE_PARAMETER' parameters='BatchNormParam'>
 				<method_info nloc='19' complexity='1' token_count='124' nesting_level='3' start_line='69' end_line='87'></method_info>
 			<added_lines>82</added_lines>
 			<deleted_lines>82</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\nn\batch_norm.cc' new_name='src\operator\nn\batch_norm.cc'>
 		<file_info nloc='458' complexity='59' token_count='3904'></file_info>
 		<modified_lines>
 			<added_lines>513,514</added_lines>
 			<deleted_lines>513</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='src\operator\nn\layer_norm-inl.h'>
 		<file_info nloc='229' complexity='15' token_count='2256'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='src\operator\nn\layer_norm.cc'>
 		<file_info nloc='111' complexity='4' token_count='781'></file_info>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='src\operator\nn\layer_norm.cu'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_gluon.py' new_name='tests\python\unittest\test_gluon.py'>
 		<file_info nloc='654' complexity='91' token_count='6850'></file_info>
 		<method name='test_layernorm' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='28' nesting_level='0' start_line='385' end_line='387'></method_info>
 			<added_lines>385,386,387</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>384,388</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_operator.py' new_name='tests\python\unittest\test_operator.py'>
 		<file_info nloc='4189' complexity='654' token_count='53512'></file_info>
 		<method name='test_layer_norm' parameters=''>
 				<method_info nloc='6' complexity='5' token_count='78' nesting_level='0' start_line='2449' end_line='2454'></method_info>
 			<added_lines>2449,2450,2451,2452,2453,2454</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='check_layer_normalization.npy_layer_norm' parameters='data,gamma,beta,axis,eps'>
 				<method_info nloc='11' complexity='3' token_count='122' nesting_level='1' start_line='2417' end_line='2427'></method_info>
 			<added_lines>2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='check_layer_normalization' parameters='in_shape,axis,eps,dtype'>
 				<method_info nloc='21' complexity='2' token_count='297' nesting_level='0' start_line='2416' end_line='2447'></method_info>
 			<added_lines>2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>2448,2455,2456</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
