<bug_data>
<bug id='19463' author='sxjscience' open_date='2020-11-02T07:18:05Z' closed_time='2020-11-19T23:58:33Z'>
 	<summary>[Bug][AMP][2.0] AMP issue of the concatenate operator</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 Reproducible example:
 import mxnet as mx
 from mxnet.gluon import nn
 from mxnet import amp
 mx.npx.set_np()
 amp.init()
 
 
 class Foo(nn.HybridBlock):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.dense0 = nn.Dense(16, in_units=8)
 
     def forward(self, x):
         return mx.np.concatenate([self.dense0(x), x], axis=-1)
 
 foo = Foo()
 foo.initialize(ctx=mx.gpu())
 
 data = mx.np.random.normal(0, 1, (32, 8), ctx=mx.gpu())
 out = foo(data)
 print(out.dtype)
 Output:
 &lt;denchmark-code&gt;---------------------------------------------------------------------------
 MXNetError                                Traceback (most recent call last)
 &lt;ipython-input-9-07e1c93ce642&gt; in &lt;module&gt;
      18 
      19 data = mx.np.random.normal(0, 1, (32, 8), ctx=mx.gpu())
 ---&gt; 20 out = foo(data)
      21 print(out.dtype)
 
 ~/.local/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, x, *args)
    1419             if not self._active:
    1420                 # Normal imperative computation of forward()
 -&gt; 1421                 return super().__call__(x, *args)
    1422 
    1423             if dc.is_deferred_compute():
 
 ~/.local/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)
     709             hook(self, args)
     710 
 --&gt; 711         out = self.forward(*args)
     712 
     713         for hook in self._forward_hooks.values():
 
 &lt;ipython-input-9-07e1c93ce642&gt; in forward(self, x)
      12 
      13     def forward(self, x):
 ---&gt; 14         return mx.np.concatenate([self.dense0(x), x], axis=-1)
      15 
      16 foo = Foo()
 
 ~/.local/lib/python3.6/site-packages/mxnet/numpy/multiarray.py in concatenate(seq, axis, out)
    6520     array([1., 2., 3., 4., 5., 6.])
    6521     """
 -&gt; 6522     return _mx_nd_np.concatenate(seq, axis=axis, out=out)
    6523 
    6524 
 
 ~/.local/lib/python3.6/site-packages/mxnet/ndarray/numpy/_op.py in concatenate(seq, axis, out)
    4522            [3., 4., 6.]])
    4523     """
 -&gt; 4524     return _api_internal.concatenate(*seq, axis, out)
    4525 
    4526 
 
 ~/.local/lib/python3.6/site-packages/mxnet/_ffi/_ctypes/function.py in __call__(self, *args)
     113                 self.handle, values, tcodes, ctypes.c_int(num_args),
     114                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:
 --&gt; 115             raise get_last_ffi_error()
     116         _ = temp_args
     117         _ = args
 
 MXNetError: MXNetError: Type inconsistent, Provided = float32, inferred type = float16
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://github.com/mk-61&gt;@mk-61&lt;/denchmark-link&gt;
  Would you take a look on this? I'm not super familiar with the amp code so it may take me more time to resolve this issue.
 	</description>
 	<comments>
 		<comment id='1' author='sxjscience' date='2020-11-03T23:49:13Z'>
 		Removed the  because &lt;denchmark-link:https://github.com/mk-61&gt;@mk-61&lt;/denchmark-link&gt;
  is able to reproduce the issue per offline discussion.
 		</comment>
 	</comments>
 </bug>
<commit id='66488662ea6d33b8aeb64b632ea45194181f794a' author='mk-61' date='2020-11-19 15:58:32-08:00'>
 	<dmm_unit complexity='0.3448275862068966' interfacing='1.0' size='0.3448275862068966'></dmm_unit>
 	<modification change_type='MODIFY' old_name='ci\docker\runtime_functions.sh' new_name='ci\docker\runtime_functions.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>783,785,837,842,857,862,874,879,905,910</added_lines>
 			<deleted_lines>783,836,855,871,901</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\mxnet\amp\amp.py' new_name='python\mxnet\amp\amp.py'>
 		<file_info nloc='605' complexity='107' token_count='4571'></file_info>
 		<method name='_get_nd_fun_to_wrap' parameters='name,module,submodule_dict'>
 				<method_info nloc='17' complexity='5' token_count='92' nesting_level='0' start_line='67' end_line='83'></method_info>
 			<added_lines>83</added_lines>
 			<deleted_lines>83</deleted_lines>
 		</method>
 		<method name='_get_np_fun_to_wrap' parameters='name,ns_prefix'>
 				<method_info nloc='19' complexity='8' token_count='158' nesting_level='0' start_line='85' end_line='103'></method_info>
 			<added_lines>90,92,93,94,95,96,97,98,99,100,101,102,103</added_lines>
 			<deleted_lines>90,92,93,94,95,96</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>219,220,229,230,239,240,247,248</added_lines>
 			<deleted_lines>212,213,219,220,224,225,230,231,236,237,242,243,244,247,248,253,254</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\python\gpu\test_amp_init.py'>
 		<file_info nloc='27' complexity='5' token_count='225'></file_info>
 	</modification>
 </commit>
</bug_data>
