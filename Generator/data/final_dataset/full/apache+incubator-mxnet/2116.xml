<bug_data>
<bug id='2116' author='helloworldlxb' open_date='2016-05-12T03:57:15Z' closed_time='2018-09-25T05:42:34Z'>
 	<summary>Shape of labels does not match shape of predictions</summary>
 	<description>
 I'm using the official LSTM model with my own iterator(&lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/2105&gt;#2105&lt;/denchmark-link&gt;
 ).
 However, it reports "Shape of labels does not match shape of predictions" even though I reshaped the variable label in the model.
 The codes for the model:
 &lt;denchmark-code&gt;def lstm_unroll(batch_size, num_lstm_layer, seq_len, 
                 num_hidden, num_label, dropout=0.):
 
     # embed_weight = mx.sym.Variable("embed_weight")
     cls_weight = mx.sym.Variable("cls_weight")
     cls_bias = mx.sym.Variable("cls_bias")
     param_cells = []
     last_states = []
     for i in range(num_lstm_layer):
         param_cells.append(LSTMParam(i2h_weight=mx.sym.Variable("l%d_i2h_weight" % i),
                                      i2h_bias=mx.sym.Variable("l%d_i2h_bias" % i),
                                      h2h_weight=mx.sym.Variable("l%d_h2h_weight" % i),
                                      h2h_bias=mx.sym.Variable("l%d_h2h_bias" % i)))
         state = LSTMState(c=mx.sym.Variable("l%d_init_c" % i),
                           h=mx.sym.Variable("l%d_init_h" % i))
         last_states.append(state)
     assert(len(last_states) == num_lstm_layer)
 
     # convolution layers
     data = mx.sym.Variable('data')
     label = mx.sym.Variable('softmax_label')
     conv1_weight = mx.sym.Variable('conv1_weight')
     conv1_bias = mx.sym.Variable('conv1_bias')
     conv2_weight = mx.sym.Variable('conv2_weight')
     conv2_bias = mx.sym.Variable('conv2_bias')
     # pool2 = [[]] * seq_len
     # for i in range(seq_len):
     # print data.infer_shape()
     pool = []
     data = mx.sym.SliceChannel(data=data, num_outputs=batch_size, axis=0, squeeze_axis=True)
     for i in range(batch_size):
         conv1 = mx.sym.Convolution(data=data[i], kernel=(3,3), num_filter=100, weight=conv1_weight, bias=conv1_bias)
         # print conv1.infer_shape()
         conv1 = mx.sym.Activation(data=conv1, act_type='relu')
         pool1 = mx.sym.Pooling(data=conv1, kernel=(3,3), stride=(2,2), pool_type='max')
         conv2 = mx.sym.Convolution(data=pool1, kernel=(3,3), num_filter=100, weight=conv2_weight, bias=conv2_bias)
         conv2 = mx.sym.Activation(data=conv2, act_type='relu')
         pool2 = mx.sym.Pooling(data=conv2, kernel=(3,3), stride=(2,2), pool_type='max')
         # shape = pool2.infer_shape(**{'data':(batch_size, seq_len, 1, 120, 160)})[-2][0]
         pool2 = mx.sym.Reshape(data=pool2, target_shape=(1, 500, 100, 28, 38))
         pool.append(pool2)
     # embed = mx.sym.Embedding(data=data, input_dim=input_size,
     #                          weight=embed_weight, output_dim=num_embed, name='embed')
     # wordvec = mx.sym.SliceChannel(data=embed, num_outputs=seq_len, squeeze_axis=1)
     pool = mx.sym.Concat(*pool, dim=0)
     # pool = mx.sym.SliceChannel(data=pool, num_outputs=batch_size, axis=0)
     data_seq = mx.sym.SliceChannel(data=pool, num_outputs=seq_len, axis=1, squeeze_axis=True)
     hidden_all = []
     for seqidx in range(seq_len):
         hidden = data_seq[seqidx]
         # stack LSTM
         for i in range(num_lstm_layer):
             if i == 0:
                 dp_ratio = 0.
             else:
                 dp_ratio = dropout
             next_state = lstm(num_hidden, indata=hidden,
                               prev_state=last_states[i],
                               param=param_cells[i],
                               seqidx=seqidx, layeridx=i, dropout=dp_ratio)
             hidden = next_state.h
             last_states[i] = next_state
         # decoder
         if dropout &gt; 0.:
             hidden = mx.sym.Dropout(data=hidden, p=dropout)
         hidden_all.append(hidden)
 
     hidden_concat = mx.sym.Concat(*hidden_all, dim=0)
 
     pred = mx.sym.FullyConnected(data=hidden_concat, num_hidden=num_label,
                                  weight=cls_weight, bias=cls_bias, name='pred')
 
     ################################################################################
     # Make label the same shape as our produced data path
     # I did not observe big speed difference between the following two ways
 
     label = mx.sym.transpose(data=label)
     label = mx.sym.Reshape(data=label, target_shape=(batch_size * seq_len,))
 
     #label_slice = mx.sym.SliceChannel(data=label, num_outputs=seq_len)
     #label = [label_slice[t] for t in range(seq_len)]
     #label = mx.sym.Concat(*label, dim=0)
     #label = mx.sym.Reshape(data=label, target_shape=(0,))
     ################################################################################
     # return label
     sm = mx.sym.SoftmaxOutput(data=pred, label=label, name='softmax')
 
     return sm
 &lt;/denchmark-code&gt;
 
 Thank you in advance!
 	</description>
 	<comments>
 		<comment id='1' author='helloworldlxb' date='2016-05-12T07:26:00Z'>
 		Do you get this error in metric?
 		</comment>
 		<comment id='2' author='helloworldlxb' date='2016-05-12T07:31:22Z'>
 		&lt;denchmark-link:https://github.com/Godricly&gt;@Godricly&lt;/denchmark-link&gt;
  Yes. The full traceback:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "video_lstm_2.py", line 277, in &lt;module&gt;
     model = mx.model.FeedForward.create(m, X=iter, eval_data=val_iter, num_epoch
  = 200, learning_rate=0.01, epoch_end_callback=a_epoch, batch_end_callback=mx.ca
 llback.Speedometer(batch_size))
   File "F:\PublicToolbox\Anaconda\lib\site-packages\mxnet-0.5.0-py2.7.egg\mxnet\
 model.py", line 900, in create
     eval_batch_end_callback=eval_batch_end_callback)
   File "F:\PublicToolbox\Anaconda\lib\site-packages\mxnet-0.5.0-py2.7.egg\mxnet\
 model.py", line 783, in fit
     sym_gen=self.sym_gen)
   File "F:\PublicToolbox\Anaconda\lib\site-packages\mxnet-0.5.0-py2.7.egg\mxnet\
 model.py", line 245, in _train_multi_device
     executor_manager.update_metric(eval_metric, data_batch.label)
   File "F:\PublicToolbox\Anaconda\lib\site-packages\mxnet-0.5.0-py2.7.egg\mxnet\
 executor_manager.py", line 406, in update_metric
     self.curr_execgrp.update_metric(metric, labels)
   File "F:\PublicToolbox\Anaconda\lib\site-packages\mxnet-0.5.0-py2.7.egg\mxnet\
 executor_manager.py", line 262, in update_metric
     metric.update(labels_slice, texec.outputs)
   File "F:\PublicToolbox\Anaconda\lib\site-packages\mxnet-0.5.0-py2.7.egg\mxnet\
 metric.py", line 141, in update
     check_label_shapes(label, pred_label)
   File "F:\PublicToolbox\Anaconda\lib\site-packages\mxnet-0.5.0-py2.7.egg\mxnet\
 metric.py", line 20, in check_label_shapes
     "predictions {}".format(label_shape, pred_shape))
 ValueError: Shape of labels 1 does not match shape of predictions 500
 &lt;/denchmark-code&gt;
 
 And I tried to print the labels, it should have been an NDArray shaped like (500,), however it actually shaped like (1,500,)
 		</comment>
 		<comment id='3' author='helloworldlxb' date='2016-05-12T08:07:14Z'>
 		A stupid way I used is to write another metric to bypass this check and handle your data label urself. I think mxnet is giving you the shape of your raw label(before reshape). I can't remember the exact reason as my coworker get into this problem before, It probably because the metric is using data_batch.label in its update in the model.py. &lt;denchmark-link:https://github.com/piiswrong&gt;@piiswrong&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='4' author='helloworldlxb' date='2016-07-18T09:50:12Z'>
 		&lt;denchmark-link:https://github.com/helloworldlxb&gt;@helloworldlxb&lt;/denchmark-link&gt;
  Hi! Have you fixed your problem? I am having exactly the same issue here. Any wisdom please? Many thanks!
 		</comment>
 		<comment id='5' author='helloworldlxb' date='2016-10-23T17:36:43Z'>
 		For everyone's information, hereby I would like to share my solution to this problem. Although the structure of my customized networks is different, I also encountered the same error, i.e. "Shape of labels does not match shape of predictions".
 I agree with &lt;denchmark-link:https://github.com/Godricly&gt;@Godricly&lt;/denchmark-link&gt;
   that this is kind of a bug of the metric. Even we have reshaped the label to 1D vector, the metric will use the shape provided by the data batch, i.e. (1, 500,) in your case. Hence, to address this issue, the straight way is to modify your customized DataIter code to make sure the labels provided are a 1D vector.
 However, if you have labels for each time step, the solution above doesn't work. Here is another ugly trick for both cases. Just modify the codes in the metric.py (of course, you need root/sudo permission) to squeeze the labels. The squeeze function will remove the single dimension. If I remember correctly, there is one line like "label = labels[i].asnumpy().astype("int32")". Below it, you could add a new line "label = label.squeeze()". I know this sounds crazy, but it works anyway.
 Hope my reply would help you guys.
 		</comment>
 		<comment id='6' author='helloworldlxb' date='2017-09-28T06:59:34Z'>
 		This issue is closed due to lack of activity in the last 90 days. Feel free to reopen if this is still an active issue. Thanks!
 		</comment>
 		<comment id='7' author='helloworldlxb' date='2018-01-28T09:17:29Z'>
 		So mxnet just ignore this bug and leave it alone???
 		</comment>
 		<comment id='8' author='helloworldlxb' date='2018-01-28T16:52:34Z'>
 		&lt;denchmark-link:https://github.com/Matafight&gt;@Matafight&lt;/denchmark-link&gt;
  are you still running into this issue on certain metric?
 		</comment>
 		<comment id='9' author='helloworldlxb' date='2018-01-29T04:46:30Z'>
 		&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
  yes, at first, I thought there may be sth wrong with my own iterator, just now I tried with mx.io.NDArrayIter, the same error occuredã€‚
 		</comment>
 		<comment id='10' author='helloworldlxb' date='2018-01-29T04:52:31Z'>
 		&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
  the error  is:
 
 And the simplified codes that can reproduce the above issue is as follows:
 #_*_coding:utf-8_*_
 import mxnet as mx
 import mxnet.ndarray as nd
 
 #setup parameters
 def get_parameters():
     batch_size = 128
     embed_dim = 200
     num_steps = 90
     num_hidden = 100
     return batch_size,embed_dim,num_steps,num_hidden
 
 def try_gpu():
     try:
         ctx=mx.gpu()
         _ = nd.array([0],ctx=ctx)
     except:
         ctx=mx.cpu()
     return ctx
 
 #setup context
 context = try_gpu()
 
 #load data
 def test_mx_io_dataiter(batch_size,num_steps):
     corpus_vec = nd.ones(shape = (1024,num_steps),ctx=context)
     dataiter = mx.io.NDArrayIter(data = corpus_vec,label=corpus_vec,batch_size=batch_size,data_name='data',label_name='softmax_label')
     return dataiter
 
 def poem_rnn(batch_size,vocab_size,embed_dim,num_hidden,num_steps):
     seq_input = mx.symbol.Variable('data')
     label = mx.sym.Variable('softmax_label')
     embedded_seq = mx.symbol.Embedding(data=seq_input, 
                                        input_dim=vocab_size, 
                                        output_dim=embed_dim)
     lstm_cell = mx.rnn.LSTMCell(num_hidden=num_hidden)
     #NTC : batch_size *num_steps*input_embed_dim
     outputs, _ = lstm_cell.unroll(length=num_steps, 
                                        inputs=embedded_seq, 
                                        layout='NTC', 
                                        merge_outputs=True)
 
     pred = mx.sym.Reshape(outputs,shape=(-1,num_hidden))
     pred = mx.sym.FullyConnected(data = pred,num_hidden=vocab_size,name='pred')
     label = mx.sym.Reshape(label,shape=(-1,))
     pred = mx.sym.SoftmaxOutput(data = pred,label=label,name='softmax')
     return pred
 
 
 batch_size,embed_dim,num_steps,num_hidden = get_parameters()
 vocab_size  = 1500
 
 data_iter = test_mx_io_dataiter(batch_size,num_steps)
 output = poem_rnn(batch_size,vocab_size,embed_dim,num_hidden,num_steps)
 mod = mx.mod.Module(output,data_names=["data"],label_names=["softmax_label"])
 model_prefix = 'poem_rnn'
 save_period = 1
 checkpoint = mx.callback.do_checkpoint(model_prefix,period = save_period)
 mod.fit(data_iter,num_epoch=5,epoch_end_callback=checkpoint)
 I'am not sure  if it is actually the same bug or there is some other bugs in my codes
 		</comment>
 		<comment id='11' author='helloworldlxb' date='2018-01-29T06:42:33Z'>
 		mod.fit(data_iter,num_epoch=5,epoch_end_callback=checkpoint)
 The default evel_metric above is 'acc'.
 However, when I choose to use other metrics,the error disappear
 mod.fit(data_iter, eval_metric='ce',  num_epoch=5, epoch_end_callback=checkpoint)
 or
 mod.fit(data_iter, eval_metric=mx.metric.Perplexity(ignore_label=-1),  num_epoch=5, epoch_end_callback=checkpoint)
 I think we can avoid this issue by specifying a specific evel_metric.
 		</comment>
 		<comment id='12' author='helloworldlxb' date='2018-04-06T21:55:23Z'>
 		&lt;denchmark-link:https://github.com/helloworldlxb&gt;@helloworldlxb&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/Matafight&gt;@Matafight&lt;/denchmark-link&gt;
  this issue should be fix by PR &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/10446&gt;#10446&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='13' author='helloworldlxb' date='2018-09-25T04:21:28Z'>
 		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
  Please close this issue as it has been fixed with the PR.
 &lt;denchmark-link:https://github.com/helloworldlxb&gt;@helloworldlxb&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/Matafight&gt;@Matafight&lt;/denchmark-link&gt;
  Please feel free to reopen this issue if you encounter it again.
 		</comment>
 	</comments>
 </bug>
<commit id='e42f7e08bb5d8277bd87f81babf42cb5121ec160' author='Lai Wei' date='2018-04-10 17:27:36-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.8181818181818182' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\mxnet\metric.py' new_name='python\mxnet\metric.py'>
 		<file_info nloc='874' complexity='116' token_count='3740'></file_info>
 		<method name='update' parameters='self,labels,preds'>
 				<method_info nloc='12' complexity='3' token_count='122' nesting_level='1' start_line='401' end_line='427'></method_info>
 			<added_lines>420,421,422,426,427</added_lines>
 			<deleted_lines>423,424</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_metric.py' new_name='tests\python\unittest\test_metric.py'>
 		<file_info nloc='118' complexity='9' token_count='1560'></file_info>
 		<method name='test_acc_2d_label' parameters=''>
 				<method_info nloc='9' complexity='1' token_count='180' nesting_level='0' start_line='57' end_line='66'></method_info>
 			<added_lines>57,58,59,60,61,62,63,64,65,66</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>67</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
