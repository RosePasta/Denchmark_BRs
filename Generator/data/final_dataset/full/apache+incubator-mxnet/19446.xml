<bug_data>
<bug id='19446' author='bgawrych' open_date='2020-10-29T13:55:04Z' closed_time='2020-11-02T18:02:40Z'>
 	<summary>net.optimize_for doesn't work with numpy semantics</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 Forward pass after calling optimize_for with specific backend doesn't work. I'm not sure what this error mean, but found a way to overcome this (ugly way :))
 Problem occurs on master and 1.x branches
 &lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "../d.py", line 23, in &lt;module&gt;
     print(net(a, b))
   File "/home/bgawrych/Desktop/mxnet/python/mxnet/gluon/block.py", line 1407, in __call__
     return super().__call__(x, *args)
   File "/home/bgawrych/Desktop/mxnet/python/mxnet/gluon/block.py", line 716, in __call__
     _check_all_np_ndarrays(out)
   File "/home/bgawrych/Desktop/mxnet/python/mxnet/gluon/utils.py", line 480, in _check_all_np_ndarrays
     raise TypeError("Block's output ndarrays/symbols must be of type `mxnet.numpy.ndarray`"
 TypeError: Block's output ndarrays/symbols must be of type `mxnet.numpy.ndarray` or `mxnet.symbol.numpy._Symbol`, while got output type &lt;class 'mxnet.ndarray.ndarray.NDArray'&gt;
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;import mxnet as mx
 from mxnet.gluon import HybridBlock
 
 mx.npx.set_np()
 
 class TestBlock(HybridBlock):
     def __init__(self):
         super(TestBlock, self).__init__()
         self.d = mx.gluon.nn.Dense(1)
     def hybrid_forward(self, F, a, b, *args):
         res = self.d.hybrid_forward(F, a, b)
         return res
 
 a = mx.np.random.uniform(low=-1, high=1, size=(1,1))
 b = mx.np.random.uniform(low=-1, high=1, size=(1,1))
 
 net = TestBlock()
 net.initialize()
 net.hybridize()
 
 print(net(a, b))
 net.optimize_for(a, b, backend="MKLDNN")
 #print(net(a, b)) # &lt;---- this line doesn't work now - we need to reload symbol with JSON
 inputs, sym = net._cached_graph
 sym = mx.sym.np._symbol.load_json(sym.tojson())
 x = mx.gluon.SymbolBlock(sym, [mx.sym.var('data0'), mx.sym.var('data1')], net.collect_params())
 
 print(x(a, b))
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;
 
 
 Add ConvertShapeAttrToNumPyCompatible(&amp;g); in MXOptimizeForBackend- doesn't help
 
 &lt;denchmark-link:https://github.com/samskalicky&gt;@samskalicky&lt;/denchmark-link&gt;
  maybe you will be able to help
 	</description>
 	<comments>
 		<comment id='1' author='bgawrych' date='2020-10-30T16:36:10Z'>
 		Thanks for the code to reproduce &lt;denchmark-link:https://github.com/bgawrych&gt;@bgawrych&lt;/denchmark-link&gt;
 , I can reproduce on teh v1.8.x branch. will take a look
 		</comment>
 		<comment id='2' author='bgawrych' date='2020-10-30T17:08:45Z'>
 		After checking with &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
  it looks like we need to do something like:
 &lt;denchmark-code&gt;# Partition the graph.                                                                                                               
 out = out.optimize_for(self._backend, arg_dict, aux_dict, ctx, **self._backend_opts)
 
 # convert to numpy symbol if needed                                                                                                  
 if _mx_npx.is_np_array():
     out = out.as_np_ndarray()
 
 #update cached graph with partitioned graph                                                                                          
 self._cached_graph = data, out
 &lt;/denchmark-code&gt;
 
 here where we call optimize_for in the Gluon block:
 
 
 
 incubator-mxnet/python/mxnet/gluon/block.py
 
 
          Line 1039
       in
       0faecf0
 
 
 
 
 
 
  out = out.optimize_for(self._backend, arg_dict, aux_dict, ctx, **self._backend_opts) 
 
 
 
 
 
 With this change it seems to be working. I'll create a PR with this on v1.x and master branches. I guess there hasnt been any testing of Numpy functionality with optimize_for yet....
 		</comment>
 	</comments>
 </bug>
<commit id='33d94f1d59335f504ed5b9a7b32f0e81a5d5da56' author='Sam Skalicky' date='2020-11-02 10:02:39-08:00'>
 	<dmm_unit complexity='0.9' interfacing='0.85' size='0.9'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\mxnet\gluon\block.py' new_name='python\mxnet\gluon\block.py'>
 		<file_info nloc='1127' complexity='349' token_count='8071'></file_info>
 		<method name='_build_cache' parameters='self,args'>
 				<method_info nloc='75' complexity='31' token_count='652' nesting_level='1' start_line='994' end_line='1089'></method_info>
 			<added_lines>1041,1042,1043,1044</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_numpy_gluon.py' new_name='tests\python\unittest\test_numpy_gluon.py'>
 		<file_info nloc='449' complexity='85' token_count='4608'></file_info>
 		<method name='test_optimize_for.__init__' parameters='self'>
 				<method_info nloc='3' complexity='1' token_count='29' nesting_level='2' start_line='424' end_line='426'></method_info>
 			<added_lines>424,425,426</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_optimize_for.hybrid_forward' parameters='self,F,a,b,args'>
 				<method_info nloc='3' complexity='1' token_count='30' nesting_level='2' start_line='427' end_line='429'></method_info>
 			<added_lines>427,428,429</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_optimize_for' parameters=''>
 				<method_info nloc='12' complexity='1' token_count='113' nesting_level='0' start_line='422' end_line='440'></method_info>
 			<added_lines>422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>421,441,442</added_lines>
 			<deleted_lines>400</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
