<bug_data>
<bug id='10026' author='marcoabreu' open_date='2018-03-07T18:23:34Z' closed_time='2018-08-17T21:51:41Z'>
 	<summary>MXNET_MKLDNN_DEBUG=1 produces errors</summary>
 	<description>
 &lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/PR-9995/32/pipeline/483&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/PR-9995/32/pipeline/483&lt;/denchmark-link&gt;
 
 Setting MXNET_MKLDNN_DEBUG=1 as environment variable will produce the following error in tests. This happens across all configurations and seeds. I do not think that this is a test failure.
 &lt;denchmark-code&gt;======================================================================
 
 ERROR: test_gluon_model_zoo.test_models
 
 ----------------------------------------------------------------------
 
 Traceback (most recent call last):
 
   File "/usr/local/lib/python2.7/dist-packages/nose/case.py", line 197, in runTest
 
     self.test(*self.arg)
 
   File "/work/mxnet/tests/python/unittest/common.py", line 157, in test_new
 
     orig_test(*args, **kwargs)
 
   File "/work/mxnet/tests/python/unittest/test_gluon_model_zoo.py", line 50, in test_models
 
     model(mx.nd.random.uniform(shape=data_shape)).wait_to_read()
 
   File "/work/mxnet/python/mxnet/ndarray/ndarray.py", line 1650, in wait_to_read
 
     check_call(_LIB.MXNDArrayWaitToRead(self.handle))
 
   File "/work/mxnet/python/mxnet/base.py", line 149, in check_call
 
     raise MXNetError(py_str(_LIB.MXGetLastError()))
 
 MXNetError: [17:10:12] src/operator/nn/mkldnn/mkldnn_base.cc:395: Check failed: similar 
 
 
 
 Stack trace returned 10 entries:
 
 [bt] (0) /work/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f06ccf3745b]
 
 [bt] (1) /work/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f06ccf38478]
 
 [bt] (2) /work/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::OpCheck::Run(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt;, nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;)+0x3ca8) [0x7f06ccf54198]
 
 [bt] (3) /work/mxnet/python/mxnet/../../lib/libmxnet.so(+0x2a910d9) [0x7f06cf55a0d9]
 
 [bt] (4) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::imperative::PushFComputeEx(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x7c) [0x7f06cf77608c]
 
 [bt] (5) /work/mxnet/python/mxnet/../../lib/libmxnet.so(+0x3148fdb) [0x7f06cfc11fdb]
 
 [bt] (6) /work/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0xcb5) [0x7f06cfc0b1a5]
 
 [bt] (7) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr&lt;dmlc::ManualEvent&gt;)#1}&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptr&lt;dmlc::ManualEvent&gt;&amp;&amp;)+0xd9) [0x7f06cfc1d309]
 
 [bt] (8) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::thread::_Impl&lt;std::_Bind_simple&lt;std::function&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; &gt;::_M_run()+0x4a) [0x7f06cfc1c43a]
 
 [bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80) [0x7f06d7ca4c80]
 
 
 
 
 
 -------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------
 
 ResNetV1(
 
   (features): HybridSequential(
 
     (0): Conv2D(None -&gt; 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
 
     (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
     (2): Activation(relu)
 
     (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)
 
     (4): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (5): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(64 -&gt; 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
         (downsample): HybridSequential(
 
           (0): Conv2D(64 -&gt; 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (6): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(128 -&gt; 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
         (downsample): HybridSequential(
 
           (0): Conv2D(128 -&gt; 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (7): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
         (downsample): HybridSequential(
 
           (0): Conv2D(256 -&gt; 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)
 
   )
 
   (output): Dense(512 -&gt; 1000, linear)
 
 )
 
 ResNetV1(
 
   (features): HybridSequential(
 
     (0): Conv2D(None -&gt; 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
 
     (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
     (2): Activation(relu)
 
     (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)
 
     (4): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (2): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (5): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(64 -&gt; 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
         (downsample): HybridSequential(
 
           (0): Conv2D(64 -&gt; 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (2): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (3): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (6): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(128 -&gt; 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
         (downsample): HybridSequential(
 
           (0): Conv2D(128 -&gt; 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (2): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (3): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (4): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (5): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (7): HybridSequential(
 
       (0): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(256 -&gt; 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
         (downsample): HybridSequential(
 
           (0): Conv2D(256 -&gt; 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (1): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
       (2): BasicBlockV1(
 
         (body): HybridSequential(
 
           (0): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
           (2): Activation(relu)
 
           (3): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
 
           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)
 
         )
 
       )
 
     )
 
     (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)
 
   )
 
   (output): Dense(512 -&gt; 1000, linear)
 
 )
 
 
 
 --------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------
 
 -------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
 
 common: INFO: Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=1825457337 to reproduce.
 
 common: INFO: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1579343143 to reproduce.
 
 --------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='marcoabreu' date='2018-03-08T11:03:59Z'>
 		&lt;denchmark-link:https://issues.apache.org/jira/browse/MXNET-60&gt;https://issues.apache.org/jira/browse/MXNET-60&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='marcoabreu' date='2018-03-22T06:21:46Z'>
 		&lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/cjolivier01&gt;@cjolivier01&lt;/denchmark-link&gt;
   This is a very useful function to verify the correctness of MKL-DNN OP.
 After debugging, there are two types of failures in DEBUG mode.
  &lt;denchmark-link:https://github.com/cjolivier01&gt;@cjolivier01&lt;/denchmark-link&gt;
 
 The current both rtol and atol in SimilarArray are 1e-3.
 In the fail case, the two data are  and . Because these two numbers are small, the atol is larger than 1e-3. But I think this difference is acceptable.
 I suggest that we change the atol to 1e-2.
 
 
 
 incubator-mxnet/src/operator/nn/mkldnn/mkldnn_base.cc
 
 
          Line 391
       in
       46e47cb
 
 
 
 
 
 
  bool similar = SimilarArray&lt;DType&gt;(outputs[i], outputs_[i], 1e-3, 1e-3); 
 
 
 
 
 
 
 [15:28:36] src/operator/nn/mkldnn/mkldnn_base.cc:346: data1[i]: 0.675079 data2[i]: 0.677384
 [15:28:36] src/operator/nn/mkldnn/mkldnn_base.cc:347: atol + rtol * std::abs(data2[i]):0.00167738
 Abs(abs(data1[i] - data2[i] )= 0.002305
 if (std::abs(data1[i] - data2[i]) &gt; atol + rtol * std::abs(data2[i])) return false
 
  &lt;denchmark-link:https://github.com/piiswrong&gt;@piiswrong&lt;/denchmark-link&gt;
 
 
 
 I suggest changing this to  so that the MKL-DNN flatten function will be used.
 After I changed these two, all cases passed.
 &lt;denchmark-code&gt;[patric@mlt-skx080 master]$ git diff
 diff --git a/3rdparty/mkldnn b/3rdparty/mkldnn
 --- a/3rdparty/mkldnn
 +++ b/3rdparty/mkldnn
 @@ -1 +1 @@
 -Subproject commit f5218ff4fd2d16d13aada2e632afd18f2514fee3
 +Subproject commit f5218ff4fd2d16d13aada2e632afd18f2514fee3-dirty
 diff --git a/python/mxnet/gluon/nn/basic_layers.py b/python/mxnet/gluon/nn/basic_layers.py
 index 3801c84..b7bba8a 100644
 --- a/python/mxnet/gluon/nn/basic_layers.py
 +++ b/python/mxnet/gluon/nn/basic_layers.py
 @@ -405,7 +405,7 @@ class Flatten(HybridBlock):
          super(Flatten, self).__init__(**kwargs)
 
      def hybrid_forward(self, F, x):
 -        return x.reshape((0, -1))
 +        return F.Flatten(x)
 
      def __repr__(self):
          return self.__class__.__name__
 diff --git a/src/operator/nn/mkldnn/mkldnn_base.cc b/src/operator/nn/mkldnn/mkldnn_base.cc
 index 820cca1..c9582cd 100644
 --- a/src/operator/nn/mkldnn/mkldnn_base.cc
 +++ b/src/operator/nn/mkldnn/mkldnn_base.cc
 @@ -388,7 +388,7 @@ void OpCheck::Run(mxnet::FCompute fn, const nnvm::NodeAttrs &amp;attrs,
      if (req[i] == kNullOp)
        continue;
      MSHADOW_TYPE_SWITCH(outputs[i].dtype(), DType, {
 -      bool similar = SimilarArray&lt;DType&gt;(outputs[i], outputs_[i], 1e-3, 1e-3);
 +      bool similar = SimilarArray&lt;DType&gt;(outputs[i], outputs_[i], 1e-3, 1e-2);
        if (!similar) {
          LOG(ERROR) &lt;&lt; attrs.op-&gt;name &lt;&lt; " fails";
        }
 &lt;/denchmark-code&gt;
 
  &lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 
 In theory, this implementation should work with MKL-DNN type as well. But it doesn't.
 I think there is a bug.  I am still debugging this now.
 		</comment>
 		<comment id='3' author='marcoabreu' date='2018-03-27T08:41:58Z'>
 		&lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
  We got the root cause of 3) in above comments. It's not the MKL-DNN implementation issues. Just need to improve the test method under MXNET_MKLDNN_DEBUG
 &lt;denchmark-link:https://github.com/cjolivier01&gt;@cjolivier01&lt;/denchmark-link&gt;
  Thanks for the nice functionality to check the results of MKL-DNN.
 We found there is a special situation which needs to be improved as 3) in my above comments.
 The  is executed to change the view of NDAarry but the real data don't change yet.
 When come to debug mode, OpCheck tries to get the MKLDNN memory without checking if it is a view, so the case fails.
 
 
 
 incubator-mxnet/src/operator/nn/mkldnn/mkldnn_base.cc
 
 
          Line 357
       in
       bd9b9c8
 
 
 
 
 
 
  auto mem = inputs_[i].GetMKLDNNData(); 
 
 
 
 
 
 So, a possible fix for OpCheck.Init,  &lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
  please help take a review.
 &lt;denchmark-code&gt;+    //auto mem = inputs_[i].GetMKLDNNData();
 +    NDArray data = inputs_[i];
 +    const TShape&amp; ishape = inputs_[i].shape();
 +    if (data.IsMKLDNNData() &amp;&amp; data.IsView())
 +        data = data.MKLDNNDataReshape(Shape2(ishape.ProdShape(0, ishape.ndim()-1),
 +                                     ishape[ishape.ndim()-1]));
 +    auto mem = data.GetMKLDNNData(); 
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='4' author='marcoabreu' date='2018-03-28T00:54:36Z'>
 		When I wrote the test, I followed the python test. &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L470&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L470&lt;/denchmark-link&gt;
 
 When assert_almost_equal is called &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L1313&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L1313&lt;/denchmark-link&gt;
 , it uses 1e-3 for both rtol and atol. I didn't know why the test fails.
 		</comment>
 		<comment id='5' author='marcoabreu' date='2018-03-28T00:58:16Z'>
 		As for modifying OpCheck.Init, you can do
 &lt;denchmark-code&gt;  if (data.IsMKLDNNData() &amp;&amp; data.IsView())
     data = in_data[fullc::kData].Reorder2Default();
 &lt;/denchmark-code&gt;
 
 Please see the example here: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/mkldnn/mkldnn_fully_connected.cc#L95&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/mkldnn/mkldnn_fully_connected.cc#L95&lt;/denchmark-link&gt;
 
 This is how we should deal with reshaped MKLDNN NDArray. Unfortunately, we can't do in-place layout conversion in the NDArray, which caused a race condition.
 		</comment>
 		<comment id='6' author='marcoabreu' date='2018-03-28T01:04:16Z'>
 		&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
  I have looked into Reorder2Default, but it will also convert to the original shape rather than the new shape of 'reshape'.
 And that's another point we want to improve later.
 		</comment>
 		<comment id='7' author='marcoabreu' date='2018-07-13T20:45:04Z'>
 		&lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;
  I believe when you call copyfrom it will convert the input memory shape into the same shape as the target. so if you Reorder2Default but then call copyfrom the mkldnn memory will be the new shape
 &lt;denchmark-code&gt;NDArray data = inputs_[i];
 inputs.emplace_back(data.shape(), ctx, false, data.dtype());
 if (data.IsMKLDNNData() &amp;&amp; data.IsView())
     data = in_data[fullc::kData].Reorder2Default();
 auto mem = inputs_[i].GetMKLDNNData();
 inputs[i].CopyFrom(*mem);
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='8' author='marcoabreu' date='2018-08-07T19:21:47Z'>
 		&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/12069&gt;#12069&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='9' author='marcoabreu' date='2018-08-17T21:46:51Z'>
 		above PR addresses issue. &lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
  can you close?
 		</comment>
 	</comments>
 </bug>
<commit id='525ead9caaf49035b0310ef7c8b686b393463760' author='Alexander Zai' date='2018-08-14 10:22:44-07:00'>
 	<dmm_unit complexity='0.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='ci\docker\runtime_functions.sh' new_name='ci\docker\runtime_functions.sh'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>584,594,603,612,644,671,681</added_lines>
 			<deleted_lines>584,585,586,596,597,598,607,608,609,618,619,620,652,653,654,681,682,683,693,694,695</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\mxnet\gluon\nn\basic_layers.py' new_name='python\mxnet\gluon\nn\basic_layers.py'>
 		<file_info nloc='617' complexity='66' token_count='2602'></file_info>
 		<method name='hybrid_forward' parameters='self,F,x'>
 				<method_info nloc='2' complexity='1' token_count='16' nesting_level='1' start_line='429' end_line='430'></method_info>
 			<added_lines>430</added_lines>
 			<deleted_lines>430</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\nn\lrn.cc' new_name='src\operator\nn\lrn.cc'>
 		<file_info nloc='159' complexity='18' token_count='1220'></file_info>
 		<modified_lines>
 			<added_lines>207,208</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\nn\mkldnn\mkldnn_base.cc' new_name='src\operator\nn\mkldnn\mkldnn_base.cc'>
 		<file_info nloc='460' complexity='138' token_count='4305'></file_info>
 		<method name='mxnet::SimilarArray' parameters='arr1,arr2,rtol,atol'>
 				<method_info nloc='106' complexity='29' token_count='1099' nesting_level='1' start_line='431' end_line='556'></method_info>
 			<added_lines>476,477,478,479,480,499,500,501,502,503,519</added_lines>
 			<deleted_lines>476,477,478,512</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
