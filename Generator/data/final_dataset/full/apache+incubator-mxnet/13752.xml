<bug_data>
<bug id='13752' author='eric-haibin-lin' open_date='2019-01-01T22:14:19Z' closed_time='2019-03-17T06:48:31Z'>
 	<summary>Adam, AdaMax and FTML cannot be used with Trainer(update_on_kv=False)</summary>
 	<description>
 These optimizers adjust the learning rate by the number of optimization steps, which is problematic if  is set and multiple GPUs share the same optimizer object (leading to incorrect count of optimization steps). For example: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1093&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1093&lt;/denchmark-link&gt;
 
 cc &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='63ed258063137421e6d4def30435014ab57fb468' author='Przemyslaw Tredak' date='2019-03-16 23:48:30-07:00'>
 	<dmm_unit complexity='0.0' interfacing='0.6' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='python\mxnet\gluon\trainer.py' new_name='python\mxnet\gluon\trainer.py'>
 		<file_info nloc='303' complexity='92' token_count='1876'></file_info>
 		<method name='_init_kvstore' parameters='self'>
 				<method_info nloc='45' complexity='20' token_count='329' nesting_level='1' start_line='169' end_line='252'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>244,245,246,247</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='python\mxnet\optimizer\optimizer.py' new_name='python\mxnet\optimizer\optimizer.py'>
 		<file_info nloc='1282' complexity='215' token_count='7407'></file_info>
 		<method name='_set_current_context' parameters='self,device_id'>
 				<method_info nloc='4' complexity='2' token_count='35' nesting_level='1' start_line='384' end_line='394'></method_info>
 			<added_lines>384,385,386,387,388,389,390,391,392,393,394</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='__call__' parameters='self,index,grad,weight'>
 				<method_info nloc='46' complexity='14' token_count='414' nesting_level='1' start_line='1629' end_line='1677'></method_info>
 			<added_lines>1639,1640</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>109,110,395</added_lines>
 			<deleted_lines>109</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_gluon_trainer.py' new_name='tests\python\unittest\test_gluon_trainer.py'>
 		<file_info nloc='241' complexity='40' token_count='2489'></file_info>
 		<method name='test_trainer_lr_sched' parameters=''>
 				<method_info nloc='36' complexity='7' token_count='344' nesting_level='0' start_line='256' end_line='293'></method_info>
 			<added_lines>275,282,283,284,285,286,287,288,289,290,291,292</added_lines>
 			<deleted_lines>275,276,283,284,285,286,287,288,289</deleted_lines>
 		</method>
 		<method name='test_trainer_invalid_lr_sched' parameters=''>
 				<method_info nloc='15' complexity='2' token_count='145' nesting_level='0' start_line='276' end_line='290'></method_info>
 			<added_lines>282,283,284,285,286,287,288,289,290</added_lines>
 			<deleted_lines>276,283,284,285,286,287,288,289</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
