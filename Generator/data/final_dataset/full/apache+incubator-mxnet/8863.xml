<bug_data>
<bug id='8863' author='shuokay' open_date='2017-11-29T07:47:29Z' closed_time='2017-12-11T02:52:35Z'>
 	<summary>[gluon] segmentation error when getting the shape of output ndarray of transpose conv</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;----------Python Info----------
 ('Version      :', '2.7.12')
 ('Compiler     :', 'GCC 5.4.0 20160609')
 ('Build        :', ('default', 'Nov 19 2016 06:48:10'))
 ('Arch         :', ('64bit', 'ELF'))
 ------------Pip Info-----------
 ('Version      :', '9.0.1')
 ('Directory    :', '/usr/local/lib/python2.7/dist-packages/pip')
 ----------MXNet Info-----------
 ('Version      :', '0.12.1')
 ('Directory    :', '/home/luban/incubator-mxnet/python/mxnet')
 Traceback (most recent call last):
   File "diagnose.py", line 171, in &lt;module&gt;
     check_mxnet()
   File "diagnose.py", line 113, in check_mxnet
     except FileNotFoundError:
 NameError: global name 'FileNotFoundError' is not defined
 &lt;/denchmark-code&gt;
 
 Package used (Python/R/Scala/Julia): Python2.7.12
 MXNet commit hash: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/91ffd691bf865833a51ca793eda125b7f9befc18&gt;91ffd69&lt;/denchmark-link&gt;
 
 Build config:
 &lt;denchmark-code&gt;#-------------------------------------------------------------------------------
 #  Template configuration for compiling mxnet
 #
 #  If you want to change the configuration, please use the following
 #  steps. Assume you are on the root directory of mxnet. First copy the this
 #  file so that any local changes will be ignored by git
 #
 #  $ cp make/config.mk .
 #
 #  Next modify the according entries, and then compile by
 #
 #  $ make
 #
 #  or build in parallel with 8 threads
 #
 #  $ make -j8
 #-------------------------------------------------------------------------------
 
 #---------------------
 # choice of compiler
 #--------------------
 
 export CC = gcc
 export CXX = g++
 export NVCC = nvcc
 
 # whether compile with options for MXNet developer
 DEV = 0
 
 # whether compile with debug
 DEBUG = 0
 
 # whether compile with profiler
 USE_PROFILER =
 
 # whether to turn on signal handler (e.g. segfault logger)
 USE_SIGNAL_HANDLER =
 
 # the additional link flags you want to add
 ADD_LDFLAGS = -L/usr/local/nvidia/lib64
 
 # the additional compile flags you want to add
 ADD_CFLAGS =
 
 #---------------------------------------------
 # matrix computation libraries for CPU/GPU
 #---------------------------------------------
 
 # whether use CUDA during compile
 USE_CUDA = 1
 
 # add the path to CUDA library to link and compile flag
 # if you have already add them to environment variable, leave it as NONE
 # USE_CUDA_PATH = /usr/local/cuda
 USE_CUDA_PATH = /usr/local/cuda
 
 # whether use CuDNN R3 library
 USE_CUDNN = 1
 
 #whether to use NCCL library
 USE_NCCL = 0
 #add the path to NCCL library
 USE_NCCL_PATH = NONE
 
 # whether use opencv during compilation
 # you can disable it, however, you will not able to use
 # imbin iterator
 USE_OPENCV = 1
 
 #whether use libjpeg-turbo for image decode without OpenCV wrapper
 USE_LIBJPEG_TURBO = 0
 #add the path to libjpeg-turbo library
 USE_LIBJPEG_TURBO_PATH = NONE
 
 # use openmp for parallelization
 USE_OPENMP = 1
 
 # MKL ML Library for Intel CPU/Xeon Phi
 # Please refer to MKL_README.md for details
 
 # MKL ML Library folder, need to be root for /usr/local
 # Change to User Home directory for standard user
 # For USE_BLAS!=mkl only
 MKLML_ROOT=/usr/local
 
 # whether use MKL2017 library
 USE_MKL2017 = 0
 
 # whether use MKL2017 experimental feature for high performance
 # Prerequisite USE_MKL2017=1
 USE_MKL2017_EXPERIMENTAL = 0
 
 # whether use NNPACK library
 USE_NNPACK = 0
 
 # choose the version of blas you want to use
 # can be: mkl, blas, atlas, openblas
 # in default use atlas for linux while apple for osx
 UNAME_S := $(shell uname -s)
 ifeq ($(UNAME_S), Darwin)
 USE_BLAS = apple
 else
 USE_BLAS = openblas
 endif
 
 # whether use lapack during compilation
 # only effective when compiled with blas versions openblas/apple/atlas/mkl
 USE_LAPACK = 1
 
 # path to lapack library in case of a non-standard installation
 USE_LAPACK_PATH =
 
 # by default, disable lapack when using MKL
 # switch on when there is a full installation of MKL available (not just MKL2017/MKL_ML)
 ifeq ($(USE_BLAS), mkl)
 USE_LAPACK = 0
 endif
 
 # add path to intel library, you may need it for MKL, if you did not add the path
 # to environment variable
 USE_INTEL_PATH = NONE
 
 # If use MKL only for BLAS, choose static link automatically to allow python wrapper
 ifeq ($(USE_MKL2017), 0)
 ifeq ($(USE_BLAS), mkl)
 USE_STATIC_MKL = 1
 endif
 else
 USE_STATIC_MKL = NONE
 endif
 
 #----------------------------
 # Settings for power and arm arch
 #----------------------------
 ARCH := $(shell uname -a)
 ifneq (,$(filter $(ARCH), armv6l armv7l powerpc64le ppc64le aarch64))
 	USE_SSE=0
 else
 	USE_SSE=1
 endif
 
 #----------------------------
 # distributed computing
 #----------------------------
 
 # whether or not to enable multi-machine supporting
 USE_DIST_KVSTORE = 0
 
 # whether or not allow to read and write HDFS directly. If yes, then hadoop is
 # required
 USE_HDFS = 0
 
 # path to libjvm.so. required if USE_HDFS=1
 LIBJVM=$(JAVA_HOME)/jre/lib/amd64/server
 
 # whether or not allow to read and write AWS S3 directly. If yes, then
 # libcurl4-openssl-dev is required, it can be installed on Ubuntu by
 # sudo apt-get install -y libcurl4-openssl-dev
 USE_S3 = 0
 
 #----------------------------
 # performance settings
 #----------------------------
 # Use operator tuning
 USE_OPERATOR_TUNING = 1
 
 # Use gperftools if found
 USE_GPERFTOOLS = 1
 
 # Use JEMalloc if found, and not using gperftools
 USE_JEMALLOC = 1
 
 #----------------------------
 # additional operators
 #----------------------------
 
 # path to folders containing projects specific operators that you don't want to put in src/operators
 EXTRA_OPERATORS =
 
 #----------------------------
 # other features
 #----------------------------
 
 # Create C++ interface package
 USE_CPP_PACKAGE = 0
 
 #----------------------------
 # plugins
 #----------------------------
 
 # whether to use caffe integration. This requires installing caffe.
 # You also need to add CAFFE_PATH/build/lib to your LD_LIBRARY_PATH
 # CAFFE_PATH = $(HOME)/caffe
 # MXNET_PLUGINS += plugin/caffe/caffe.mk
 
 # whether to use torch integration. This requires installing torch.
 # You also need to add TORCH_PATH/install/lib to your LD_LIBRARY_PATH
 # TORCH_PATH = $(HOME)/torch
 # MXNET_PLUGINS += plugin/torch/torch.mk
 
 # WARPCTC_PATH = $(HOME)/warp-ctc
 # MXNET_PLUGINS += plugin/warpctc/warpctc.mk
 
 # whether to use sframe integration. This requires build sframe
 # git@github.com:dato-code/SFrame.git
 # SFRAME_PATH = $(HOME)/SFrame
 # MXNET_PLUGINS += plugin/sframe/plugin.mk
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;
 
 [1]    17341 segmentation fault (core dumped)  python resnet-fcn.py
 &lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;
 
 import mxnet as mx
 from mxnet import gluon
 from mxnet import nd
 ctx = mx.gpu(1)
 net4 = gluon.nn.Sequential()
 conv_t4 = gluon.nn.Conv2DTranspose(1024, kernel_size=(4, 4), strides=(2, 2), padding=(1, 1))
 conv_t4.initialize(init=mx.init.Xavier(), ctx=ctx)
 x4 = nd.uniform(low=0, high=255, shape=(2, 1024, 14, 14), ctx=ctx)
 net4.add(conv_t4)
 print (net4(x4)).shape
 	</description>
 	<comments>
 		<comment id='1' author='shuokay' date='2017-11-29T18:42:43Z'>
 		It's a bug in the sampling op. The following code would crash at commit &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/3226bce2c912bd0289be171c7025a6651e025407&gt;3226bce&lt;/denchmark-link&gt;
 :
  import mxnet as mx
  from mxnet import nd
  ctx = mx.cpu(0)
  x4 = nd.uniform(low=0, high=255, shape=(2, 1024, 14, 14), ctx=ctx)
  print(x4.shape)
 		</comment>
 		<comment id='2' author='shuokay' date='2017-11-29T22:00:58Z'>
 		Core dump at sample_op.h:266. Maybe related to this PR: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/8179&gt;#8179&lt;/denchmark-link&gt;
 
 Note: NaiveEngine can run through though.
 &lt;denchmark-link:https://github.com/asmushetzel&gt;@asmushetzel&lt;/denchmark-link&gt;
 
 &lt;denchmark-code&gt;(gdb) bt
 #0  0x00000000010e40c0 in ?? ()
 #1  0x00007fffe78cd9bc in mxnet::op::Scalar2Array&lt;mshadow::cpu, float&gt;::~Scalar2Array (this=0x7fffb58d5e80, __in_chrg=&lt;optimized out&gt;) at src/operator/random/./sample_op.h:266
 #2  0x00007fffe78c4df7 in mxnet::op::SampleMaster&lt;mshadow::cpu, mxnet::op::UniformSampler&lt;mshadow::cpu&gt; &gt;::op (attrs=..., ctx=..., req=@0x1d739d0: mxnet::kWriteTo, 
     outputs=0x7fffb58d66e0) at src/operator/random/./sample_op.h:295
 #3  0x00007fffe78ba92c in mxnet::op::Sample_&lt;mshadow::cpu, mxnet::op::UniformSampler&lt;mshadow::cpu&gt; &gt; (attrs=..., ctx=..., inputs=std::vector of length 0, capacity 0, 
     req=std::vector of length 1, capacity 1 = {...}, outputs=std::vector of length 1, capacity 1 = {...}) at src/operator/random/./sample_op.h:454
 #4  0x00007fffe77b3700 in std::_Function_handler&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;), void (*)(nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt;::_M_invoke(std::_Any_data const&amp;, nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;) (__functor=..., __args#0=..., 
     __args#1=..., __args#2=std::vector of length 0, capacity 0, __args#3=std::vector of length 1, capacity 1 = {...}, __args#4=std::vector of length 1, capacity 1 = {...})
     at /usr/include/c++/5/functional:1871
 #5  0x00007fffe994ead2 in std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt;::operator()(nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;) const (this=0x1d31858, __args#0=..., __args#1=..., __args#2=std::vector of length 0, capacity 0, __args#3=std::vector of length 1, capacity 1 = {...}, 
     __args#4=std::vector of length 1, capacity 1 = {...}) at /usr/include/c++/5/functional:2267
 #6  0x00007fffe9948ce0 in mxnet::imperative::PushFCompute(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const (__closure=0x1d317d0, rctx=...) at src/imperative/./imperative_utils.h:360
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='3' author='shuokay' date='2017-12-01T17:29:48Z'>
 		looking into it
 		</comment>
 		<comment id='4' author='shuokay' date='2017-12-10T12:35:49Z'>
 		&lt;denchmark-link:https://github.com/shuokay&gt;@shuokay&lt;/denchmark-link&gt;
  PR8935 which avoids this special allocation has been merged. Can you do a recheck on your side whether things work ok now?
 		</comment>
 		<comment id='5' author='shuokay' date='2017-12-11T02:52:35Z'>
 		&lt;denchmark-link:https://github.com/asmushetzel&gt;@asmushetzel&lt;/denchmark-link&gt;
 
 No segmentation fault and gloun exit normally. Thanks for your work.
 		</comment>
 	</comments>
 </bug>
<commit id='01b7ae3a4db14a47f426882b0fbf2e32192a3954' author='moin' date='2017-12-08 16:55:36-08:00'>
 	<dmm_unit complexity='1.0' interfacing='0.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\operator\random\sample_op.h' new_name='src\operator\random\sample_op.h'>
 		<file_info nloc='445' complexity='26' token_count='3515'></file_info>
 		<method name='mxnet::op::Scalar2Array::Scalar2Array' parameters='scalar,ctx'>
 				<method_info nloc='6' complexity='1' token_count='98' nesting_level='3' start_line='259' end_line='264'></method_info>
 			<added_lines>259,260,261,262,263,264</added_lines>
 			<deleted_lines>259,260,261,262,263,264</deleted_lines>
 		</method>
 		<method name='mxnet::op::SampleMaster&lt;xpu,GammaSampler&lt;xpu&gt;&gt;::op' parameters='attrs,ctx,req,outputs'>
 				<method_info nloc='18' complexity='1' token_count='196' nesting_level='3' start_line='316' end_line='333'></method_info>
 			<added_lines>324,325,326,327,331</added_lines>
 			<deleted_lines>319</deleted_lines>
 		</method>
 		<method name='mxnet::op::SampleMaster&lt;xpu,NormalSampler&lt;xpu&gt;&gt;::op' parameters='attrs,ctx,req,outputs'>
 				<method_info nloc='17' complexity='1' token_count='184' nesting_level='3' start_line='295' end_line='311'></method_info>
 			<added_lines>302,303,304,305,309</added_lines>
 			<deleted_lines>295,296,300</deleted_lines>
 		</method>
 		<method name='mxnet::op::SampleMaster&lt;xpu,ExponentialSampler&lt;xpu&gt;&gt;::op' parameters='attrs,ctx,req,outputs'>
 				<method_info nloc='16' complexity='1' token_count='180' nesting_level='3' start_line='338' end_line='353'></method_info>
 			<added_lines>345,346,347,351</added_lines>
 			<deleted_lines>339,353</deleted_lines>
 		</method>
 		<method name='mxnet::op::Scalar2Array::Ref' parameters=''>
 				<method_info nloc='1' complexity='1' token_count='17' nesting_level='3' start_line='268' end_line='268'></method_info>
 			<added_lines>268</added_lines>
 			<deleted_lines>268</deleted_lines>
 		</method>
 		<method name='mxnet::op::SampleMaster&lt;xpu,PoissonSampler&lt;xpu&gt;&gt;::op' parameters='attrs,ctx,req,outputs'>
 				<method_info nloc='16' complexity='1' token_count='180' nesting_level='3' start_line='358' end_line='373'></method_info>
 			<added_lines>365,366,367,371</added_lines>
 			<deleted_lines>358,372,373</deleted_lines>
 		</method>
 		<method name='mxnet::op::SampleMaster&lt;xpu,GeneralizedNegativeBinomialSampler&lt;xpu&gt;&gt;::op' parameters='attrs,ctx,req,outputs'>
 				<method_info nloc='20' complexity='1' token_count='196' nesting_level='3' start_line='399' end_line='418'></method_info>
 			<added_lines>409,410,411,412,416</added_lines>
 			<deleted_lines>414,415</deleted_lines>
 		</method>
 		<method name='mxnet::op::Scalar2Array::~Scalar2Array' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='16' nesting_level='3' start_line='265' end_line='267'></method_info>
 			<added_lines>265</added_lines>
 			<deleted_lines>265,266,267</deleted_lines>
 		</method>
 		<method name='mxnet::op::AllocContext&lt;cpu&gt;' parameters=''>
 				<method_info nloc='1' complexity='1' token_count='15' nesting_level='2' start_line='252' end_line='252'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>252</deleted_lines>
 		</method>
 		<method name='mxnet::op::SampleMaster&lt;xpu,UniformSampler&lt;xpu&gt;&gt;::op' parameters='attrs,ctx,req,outputs'>
 				<method_info nloc='17' complexity='1' token_count='186' nesting_level='3' start_line='274' end_line='290'></method_info>
 			<added_lines>281,282,283,284,288</added_lines>
 			<deleted_lines>274,277,278,279,280</deleted_lines>
 		</method>
 		<method name='mxnet::op::GetSamplingTempData' parameters='N,p1,p2,ctx,seeds,parm1,parm2'>
 				<method_info nloc='17' complexity='1' token_count='312' nesting_level='2' start_line='248' end_line='266'></method_info>
 			<added_lines>248,249,250,251,254,255,256,257,258,259,260,261,262,263,264,265</added_lines>
 			<deleted_lines>248,249,250,251,252,253,254,255,257,258,259,260,261,262,263,264,265,266</deleted_lines>
 		</method>
 		<method name='mxnet::op::SampleMaster&lt;xpu,NegativeBinomialSampler&lt;xpu&gt;&gt;::op' parameters='attrs,ctx,req,outputs'>
 				<method_info nloc='17' complexity='1' token_count='196' nesting_level='3' start_line='378' end_line='394'></method_info>
 			<added_lines>386,387,388,392</added_lines>
 			<deleted_lines>392,393</deleted_lines>
 		</method>
 		<method name='mxnet::op::AllocContext&lt;gpu&gt;' parameters=''>
 				<method_info nloc='1' complexity='1' token_count='15' nesting_level='2' start_line='254' end_line='254'></method_info>
 			<added_lines>254</added_lines>
 			<deleted_lines>254</deleted_lines>
 		</method>
 		<method name='mxnet::op::Scalar2Array::GetTensor' parameters=''>
 				<method_info nloc='1' complexity='1' token_count='25' nesting_level='3' start_line='269' end_line='269'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>269</deleted_lines>
 		</method>
 		<method name='mxnet::op::GetSeeds' parameters='N,ctx'>
 				<method_info nloc='8' complexity='1' token_count='109' nesting_level='2' start_line='274' end_line='281'></method_info>
 			<added_lines>281</added_lines>
 			<deleted_lines>274,277,278,279,280</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>245,246</added_lines>
 			<deleted_lines>245,246,247,270,271,272,273,314,315,334,335,354,377,397,419</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
