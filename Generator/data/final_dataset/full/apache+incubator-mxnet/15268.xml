<bug_data>
<bug id='15268' author='Ishitori' open_date='2019-06-18T18:31:26Z' closed_time='2019-06-21T01:41:01Z'>
 	<summary>Backward doesn't work on LSTM with sequence_length</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 LSTM with out-of-the-box variable length was introduced in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/14208/&gt;this PR&lt;/denchmark-link&gt;
 . I tried to use it, and while the forward pass works well, the backward pass fails.
 I provide minimum reproducible example. To my best knowledge, the backward pass is not covered with a unit test.
 &lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;
 
 The latest version with --pre
 Package used (Python/R/Scala/Julia):
 Python
 &lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;MXNetError: [17:18:04] src/operator/./rnn-inl.h:1006: Check failed: in_data.size() == num_inputs (4 vs. 5) : 
 Stack trace:
   [bt] (0) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x4a157b) [0x7fdedb45957b]
   [bt] (1) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x507b9ad) [0x7fdee00339ad]
   [bt] (2) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x50b5cac) [0x7fdee006dcac]
   [bt] (3) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#3}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x396) [0x7fdedd6b3d36]
   [bt] (4) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#4}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext)+0x5d) [0x7fdedd6b43cd]
   [bt] (5) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x264c4f9) [0x7fdedd6044f9]
   [bt] (6) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2658961) [0x7fdedd610961]
   [bt] (7) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x265be70) [0x7fdedd613e70]
   [bt] (8) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x265c106) [0x7fdedd614106]
 
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;
 
 You have to use GPU to run it, as this feature is GPU only.
 The devil is in the fact that the backward fails silently and mx.nd.waitall() is necessary at the end
 import mxnet as mx
 import numpy as np
 from mxnet.gluon import nn
 from mxnet.gluon.rnn import LSTM
 
 ctx = mx.gpu(0)
 
 label = mx.nd.array([1, 2, 3, 4, 5, 6, 7], ctx=ctx)
 # random numbers, but with ones at the end as a padding symbol
 x = mx.nd.array([[5434, 3232, 776, 323, 1, 1, 1], [4353, 545, 37, 23, 23, 545, 1]], ctx=ctx)
 
 embedding = nn.Embedding(input_dim=6000,
                           output_dim=100,
                           weight_initializer=mx.initializer.Uniform(0.001))
 
 lstm = LSTM(hidden_size=100,
             num_layers=1, dropout=0.2, bidirectional=True,
             use_sequence_length=True)
 
 dense = nn.Dense(1)
 l1 = mx.gluon.loss.L1Loss()
 
 embedding.initialize(ctx=ctx)
 lstm.initialize(ctx=ctx)
 dense.initialize(ctx=ctx)
 
 
 with mx.autograd.record():
     x_mask = x != 1
     x_len = mx.nd.sum(x_mask, axis=1).astype(np.int32)    
     state = lstm.begin_state(batch_size=x.shape[0], ctx=x.context)
     x_emb = embedding(x)
     x_emb = x_emb.transpose((1, 0, 2))
     a, _ = lstm(x_emb, states=state, sequence_length=x_len)
     out = dense(a)
     loss = l1(out, label)
     # this prints the loss, showing that forward pass works fine
     print(loss)
 
 # this one will fail
 loss.backward()
 mx.nd.waitall()
 	</description>
 	<comments>
 		<comment id='1' author='Ishitori' date='2019-06-18T18:31:32Z'>
 		Hey, this is the MXNet Label Bot.
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
 Here are my recommended labels: Bug
 		</comment>
 		<comment id='2' author='Ishitori' date='2019-06-18T18:34:12Z'>
 		&lt;denchmark-link:https://github.com/stephenrawls&gt;@stephenrawls&lt;/denchmark-link&gt;
 , any help with that?
 		</comment>
 		<comment id='3' author='Ishitori' date='2019-06-18T20:39:58Z'>
 		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
  add [Bug, Gluon]
 		</comment>
 		<comment id='4' author='Ishitori' date='2019-06-18T22:12:50Z'>
 		I could reproduce this issue. Here is a full callstack.
 ubuntu@ip-172-31-31-181:~$ python lstm_test.py
 [1.0000387 2.0000887 3.000114  4.000115  5.000068  6.0000405 7.       ]
 &lt;NDArray 7 &lt;denchmark-link:https://github.com/gpu&gt;@gpu&lt;/denchmark-link&gt;
 (0)&gt;
 Traceback (most recent call last):
 File "lstm_test.py", line 42, in 
 mx.nd.waitall()
 File "/home/ubuntu/incubator-mxnet/python/mxnet/ndarray/ndarray.py", line 166, in waitall
 check_call(_LIB.MXNDArrayWaitAll())
 File "/home/ubuntu/incubator-mxnet/python/mxnet/base.py", line 253, in check_call
 raise MXNetError(py_str(_LIB.MXGetLastError()))
 mxnet.base.MXNetError: [22:04:02] src/operator/./rnn-inl.h:1006: Check failed: in_data.size() == num_inputs (4 vs. 5) :
 Stack trace:
 [bt] (0) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7ffa222a1082]
 [bt] (1) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::RNNOp&lt;mshadow::gpu, float, float&gt;::Backward(mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;)+0x1dc) [0x7ffa26c335bc]
 [bt] (2) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::op::RNNStatefulGradComputemshadow::gpu(mxnet::OpStatePtr const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;)+0x21b6) [0x7ffa26c68186]
 [bt] (3) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocatormxnet::Resource &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;unsigned int, std::allocator &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/3&gt;#3&lt;/denchmark-link&gt;
 }::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x1333) [0x7ffa24680473]
 [bt] (4) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocatormxnet::Resource &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;unsigned int, std::allocator &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/4&gt;#4&lt;/denchmark-link&gt;
 }&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x1d) [0x7ffa2468173d]
 [bt] (5) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/1&gt;#1&lt;/denchmark-link&gt;
 }&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;, mxnet::engine::CallbackOnComplete&amp;&amp;)+0x1ec) [0x7ffa24e3f3dc]
 [bt] (6) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x945) [0x7ffa24e42c35]
 [bt] (7) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker&lt;(dmlc::ConcurrentQueueType)0&gt;(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock&lt;(dmlc::ConcurrentQueueType)0&gt;, bool)::{lambda()&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/4&gt;#4&lt;/denchmark-link&gt;
 }::operator()() const::{lambda(std::shared_ptrdmlc::ManualEvent)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/1&gt;#1&lt;/denchmark-link&gt;
 }&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptrdmlc::ManualEvent&amp;&amp;)+0x4e) [0x7ffa24e5a8fe]
 		</comment>
 		<comment id='5' author='Ishitori' date='2019-06-18T22:49:43Z'>
 		Looking at this now.
 		</comment>
 		<comment id='6' author='Ishitori' date='2019-06-19T05:49:29Z'>
 		I think I have a solution, at least I have tested locally and it appears to work.
 Just need to update unit tests and then I will file a PR.
 		</comment>
 		<comment id='7' author='Ishitori' date='2019-06-19T06:00:06Z'>
 		Thanks &lt;denchmark-link:https://github.com/Ishitori&gt;@Ishitori&lt;/denchmark-link&gt;
   for the catch!
 Hi &lt;denchmark-link:https://github.com/stephenrawls&gt;@stephenrawls&lt;/denchmark-link&gt;
 , could you tag &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
  and me in your PR? We would like to include your fix in MXNet 1.5.0 release. This feature is already included in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/releases/tag/1.5.0.rc1&gt;1.5.0.rc1&lt;/denchmark-link&gt;
 
 Thanks!
 		</comment>
 		<comment id='8' author='Ishitori' date='2019-06-20T20:06:52Z'>
 		I verified that the issue is fixed in the latest code.
 &lt;denchmark-link:https://github.com/lanking520&gt;@lanking520&lt;/denchmark-link&gt;
   I would recommend closing this issue.
 		</comment>
 	</comments>
 </bug>
<commit id='4d9667121ae6fb643f2a02ab15e25231ed756cde' author='stephenrawls' date='2019-06-19 20:55:33-07:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\operator\rnn-inl.h' new_name='src\operator\rnn-inl.h'>
 		<file_info nloc='1336' complexity='180' token_count='8313'></file_info>
 		<method name='mxnet::op::CreateRNNState' parameters='attrs,ctx,in_shapes,in_types'>
 				<method_info nloc='26' complexity='4' token_count='187' nesting_level='2' start_line='1577' end_line='1603'></method_info>
 			<added_lines>1586,1587,1588,1589,1590</added_lines>
 			<deleted_lines>1586,1587</deleted_lines>
 		</method>
 		<method name='mxnet::op::RNNStatefulGradCompute' parameters='state,ctx,inputs,req,outputs'>
 				<method_info nloc='38' complexity='6' token_count='336' nesting_level='2' start_line='1640' end_line='1687'></method_info>
 			<added_lines>1655,1675,1676,1677,1678,1679,1680,1681,1682,1683</added_lines>
 			<deleted_lines>1652</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\gpu\test_gluon_gpu.py' new_name='tests\python\gpu\test_gluon_gpu.py'>
 		<file_info nloc='391' complexity='50' token_count='4056'></file_info>
 		<method name='check_layer_bidirectional_varseqlen.__init__' parameters='self,size,kwargs'>
 				<method_info nloc='5' complexity='1' token_count='69' nesting_level='2' start_line='231' end_line='235'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>231,232,233,234,235</deleted_lines>
 		</method>
 		<method name='check_layer_bidirectional_varseqlen' parameters='size,in_size'>
 				<method_info nloc='44' complexity='8' token_count='528' nesting_level='0' start_line='229' end_line='289'></method_info>
 			<added_lines>238,245,250,252,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289</added_lines>
 			<deleted_lines>230,231,232,233,234,235,236,237,238,239,240,241,242,251,258,259,265,266,267,268,269,275</deleted_lines>
 		</method>
 		<method name='check_layer_bidirectional_varseqlen.forward' parameters='self,inpt,sequence_length'>
 				<method_info nloc='6' complexity='1' token_count='70' nesting_level='2' start_line='237' end_line='242'></method_info>
 			<added_lines>238</added_lines>
 			<deleted_lines>237,238,239,240,241,242</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
