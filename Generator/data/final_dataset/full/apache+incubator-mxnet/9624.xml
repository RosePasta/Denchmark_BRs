<bug_data>
<bug id='9624' author='solin319' open_date='2018-01-30T01:36:43Z' closed_time='2020-09-26T18:48:45Z'>
 	<summary>problem when set fix_gamma=True in batchnorm</summary>
 	<description>
 If fix_gamma is true, then set gamma to 1 and its gradient to 0.
 But the value of gamma will be changed during parameters update. So the gamma saved in param file was not 1. These will bring a problem in convert MXNet parameters to other deep-learning platforms.
 This problem was caused by we set a default weight-decay in SGD optimizer.
 We must define variable gamma with wd_mult=0 to fix gamma=1 during training.
 Can MXNet set wd of gamma to 0 automatically when fix_gamma=1?
 	</description>
 	<comments>
 		<comment id='1' author='solin319' date='2018-02-01T20:40:21Z'>
 		This is a legacy design defect. When fix_gamma is true there shouldn't be a gamma parameter.
 In the future this could be solved by creating new operator batch_norm and deprecating BatchNorm
 		</comment>
 		<comment id='2' author='solin319' date='2018-02-27T20:19:33Z'>
 		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
  : Tag: Bug
 		</comment>
 		<comment id='3' author='solin319' date='2018-06-14T08:04:33Z'>
 		&lt;denchmark-link:https://github.com/solin319&gt;@solin319&lt;/denchmark-link&gt;
  how did you check the value of gamma? I suspect I am facing the same issue and hence, want to verify. But unfortunately, I couldn't find gamma in either arg_params or aux_params.
 		</comment>
 		<comment id='4' author='solin319' date='2019-03-14T00:52:34Z'>
 		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
  add [Operator]
 		</comment>
 		<comment id='5' author='solin319' date='2019-04-15T20:24:10Z'>
 		&lt;denchmark-link:https://github.com/solin319&gt;@solin319&lt;/denchmark-link&gt;
  I guess, the fix is already merged and closed.
 &lt;denchmark-link:https://github.com/anirudh2290&gt;@anirudh2290&lt;/denchmark-link&gt;
  Can we close this one.
 		</comment>
 		<comment id='6' author='solin319' date='2019-04-15T21:08:16Z'>
 		&lt;denchmark-link:https://github.com/Vikas89&gt;@Vikas89&lt;/denchmark-link&gt;
  the fix is only for coreml converter. The operator hasnt been fixed yet.
 		</comment>
 		<comment id='7' author='solin319' date='2020-09-26T08:55:21Z'>
 		&lt;denchmark-link:https://github.com/solin319&gt;@solin319&lt;/denchmark-link&gt;
  Does it means that the parameters would be updated even its grad_req is set to "null" if the wd_mult is not set to zero?
 I think this behavior is unexpected.
 		</comment>
 		<comment id='8' author='solin319' date='2020-09-26T18:48:45Z'>
 		I think &lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
  fixed this issue in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18500&gt;#18500&lt;/denchmark-link&gt;
 . Now the batchnorm op would respond to grad_req=null correctly.
 		</comment>
 	</comments>
 </bug>
<commit id='0f7d33d7470a444cf9b3f6644bdc41a22f7afe85' author='Zhao HG' date='2019-01-16 15:59:43-08:00'>
 	<dmm_unit complexity='1.0' interfacing='0.84375' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tools\coreml\converter\_layers.py' new_name='tools\coreml\converter\_layers.py'>
 		<file_info nloc='296' complexity='54' token_count='2046'></file_info>
 		<method name='convert_batchnorm' parameters='net,node,module,builder'>
 				<method_info nloc='30' complexity='4' token_count='230' nesting_level='0' start_line='453' end_line='501'></method_info>
 			<added_lines>475,476,477,481,482,490,491</added_lines>
 			<deleted_lines>475,476</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tools\coreml\test\test_mxnet_converter.py' new_name='tools\coreml\test\test_mxnet_converter.py'>
 		<file_info nloc='825' complexity='56' token_count='5776'></file_info>
 		<method name='test_batch_norm_with_fix_gamma' parameters='self'>
 				<method_info nloc='27' complexity='1' token_count='195' nesting_level='1' start_line='941' end_line='973'></method_info>
 			<added_lines>941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>974</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
