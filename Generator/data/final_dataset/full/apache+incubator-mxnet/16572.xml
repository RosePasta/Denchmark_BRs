<bug_data>
<bug id='16572' author='haojin2' open_date='2019-10-21T21:34:07Z' closed_time='2019-10-23T04:38:13Z'>
 	<summary>cudaErrorInvalidResourceHandle error when running some RNN models</summary>
 	<description>
 Both this &lt;denchmark-link:https://github.com/d2l-ai/d2l-en/blob/numpy2/chapter_natural-language-processing/sentiment-analysis-rnn.md&gt;notebook&lt;/denchmark-link&gt;
  and this &lt;denchmark-link:https://github.com/d2l-ai/d2l-en/blob/master/chapter_natural-language-processing/sentiment-analysis-rnn.md&gt;notebook&lt;/denchmark-link&gt;
  in dive into deep learning textbook errors with  on &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/rnn-inl.h#L1505&gt;line 1505 of src/operator/rnn-inl.h&lt;/denchmark-link&gt;
 .
 	</description>
 	<comments>
 		<comment id='1' author='haojin2' date='2019-10-21T21:35:02Z'>
 		I'm using CUDA 10.1 + CUDNN 7.6.4 on 4 V100 GPUs (p3.8xlarge instances). Reverting &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16391&gt;#16391&lt;/denchmark-link&gt;
  got both notebooks back to normal.
 		</comment>
 		<comment id='2' author='haojin2' date='2019-10-21T21:35:52Z'>
 		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
  Could you guys help with this issue here?
 		</comment>
 		<comment id='3' author='haojin2' date='2019-10-22T20:52:57Z'>
 		I will take a look at this.  Thanks for pointing this out.
 		</comment>
 		<comment id='4' author='haojin2' date='2019-10-22T22:20:18Z'>
 		Glancing over the code knowing now of this issue, I see a problem that may be related.  If the code has been compiled for CUDA/CUDNN use, and then an RNNOp is instantiated on a system with no GPU, the code will fail at the call to cudaEventCreateWithFlags() in the constructor.  Calling this lazily on first use (on a gpu) would solve this.  I suspect the approach will also fix the issues with the notebooks that you point out, although I have not verified this.
 Are you blocked by this?  It might take me a day or two to make a proper PR with the fix properly verified.
 		</comment>
 		<comment id='5' author='haojin2' date='2019-10-22T22:38:29Z'>
 		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
  Actually the 1.6.0 release could be blocked by this. Please do lemme know if you need any help on verification/reproduction/fixing so that we could streamline your fix. Thanks!
 		</comment>
 		<comment id='6' author='haojin2' date='2019-10-22T22:44:42Z'>
 		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
  BTW I mentioned earlier that I was running with 4 V100 GPUs.
 		</comment>
 		<comment id='7' author='haojin2' date='2019-10-22T23:43:56Z'>
 		&lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
  has postulated that the problem involves having the main python thread create the RNNOp (and its held cuda event) with either no context or a GPU-0 context, then having the event recorded on a stream of a different GPU.  I will be pushing a fix momentarily that delays creating the cuda event until first use, which should correct this scenario.
 I have verified the PR reinstates proper behavior when run on a system with no GPU.  I appreciate your offer to see if the PR cures this issue you raised with the notebooks.  Thanks!
 		</comment>
 		<comment id='8' author='haojin2' date='2019-10-23T04:38:12Z'>
 		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
  Fix merged, now closing this issue.
 		</comment>
 	</comments>
 </bug>
<commit id='b05d72ac5697904e8d00954402174a7891c65ece' author='Dick Carter' date='2019-10-22 21:35:31-07:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\operator\rnn-inl.h' new_name='src\operator\rnn-inl.h'>
 		<file_info nloc='1315' complexity='173' token_count='8222'></file_info>
 		<method name='mxnet::op::RNNOp::~RNNOp' parameters=''>
 				<method_info nloc='30' complexity='6' token_count='230' nesting_level='3' start_line='525' end_line='560'></method_info>
 			<added_lines>540,541</added_lines>
 			<deleted_lines>541</deleted_lines>
 		</method>
 		<method name='mxnet::op::RNNOp::SyncDgrad' parameters=''>
 				<method_info nloc='9' complexity='4' token_count='47' nesting_level='3' start_line='1501' end_line='1511'></method_info>
 			<added_lines>1505,1506,1507,1508</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='mxnet::op::RNNOp::RNNOp' parameters='param,ctx'>
 				<method_info nloc='81' complexity='21' token_count='541' nesting_level='3' start_line='418' end_line='524'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>496</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1542</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
