<bug_data>
<bug id='15759' author='sxjscience' open_date='2019-08-06T00:16:21Z' closed_time='2019-09-05T18:33:47Z'>
 	<summary>[Optimizer][Bug] Gradient is mutated in the Adam optimizer</summary>
 	<description>
 In the implementation of Adam: grad, mean and var are all mutated (See &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op-inl.h#L1307-L1315&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op-inl.h#L1307-L1315&lt;/denchmark-link&gt;
 ). However, the  flag is only set to {2, 3}, which should be {1, 2, 3}. (See &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op.cc#L699&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op.cc#L699&lt;/denchmark-link&gt;
 )
 To reproduce the bug, you may check the value of the gradient before/after  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1226-L1227&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1226-L1227&lt;/denchmark-link&gt;
  .
 We can add the following into optimizer.py
 grad1 = grad.asnumpy()
 adam_update(weight, grad, mean, var, out=weight, ...)
 grad2 = grad.asnumpy()
 import numpy as np
 np.testing.assert_allclose(grad1, grad2)
 Create an adam optimizer with wd=1E-3 and train any model. You will find that the gradient has been mutated.
 	</description>
 	<comments>
 		<comment id='1' author='sxjscience' date='2019-08-06T00:16:23Z'>
 		Hey, this is the MXNet Label Bot.
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
 Here are my recommended labels: Bug
 		</comment>
 		<comment id='2' author='sxjscience' date='2019-08-07T14:30:46Z'>
 		On looking at other optimizer's updater,
 These are the cases where original grad is being updated.
 Will update the PR accordingly.
 RMSPropAlexUpdate
 
 
 
 incubator-mxnet/src/operator/optimizer_op-inl.h
 
 
         Lines 1600 to 1619
       in
       7186123
 
 
 
 
 
 
  inline void RMSPropAlexUpdate(const nnvm::NodeAttrs &amp;attrs, 
 
 
 
  const OpContext &amp;ctx, 
 
 
 
  const std::vector&lt;TBlob&gt; &amp;inputs, 
 
 
 
  const std::vector&lt;OpReqType&gt; &amp;req, 
 
 
 
  const std::vector&lt;TBlob&gt; &amp;outputs) { 
 
 
 
  using namespace mshadow; 
 
 
 
  using namespace mshadow::expr; 
 
 
 
  using namespace mshadow_op; 
 
 
 
  const RMSPropAlexParam &amp;param = nnvm::get&lt;RMSPropAlexParam&gt;(attrs.parsed); 
 
 
 
    Stream&lt;xpu&gt; *s = ctx.get_stream&lt;xpu&gt;(); 
 
 
 
  MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; weight = inputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; grad = inputs[1].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; state_n = inputs[2].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; state_g = inputs[3].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; delta = inputs[4].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; out = outputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
  
 
 
 
      grad = scalar&lt;DType&gt;(param.rescale_grad) * grad + 
 
 
 
             scalar&lt;DType&gt;(param.wd) * weight; 
 
 
 
 
 
 RMSPropUpdate
 
 
 
 incubator-mxnet/src/operator/optimizer_op-inl.h
 
 
         Lines 1691 to 1708
       in
       7186123
 
 
 
 
 
 
  template &lt;typename xpu&gt; 
 
 
 
  inline void RMSPropUpdate(const nnvm::NodeAttrs &amp;attrs, const OpContext &amp;ctx, 
 
 
 
  const std::vector&lt;TBlob&gt; &amp;inputs, 
 
 
 
  const std::vector&lt;OpReqType&gt; &amp;req, 
 
 
 
  const std::vector&lt;TBlob&gt; &amp;outputs) { 
 
 
 
  using namespace mshadow; 
 
 
 
  using namespace mshadow::expr; 
 
 
 
  using namespace mshadow_op; 
 
 
 
  const RMSPropParam &amp;param = nnvm::get&lt;RMSPropParam&gt;(attrs.parsed); 
 
 
 
    Stream&lt;xpu&gt; *s = ctx.get_stream&lt;xpu&gt;(); 
 
 
 
  MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; weight = inputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; grad = inputs[1].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; state_n = inputs[2].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; out = outputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
  
 
 
 
      grad = scalar&lt;DType&gt;(param.rescale_grad) * grad + 
 
 
 
             scalar&lt;DType&gt;(param.wd) * weight; 
 
 
 
 
 
 FtrlUpdate
 
 
 
 incubator-mxnet/src/operator/optimizer_op-inl.h
 
 
         Lines 1784 to 1803
       in
       7186123
 
 
 
 
 
 
  template&lt;typename xpu&gt; 
 
 
 
  inline void FtrlUpdate(const nnvm::NodeAttrs&amp; attrs, 
 
 
 
  const OpContext &amp;ctx, 
 
 
 
  const std::vector&lt;TBlob&gt; &amp;inputs, 
 
 
 
  const std::vector&lt;OpReqType&gt; &amp;req, 
 
 
 
  const std::vector&lt;TBlob&gt; &amp;outputs) { 
 
 
 
  using namespace mshadow; 
 
 
 
  using namespace mshadow::expr; 
 
 
 
  using namespace mshadow_op; 
 
 
 
  const FtrlParam&amp; param = nnvm::get&lt;FtrlParam&gt;(attrs.parsed); 
 
 
 
    Stream&lt;xpu&gt;* s = ctx.get_stream&lt;xpu&gt;(); 
 
 
 
  MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; weight = inputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; grad = inputs[1].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; z = inputs[2].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; n = inputs[3].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
      Tensor&lt;xpu, 2, DType&gt; out = outputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 
 
 
 
  
 
 
 
      grad = scalar&lt;DType&gt;(param.rescale_grad) * grad; 
 
 
 
  
 
 
 
 
 
 		</comment>
 		<comment id='3' author='sxjscience' date='2019-09-05T18:34:58Z'>
 		Thanks &lt;denchmark-link:https://github.com/kshitij12345&gt;@kshitij12345&lt;/denchmark-link&gt;
  !
 		</comment>
 	</comments>
 </bug>
<commit id='d60be31df6b05385cd8adc4b8b26b34a33e7693c' author='kshitij12345' date='2019-09-05 11:33:43-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.6326530612244898' size='0.10204081632653061'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\operator\optimizer_op-inl.h' new_name='src\operator\optimizer_op-inl.h'>
 		<file_info nloc='2032' complexity='200' token_count='17667'></file_info>
 		<method name='mxnet::op::AdamUpdateKernel::Map' parameters='i,out_data,mean_data,var_data,weight_data,grad_data,clip_gradient,rescale_grad,beta1,beta2,lr,wd,epsilon,req'>
 				<method_info nloc='17' complexity='2' token_count='185' nesting_level='3' start_line='1298' end_line='1317'></method_info>
 			<added_lines>1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317</added_lines>
 			<deleted_lines>1302,1303,1304,1314,1315,1316,1317</deleted_lines>
 		</method>
 		<method name='mxnet::op::RMSPropAlexUpdateKernel::Map' parameters='i,out_data,state_n_data,state_g_data,delta_data,weight_data,grad_data,clip_gradient,rescale_grad,gamma1,gamma2,lr,wd,clip_weights,epsilon,req'>
 				<method_info nloc='28' complexity='3' token_count='267' nesting_level='3' start_line='1613' end_line='1643'></method_info>
 			<added_lines>1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643</added_lines>
 			<deleted_lines>1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643</deleted_lines>
 		</method>
 		<method name='mxnet::op::RMSPropAlexUpdate' parameters='attrs,ctx,inputs,req,outputs'>
 				<method_info nloc='23' complexity='1' token_count='314' nesting_level='2' start_line='1647' end_line='1670'></method_info>
 			<added_lines>1652,1656,1657,1658,1659,1660,1661,1663,1664,1665,1666,1667,1668</added_lines>
 			<deleted_lines>1647,1648,1649</deleted_lines>
 		</method>
 		<method name='mxnet::op::FtrlUpdate' parameters='attrs,ctx,inputs,req,outputs'>
 				<method_info nloc='21' complexity='1' token_count='328' nesting_level='2' start_line='1819' end_line='1841'></method_info>
 			<added_lines>1824,1825,1835,1836,1837,1838,1839</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='mxnet::op::AdamUpdate' parameters='attrs,ctx,inputs,req,outputs'>
 				<method_info nloc='22' complexity='1' token_count='338' nesting_level='2' start_line='1321' end_line='1343'></method_info>
 			<added_lines>1326,1336,1337,1338,1339,1340,1341</added_lines>
 			<deleted_lines>1321,1322,1323,1324,1325,1326,1327,1328,1329</deleted_lines>
 		</method>
 		<method name='mxnet::op::FtrlUpdateKernel::Map' parameters='i,out_data,n_data,z_data,weight_data,grad_data,clip_gradient,rescale_grad,beta,lamda1,lr,wd,req'>
 				<method_info nloc='20' complexity='2' token_count='213' nesting_level='3' start_line='1793' end_line='1815'></method_info>
 			<added_lines>1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815</added_lines>
 			<deleted_lines>1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815</deleted_lines>
 		</method>
 		<method name='mxnet::op::RMSPropUpdate' parameters='attrs,ctx,inputs,req,outputs'>
 				<method_info nloc='19' complexity='1' token_count='268' nesting_level='2' start_line='1738' end_line='1757'></method_info>
 			<added_lines>1742,1746,1747,1748,1749,1751,1752,1753,1754,1755</added_lines>
 			<deleted_lines>1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748</deleted_lines>
 		</method>
 		<method name='mxnet::op::RMSPropUpdateKernel::Map' parameters='i,out_data,state_n_data,weight_data,grad_data,clip_gradient,rescale_grad,gamma1,lr,wd,clip_weights,epsilon,req'>
 				<method_info nloc='20' complexity='3' token_count='184' nesting_level='3' start_line='1712' end_line='1734'></method_info>
 			<added_lines>1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734</added_lines>
 			<deleted_lines>1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>1296,1297,1318,1319,1611,1612,1644,1645,1710,1711,1735,1736,1791,1792,1816,1817</added_lines>
 			<deleted_lines>1318,1319,1320,1605,1606,1607,1611,1612,1645,1646,1696,1697,1698,1702,1703,1704,1705,1707,1708,1709,1710,1711,1735,1736,1737,1790,1791,1792,1816,1817,1818</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_ndarray.py' new_name='tests\python\unittest\test_ndarray.py'>
 		<file_info nloc='1596' complexity='241' token_count='21557'></file_info>
 		<method name='test_update_ops_mutation' parameters=''>
 				<method_info nloc='31' complexity='1' token_count='452' nesting_level='0' start_line='1891' end_line='1952'></method_info>
 			<added_lines>1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_update_ops_mutation.test_op' parameters='op,num_inputs,mutated_inputs,kwargs'>
 				<method_info nloc='16' complexity='4' token_count='149' nesting_level='1' start_line='1899' end_line='1924'></method_info>
 			<added_lines>1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_update_ops_mutation.assert_mutate' parameters='x,y,op'>
 				<method_info nloc='3' complexity='1' token_count='27' nesting_level='1' start_line='1892' end_line='1894'></method_info>
 			<added_lines>1892,1893,1894</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_update_ops_mutation.assert_unchanged' parameters='x,y,op'>
 				<method_info nloc='2' complexity='1' token_count='19' nesting_level='1' start_line='1896' end_line='1897'></method_info>
 			<added_lines>1896,1897</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>31,1890,1953,1954</added_lines>
 			<deleted_lines>31</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
