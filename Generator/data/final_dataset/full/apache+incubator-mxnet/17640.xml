<bug_data>
<bug id='17640' author='apeforest' open_date='2020-02-20T22:35:39Z' closed_time='2020-02-26T06:25:42Z'>
 	<summary>[OPPERF] opperf error out if I use python timer instead of the builtin mxnet profiler</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 Seems the python timer did not parse the argument to broadcast_axis properly. It is fine if I use the 'native' profiler.
 &lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "perf.py", line 12, in &lt;module&gt;
     warmup=20, runs=100, profiler='python')
   File "/home/ubuntu/src/incubator-mxnet/benchmark/opperf/utils/benchmark_utils.py", line 177, in run_performance_test
     benchmark_result = _run_nd_operator_performance_test(op, inputs, run_backward, warmup, runs, kwargs_list, profiler)
   File "/home/ubuntu/src/incubator-mxnet/benchmark/opperf/utils/benchmark_utils.py", line 114, in _run_nd_operator_performance_test
     _, _ = benchmark_helper_func(op, warmup, **kwargs_list[0])
   File "/home/ubuntu/src/incubator-mxnet/benchmark/opperf/utils/profiler_utils.py", line 251, in python_profile_it
     modified_args = (args[0], 1, args[2])
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;#!/usr/bin/python
 import mxnet as mx
 from mxnet import nd
 
 from benchmark.opperf.utils.benchmark_utils import run_performance_test
 
 mx.random.seed(17)
 res = run_performance_test(nd.broadcast_axis, run_backward=True, dtype='float32', ctx=mx.gpu(),
                            inputs=[
                                {"data" : (1, 1024, 1), "axis" : (0, 2), "size" : (1024, 8)}
                            ],
                            warmup=20, runs=100, profiler='python')
 print(res)
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='apeforest' date='2020-02-20T22:36:10Z'>
 		&lt;denchmark-link:https://github.com/ChaiBapchya&gt;@ChaiBapchya&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/connorgoggins&gt;@connorgoggins&lt;/denchmark-link&gt;
  Would you like to take a look at this?
 		</comment>
 		<comment id='2' author='apeforest' date='2020-02-20T22:38:11Z'>
 		&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
  working to reproduce now.
 		</comment>
 		<comment id='3' author='apeforest' date='2020-02-20T23:10:56Z'>
 		Reproduced on CPU, debugging now.
 		</comment>
 		<comment id='4' author='apeforest' date='2020-02-20T23:38:37Z'>
 		Implemented fix here: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/17642&gt;[OpPerf] Fixed Python profiler bug&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='1906eff6b8a1eb96903f61583e722d1819ac657e' author='Connor Goggins' date='2020-02-25 22:25:41-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='benchmark\opperf\utils\profiler_utils.py' new_name='benchmark\opperf\utils\profiler_utils.py'>
 		<file_info nloc='151' complexity='29' token_count='729'></file_info>
 		<method name='python_profile' parameters='func'>
 				<method_info nloc='4' complexity='1' token_count='17' nesting_level='0' start_line='223' end_line='280'></method_info>
 			<added_lines>251</added_lines>
 			<deleted_lines>251</deleted_lines>
 		</method>
 		<method name='python_profile.python_profile_it' parameters='args,kwargs'>
 				<method_info nloc='26' complexity='4' token_count='194' nesting_level='1' start_line='249' end_line='279'></method_info>
 			<added_lines>251</added_lines>
 			<deleted_lines>251</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
