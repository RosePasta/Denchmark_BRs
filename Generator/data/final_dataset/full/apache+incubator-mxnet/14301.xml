<bug_data>
<bug id='14301' author='ashokei' open_date='2019-03-02T07:34:24Z' closed_time='2019-03-05T08:14:59Z'>
 	<summary>SoftmaxOutput crashes with normalization "valid"</summary>
 	<description>
 &lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;
 
 &lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;
 
 ubuntu 16.04 default build
 and, run below script.
 &lt;denchmark-code&gt;import numpy as np                                                
 import mxnet as mx                                                
 xpu = mx.cpu()                                                    
 x = mx.sym.Variable('x')                                          
 label = mx.sym.Variable('label')                                  
 x_nd = mx.nd.array([[1, 6, 4, 2],[1, 6, 4, 2]], ctx=xpu)          
 grad_x = mx.nd.zeros((2,4), ctx=xpu)                              
 label_nd = mx.nd.array([1,1], ctx=xpu)                            
                                                                   
 sym = mx.sym.SoftmaxOutput(data=x, label=label, ignore_label=0,   
                            use_ignore=True, normalization="valid")
 ex = sym.bind(ctx=xpu, args={'x': x_nd, 'label': label_nd},       
               args_grad={'x': grad_x})                            
                                                                   
 ex.forward(is_train=True)                                         
 softmax_out = ex.outputs[0].asnumpy()                             
 ex.backward(is_train=True)                                        
 
 &lt;/denchmark-code&gt;
 
 MXNet commit hash:
 &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/fb4f9d55382538fe688638b741830d84ae0d783e&gt;fb4f9d5&lt;/denchmark-link&gt;
 
 Build config:
 make
 &lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;
 
 &lt;denchmark-code&gt;terminated by signal SIGSEGV (Address boundary error)
 &lt;/denchmark-code&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='ashokei' date='2019-03-02T07:34:26Z'>
 		Hey, this is the MXNet Label Bot.
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
 Here are my recommended labels: Bug
 		</comment>
 		<comment id='2' author='ashokei' date='2019-03-02T08:51:09Z'>
 		Thanks for your report!
 reproduce the bug in MXNet &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/fb4f9d55382538fe688638b741830d84ae0d783e&gt;fb4f9d5&lt;/denchmark-link&gt;
 
 It is strange that the address of ctx.requested[softmaxout_enum::kTempSpace] is 0 in src/operator/softmax_output-inl.h   .
 ctx.requested.size() is 0 in Backward.
 It is a bug that BackwardResource is not called when ex.backward(is_train=true) is called.
 I do not know why SoftmaxOutput is not a legacy operator.
 		</comment>
 		<comment id='3' author='ashokei' date='2019-03-04T00:04:44Z'>
 		I too recently saw an issue with Softmax that generated a segfault.  This behavior began with the Softmax operator changes introduced by &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/13699&gt;#13699&lt;/denchmark-link&gt;
  and occurs when the framework is compiled with USE_MKLDNN=0.  The failing test is with sockeye:
 &lt;denchmark-code&gt;test/integration/test_constraints_int.py::test_constraints[--encoder rnn --decoder rnn --num-layers 1 --rnn-cell-type lstm --rnn-num-hidden 8 --num-embed 4  --rnn-attention-type mlp --rnn-attention-num-hidden 8 --loss cross-entropy --optimized-metric perplexity --max-updates 2 --checkpoint-frequency 2 --optimizer adam --initial-learning-rate 0.01 --batch-type sentence  --decode-and-evaluate 0-2-10] ./test.sh: line 3:    62 Segmentation fault      python setup.py test
 ++ RV=139
 &lt;/denchmark-code&gt;
 
 Perhaps you could verify that your fix corrects this behavior?
 		</comment>
 		<comment id='4' author='ashokei' date='2019-03-04T06:15:06Z'>
 		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
 
 Hi! I test sockeye in my laptop with MXNet(master) with 
 All tests pass except for test/unit/test_inference.py::test_topk_func
 &lt;denchmark-code&gt;test/unit/test_inference.py::test_topk_func[1-5-200] FAILED              [ 60%]
 test/unit/test_inference.py::test_topk_func[5-5-200] FAILED              [ 60%]
 test/unit/test_inference.py::test_topk_func[1-1-200] PASSED              [ 60%]
 test/unit/test_inference.py::test_topk_func[5-1-200] PASSED              [ 60%]
 test/unit/test_inference.py::test_topk_func[10-10-100] FAILED            [ 60%]
 &lt;/denchmark-code&gt;
 
 In sockeye, there is no any Softmax with normalization valid, so it couldn't trigger the bug in this issue.
 		</comment>
 		<comment id='5' author='ashokei' date='2019-03-04T21:09:58Z'>
 		I can also confirm this issue, it happens only when normalization-"valid" and while executing the Executor.backward function call. For instance this sample code works fine -
 &lt;denchmark-code&gt;import mxnet as mx                                                                                                                                            
 import numpy as np
 
 xpu = mx.cpu()
 x_nd = mx.nd.array([[1, 6, 4, 2],[1, 6, 4, 2]], ctx=xpu)    
 grad_x = mx.nd.zeros((2,4), ctx=xpu)    
 label_nd = mx.nd.array([1,1], ctx=xpu)
 
 x_nd.attach_grad()
 
 with mx.autograd.record():
     y = mx.nd.SoftmaxOutput(data=x_nd, label=label_nd, ignore_label=0, use_ignore=True) #, normalization="valid")
 
 y.backward()
 print(x_nd.grad)
 &lt;/denchmark-code&gt;
 
 So the bug is with the gradient calculation of softmax output when normalization="valid"
 		</comment>
 		<comment id='6' author='ashokei' date='2019-03-04T21:48:16Z'>
 		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
  Sockeye can use 'valid' normalization in its SoftmaxOutput operator use, see &lt;denchmark-link:https://github.com/awslabs/sockeye/blob/2f44099cd4f488bd8d348d74e9ae85095f72501e/sockeye/loss.py#L112&gt;here&lt;/denchmark-link&gt;
 .
 The failure you are observing for  is related to &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/13862&gt;#13862&lt;/denchmark-link&gt;
 , which is an open problem.
 		</comment>
 		<comment id='7' author='ashokei' date='2019-03-04T23:02:26Z'>
 		&lt;denchmark-link:https://github.com/fhieber&gt;@fhieber&lt;/denchmark-link&gt;
  Sorry that I overlooked it.
 &lt;denchmark-link:https://github.com/anirudhacharya&gt;@anirudhacharya&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/14302&gt;#14302&lt;/denchmark-link&gt;
  will address the problem.
 		</comment>
 	</comments>
 </bug>
<commit id='5065f13dfee6153df573339067954f22257ef9a6' author='JackieWu' date='2019-03-05 16:14:58+08:00'>
 	<dmm_unit complexity='0.058823529411764705' interfacing='0.058823529411764705' size='0.058823529411764705'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\operator\softmax_output-inl.h' new_name='src\operator\softmax_output-inl.h'>
 		<file_info nloc='405' complexity='64' token_count='3637'></file_info>
 		<method name='mxnet::op::SoftmaxOutputProp::BackwardResource' parameters='in_shape'>
 				<method_info nloc='4' complexity='1' token_count='20' nesting_level='3' start_line='429' end_line='432'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>429,430,431,432</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>433</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='src\operator\softmax_output.cc' new_name='src\operator\softmax_output.cc'>
 		<file_info nloc='225' complexity='21' token_count='1469'></file_info>
 		<modified_lines>
 			<added_lines>265,266,267</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_operator.py' new_name='tests\python\unittest\test_operator.py'>
 		<file_info nloc='6271' complexity='1004' token_count='78811'></file_info>
 		<method name='test_softmax_output_normalization._softmaxoutput_normalization' parameters='multi_output,use_ignore,normalization'>
 				<method_info nloc='48' complexity='11' token_count='414' nesting_level='1' start_line='6501' end_line='6559'></method_info>
 			<added_lines>6501,6502,6503,6504,6505,6506,6507,6508,6509,6510,6511,6512,6513,6514,6515,6516,6517,6518,6519,6520,6521,6522,6523,6524,6525,6526,6527,6528,6529,6530,6531,6532,6533,6534,6535,6536,6537,6538,6539,6540,6541,6542,6543,6544,6545,6546,6547,6548,6549,6550,6551,6552,6553,6554,6555,6556,6557,6558,6559</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_softmax_output_normalization' parameters=''>
 				<method_info nloc='7' complexity='4' token_count='43' nesting_level='0' start_line='6500' end_line='6565'></method_info>
 			<added_lines>6500,6501,6502,6503,6504,6505,6506,6507,6508,6509,6510,6511,6512,6513,6514,6515,6516,6517,6518,6519,6520,6521,6522,6523,6524,6525,6526,6527,6528,6529,6530,6531,6532,6533,6534,6535,6536,6537,6538,6539,6540,6541,6542,6543,6544,6545,6546,6547,6548,6549,6550,6551,6552,6553,6554,6555,6556,6557,6558,6559,6560,6561,6562,6563,6564,6565</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>6499,6566,6567,6605,6628</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
