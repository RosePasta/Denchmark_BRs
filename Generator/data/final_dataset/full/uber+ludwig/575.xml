<bug_data>
<bug id='575' author='hamletbatista' open_date='2019-11-20T19:55:43Z' closed_time='2020-02-27T23:42:36Z'>
 	<summary>Can't parse SavedModel to use in TensorflowJs</summary>
 	<description>
 Describe the bug
 A clear and concise description of what the bug is.
 I'm trying to export a trained model so I can run inference using TensorflowJs, but the exported .pb doesn't work with the TensorflowJs converter tool. I get this error:
 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
 Instructions for updating:
 If using Keras pass *_constraint arguments to layers.
 Traceback (most recent call last):
 File "/usr/local/bin/tensorflowjs_converter", line 8, in 
 sys.exit(pip_main())
 File "/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py", line 638, in pip_main
 main([' '.join(sys.argv[1:])])
 File "/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py", line 642, in main
 convert(argv[0].split(' '))
 File "/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py", line 591, in convert
 strip_debug_ops=args.strip_debug_ops)
 File "/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 419, in convert_tf_saved_model
 model = load(saved_model_dir, saved_model_tags)
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load.py", line 519, in load
 return load_internal(export_dir, tags)
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load.py", line 550, in load_internal
 root = load_v1_in_v2.load(export_dir, tags)
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py", line 239, in load
 return loader.load(tags=tags)
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py", line 222, in load
 signature_functions = self._extract_signatures(wrapped, meta_graph_def)
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py", line 138, in _extract_signatures
 signature_fn = wrapped.prune(feeds=feeds, fetches=fetches)
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/wrap_function.py", line 320, in prune
 sources=flat_feeds + self.graph.internal_captures)
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/lift_to_graph.py", line 260, in lift_to_graph
 add_sources=add_sources))
 File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/op_selector.py", line 413, in map_subgraph
 % (repr(init_tensor), repr(op), _path_from(op, init_tensor, sources)))
 tensorflow.python.ops.op_selector.UnliftableError: A SavedModel signature needs an input for each placeholder the signature's outputs use. An output for signature 'predict' depends on a placeholder which is not an input (i.e. the placeholder is not fed a value).
 Unable to lift tensor &lt;tf.Tensor 'Category0/predictions_Category0/predictions_Category0:0' shape=(?,) dtype=int64&gt; because it depends transitively on placeholder &lt;tf.Operation 'is_training' type=Placeholder&gt; via at least one path, e.g.: Category0/predictions_Category0/predictions_Category0 (ArgMax) &lt;- Category0/predictions_Category0/add (Add) &lt;- Category0/predictions_Category0/MatMul (MatMul) &lt;- concat_combiner/concat_combiner (Identity) &lt;- concat_combiner/concat (Identity) &lt;- Questions/Questions (Identity) &lt;- Questions/dropout/cond/Merge (Merge) &lt;- Questions/dropout/cond/dropout/mul_1 (Mul) &lt;- Questions/dropout/cond/dropout/Cast (Cast) &lt;- Questions/dropout/cond/dropout/GreaterEqual (GreaterEqual) &lt;- Questions/dropout/cond/dropout/rate (Const) &lt;- Questions/dropout/cond/switch_t (Identity) &lt;- Questions/dropout/cond/Switch (Switch) &lt;- is_training (Placeholder)
 
 Steps to reproduce the behavior:
 You can follow my steps in this colab notebook &lt;denchmark-link:https://colab.research.google.com/drive/1c1REIK3G5FzwuCxmO8R0xA_0ODDlC57z#scrollTo=vNudSgJAZ7JB&gt;https://colab.research.google.com/drive/1c1REIK3G5FzwuCxmO8R0xA_0ODDlC57z#scrollTo=vNudSgJAZ7JB&lt;/denchmark-link&gt;
 
 Please provide code, yaml definition file and a sample of data in order to entirely reproduce the issue.
 Issues that are not reproducible will be ignored.
 Everything is in the Colab notebook.
 Expected behavior
 A clear and concise description of what you expected to happen.
 I am hoping to load the trained model in TensorflowJs
 Screenshots
 If applicable, add screenshots to help explain your problem.
 Environment (please complete the following information):
 
 OS: macOS
 Version: Catalina
 Python version: 3.6.8
 Ludwig version: 0.2.1
 
 
 Add any other context about the problem here.
 I tried the ideas in this article &lt;denchmark-link:https://github.com/ludwig-ai/ludwig/issues/329#issuecomment-548854347&gt;#329 (comment)&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='hamletbatista' date='2019-11-21T19:52:53Z'>
 		This is not a functionality we currently support, so will mark it as enhancement, but at the same time from the log it seems the problem has to do with the way we save the SavedModel rather than a tfjs specific thing, so we will investigate the issue, and that could potentially solve the problem for tfjs too.
 		</comment>
 		<comment id='2' author='hamletbatista' date='2019-11-26T00:13:46Z'>
 		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
  thanks
 		</comment>
 		<comment id='3' author='hamletbatista' date='2020-02-04T18:05:44Z'>
 		Hey &lt;denchmark-link:https://github.com/hamletbatista&gt;@hamletbatista&lt;/denchmark-link&gt;
 , would you mind providing the model_definition.yaml which is referenced in the codelab? Thanks
 		</comment>
 		<comment id='4' author='hamletbatista' date='2020-02-04T21:08:29Z'>
 		Actually never mind, I see it in the template.
 		</comment>
 		<comment id='5' author='hamletbatista' date='2020-02-04T22:25:33Z'>
 		&lt;denchmark-link:https://github.com/ydudin3&gt;@ydudin3&lt;/denchmark-link&gt;
  glad to know. Please let me know if are able to get this to work
 		</comment>
 		<comment id='6' author='hamletbatista' date='2020-02-06T01:37:08Z'>
 		&lt;denchmark-link:https://github.com/hamletbatista&gt;@hamletbatista&lt;/denchmark-link&gt;
  it seems from the logs that output tensors get appended to the input_tensors list
 For example printing input_tensors yields:
 {'Category0': &lt;tf.Tensor 'Category0/Category0_placeholder:0' shape=(?,) dtype=int64&gt;, 'Category2': &lt;tf.Tensor 'Category2/Category2_placeholder:0' shape=(?,) dtype=int64&gt;, 'Questions': &lt;tf.Tensor 'Questions/Questions_placeholder:0' shape=(?, ?) dtype=int32&gt;}
 It looks like get_tensors function has these few lines in it:
 for output_feature in model_definition['output_features']: input_tensors[output_feature['name']] = getattr(model, output_feature['name'])
 Is this intentional? I wonder if that's what's causing model load failure.
 		</comment>
 		<comment id='7' author='hamletbatista' date='2020-02-06T20:41:23Z'>
 		That is in the cell after "try it again". Not sure why you are running it that way instead of using model.save_savedmodel().
 		</comment>
 		<comment id='8' author='hamletbatista' date='2020-02-06T21:02:34Z'>
 		
 That is in the cell after "try it again". Not sure why you are running it that way instead of using model.save_savedmodel().
 
 I tried that first. See cells above.
 &lt;denchmark-link:https://user-images.githubusercontent.com/1514243/73978311-133cd880-48fa-11ea-92d8-ba268f75744c.png&gt;&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='9' author='hamletbatista' date='2020-02-06T21:04:42Z'>
 		
 @hamletbatista it seems from the logs that output tensors get appended to the input_tensors list
 For example printing input_tensors yields:
 {'Category0': &lt;tf.Tensor 'Category0/Category0_placeholder:0' shape=(?,) dtype=int64&gt;, 'Category2': &lt;tf.Tensor 'Category2/Category2_placeholder:0' shape=(?,) dtype=int64&gt;, 'Questions': &lt;tf.Tensor 'Questions/Questions_placeholder:0' shape=(?, ?) dtype=int32&gt;}
 It looks like get_tensors function has these few lines in it:
 for output_feature in model_definition['output_features']: input_tensors[output_feature['name']] = getattr(model, output_feature['name'])
 Is this intentional? I wonder if that's what's causing model load failure.
 
 I haven't touched this code in a while, but I added links in the comments to where I was finding suggestions to fix the issue.
 The suggestion came from this comment it appears &lt;denchmark-link:https://github.com/ludwig-ai/ludwig/issues/329#issuecomment-508777581&gt;#329 (comment)&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='10' author='hamletbatista' date='2020-02-06T21:06:14Z'>
 		I took a different route to solve this using HuggingFace's library, but yours would make the tutorial much simpler to follow
 		</comment>
 		<comment id='11' author='hamletbatista' date='2020-02-06T21:24:25Z'>
 		
 I tried that first. See cells above.
 
 Yes but it is commented out, and I don't see errors there, what was wrong with it?
 
 I took a different route to solve this using HuggingFace's library, but yours would make the tutorial much simpler to follow
 
 We are adding import of Huggingface's transformer library in the next version of Ludwig. Wonder how are you serving it as even the smaller distilled model is really expensive to use at inference time.
 		</comment>
 		<comment id='12' author='hamletbatista' date='2020-02-06T23:12:33Z'>
 		
 
 I tried that first. See cells above.
 
 Yes but it is commented out, and I don't see errors there, what was wrong with it?
 
 There was no error or stack trace. The issue was the file generated seemed corrupted or incomplete when I tried to load it.
 I will give it another try over the weekend.
 
 
 I took a different route to solve this using HuggingFace's library, but yours would make the tutorial much simpler to follow
 
 We are adding import of Huggingface's transformer library in the next version of Ludwig. Wonder how are you serving it as even the smaller distilled model is really expensive to use at inference time.
 
 Yes. I have that problem and this is mostly a learning exercise to teach marketers, not for production use. I have in my queue to investigate this research next &lt;denchmark-link:https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/&gt;https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/&lt;/denchmark-link&gt;
 
 It seems to solve that issue.
 		</comment>
 		<comment id='13' author='hamletbatista' date='2020-02-07T01:20:05Z'>
 		
 There was no error or stack trace. The issue was the file generated seemed corrupted or incomplete when I tried to load it.
 
 Got it, but you can understand that I need to se the error you were getting about corrupted file, otherwise it's difficult to figure out what the problem is.
 Ideally you could provide a minimal self contained reproducible zip containing either data and a python script (data can be generated with the data/dataset_sythesizer.py script if you can't share it) or data + yaml file + command to run it.
 
 Yes. I have that problem and this is mostly a learning exercise to teach marketers, not for production use. I have in my queue to investigate this research next https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/
 It seems to solve that issue.
 
 It was tested on 3 layers bert, the latency is much higher on the full model. Still, it's a step forward ;)
 Anyway, I thought you usecase was fast inference at deployment time, but if your goal is just demoing, and you don't care about a super scalable inference pipeline, then you can train a model with Ludwig and then serve it with
 &lt;denchmark-code&gt;ludwig serve --model_path path/to/trained/model
 &lt;/denchmark-code&gt;
 
 and it will launch a REST API server you can query easily. More info in the &lt;denchmark-link:https://uber.github.io/ludwig/user_guide/#serve&gt;User Guide&lt;/denchmark-link&gt;
 .
 		</comment>
 		<comment id='14' author='hamletbatista' date='2020-02-07T09:24:36Z'>
 		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
  just FYI:  works incorrectly now. That's because of wrong placeholders' names &lt;denchmark-link:https://github.com/ludwig-ai/ludwig/issues/329#issuecomment-548854347&gt;#329 (comment)&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='15' author='hamletbatista' date='2020-02-07T12:45:38Z'>
 		
 
 There was no error or stack trace. The issue was the file generated seemed corrupted or incomplete when I tried to load it.
 
 Got it, but you can understand that I need to se the error you were getting about corrupted file, otherwise it's difficult to figure out what the problem is.
 Ideally you could provide a minimal self contained reproducible zip containing either data and a python script (data can be generated with the data/dataset_sythesizer.py script if you can't share it) or data + yaml file + command to run it.
 
 Yes. I will have time over the weekend :)
 
 
 Yes. I have that problem and this is mostly a learning exercise to teach marketers, not for production use. I have in my queue to investigate this research next https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/
 It seems to solve that issue.
 
 It was tested on 3 layers bert, the latency is much higher on the full model. Still, it's a step forward ;)
 
 Interesting. I will see if I can get decent accuracy. Thanks for the insights.
 
 Anyway, I thought you usecase was fast inference at deployment time, but if your goal is just demoing, and you don't care about a super scalable inference pipeline, then you can train a model with Ludwig and then serve it with
 ludwig serve --model_path path/to/trained/model
 
 and it will launch a REST API server you can query easily. More info in the User Guide.
 
 I need to run the model in JS to embed in Google Sheets and Excel. Fetching from a serving URL would be my fall back option.
 Thanks
 		</comment>
 		<comment id='16' author='hamletbatista' date='2020-02-07T18:51:52Z'>
 		
 @w4nderlust just FYI: save_savemodel works incorrectly now. That's because of wrong placeholders' names #329 (comment)
 
 Thanks, yes we are working on it. &lt;denchmark-link:https://github.com/ydudin3&gt;@ydudin3&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='17' author='hamletbatista' date='2020-02-27T23:42:36Z'>
 		The merged PR should have solved the issue. There's also an integration test for SavedModel now that shows how to load and save SavedModels and what kind of preprocessing and postprocessing you need to do in order to map data to tensors and prediction tensors to data: &lt;denchmark-link:https://github.com/uber/ludwig/blob/master/tests/integration_tests/test_savedmodel.py&gt;https://github.com/uber/ludwig/blob/master/tests/integration_tests/test_savedmodel.py&lt;/denchmark-link&gt;
 . Let us know if you have further problems.
 		</comment>
 		<comment id='18' author='hamletbatista' date='2020-02-28T13:40:36Z'>
 		Thanks. I will check this out. This was sorely needed!
 		</comment>
 	</comments>
 </bug>
<commit id='4039f93e605ed929e4f4a75c725bb70e148df92b' author='Piero Molino' date='2020-02-27 15:39:59-08:00'>
 	<dmm_unit complexity='0.0' interfacing='0.8955223880597015' size='0.07462686567164178'></dmm_unit>
 	<modification change_type='MODIFY' old_name='ludwig\cli.py' new_name='ludwig\cli.py'>
 		<file_info nloc='84' complexity='13' token_count='467'></file_info>
 		<method name='export' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='34' nesting_level='1' start_line='106' end_line='109'></method_info>
 			<added_lines>106,107,108,109</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='saved_model_predict' parameters='self'>
 				<method_info nloc='4' complexity='1' token_count='34' nesting_level='1' start_line='111' end_line='114'></method_info>
 			<added_lines>111,112,113,114</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>110,115</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='ludwig\models\model.py' new_name='ludwig\models\model.py'>
 		<file_info nloc='1404' complexity='68' token_count='6548'></file_info>
 		<method name='save_savedmodel' parameters='self,save_path'>
 				<method_info nloc='41' complexity='7' token_count='252' nesting_level='1' start_line='1400' end_line='1448'></method_info>
 			<added_lines>1402,1403,1404,1405,1406,1408,1409,1410,1412,1413,1414,1415,1416,1417,1418,1419,1420,1422,1423,1424,1425,1426,1427,1428,1429,1430,1432,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448</added_lines>
 			<deleted_lines>1403,1404,1405,1406,1409,1410,1411,1412,1413,1416,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1435</deleted_lines>
 		</method>
 		<method name='load' parameters='load_path,use_horovod'>
 				<method_info nloc='12' complexity='1' token_count='54' nesting_level='1' start_line='1435' end_line='1446'></method_info>
 			<added_lines>1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446</added_lines>
 			<deleted_lines>1435</deleted_lines>
 		</method>
 		<method name='load' parameters='load_path,gpus,gpu_fraction,use_horovod'>
 				<method_info nloc='14' complexity='1' token_count='82' nesting_level='1' start_line='1454' end_line='1467'></method_info>
 			<added_lines>1454,1465,1466</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines>38</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='ADD' old_name='None' new_name='tests\integration_tests\test_savedmodel.py'>
 		<file_info nloc='120' complexity='6' token_count='648'></file_info>
 	</modification>
 </commit>
</bug_data>
