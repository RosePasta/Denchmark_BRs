<bug_data>
<bug id='3048' author='marii-moe' open_date='2020-12-02T09:30:20Z' closed_time='2020-12-02T14:35:20Z'>
 	<summary>Gradient Accumulation + Mixed Precision shows artificially high training loss</summary>
 	<description>
 Example of issue on forum: &lt;denchmark-link:https://forums.fast.ai/t/what-is-the-v2-equivalent-of-accumulatescheduler/66409/13&gt;https://forums.fast.ai/t/what-is-the-v2-equivalent-of-accumulatescheduler/66409/13&lt;/denchmark-link&gt;
 
 Please confirm you have the latest versions of fastai, fastcore, fastscript, and nbdev prior to reporting a bug (delete one): YES
 Describe the bug
 The bug occurs when Gradient Accumulation and the MixedPrecision Callback are both used. Gradient Accumulation run before Mixed Precision and causes the after_backwards to not be run, meaning that the loss is not unscaled before it is logged. This means that very large losses such as 6000000+ to be logged.
 To Reproduce
 Steps to reproduce the behavior:
 Run this code:
 &lt;denchmark-code&gt;seed=random.randint(0,2**32-1)
 
 with no_random(seed): 
     db=synth_dbunch(bs=8,n_train=1,n_valid=1,cuda=True)
     learn = synth_learner(data=db)
     learn.fit(1, lr=0.01)
 #start without gradient overflow
 max_loss_scale=2048.0
 with no_random(seed): 
     db=synth_dbunch(bs=1,n_train=8,n_valid=8,cuda=True)
     learn = synth_learner(data=db,cbs=[GradientAccumulation(n_acc=8)])
     learn.to_fp16(max_loss_scale=max_loss_scale)
     learn.fit(1, lr=0.01)
 &lt;/denchmark-code&gt;
 
 The training loss will be very high, 5000+ for fp16. fp32 will be reasonable.
 Expected behavior
 Similar training loss between fp32 and fp16 versions. &lt;2 difference in loss.
 Additional context
 I have already gotten a fix, bill be submitting it in pull request soon.
 	</description>
 	<comments>
 	</comments>
 </bug>
<commit id='d5599538539e8c8cc2bcde5cd378a241c8270232' author='Marii' date='2020-12-02 06:35:20-08:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fastai\callback\fp16.py' new_name='fastai\callback\fp16.py'>
 		<file_info nloc='132' complexity='53' token_count='1335'></file_info>
 		<method name='after_backward' parameters='self'>
 				<method_info nloc='17' complexity='10' token_count='149' nesting_level='1' start_line='97' end_line='115'></method_info>
 			<added_lines>101</added_lines>
 			<deleted_lines>98,104</deleted_lines>
 		</method>
 		<method name='after_batch' parameters='self'>
 				<method_info nloc='2' complexity='2' token_count='20' nesting_level='1' start_line='120' end_line='122'></method_info>
 			<added_lines>120,121,122</added_lines>
 			<deleted_lines>121</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>75,137</added_lines>
 			<deleted_lines>75</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\18_callback.fp16.ipynb' new_name='nbs\18_callback.fp16.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>562,566,592,611,612,613,628,698,699,704,705,710,711,759,760,765,766,771,772,834,835,840,841,846,847,893,894,899,900,905,906,1006,1007,1012,1013,1018,1019</added_lines>
 			<deleted_lines>562,566,589,595,612,696,697,702,703,708,709,757,758,763,764,769,770,832,833,838,839,844,845,891,892,897,898,903,904,1004,1005,1010,1011,1016,1017</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\18a_callback.training.ipynb' new_name='nbs\18a_callback.training.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>39,40,41,42,43,44,45,46,47,48,146,216,217,245,246,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,403,404,432,433,438,439,577,578,625,626</added_lines>
 			<deleted_lines>39,137,207,208,236,237,293,294,299,300,328,329,334,335,473,474,521,522</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
