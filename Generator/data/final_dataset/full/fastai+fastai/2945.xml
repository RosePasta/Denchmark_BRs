<bug_data>
<bug id='2945' author='rsayn' open_date='2020-11-06T09:04:33Z' closed_time='2020-11-13T17:44:50Z'>
 	<summary>[Model serialization] Exporting TabularLearner via learn.export() leads to huge file size</summary>
 	<description>
 Please confirm you have the latest versions of fastai, fastcore, fastscript, and nbdev prior to reporting a bug (delete one): YES
 Describe the bug
 Exporting TabularLearner via learn.export() leads to huge Pickle file size (&gt;80MB).
 To Reproduce
 Steps to reproduce the behavior:
 
 Create a TabularLearner
 Train it
 Export it to pickle file via learn.export(filepath)
 
 Expected behavior
 The Pickle file should be smaller in size.
 Error with full stack trace
 N/A
 Additional context
 By creating different learners with DataFrames of varying size, I noticed that the size of the pickled file increases with the dataset dimension, although after re-loading the serialized file learn.dls is empty as expected.
 	</description>
 	<comments>
 		<comment id='1' author='rsayn' date='2020-11-06T16:11:09Z'>
 		There's a temporary solution here: &lt;denchmark-link:https://walkwithfastai.com/tab.export#Exporting-our-TabularPandas&gt;https://walkwithfastai.com/tab.export#Exporting-our-TabularPandas&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='rsayn' date='2020-11-06T22:25:16Z'>
 		I've narrowed down the issue to the ReadTabBatch transform, we're always storing a copy of the dataframe into memory through this
 		</comment>
 		<comment id='3' author='rsayn' date='2020-11-07T18:57:32Z'>
 		Should be fixed now.
 		</comment>
 		<comment id='4' author='rsayn' date='2020-11-12T10:28:13Z'>
 		Hi, I upgraded Fastai to the lastest version (2.1.5) and I'm still experiencing the same issue.
 I created a notebook on Colab to reproduce the problem:
 &lt;denchmark-link:https://colab.research.google.com/drive/1yvQwIrC9zfI0jq5xZq_h4JVkoXWszcHC?usp=sharing&gt;https://colab.research.google.com/drive/1yvQwIrC9zfI0jq5xZq_h4JVkoXWszcHC?usp=sharing&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='5' author='rsayn' date='2020-11-12T10:43:53Z'>
 		I've done the same (after upgrading to 2.1.5) and I also experienced the same issue:
 Model trained with adult.csv, exported to a 161K file.
 Model trained with the same dataset, but containing 500K rows, exported to a 8Mb file.
 &lt;denchmark-link:https://colab.research.google.com/drive/1zhSKeJCB5CvTiQKgYWubey9w1VzbNiG2?usp=sharing&gt;https://colab.research.google.com/drive/1zhSKeJCB5CvTiQKgYWubey9w1VzbNiG2?usp=sharing&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='6' author='rsayn' date='2020-11-12T14:02:29Z'>
 		&lt;denchmark-link:https://github.com/claudiobottari&gt;@claudiobottari&lt;/denchmark-link&gt;
  I've found the issue. It's after we  the model
 		</comment>
 		<comment id='7' author='rsayn' date='2020-11-12T15:34:13Z'>
 		Here's a minimal reproducer showing that there is a duplicate validation dataframe being added after fit:
 &lt;denchmark-link:https://gist.github.com/muellerzr/df3fc4a12b021be85639afddab3c5d32&gt;https://gist.github.com/muellerzr/df3fc4a12b021be85639afddab3c5d32&lt;/denchmark-link&gt;
 
 &lt;denchmark-link:https://github.com/jph00&gt;@jph00&lt;/denchmark-link&gt;
  we should reopen this issue
 		</comment>
 		<comment id='8' author='rsayn' date='2020-11-13T09:00:33Z'>
 		The problem is that  is not deleting its  attribute after the fit. I modified &lt;denchmark-link:https://github.com/muellerzr&gt;@muellerzr&lt;/denchmark-link&gt;
  gist showing how to solve the problem.
 &lt;denchmark-link:https://gist.github.com/ababino/2a2c67ac264e2ed8c95144377b9be2b4&gt;https://gist.github.com/ababino/2a2c67ac264e2ed8c95144377b9be2b4&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='9' author='rsayn' date='2020-12-02T13:01:46Z'>
 		&lt;denchmark-link:https://github.com/muellerz&gt;@muellerz&lt;/denchmark-link&gt;
  can you provide a snippet on how to deserialize a bugged model and re-serialize it with the correct size?
 I have a lot of models affected by this issue that I can't retrain.
 Thanks in advance.
 		</comment>
 	</comments>
 </bug>
<commit id='6dbbb9088b8354a1f26dc2927af6535ed39b08f7' author='Zachary Mueller' date='2020-11-06 19:29:26-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fastai\tabular\core.py' new_name='fastai\tabular\core.py'>
 		<file_info nloc='291' complexity='107' token_count='3638'></file_info>
 		<method name='_maybe_expand' parameters='o'>
 				<method_info nloc='5' complexity='2' token_count='31' nesting_level='0' start_line='315' end_line='336'></method_info>
 			<added_lines>320</added_lines>
 			<deleted_lines>320</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\40_tabular.core.ipynb' new_name='nbs\40_tabular.core.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>1631</added_lines>
 			<deleted_lines>1631</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
