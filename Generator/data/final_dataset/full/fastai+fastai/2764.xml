<bug_data>
<bug id='2764' author='bpisaacoff' open_date='2020-09-06T14:14:35Z' closed_time='2020-09-17T04:18:20Z'>
 	<summary>learn.tta() fails on a learner imported with load_learner()</summary>
 	<description>
 Please confirm you have the latest versions of fastai, fastcore, fastscript, and nbdev prior to reporting a bug (delete one): YES
 Describe the bug
 learn.tta() fails on a learner which is imported with load_learner()
 
 Colab notebook reproducing the behaivor at &lt;denchmark-link:https://colab.research.google.com/drive/1l1tnOtboOwOhsYjwi8K8cUEPIPpDJMaU?usp=sharing&gt;https://colab.research.google.com/drive/1l1tnOtboOwOhsYjwi8K8cUEPIPpDJMaU?usp=sharing&lt;/denchmark-link&gt;
 
 Steps to reproduce the behavior:
 
 Train a learner on a dataloader w/ augmentations.
 Create a test dataloader tst_dl=learn.dls.test_dl(get_image_files(path))
 Run preds, targs = learn.tta(dl=tst_dl)
 This runs without error. (However targs returns None which is not expected)
 
 The error arises from exporting the learner and then loading. After step 1 above
 
 Export the learner learn.export('/content/learnfile.pkl')
 Load the learner back in in_learn=load_learner('/content/learnfile.pkl')
 Create a test dataloader tst_dl=in_learn.dls.test_dl(get_image_files(path))
 Running .tta() now produces an error. preds, targs = in_learn.tta(dl=tst_dl)
 
 Expected behavior
 .tta() should work on the loaded learner exactly as it did on the learner which was just trained.
 Error with full stack trace
 epoch	train_loss	valid_loss	accuracy	time
 ---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 &lt;ipython-input-16-3d9acc24d6be&gt; in &lt;module&gt;()
 ----&gt; 1 preds, targs = in_learn.tta(dl=tst_dl)
 
 13 frames
 /usr/local/lib/python3.6/dist-packages/fastai/learner.py in tta(self, ds_idx, dl, n, item_tfms, batch_tfms, beta, use_max)
     562     if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)
     563     try:
 --&gt; 564         self(_before_epoch)
     565         with dl.dataset.set_split_idx(0), self.no_mbar():
     566             if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))
 
 /usr/local/lib/python3.6/dist-packages/fastai/learner.py in __call__(self, event_name)
     131     def ordered_cbs(self, event): return [cb for cb in sort_by_run(self.cbs) if hasattr(cb, event)]
     132 
 --&gt; 133     def __call__(self, event_name): L(event_name).map(self._call_one)
     134 
     135     def _call_one(self, event_name):
 
 /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in map(self, f, *args, **kwargs)
     381              else f.format if isinstance(f,str)
     382              else f.__getitem__)
 --&gt; 383         return self._new(map(g, self))
     384 
     385     def filter(self, f, negate=False, **kwargs):
 
 /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in _new(self, items, *args, **kwargs)
     331     @property
     332     def _xtra(self): return None
 --&gt; 333     def _new(self, items, *args, **kwargs): return type(self)(items, *args, use_list=None, **kwargs)
     334     def __getitem__(self, idx): return self._get(idx) if is_indexer(idx) else L(self._get(idx), use_list=None)
     335     def copy(self): return self._new(self.items.copy())
 
 /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in __call__(cls, x, *args, **kwargs)
      45             return x
      46 
 ---&gt; 47         res = super().__call__(*((x,) + args), **kwargs)
      48         res._newchk = 0
      49         return res
 
 /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in __init__(self, items, use_list, match, *rest)
     322         if items is None: items = []
     323         if (use_list is not None) or not _is_array(items):
 --&gt; 324             items = list(items) if use_list else _listify(items)
     325         if match is not None:
     326             if is_coll(match): match = len(match)
 
 /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in _listify(o)
     235     if isinstance(o, list): return o
     236     if isinstance(o, str) or _is_array(o): return [o]
 --&gt; 237     if is_iter(o): return list(o)
     238     return [o]
     239 
 
 /usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in __call__(self, *args, **kwargs)
     298             if isinstance(v,_Arg): kwargs[k] = args.pop(v.i)
     299         fargs = [args[x.i] if isinstance(x, _Arg) else x for x in self.pargs] + args[self.maxi+1:]
 --&gt; 300         return self.fn(*fargs, **kwargs)
     301 
     302 # Cell
 
 /usr/local/lib/python3.6/dist-packages/fastai/learner.py in _call_one(self, event_name)
     135     def _call_one(self, event_name):
     136         assert hasattr(event, event_name), event_name
 --&gt; 137         [cb(event_name) for cb in sort_by_run(self.cbs)]
     138 
     139     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)
 
 /usr/local/lib/python3.6/dist-packages/fastai/learner.py in &lt;listcomp&gt;(.0)
     135     def _call_one(self, event_name):
     136         assert hasattr(event, event_name), event_name
 --&gt; 137         [cb(event_name) for cb in sort_by_run(self.cbs)]
     138 
     139     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)
 
 /usr/local/lib/python3.6/dist-packages/fastai/callback/core.py in __call__(self, event_name)
      42                (self.run_valid and not getattr(self, 'training', False)))
      43         res = None
 ---&gt; 44         if self.run and _run: res = getattr(self, event_name, noop)()
      45         if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit
      46         return res
 
 /usr/local/lib/python3.6/dist-packages/fastai/callback/progress.py in before_epoch(self)
      21 
      22     def before_epoch(self):
 ---&gt; 23         if getattr(self, 'mbar', False): self.mbar.update(self.epoch)
      24 
      25     def before_train(self):    self._launch_pbar()
 
 /usr/local/lib/python3.6/dist-packages/fastprogress/fastprogress.py in update(self, val)
      92             yield o
      93 
 ---&gt; 94     def update(self, val): self.main_bar.update(val)
      95 
      96 # Cell
 
 /usr/local/lib/python3.6/dist-packages/fastprogress/fastprogress.py in update(self, val)
      57         elif val &lt;= self.first_its or val &gt;= self.last_v + self.wait_for or val &gt;= self.total:
      58             cur_t = time.time()
 ---&gt; 59             avg_t = (cur_t - self.start_t) / val
      60             self.wait_for = max(int(self.update_every / (avg_t+1e-8)),1)
      61             self.pred_t = avg_t * self.total
 
 AttributeError: 'NBProgressBar' object has no attribute 'start_t'
 Additional context
 Running on Google colab
 	</description>
 	<comments>
 		<comment id='1' author='bpisaacoff' date='2020-09-07T20:30:54Z'>
 		The error is not due to exporting the learner. I verified it. There is something wrong in learn.tta(). If you run learn.tta() before exporting then the code will still fail with the same error.
 UPDATE: The error occurs when we run learn.tta multiple times. When I first ran learn.tta it worked as it should. But when I ran it again learn.tta I got the above error.
 learn.epoch = 0 solves the issue. I think the reason is in MasterBar.update start_t is only defined when val=0 i.e. epoch=0.
 		</comment>
 		<comment id='2' author='bpisaacoff' date='2020-09-17T02:20:13Z'>
 		I'm looking into this. Just FYI on this:
 
 However targs returns None which is not expected
 
 test_dl has no targs, so this is expected.
 		</comment>
 		<comment id='3' author='bpisaacoff' date='2020-09-17T04:17:56Z'>
 		Many thanks for the clear issue and repro! :) Should be fixed now - was just missing an initializer for learn.epoch.
 		</comment>
 	</comments>
 </bug>
<commit id='4c74087e6a150e3e2c898f27931646e8364f983d' author='Jeremy Howard' date='2020-09-16 21:18:05-07:00'>
 	<dmm_unit complexity='0.1111111111111111' interfacing='1.0' size='0.1111111111111111'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fastai\__init__.py' new_name='fastai\__init__.py'>
 		<file_info nloc='1' complexity='0' token_count='3'></file_info>
 		<modified_lines>
 			<added_lines>1</added_lines>
 			<deleted_lines>1</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fastai\_nbdev.py' new_name='fastai\_nbdev.py'>
 		<file_info nloc='855' complexity='1' token_count='3338'></file_info>
 		<modified_lines>
 			<added_lines>447</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fastai\callback\core.py' new_name='fastai\callback\core.py'>
 		<file_info nloc='111' complexity='44' token_count='1090'></file_info>
 		<method name='before_fit' parameters='self'>
 				<method_info nloc='6' complexity='3' token_count='80' nesting_level='1' start_line='62' end_line='67'></method_info>
 			<added_lines>64</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fastai\callback\progress.py' new_name='fastai\callback\progress.py'>
 		<file_info nloc='89' complexity='30' token_count='870'></file_info>
 		<modified_lines>
 			<added_lines>27,28</added_lines>
 			<deleted_lines>27,28</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fastai\learner.py' new_name='fastai\learner.py'>
 		<file_info nloc='485' complexity='152' token_count='5311'></file_info>
 		<method name='_tta' parameters='Learner,ds_idx,dl,n,item_tfms,batch_tfms,beta,use_max'>
 				<method_info nloc='11' complexity='5' token_count='189' nesting_level='0' start_line='549' end_line='559'></method_info>
 			<added_lines></added_lines>
 			<deleted_lines>549,550,551,552,553,554,555,556,557,558,559</deleted_lines>
 		</method>
 		<method name='tta' parameters='Learner,ds_idx,dl,n,item_tfms,batch_tfms,beta,use_max'>
 				<method_info nloc='20' complexity='11' token_count='294' nesting_level='0' start_line='554' end_line='574'></method_info>
 			<added_lines>570</added_lines>
 			<deleted_lines>554,555,556,557,558,559,560</deleted_lines>
 		</method>
 		<method name='fit' parameters='self,n_epoch,lr,wd,cbs,reset_opt'>
 				<method_info nloc='8' complexity='6' token_count='109' nesting_level='1' start_line='200' end_line='207'></method_info>
 			<added_lines>206</added_lines>
 			<deleted_lines>206</deleted_lines>
 		</method>
 		<method name='to_detach_from_dl' parameters='Learner,NoneType'>
 				<method_info nloc='2' complexity='2' token_count='63' nesting_level='0' start_line='338' end_line='339'></method_info>
 			<added_lines>338,339</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>5,6,337,340</added_lines>
 			<deleted_lines>5,6,548,579,580</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\13_callback.core.ipynb' new_name='nbs\13_callback.core.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>439,454,617,1057,1071,1108,1110</added_lines>
 			<deleted_lines>439,616</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\13a_learner.ipynb' new_name='nbs\13a_learner.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>409,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,671,672,673,674,675,676,677,678,679,680,681,682,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1276,1277,1278,1279,1280,1281,1282,1283,1284,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1957,1958,1960,1968,1969,1971,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2684,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2873,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3406,3448,3449,3450,3575,3599,3601,3602</added_lines>
 			<deleted_lines>409,542,602,637,733,882,950,986,1002,1113,1129,1139,1161,1180,1201,1217,1228,1272,1301,1330,1346,1615,1624,1672,1690,1699,1708,1717,1762,1823,1881,1925,2189,2212,2221,2230,2239,2248,2264,2276,2303,2344,2535,2590,2615,2636,2707,2749,2750,2751,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2896,2897,2921,2923</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\16_callback.progress.ipynb' new_name='nbs\16_callback.progress.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>82,83</added_lines>
 			<deleted_lines>82,83</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='settings.ini' new_name='settings.ini'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>5</added_lines>
 			<deleted_lines>5</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
