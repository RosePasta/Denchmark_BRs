<bug_data>
<bug id='2892' author='muellerzr' open_date='2020-10-21T02:48:59Z' closed_time='2020-11-03T21:41:48Z'>
 	<summary>Learn.load and LRFinder not functioning properly for the optimizer states</summary>
 	<description>
 Currently there is a bug where if you run  twice in a row, you will get very different results such as below:
 &lt;denchmark-link:https://user-images.githubusercontent.com/7831895/96666881-f7f9e800-1325-11eb-858a-9eae1ae82b12.png&gt;&lt;/denchmark-link&gt;
 
 This sort of pattern is common in models that already have trained weights. Since we know that the models weights are stored away, my investigation led me to believe that something would be wrong with how we are loading in the optimizer in learn.load.
 The stem of the issue is the fact that if self.opt is none we call create_opt and then pass this new opt into load_model as seen below:
 def load(self, file, with_opt=None, device=None, **kwargs):
         if device is None and hasattr(self.dls, 'device'): device = self.dls.device
         if self.opt is None: self.create_opt()
         file = join_path_file(file, self.path/self.model_dir, ext='.pth')
         load_model(file, self.model, self.opt, device=device, **kwargs)
         return self
 What winds up happening is the same optimizer state is being used, despite us trying to load in new weights because self.opt is not None when it goes in. The proposed fix is that if with_opt is None, we set self.opt to None. This allows for a much better graph that we could expect:
 @patch
 def load(self:Learner, file, with_opt=None, device=None, **kwargs):
     if device is None and hasattr(self.dls, 'device'): device = self.dls.device
     if with_opt is None: self.opt=None
     file = join_path_file(file, self.path/self.model_dir, ext='.pth')
     load_model(file, self.model, self.opt, device=device, **kwargs)
     return self
 &lt;denchmark-link:https://user-images.githubusercontent.com/7831895/96667077-5b841580-1326-11eb-856a-c35537a094c6.png&gt;&lt;/denchmark-link&gt;
 
 Let me know if this fix looks okay to you &lt;denchmark-link:https://github.com/jph00&gt;@jph00&lt;/denchmark-link&gt;
 , or if you believe there is a better solution for addressing the issue
 	</description>
 	<comments>
 		<comment id='1' author='muellerzr' date='2020-10-21T03:16:12Z'>
 		Currently it doesn't seem to work with mixed precision, here is the stack trace:
 ---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 &lt;ipython-input-20-979fb3ad48c2&gt; in &lt;module&gt;
       6     load_model(file, self.model, self.opt, device=device, **kwargs)
       7     return self
 ----&gt; 8 learn.lr_find()
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/schedule.py in lr_find(self, start_lr, end_lr, num_it, stop_div, show_plot, suggestions)
     226     n_epoch = num_it//len(self.dls.train) + 1
     227     cb=LRFinder(start_lr=start_lr, end_lr=end_lr, num_it=num_it, stop_div=stop_div)
 --&gt; 228     with self.no_logging(): self.fit(n_epoch, cbs=cb)
     229     if show_plot: self.recorder.plot_lr_find()
     230     if suggestions:
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/logargs.py in _f(*args, **kwargs)
      54         init_args.update(log)
      55         setattr(inst, 'init_args', init_args)
 ---&gt; 56         return inst if to_return else f(*args, **kwargs)
      57     return _f
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)
     205             self.opt.set_hypers(lr=self.lr if lr is None else lr)
     206             self.n_epoch = n_epoch
 --&gt; 207             self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
     208 
     209     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
     155         try:       self(f'before_{event_type}')       ;f()
     156         except ex: self(f'after_cancel_{event_type}')
 --&gt; 157         finally:   self(f'after_{event_type}')        ;final()
     158 
     159     def all_batches(self):
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in __call__(self, event_name)
     131     def ordered_cbs(self, event): return [cb for cb in sort_by_run(self.cbs) if hasattr(cb, event)]
     132 
 --&gt; 133     def __call__(self, event_name): L(event_name).map(self._call_one)
     134 
     135     def _call_one(self, event_name):
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in map(self, f, *args, **kwargs)
     270              else f.format if isinstance(f,str)
     271              else f.__getitem__)
 --&gt; 272         return self._new(map(g, self))
     273 
     274     def filter(self, f, negate=False, **kwargs):
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in _new(self, items, *args, **kwargs)
     216     @property
     217     def _xtra(self): return None
 --&gt; 218     def _new(self, items, *args, **kwargs): return type(self)(items, *args, use_list=None, **kwargs)
     219     def __getitem__(self, idx): return self._get(idx) if is_indexer(idx) else L(self._get(idx), use_list=None)
     220     def copy(self): return self._new(self.items.copy())
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in __call__(cls, x, *args, **kwargs)
     197     def __call__(cls, x=None, *args, **kwargs):
     198         if not args and not kwargs and x is not None and isinstance(x,cls): return x
 --&gt; 199         return super().__call__(x, *args, **kwargs)
     200 
     201 # Cell
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in __init__(self, items, use_list, match, *rest)
     207         if items is None: items = []
     208         if (use_list is not None) or not _is_array(items):
 --&gt; 209             items = list(items) if use_list else _listify(items)
     210         if match is not None:
     211             if is_coll(match): match = len(match)
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in _listify(o)
     114     if isinstance(o, list): return o
     115     if isinstance(o, str) or _is_array(o): return [o]
 --&gt; 116     if is_iter(o): return list(o)
     117     return [o]
     118 
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in __call__(self, *args, **kwargs)
     177             if isinstance(v,_Arg): kwargs[k] = args.pop(v.i)
     178         fargs = [args[x.i] if isinstance(x, _Arg) else x for x in self.pargs] + args[self.maxi+1:]
 --&gt; 179         return self.fn(*fargs, **kwargs)
     180 
     181 # Cell
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _call_one(self, event_name)
     135     def _call_one(self, event_name):
     136         assert hasattr(event, event_name), event_name
 --&gt; 137         [cb(event_name) for cb in sort_by_run(self.cbs)]
     138 
     139     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in &lt;listcomp&gt;(.0)
     135     def _call_one(self, event_name):
     136         assert hasattr(event, event_name), event_name
 --&gt; 137         [cb(event_name) for cb in sort_by_run(self.cbs)]
     138 
     139     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/core.py in __call__(self, event_name)
      42                (self.run_valid and not getattr(self, 'training', False)))
      43         res = None
 ---&gt; 44         if self.run and _run: res = getattr(self, event_name, noop)()
      45         if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit
      46         return res
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/fp16.py in after_fit(self)
     123     def after_fit(self):
     124         if not hasattr(self,'master_pgs'): return
 --&gt; 125         _copy_state(self.learn.opt, self.master_pgs, self.model_pgs)
     126         self.learn.opt.param_lists  = self.old_pgs
     127         delattr(self, "master_pgs")
 
 ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/fp16.py in _copy_state(opt, pgs1, pgs2)
      58 # Cell
      59 def _copy_state(opt, pgs1, pgs2):
 ---&gt; 60     opt.param_lists = pgs2
      61     for pg1,pg2 in zip(pgs1, pgs2):
      62         for p1,p2 in zip(pg1, pg2): opt.state[p2] = copy_clone(opt.state.pop(p1, {}))
 
 AttributeError: 'NoneType' object has no attribute 'param_lists'
 I don't know enough about fp16 to know where to start debugging or the obvious red flags
 		</comment>
 		<comment id='2' author='muellerzr' date='2020-10-21T07:28:54Z'>
 		I have the same issue here.
 Also maybe related, some models with fp16 diverge if I do a lr_find before calling fit. So I do lr_find, then recreate the learner and call fit afterwards.
 		</comment>
 		<comment id='3' author='muellerzr' date='2020-11-03T21:38:40Z'>
 		You didn't provide a repro so I don't actually know if I fixed it - but I made a change to load back the orig opt after lr_find, so hopefully it's fixed now. I couldn't repro the fp16 issue at all. Here's what I did
 &lt;denchmark-code&gt;from fastai.vision.all import *
 
 path = untar_data(URLs.PETS)
 files = get_image_files(path/"images")
 def label_func(f): return f[0].isupper()
 dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224), n_workers=8)
 learn = cnn_learner(dls, resnet34, metrics=error_rate).to_fp16()
 learn.lr_find()
 &lt;/denchmark-code&gt;
 
 Please reopen if there's still a problem and you can provide a minimal repro.
 		</comment>
 		<comment id='4' author='muellerzr' date='2020-11-17T21:48:42Z'>
 		I I have similar issue and am using the lastest version v2.1.5.
 If I insert a lr_find in front of the fit, it will change the training result. Please see screenshot below
 &lt;denchmark-link:https://user-images.githubusercontent.com/26617297/99454500-5e0c7600-298b-11eb-9da2-392fb989e3a4.png&gt;&lt;/denchmark-link&gt;
 
 reproducible code:
 &lt;denchmark-code&gt;from fastai.vision.all import *
 def is_cat(x): return x[0].isupper()
 path = untar_data(URLs.PETS)/'images'
 set_seed(42,True)
 dls = ImageDataLoaders.from_name_func(
     path, get_image_files(path), valid_pct=0.2, seed=42,
     label_func=is_cat, item_tfms=Resize(224),shuffle_train= False)
 learn = cnn_learner(dls, resnet34, metrics=error_rate)
 learn.fit(1)
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-code&gt;set_seed(42,True)
 dls = ImageDataLoaders.from_name_func(
     path, get_image_files(path), valid_pct=0.2, seed=42,
     label_func=is_cat, item_tfms=Resize(224),shuffle_train= False)
 learn = cnn_learner(dls, resnet34, metrics=error_rate)
 learn.lr_find()
 
 learn.fit(1)
 &lt;/denchmark-code&gt;
 
 		</comment>
 		<comment id='5' author='muellerzr' date='2021-01-08T10:43:08Z'>
 		the issue is still there on latest 2.2.3
 		</comment>
 	</comments>
 </bug>
<commit id='2b058673fd26733bd68b1dc0e0b53e58247278b3' author='Jeremy Howard' date='2020-11-03 13:41:33-08:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='fastai\callback\schedule.py' new_name='fastai\callback\schedule.py'>
 		<file_info nloc='170' complexity='46' token_count='2008'></file_info>
 		<method name='after_fit' parameters='self'>
 				<method_info nloc='6' complexity='2' token_count='50' nesting_level='1' start_line='194' end_line='199'></method_info>
 			<added_lines>198</added_lines>
 			<deleted_lines>198</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='fastai\learner.py' new_name='fastai\learner.py'>
 		<file_info nloc='485' complexity='152' token_count='5287'></file_info>
 		<method name='load_model' parameters='file,model,opt,with_opt,device,strict'>
 				<method_info nloc='14' complexity='9' token_count='130' nesting_level='0' start_line='41' end_line='54'></method_info>
 			<added_lines>50</added_lines>
 			<deleted_lines>50</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\13a_learner.ipynb' new_name='nbs\13a_learner.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>213</added_lines>
 			<deleted_lines>213</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='nbs\14_callback.schedule.ipynb' new_name='nbs\14_callback.schedule.ipynb'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>612,633,786,787,842,865,866,925,926,1046,1047,1100,1313,1318,1358,1370,1424</added_lines>
 			<deleted_lines>612,633,786,787,842,865,866,925,926,1046,1047,1100,1313,1318</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
