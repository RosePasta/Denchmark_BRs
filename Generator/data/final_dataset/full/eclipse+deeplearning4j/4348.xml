<bug_data>
<bug id='4348' author='fancyerii' open_date='2017-11-30T01:55:15Z' closed_time='2017-12-01T19:26:59Z'>
 	<summary>wrong predicting result running VGG16SparkJavaWebApp</summary>
 	<description>
 &lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;
 
 Please describe our issue, along with:
 I have run VGG16SparkJavaWebApp in &lt;denchmark-link:https://deeplearning4j.org/build_vgg_webapp&gt;https://deeplearning4j.org/build_vgg_webapp&lt;/denchmark-link&gt;
 
 I use the pretrained vgg imagenet model and I uploaded this image: &lt;denchmark-link:https://deeplearning4j.org/img/cat.jpeg&gt;https://deeplearning4j.org/img/cat.jpeg&lt;/denchmark-link&gt;
  but it predict wrong:
 'Predictions for batch : 11.939789%, toilet_tissue 8.131950%, paper_towel 6.710276%, tabby 6.015658%, tiger_cat 5.664997%, dishwasher'
 my source codes are: &lt;denchmark-link:https://gist.github.com/fancyerii/cc862b35b541039dcfa9b489020c3cae&gt;https://gist.github.com/fancyerii/cc862b35b541039dcfa9b489020c3cae&lt;/denchmark-link&gt;
 
 &lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;
 
 Please indicate relevant versions, including, if relevant:
 
 Deeplearning4j version 0.9.1
 platform information (OS, etc) Ubuntu 14.04
 CUDA version, if used
 NVIDIA driver version, if in use
 
 &lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;
 
 If you'd like to help us fix the issue by contributing some code, but would
 like guidance or help in doing so, please mention it!
 	</description>
 	<comments>
 		<comment id='1' author='fancyerii' date='2017-11-30T05:14:42Z'>
 		Seems to be basically the same as &lt;denchmark-link:https://gist.github.com/tomthetrainer/3edd4ec67107688be04ee327a6d12197&gt;https://gist.github.com/tomthetrainer/3edd4ec67107688be04ee327a6d12197&lt;/denchmark-link&gt;
 , but are you noticing anything off &lt;denchmark-link:https://github.com/tomthetrainer&gt;@tomthetrainer&lt;/denchmark-link&gt;
  ?
 		</comment>
 		<comment id='2' author='fancyerii' date='2017-11-30T05:59:45Z'>
 		I'm also getting the same kind of results with 0.9.1 and current master (0.9.2-SNAPSHOT), with either CPU or GPU:
 &lt;denchmark-code&gt;Predictions for batch  :
 	11.939786%, toilet_tissue
 	8.131945%, paper_towel
 	6.710284%, tabby
 	6.015659%, tiger_cat
 	5.665000%, dishwasher
 &lt;/denchmark-code&gt;
 
 So, I'm guessing there's something wrong with the model in the zoo. There are also issues with GoogLeNet and ResNet50: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/4139&gt;https://github.com/deeplearning4j/deeplearning4j/issues/4139&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/crockpotveggies&gt;@crockpotveggies&lt;/denchmark-link&gt;
  ?
 		</comment>
 		<comment id='3' author='fancyerii' date='2017-11-30T15:14:26Z'>
 		The models themselves haven't changed since previous versions, yet we are seeing wrong predictions. Is this a labels ordering issue...?
 		</comment>
 		<comment id='4' author='fancyerii' date='2017-11-30T18:14:04Z'>
 		I really do not think this is a bug.
 It is funny, but not a bug. I actually used that image in class and had to explain it.
 I think the curved chair of the couch is the toilet paper and the paper towels and the cabinet behind fergus, that is the cat fergus, is the dishwasher.
 And no, of course those are mis-classifications, but if we were to find activations for the hidden layers those areas would be what I would guess is triggering those classifications.
 The only way to sort out if this is a bug would be to compare with Keras and see if our labels are different from keras.
 This model uses weights imported from a keras export.
 I think this is not an issue. If someone wants to cross-check with keras that is fine.
 whatever the accuracy of VGG was, perhaps in the 90 percent range, this is just example that is in the 10 percent-ish of somewhat wrong classifications.
 If you wanted to play around, crop the image, or blur some sections to sort out what is triggering the toilet paper or the paper towel.
 		</comment>
 		<comment id='5' author='fancyerii' date='2017-12-01T03:49:50Z'>
 		&lt;denchmark-link:https://github.com/tomthetrainer&gt;@tomthetrainer&lt;/denchmark-link&gt;
  If it's not a bug in the code or the model, then it's a bug
 in the documentation. Where do those results come from:
 &lt;denchmark-link:https://deeplearning4j.org/build_vgg_webapp#example&gt;https://deeplearning4j.org/build_vgg_webapp#example&lt;/denchmark-link&gt;
  ?
 		</comment>
 		<comment id='6' author='fancyerii' date='2017-12-01T18:59:11Z'>
 		&lt;denchmark-link:https://github.com/saudet&gt;@saudet&lt;/denchmark-link&gt;
 , thanks for pointing that out..
 Interesting, perhaps I put the wrong image up there, or the multiple resizing confused things, regardless this is worth looking into further, let me try a few things
 		</comment>
 		<comment id='7' author='fancyerii' date='2017-12-01T19:08:57Z'>
 		Some testing,
 The image from the web site had been rescaled, 240*80
 shipping it for inference returns.
 Toilet tissue 11.9%  Paper towel 8.1%  Tabby 6.7%  Tiger cat 6%  Dishwasher 5.7%
 The original image 1920 * 1440 jpeg
 returns..
 Tabby 16.7%  Tiger cat 7.6%  Toilet tissue 6.4%  Egyptian cat 5.4%  Paper towel 5.2%
 The website however shows somewhat strange returns of
 16.694832%, tabby 7.550286%, tiger_cat 0.065847%, cleaver 0.000000%, cleaver 0.000000%, cleaver
 Not so strange in the first 3, but that fact that only 2 over 1 percent and only sum of 23% in top 5 is suspicious.
 I am going to call this a documentation error, fix the website with the returns from the non-scaled image, and add a comment that the scaled image performs poorly.
 		</comment>
 		<comment id='8' author='fancyerii' date='2017-12-01T19:26:59Z'>
 		I corrected the web page to show the results from the non scaled image, and added a note that the scaled image returns different results.
 I feel that is satisfactory and I am closing this.
 I could have gone one step farther and included the embarrassing results from the scaled image, I chose not to. In the face of argument I could be convinced to add that to the discussion on the web page.
 My reasoning for keeping it vague:
 Image classification can be challenging, I think it is the change in dimension ratio combined with loss of detail that causes the slop in the return of the scaled image.
 Our goal is not to teach the very details, just that the results can work well, and then again not always perfect.
 Re-open if you want me to clarify further in the docs, and/or provide a link to the image that would return the values shown in the web page.
 		</comment>
 		<comment id='9' author='fancyerii' date='2018-09-24T04:43:54Z'>
 		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
 		</comment>
 	</comments>
 </bug>
<commit id='905db4e408c44ace442e3385c0f4e6c9a3b878ed' author='tomthetrainer' date='2017-12-01 14:18:59-05:00'>
 	<dmm_unit complexity='None' interfacing='None' size='None'></dmm_unit>
 	<modification change_type='MODIFY' old_name='build_vgg_webapp.md' new_name='build_vgg_webapp.md'>
 		<file_info nloc='None' complexity='None' token_count='None'></file_info>
 		<modified_lines>
 			<added_lines>329,333</added_lines>
 			<deleted_lines>329,333</deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
