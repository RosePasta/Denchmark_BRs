<bug_data>
<bug id='118' author='astariul' open_date='2020-03-04T06:02:14Z' closed_time='2020-03-10T21:03:24Z'>
 	<summary>TypeError: FP16_DeepSpeedZeroOptimizer is not an Optimizer</summary>
 	<description>
 I'm trying to use 1-Cycle scheduler, but I meet the following error :
 
 TypeError: FP16_DeepSpeedZeroOptimizer is not an Optimizer
 
 &lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;
 
 Here is my configuration file :
 &lt;denchmark-code&gt;{
     "train_batch_size": 64,
     "train_micro_batch_size_per_gpu": 1,
     "gradient_accumulation_steps": 16,
     "optimizer": {
         "type": "Adam",
         "params": {
             "lr": 3e-05,
             "betas": [
                 0.9,
                 0.999
             ],
             "eps": 1e-8,
             "weight_decay": 0.01
         }
     },
     "gradient_clipping": 0.1,
     "scheduler": {
         "type": "OneCycle",
         "params": {
             "cycle_first_step_size": 16000,
             "cycle_first_stair_count": 8000,
             "decay_step_size": 16000,
             "cycle_min_lr": 1e-06,
             "cycle_max_lr": 3e-05,
             "decay_lr_rate": 1e-07,
             "cycle_min_mom": 0.85,
             "cycle_max_mom": 0.99,
             "decay_mom_rate": 0.0
         }
     },
     "zero_optimization": true,
     "disable_allgather": true,
     "fp16": {
         "enabled": true,
         "loss_scale": 0,
         "min_loss_scale": 1
     }
 }
 &lt;/denchmark-code&gt;
 
 When using another Scheduler (with FP16), I meet no problem.
 	</description>
 	<comments>
 		<comment id='1' author='astariul' date='2020-03-04T06:37:32Z'>
 		Thanks for reporting this bug. We will take a look at this as soon as possible. I just created two test cases that reproduce the error (one with ZeRO and one with FP16 but no ZeRO).
 &lt;denchmark-link:https://github.com/microsoft/DeepSpeed/blob/jeffra/onecycle_bug/tests/unit/test_fp16.py#L147-L246&gt;https://github.com/microsoft/DeepSpeed/blob/jeffra/onecycle_bug/tests/unit/test_fp16.py#L147-L246&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='astariul' date='2020-03-05T06:21:18Z'>
 		Note : I have the same error when I try the same configuration and LRRangeTest as scheduler.
 		</comment>
 		<comment id='3' author='astariul' date='2020-03-13T10:38:47Z'>
 		I'm still meeting this issue in deepspeed/deepspeed:latest. How should I update the docker image to pull latest code from source ?
 		</comment>
 		<comment id='4' author='astariul' date='2020-03-13T14:42:52Z'>
 		Hi @Colanim, it should be up to date. Can you tell us this info from inside your docker container?
 python -c 'import deepspeed; print("deepspeed info:", deepspeed.__version__, deepspeed.__git_branch__, deepspeed.__git_hash__)'
 Also I just looked at the lasted docker build, it prints this same version info and it looks to be aligned with the latest March 12th commit (&lt;denchmark-link:https://github.com/microsoft/DeepSpeed/commit/3d3f8d36a4e8c0b7e6358bccd90254fc7424ffcb&gt;3d3f8d3&lt;/denchmark-link&gt;
 ): &lt;denchmark-link:https://dev.azure.com/DeepSpeedMSFT/DeepSpeed/_build/results?buildId=416&amp;view=logs&amp;j=3dc8fd7e-4368-5a92-293e-d53cefc8c4b3&amp;t=a1aa9649-a94b-5ac4-3f5e-9bb6223edb04&amp;l=1717&gt;https://dev.azure.com/DeepSpeedMSFT/DeepSpeed/_build/results?buildId=416&amp;view=logs&amp;j=3dc8fd7e-4368-5a92-293e-d53cefc8c4b3&amp;t=a1aa9649-a94b-5ac4-3f5e-9bb6223edb04&amp;l=1717&lt;/denchmark-link&gt;
 
 ** info: 0.1.0 master 3d3f8d3
 		</comment>
 		<comment id='5' author='astariul' date='2020-03-14T10:07:08Z'>
 		My bad, I didn't pull the latest image..
 After doing docker pull deepspeed/deepspeed:latest, it's working üëç
 		</comment>
 	</comments>
 </bug>
<commit id='1c0b326e772e20082772fdcd10e3f95ed481d555' author='Olatunji Ruwase' date='2020-03-10 14:03:23-07:00'>
 	<dmm_unit complexity='1.0' interfacing='0.6528925619834711' size='0.4049586776859504'></dmm_unit>
 	<modification change_type='MODIFY' old_name='deepspeed\pt\deepspeed_lr_schedules.py' new_name='deepspeed\pt\deepspeed_lr_schedules.py'>
 		<file_info nloc='570' complexity='106' token_count='2811'></file_info>
 		<method name='_initialize_lr' parameters='self,optimizer,cycle_min_lr,cycle_max_lr,decay_lr_rate,last_batch_iteration'>
 				<method_info nloc='6' complexity='1' token_count='15' nesting_level='1' start_line='531' end_line='536'></method_info>
 			<added_lines>531,532,533,534,535,536</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='get_torch_optimizer' parameters='optimizer'>
 				<method_info nloc='7' complexity='4' token_count='51' nesting_level='0' start_line='288' end_line='296'></method_info>
 			<added_lines>288,289,290,291,292,293,294,295,296</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_initialize_cycle' parameters='self,cycle_first_step_size,cycle_second_step_size,cycle_first_stair_count,cycle_second_stair_count,decay_step_size'>
 				<method_info nloc='6' complexity='1' token_count='15' nesting_level='1' start_line='513' end_line='518'></method_info>
 			<added_lines>513,514,515,516,517,518</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_initialize_momentum' parameters='self,optimizer,cycle_min_mom,cycle_max_mom,decay_mom_rate,last_batch_iteration'>
 				<method_info nloc='6' complexity='1' token_count='15' nesting_level='1' start_line='546' end_line='551'></method_info>
 			<added_lines>546,547,548,549,550,551</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>16,281,282,283,284,285,286,287,297,298,345,350,352,356,483,485,486,487,488,489,490,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,523,528,529,530,537,538,539,540,542,545,552,553,554,555,556,557,558,559,560,564,565,566,671,673,674</added_lines>
 			<deleted_lines>326,327,328,333,335,339,466,467,468,470,471,472,473,475,483,488,489,493,495,496,497,498,499,500,501,502,503,504,609,611,612</deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\unit\test_fp16.py' new_name='tests\unit\test_fp16.py'>
 		<file_info nloc='218' complexity='18' token_count='1246'></file_info>
 		<method name='test_adam_fp16_onecycle_compatibility._test_adam_fp16_onecycle_compatibility' parameters='args,model,hidden_dim'>
 				<method_info nloc='12' complexity='2' token_count='94' nesting_level='1' start_line='182' end_line='193'></method_info>
 			<added_lines>182,183,184,185,186,187,188,189,190,191,192,193</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_adam_fp16_zero_onecycle_compatibility' parameters='tmpdir'>
 				<method_info nloc='37' complexity='1' token_count='152' nesting_level='0' start_line='198' end_line='248'></method_info>
 			<added_lines>198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_adam_fp16_onecycle_compatibility' parameters='tmpdir'>
 				<method_info nloc='35' complexity='1' token_count='152' nesting_level='0' start_line='147' end_line='195'></method_info>
 			<added_lines>147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_adam_fp16_zero_onecycle_compatibility._test_adam_fp16_zero_onecycle_compatibility' parameters='args,model,hidden_dim'>
 				<method_info nloc='12' complexity='2' token_count='94' nesting_level='1' start_line='233' end_line='244'></method_info>
 			<added_lines>233,234,235,236,237,238,239,240,241,242,243,244</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>145,146,196,197</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
