<bug_data>
<bug id='2207' author='ajtulloch' open_date='2018-11-30T19:54:45Z' closed_time='2018-12-01T00:30:49Z'>
 	<summary>Supporting reduction domains where the RDom depends on axis variables?</summary>
 	<description>
 Here's a minimized example of a compute declaration that I would have thought should be able to be handled by TVM. Essentially, we want the range of our summation variable to be dependent on the value stored in  corresponding length tensor. For example:
 def func(Elements, Lengths):
     def f(n, d):
         rg = tvm.reduce_axis((0, Lengths[n]))
         return tvm.sum(Elements[rg, d], axis=rg)
 
     (N,) = get_const_tuple(Lengths.shape)
     (_, D) = get_const_tuple(Elements.shape)
     return tvm.compute((N, D), f, name="Y")
 The generated IR (from tvm.lower(..) looks like:
 &lt;denchmark-code&gt;produce Y {
   for (n, 0, 10) {
     for (d, 0, 128) {
       Y[((n*128) + d)] = 0.000000f
       for (rv, 0, Lengths[n]) {
         Y[((n*128) + d)] = (Y[((n*128) + d)] + Elements[(d + (rv*128))])
       }
     }
   }
 }
 &lt;/denchmark-code&gt;
 
 which seems reasonable.  Unfortunately, tvm.build fails with
 tvm._ffi.base.TVMError: [11:49:26] /Users/tulloch/src/tvm/src/pass/make_api.cc:169: Not all Vars are passed in api_args:  'n'  does not appeared in api_args,
 that is, the variable n used in the rg RDom seems to be duplicated from it's definition as the axis used in the computation of Y.
 For a full repro, try:
 from topi.util import get_const_tuple
 import tvm
 
 def func(Elements, Lengths):
     def f(n, d):
         rg = tvm.reduce_axis((0, Lengths[n]))
         return tvm.sum(Elements[rg, d], axis=rg)
 
     (N,) = get_const_tuple(Lengths.shape)
     (_, D) = get_const_tuple(Elements.shape)
     return tvm.compute((N, D), f, name="Y")
 
 def run(N, I, D):
     Elements = tvm.placeholder(shape=(I, D), dtype="float32", name="Elements")
     Lengths = tvm.placeholder(shape=(N,), dtype="int32", name="Lengths")
     Y = func(Elements, Lengths)
     s = tvm.create_schedule([Y.op])
     print(tvm.lower(s, [Elements, Lengths, Y], simple_mode=True))
     print(tvm.save_json(Y))
     f = tvm.build(s, [Elements, Lengths, Y], target="llvm")
 
 run(N=10, I=10, D=128)
 Is it reasonable to expect this to work? If so,
 (For reference, the full IR from tvm.save_json(..) for this expression looks like:
 {
   "root": 1, 
   "nodes": [
     {
       "type_key": ""
     }, 
     {
       "type_key": "Tensor", 
       "attrs": {
         "dtype": "float32", 
         "op": "5", 
         "shape": "2", 
         "value_index": "0"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [3, 4]
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "10"
       }
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "128"
       }
     }, 
     {
       "type_key": "ComputeOp", 
       "attrs": {
         "attrs": "6", 
         "axis": "7", 
         "body": "27", 
         "name": "Y", 
         "reduce_axis": "16", 
         "tag": ""
       }
     }, 
     {
       "type_key": "StrMap"
     }, 
     {
       "type_key": "Array", 
       "data": [8, 12]
     }, 
     {
       "type_key": "IterVar", 
       "attrs": {
         "dom": "9", 
         "iter_type": "0", 
         "thread_tag": "", 
         "var": "11"
       }
     }, 
     {
       "type_key": "Range", 
       "attrs": {
         "extent": "3", 
         "min": "10"
       }
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "0"
       }
     }, 
     {
       "type_key": "Variable", 
       "attrs": {
         "dtype": "int32", 
         "name": "n"
       }
     }, 
     {
       "type_key": "IterVar", 
       "attrs": {
         "dom": "13", 
         "iter_type": "0", 
         "thread_tag": "", 
         "var": "15"
       }
     }, 
     {
       "type_key": "Range", 
       "attrs": {
         "extent": "4", 
         "min": "14"
       }
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "0"
       }
     }, 
     {
       "type_key": "Variable", 
       "attrs": {
         "dtype": "int32", 
         "name": "d"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [17]
     }, 
     {
       "type_key": "IterVar", 
       "attrs": {
         "dom": "18", 
         "iter_type": "2", 
         "thread_tag": "", 
         "var": "26"
       }
     }, 
     {
       "type_key": "Range", 
       "attrs": {
         "extent": "20", 
         "min": "19"
       }
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "0"
       }
     }, 
     {
       "type_key": "Call", 
       "attrs": {
         "args": "21", 
         "call_type": "3", 
         "dtype": "int32", 
         "func": "22", 
         "name": "Lengths", 
         "value_index": "0"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [11]
     }, 
     {
       "type_key": "PlaceholderOp", 
       "attrs": {
         "attrs": "23", 
         "dtype": "int32", 
         "name": "Lengths", 
         "shape": "24", 
         "tag": ""
       }
     }, 
     {
       "type_key": "StrMap"
     }, 
     {
       "type_key": "Array", 
       "data": [25]
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "10"
       }
     }, 
     {
       "type_key": "Variable", 
       "attrs": {
         "dtype": "int32", 
         "name": "rv"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [28]
     }, 
     {
       "type_key": "Reduce", 
       "attrs": {
         "axis": "16", 
         "combiner": "29", 
         "condition": "46", 
         "dtype": "float32", 
         "source": "38", 
         "value_index": "0"
       }
     }, 
     {
       "type_key": "CommReducer", 
       "attrs": {
         "identity_element": "36", 
         "lhs": "30", 
         "result": "34", 
         "rhs": "32"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [31]
     }, 
     {
       "type_key": "Variable", 
       "attrs": {
         "dtype": "float32", 
         "name": "x"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [33]
     }, 
     {
       "type_key": "Variable", 
       "attrs": {
         "dtype": "float32", 
         "name": "y"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [35]
     }, 
     {
       "type_key": "Add", 
       "attrs": {
         "a": "31", 
         "b": "33", 
         "dtype": "float32"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [37]
     }, 
     {
       "type_key": "FloatImm", 
       "attrs": {
         "dtype": "float32", 
         "value": "0.000000"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [39]
     }, 
     {
       "type_key": "Call", 
       "attrs": {
         "args": "40", 
         "call_type": "3", 
         "dtype": "float32", 
         "func": "41", 
         "name": "Elements", 
         "value_index": "0"
       }
     }, 
     {
       "type_key": "Array", 
       "data": [26, 15]
     }, 
     {
       "type_key": "PlaceholderOp", 
       "attrs": {
         "attrs": "42", 
         "dtype": "float32", 
         "name": "Elements", 
         "shape": "43", 
         "tag": ""
       }
     }, 
     {
       "type_key": "StrMap"
     }, 
     {
       "type_key": "Array", 
       "data": [44, 45]
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "10"
       }
     }, 
     {
       "type_key": "IntImm", 
       "attrs": {
         "dtype": "int32", 
         "value": "128"
       }
     }, 
     {
       "type_key": "UIntImm", 
       "attrs": {
         "dtype": "uint1", 
         "value": "1"
       }
     }
   ], 
   "b64ndarrays": [], 
   "attrs": {"tvm_version": "0.5.dev"}
 }
 	</description>
 	<comments>
 		<comment id='1' author='ajtulloch' date='2018-11-30T20:57:27Z'>
 		Thanks, &lt;denchmark-link:https://github.com/ajtulloch&gt;@ajtulloch&lt;/denchmark-link&gt;
  for reporting this, it is a fun bug to catch. The main of the problem cause is we call substitution too early only on the loop body before.
 Should be fixed by &lt;denchmark-link:https://github.com/apache/tvm/pull/2208&gt;#2208&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='2' author='ajtulloch' date='2018-12-01T00:18:24Z'>
 		Thanks for the fix &lt;denchmark-link:https://github.com/tqchen&gt;@tqchen&lt;/denchmark-link&gt;
 , this looks great. It should make certain sparse operations much easier to implement (and more generalizable), without having to fallback into extern ops. Thanks!
 		</comment>
 		<comment id='3' author='ajtulloch' date='2018-12-05T09:13:42Z'>
 		When we use:
 
 di= tvm.reduce_axis(( indice[axis] , M), name='di')
 
 tvm.build error：
 make_api.cc:184: Not all Vars are passed in api_args:  'i0'  does not appeared in api_args
 
 di= tvm.reduce_axis(( M , indice[axis] ), name='di')
 tvm.build：ok
 
 why?  indice[axis]=i0,  is  a axis variables. M is a  constant.
 		</comment>
 		<comment id='4' author='ajtulloch' date='2018-12-05T22:43:07Z'>
 		@Foundea could you produce a minimal runnable example, similar to e.g.
 from topi.util import get_const_tuple
 import tvm
 
 def func(Elements, Lengths):
     def f(n, d):
         rg = tvm.reduce_axis((0, Lengths[n]))
         return tvm.sum(Elements[rg, d], axis=rg)
 
     (N,) = get_const_tuple(Lengths.shape)
     (_, D) = get_const_tuple(Elements.shape)
     return tvm.compute((N, D), f, name="Y")
 
 def run(N, I, D):
     Elements = tvm.placeholder(shape=(I, D), dtype="float32", name="Elements")
     Lengths = tvm.placeholder(shape=(N,), dtype="int32", name="Lengths")
     Y = func(Elements, Lengths)
     s = tvm.create_schedule([Y.op])
     print(tvm.lower(s, [Elements, Lengths, Y], simple_mode=True))
     print(tvm.save_json(Y))
     f = tvm.build(s, [Elements, Lengths, Y], target="llvm")
 
 run(N=10, I=10, D=128)
 		</comment>
 		<comment id='5' author='ajtulloch' date='2018-12-14T01:17:33Z'>
 		&lt;denchmark-link:https://github.com/ajtulloch&gt;@ajtulloch&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/tqchen&gt;@tqchen&lt;/denchmark-link&gt;
 
 
 Hi all. This is a very interesting problem.
 I use the bug fixed version of tvm, and I edited this case a little, add cache_read/cache_write in schedule part.
 
 from topi.util import get_const_tuple
 import tvm
 
 def func(Elements, Lengths):
     def f(n, d):
         rg = tvm.reduce_axis((0, Lengths[n]))
         return tvm.sum(Elements[rg, d], axis=rg)
 
     (N,) = get_const_tuple(Lengths.shape)
     (_, D) = get_const_tuple(Elements.shape)
     return tvm.compute((N, D), f, name="Y")
 
 def run(N, I, D):
     Elements = tvm.placeholder(shape=(I, D), dtype="float32", name="Elements")
     Lengths = tvm.placeholder(shape=(N,), dtype="int32", name="Lengths")
     Y = func(Elements, Lengths)
     s = tvm.create_schedule([Y.op])
     Elements_local = s.cache_read(Elements, "local", [Y])
     Lengths_local = s.cache_read(Lengths, "local", [Y])
     Y_local = s.cache_write(Y, "local")
     print(tvm.lower(s, [Elements, Lengths, Y], simple_mode=True))
     #print(tvm.save_json(Y))
     #f = tvm.build(s, [Elements, Lengths, Y], target="llvm")
 
 run(N=10, I=10, D=128)
 
 Unfortunately, tvm.lower() fails with
 
 &lt;denchmark-code&gt;tvm._ffi.base.TVMError: [08:42:49] /home/dylan/tvm/src/schedule/schedule_dataflow_rewrite.cc:166: Check failed: iv-&gt;iter_type == kDataPar (2 vs. 0) Can only relayout with in data parallel dimensions
 &lt;/denchmark-code&gt;
 
 
 I don't known what is the main of the problem cause. I guess the problem is cause by the
 
 &lt;denchmark-code&gt;rg = tvm.reduce_axis((0, Lengths[n]))
 &lt;/denchmark-code&gt;
 
 		</comment>
 	</comments>
 </bug>
<commit id='6d32037c64e2acbfbbcd4efc8d8781b944bc74ea' author='Tianqi Chen' date='2018-11-30 16:05:04-08:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='0.9'></dmm_unit>
 	<modification change_type='MODIFY' old_name='src\op\compute_op.cc' new_name='src\op\compute_op.cc'>
 		<file_info nloc='497' complexity='100' token_count='4150'></file_info>
 		<method name='tvm::MakeComputeStmt' parameters=''>
 				<method_info nloc='37' complexity='5' token_count='387' nesting_level='1' start_line='307' end_line='351'></method_info>
 			<added_lines>325,333,335,337,338,339,345,346,347,348,349</added_lines>
 			<deleted_lines>324,331,334,336,343,344</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_schedule_schedule_ops.py' new_name='tests\python\unittest\test_schedule_schedule_ops.py'>
 		<file_info nloc='368' complexity='34' token_count='4637'></file_info>
 		<method name='test_loop_dep_reduce' parameters=''>
 				<method_info nloc='6' complexity='1' token_count='65' nesting_level='0' start_line='412' end_line='419'></method_info>
 			<added_lines>412,413,414,415,416,417,418,419</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='test_loop_dep_reduce.f' parameters='n'>
 				<method_info nloc='3' complexity='1' token_count='31' nesting_level='1' start_line='414' end_line='416'></method_info>
 			<added_lines>414,415,416</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>420,421,423</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
