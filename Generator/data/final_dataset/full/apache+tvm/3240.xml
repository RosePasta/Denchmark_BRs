<bug_data>
<bug id='3240' author='kevinthesun' open_date='2019-05-24T20:55:56Z' closed_time='2019-07-24T18:48:47Z'>
 	<summary>[TOPI] Operator overloading issue when dealing with zero-rank tensor</summary>
 	<description>
 This PR &lt;denchmark-link:https://github.com/apache/tvm/pull/1029&gt;#1029&lt;/denchmark-link&gt;
  overloads binary op for tensor to use topi broadcast op when importing topi. This will cause topi.compute to fail when zero-rank tensor appears in the fcompute body, since a topi broadcast op returns a tensor while comm_reducer requires an expr:
 import tvm
 from tvm import relay
 
 n = 10
 A = tvm.placeholder((n, ), name='A')
 scale = tvm.placeholder((), name='scale')
 k = tvm.reduce_axis((0, n), name="k")
 fcompute = lambda : tvm.sum(A[k] * scale, axis=k)
 C = tvm.compute((), fcompute, name="C")
 Error msg:
 &lt;denchmark-code&gt;Traceback (most recent call last):
   File "test.py", line 9, in &lt;module&gt;
     C = tvm.compute((), fcompute, name="C")
   File "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tvm-0.6.dev0-py3.6-macosx-10.11-x86_64.egg/tvm/api.py", line 309, in compute
     body = fcompute(*[v.var for v in dim_var])
   File "test.py", line 8, in &lt;lambda&gt;
     fcompute = lambda : tvm.sum(A[k] * scale, axis=k)
   File "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tvm-0.6.dev0-py3.6-macosx-10.11-x86_64.egg/tvm/api.py", line 819, in reducer
     return _make_reduce(expr, axis, where)
   File "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tvm-0.6.dev0-py3.6-macosx-10.11-x86_64.egg/tvm/api.py", line 795, in _make_reduce
     assert isinstance(expr, _expr.Expr)
 AssertionError
 &lt;/denchmark-code&gt;
 
 &lt;denchmark-link:https://github.com/tqchen&gt;@tqchen&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/jroesch&gt;@jroesch&lt;/denchmark-link&gt;
  &lt;denchmark-link:https://github.com/yzhliu&gt;@yzhliu&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='kevinthesun' date='2019-05-30T17:43:30Z'>
 		&lt;denchmark-link:https://github.com/kevinthesun&gt;@kevinthesun&lt;/denchmark-link&gt;
  please followup to propose a fix for this.
 		</comment>
 		<comment id='2' author='kevinthesun' date='2019-07-06T21:33:22Z'>
 		ping &lt;denchmark-link:https://github.com/kevinthesun&gt;@kevinthesun&lt;/denchmark-link&gt;
 
 		</comment>
 		<comment id='3' author='kevinthesun' date='2019-07-09T02:10:56Z'>
 		Is it possible we force binary op to be the correct implementation inside tvm.compute?
 		</comment>
 		<comment id='4' author='kevinthesun' date='2019-07-09T17:02:08Z'>
 		Because it is not context-dependent, it is harder to do force that. We could, however, avoid force  TensorSlice mul scalar to always get an Expr(which should fix your case)
 		</comment>
 		<comment id='5' author='kevinthesun' date='2019-07-13T20:32:37Z'>
 		&lt;denchmark-link:https://github.com/kevinthesun&gt;@kevinthesun&lt;/denchmark-link&gt;
  what is the status on this?
 		</comment>
 		<comment id='6' author='kevinthesun' date='2019-07-15T22:39:39Z'>
 		&lt;denchmark-link:https://github.com/tqchen&gt;@tqchen&lt;/denchmark-link&gt;
  Did you mean force mul-scalar to always get Expr?
 		</comment>
 		<comment id='7' author='kevinthesun' date='2019-07-17T23:55:29Z'>
 		it depends on the type. If it is TensorSlice that has 0-rank, we could always return Expr.
 		</comment>
 		<comment id='8' author='kevinthesun' date='2019-07-24T18:06:32Z'>
 		After i think a bit more about it. I think the problem was that we do need to represent the 0-rank tensor as TensorSlice, so in the above case, we should instead write(note the call in the scale)
 import tvm
 from tvm import relay
 
 n = 10
 A = tvm.placeholder((n, ), name='A')
 scale = tvm.placeholder((), name='scale')
 k = tvm.reduce_axis((0, n), name="k")
 fcompute = lambda : tvm.sum(A[k] * scale(), axis=k)
 C = tvm.compute((), fcompute, name="C")
 		</comment>
 		<comment id='9' author='kevinthesun' date='2019-07-24T18:12:57Z'>
 		&lt;denchmark-link:https://github.com/apache/tvm/pull/3612&gt;#3612&lt;/denchmark-link&gt;
  makes the test cases more conservative. So that the topi behavior can remains the same.
 		</comment>
 	</comments>
 </bug>
<commit id='90eee08746d7b96d834331aa910a760451330f7b' author='Tianqi Chen' date='2019-07-24 11:48:39-07:00'>
 	<dmm_unit complexity='1.0' interfacing='1.0' size='1.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tests\python\unittest\test_codegen_llvm.py' new_name='tests\python\unittest\test_codegen_llvm.py'>
 		<file_info nloc='462' complexity='69' token_count='5294'></file_info>
 		<method name='test_rank_zero' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='10' nesting_level='0' start_line='346' end_line='367'></method_info>
 			<added_lines>353,354</added_lines>
 			<deleted_lines>352,353</deleted_lines>
 		</method>
 		<method name='test_rank_zero_bound_checkers.check_llvm' parameters='n'>
 				<method_info nloc='19' complexity='2' token_count='284' nesting_level='1' start_line='370' end_line='390'></method_info>
 			<added_lines>377,378</added_lines>
 			<deleted_lines>376,377</deleted_lines>
 		</method>
 		<method name='test_rank_zero_bound_checkers' parameters=''>
 				<method_info nloc='3' complexity='1' token_count='10' nesting_level='0' start_line='369' end_line='391'></method_info>
 			<added_lines>377,378</added_lines>
 			<deleted_lines>376,377</deleted_lines>
 		</method>
 		<method name='test_rank_zero.check_llvm' parameters='n'>
 				<method_info nloc='18' complexity='2' token_count='274' nesting_level='1' start_line='347' end_line='366'></method_info>
 			<added_lines>353,354</added_lines>
 			<deleted_lines>352,353</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines>18</added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 	<modification change_type='MODIFY' old_name='topi\python\topi\generic_op_impl.py' new_name='topi\python\topi\generic_op_impl.py'>
 		<file_info nloc='25' complexity='6' token_count='226'></file_info>
 		<method name='_make_bop._tensor_bop_impl' parameters='lhs,rhs'>
 				<method_info nloc='6' complexity='3' token_count='65' nesting_level='1' start_line='56' end_line='86'></method_info>
 			<added_lines>82,83</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<method name='_make_bop' parameters='broadcast_bop,orig_bop'>
 				<method_info nloc='5' complexity='1' token_count='31' nesting_level='0' start_line='25' end_line='88'></method_info>
 			<added_lines>82,83</added_lines>
 			<deleted_lines></deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
