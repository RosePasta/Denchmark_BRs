<bug_data>
<bug id='586' author='festlv' open_date='2017-02-18T22:53:13Z' closed_time='2017-02-22T19:48:59Z'>
 	<summary>Caffe converter silently fails on (some) non-supported layer types</summary>
 	<description>
 I have been trying out caffe converter example on a custom caffe model.
 The prediction vector of the imported model was all zeroes. During debugging of this issue I found out that tiny-dnn network was correctly constructed, but loading weights had silently stopped after encountering BatchNorm layer- it can't be mapped to tiny-dnn layer type for weight loading.
 I'd suggest at least changing this line to throw an error in this case, so that unsupported layer types trigger obvious error:
 &lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/blob/master/tiny_dnn/io/caffe/layer_factory.h#L146&gt;https://github.com/tiny-dnn/tiny-dnn/blob/master/tiny_dnn/io/caffe/layer_factory.h#L146&lt;/denchmark-link&gt;
 
 	</description>
 	<comments>
 		<comment id='1' author='festlv' date='2017-02-18T23:45:20Z'>
 		&lt;denchmark-link:https://github.com/festlv&gt;@festlv&lt;/denchmark-link&gt;
  Thanks for reporting this bug! please submit a PR with the pach
 		</comment>
 		<comment id='2' author='festlv' date='2017-02-18T23:56:15Z'>
 		And also covering missing layer in tiny if you need :)
 		</comment>
 		<comment id='3' author='festlv' date='2017-02-19T09:15:24Z'>
 		Submitted a PR.
 I do need this missing layer, but I don't know enough about internals of tiny-dnn, caffe and even how exactly batchnorm works, so I won't be able to add it.
 		</comment>
 		<comment id='4' author='festlv' date='2017-02-22T19:48:59Z'>
 		The core of this issue has been fixed in &lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/587&gt;#587&lt;/denchmark-link&gt;
 .
 The missing BatchNorm support is in &lt;denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/595&gt;#595&lt;/denchmark-link&gt;
 .
 I have the following suggestions regarding caffe converter:
 
 List supported layer types in documentation (https://github.com/tiny-dnn/tiny-dnn/blob/master/tiny_dnn/io/caffe/layer_factory_impl.h#L857).
 Make the inference in general more robust (maybe a debug mode with assertions). I have been able to proceed through inference although the weights were not loaded for any layers.
 
 		</comment>
 	</comments>
 </bug>
<commit id='b26eb9df09a510af537b93e7ab12b463c9796f09' author='Reinis Veips' date='2017-02-19 11:02:23+01:00'>
 	<dmm_unit complexity='0.0' interfacing='1.0' size='0.0'></dmm_unit>
 	<modification change_type='MODIFY' old_name='tiny_dnn\io\caffe\layer_factory.h' new_name='tiny_dnn\io\caffe\layer_factory.h'>
 		<file_info nloc='100' complexity='18' token_count='768'></file_info>
 		<method name='tiny_dnn::reload_weight_from_caffe_net' parameters='layer,net'>
 				<method_info nloc='27' complexity='8' token_count='179' nesting_level='1' start_line='122' end_line='156'></method_info>
 			<added_lines>146,147,148,149,150,151</added_lines>
 			<deleted_lines>146</deleted_lines>
 		</method>
 		<modified_lines>
 			<added_lines></added_lines>
 			<deleted_lines></deleted_lines>
 		</modified_lines>
 	</modification>
 </commit>
</bug_data>
