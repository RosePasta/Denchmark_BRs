{"BR": {"BR_id": "2474", "BR_author": "Mut1nyJD", "BRopenT": "2019-11-25T22:40:56Z", "BRcloseT": "2019-12-03T06:28:45Z", "BR_text": {"BRsummary": "An ONNX model that worked in 0.5.1 now fails in 1.0.0", "BRdescription": "\n Describe the bug\n I recently upgraded my ONNX Runtime from 0.5.1 to 1.0.0 after changing my C code to adapt to the new API I then found out that my previous ONNX model does no longer run in 1.0.0. My ONNX model uses OpSet-Level 9. It is a PyTorch1.3 ONNX export of a HRNet-backbone landmark detection model.\n When loading the ONNX model in ONNXRuntime 1.0 I'll get an Exception:\n op_kernel.cc:21 onnxruntime::OpKernelContext::OpKernelContext kernel != nullptr was false. OpKernel was null\n it loaded fine in 0.5.1\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Mut1nyJD", "commentT": "2019-11-26T00:18:17Z", "comment_text": "\n \t\tIf I attempt to export that model with opset 9 I see the following error:\n \n python\\python37\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:198: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.\n ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n We recommend using opset 11 and above for models using this operator.\n \n Exporting with opset 11 produces a model that loads successfully. Is that an option? I used the export instructions from this pytorch issue: <denchmark-link:https://github.com/pytorch/pytorch/issues/23474>pytorch/pytorch#23474</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Mut1nyJD", "commentT": "2019-11-26T00:31:03Z", "comment_text": "\n \t\tI also exported using opset 9 and the model produced loads successfully. Could you please share your model as well a the requested system info:\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n ONNX Runtime installed from (source or binary):\n ONNX Runtime version:\n Python version:\n Visual Studio version (if applicable):\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version:\n GPU model and memory:\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Mut1nyJD", "commentT": "2019-11-26T07:40:49Z", "comment_text": "\n \t\tSure please find here a download link to the ONNX model\n <denchmark-link:https://mut1nyjd.stackstorage.com/s/ZwctHMbvHJM1bhf>https://mut1nyjd.stackstorage.com/s/ZwctHMbvHJM1bhf</denchmark-link>\n \n \n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n \n \n Windows 10\n \n \n ONNX Runtime installed from (source or binary):\n \n \n Binary NuGet package version\n \n \n ONNX Runtime version:\n \n \n 0.51 and 1.0.0\n \n \n Python version:\n \n \n No Python using C Backend\n \n \n Visual Studio version (if applicable):\n \n \n 15.9.8\n \n \n CUDA/cuDNN version:\n \n \n 10.1/7.1\n \n \n GPU model and memory:\n \n \n GTX1060ti 6GB VRAM\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Mut1nyJD", "commentT": "2019-11-26T11:33:24Z", "comment_text": "\n \t\tForgot to say the output I'll get from ONNX Runtime 0.5.1 on this model on a test image is pretty close to what I'll get from the PyTorch model, so the model itself does not seem to be the problem\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Mut1nyJD", "commentT": "2019-11-27T05:05:28Z", "comment_text": "\n \t\tI have not been able to reproduce on either linux or windows with CUDA 10.1. The cuDNN version I have is a bit newer on each (7.6) though but I don't expect that would matter.\n Can you try completely uninstalling ONNX Runtime and reinstalling? There are only a few places we create OpKernelContext and I can't see how any of those would have a nullptr for the kernel.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Mut1nyJD", "commentT": "2019-11-27T09:17:54Z", "comment_text": "\n \t\tHmm I tried a complete fresh install of ONNXRuntime with a new project also tried it with VS2015.\n I installed latest CuDNN 7.6.5 for 10.1 but still seeing the same error.\n I've installed the MKL-DNN version of ONNXRuntime 1.0.0 that works fine with the model.\n I will try a different GPU and/with other driver and see if that makes a difference.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "Mut1nyJD", "commentT": "2019-11-27T11:07:02Z", "comment_text": "\n \t\tCould you share the C code you're using? I tested using python so that's one other difference.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "Mut1nyJD", "commentT": "2019-11-27T11:38:32Z", "comment_text": "\n \t\tSure no problem, please find here my minimum test code\n // ONNXRuntime\n #include \"onnxruntime_c_api.h\"\n // CUDA ONNXRuntime\n #include \"cuda_provider_factory.h\"\n `const OrtApi* g_ort = OrtGetApiBase()->GetApi(ORT_API_VERSION);`\n void CheckStatus(OrtStatus* status)\n {\n \tif (status != NULL) {\n \t\tconst char* msg = g_ort->GetErrorMessage(status);\n \t\tfprintf(stderr, \"%s\\n\", msg);\n \t\tg_ort->ReleaseStatus(status);\n \t\texit(1);\n \t}\n }\n  \n int main()\n {\n \tOrtEnv* env = NULL;\n \tCheckStatus(g_ort->CreateEnv(ORT_LOGGING_LEVEL_WARNING, \"test\", &env));\n \t// initialize session options if needed\n \tOrtSessionOptions* session_options;\n \tg_ort->CreateSessionOptions(&session_options);\n \t// If you have CUDA ONNXRuntime installed otherwise don't use this line\n \tOrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 0);\n \tg_ort->SetIntraOpNumThreads(session_options, 8);\n \tg_ort->SetInterOpNumThreads(session_options, 8);\n \tg_ort->SetSessionGraphOptimizationLevel(session_options, ORT_ENABLE_BASIC);\n \tOrtSession* session = NULL;\n \tCheckStatus(g_ort->CreateSession(env, L\"hrnet_w18_landmarks.onnx\", session_options, &session));\n \tstd::cout << \"ONNX model loaded succesfull\\n\"; \n }\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "Mut1nyJD", "commentT": "2019-11-27T11:43:13Z", "comment_text": "\n \t\tIf you need a full .vcproj + code I can upload that as well\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "Mut1nyJD", "commentT": "2019-11-27T23:13:58Z", "comment_text": "\n \t\tI can reproduce the issue. Should have a fix later today.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "Mut1nyJD", "commentT": "2019-11-28T07:29:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/skottmckay>@skottmckay</denchmark-link>\n \n Great excellent! Happy you manage to reproduce it and yes sorry I misreported my setup it was CUDA 10.1/CuDNN 7.5 before I have so many CUDA/CuDNN variants installed I loose track from time to time.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "Mut1nyJD", "commentT": "2019-11-28T21:48:46Z", "comment_text": "\n \t\tTwo short term options if needed:\n \n \n disable all optimizations so constant folding doesn't run\n \n simple but has a performance cost\n g_ort->SetSessionGraphOptimizationLevel(session_options, ORT_DISABLE_ALL);\n \n \n \n run the optimizations with just the CPU execution provider enabled, save the model, and use that model with the CUDA execution provider. this is a one time step.\n \n g_ort->SetOptimizedModelFilePath(session_options, L\"hrnet_w18_landmarks.optimized.onnx\");\n comment out the line that adds the CUDA execution provider, set the optimized model file path, create a session and release it (no need to call Run). following that you should be able to use the CUDA execution provider with the optimized model\n \n \n \n \t\t"}}}, "commit": {"commit_id": "e8b327d6573845654700c048c330c9c722f3d3f8", "commit_author": "Scott McKay", "commitT": "2019-12-03 16:28:44+10:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\optimizer\\constant_folding.cc", "file_new_name": "onnxruntime\\core\\optimizer\\constant_folding.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "28,29,30,31,32,33,34,35,36,37,38,50,51,52,53,54,58,59,60,61,62,86,87", "deleted_lines": "65,66", "method_info": {"method_name": "onnxruntime::ConstantFolding::ApplyImpl", "method_params": "graph,modified,graph_level,logger", "method_startline": "14", "method_endline": "116"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\optimizer\\constant_folding.h", "file_new_name": "onnxruntime\\core\\optimizer\\constant_folding.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "19,20", "deleted_lines": "19,20", "method_info": {"method_name": "onnxruntime::ConstantFolding::ConstantFolding", "method_params": "compatible_execution_providers", "method_startline": "19", "method_endline": "20"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "onnxruntime\\core\\optimizer\\optimizer_execution_frame.cc", "file_new_name": "onnxruntime\\core\\optimizer\\optimizer_execution_frame.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "60,61,70,71,72", "deleted_lines": "60,61,70,71", "method_info": {"method_name": "onnxruntime::OptimizerExecutionFrame::Info::Info", "method_params": "nodes,initialized_tensor_set", "method_startline": "19", "method_endline": "75"}}, "hunk_1": {"Ismethod": 1, "added_lines": "122,123", "deleted_lines": "121,122", "method_info": {"method_name": "onnxruntime::OptimizerExecutionFrame::CreateNodeOutputMLValueImpl", "method_params": "ort_value,ort_value_idx,shape,nnz", "method_startline": "98", "method_endline": "129"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cuda\\cuda_execution_provider.cc", "file_new_name": "onnxruntime\\core\\providers\\cuda\\cuda_execution_provider.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1320,1321,1322,1323,1324,1327", "deleted_lines": "1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1320,1321,1322,1323,1324,1327", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::GetCapability", "method_params": "graph", "method_startline": "1259", "method_endline": "1354"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "onnxruntime\\test\\optimizer\\graph_transform_test.cc", "file_new_name": "onnxruntime\\test\\optimizer\\graph_transform_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "GraphTransformationTests,ConstantFoldingNodesOnDifferentEP", "method_startline": "134", "method_endline": "159"}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1013", "method_info": {"method_name": "onnxruntime::test::ValidateAttention", "method_params": "graph", "method_startline": "905", "method_endline": "1016"}}}}}}}