{"BR": {"BR_id": "3755", "BR_author": "alinaSarwar", "BRopenT": "2020-04-29T21:44:11Z", "BRcloseT": "2020-07-25T08:41:02Z", "BR_text": {"BRsummary": "Error creating onnxruntime session", "BRdescription": "\n Problem Description\n I'm trying to export the following pytorch model to onnx using torch scripting.\n <denchmark-code>@torch.jit.script\n def get_count(array, thresh):\n     count = torch.tensor(0)\n     for i in range(5):\n        if array[i] > thresh:\n         count = count + 1\n     return count\n \n class test_model(nn.Module):\n     def forward(self, array, thresh):\n         return get_count(array, thresh)\n </denchmark-code>\n \n I use the following to export my model to onnx.\n <denchmark-code>array = torch.tensor([10,11,5,4,6])\n thresh = torch.tensor(5)\n model = test_model()\n \n torch.onnx.export(model=model,\n                   args=(array, thresh),\n                   f=\"test_model.onnx\",\n                   verbose=True,\n                   opset_version=11,\n                   input_names=['input_data', 'threshold'])\n </denchmark-code>\n \n Here is the onnx graph that i get after export:\n <denchmark-code>graph(%input_data : Long(5),\n       %threshold : Long()):\n   %2 : Long() = onnx::Constant[value={1}]()\n   %3 : Long() = onnx::Constant[value={5}]()\n   %4 : Long() = onnx::Constant[value={0}]()\n   %5 : Tensor = onnx::Cast[to=9](%2)\n   %6 : Long() = onnx::Loop(%3, %5, %4) # <ipython-input-70-c5725febec4a>:4:4\n     block0(%i.1 : Long(), %cond : bool, %count.10 : Tensor):\n       %10 : Tensor = onnx::Gather[axis=0](%input_data, %i.1) # <ipython-input-70-c5725febec4a>:5:10\n       %11 : Tensor = onnx::Greater(%10, %threshold) # <ipython-input-70-c5725febec4a>:5:10\n       %12 : Tensor = onnx::If(%11) # <ipython-input-70-c5725febec4a>:5:7\n         block0():\n           %13 : Long() = onnx::Constant[value={1}]()\n           %14 : LongTensor = onnx::Add(%count.10, %13) # <ipython-input-70-c5725febec4a>:6:16\n           -> (%14)\n         block1():\n           -> (%count.10)\n       %15 : Tensor = onnx::Cast[to=9](%2)\n       -> (%15, %12)\n   return (%6)\n </denchmark-code>\n \n However, when I create an onnx runtime inference session using\n <denchmark-code> ort_sess = onnxruntime.InferenceSession('test_sort.onnx')\n </denchmark-code>\n \n I get the following error:\n <denchmark-code>---------------------------------------------------------------------------\n Fail                                      Traceback (most recent call last)\n <ipython-input-73-592f4c5d3a84> in <module>\n       3 inputs = (array,threshold)\n       4 \n ----> 5 ort_sess = ort.InferenceSession('test_sort.onnx')\n       6 \n       7 # ort_inputs = {ort_session.get_inputs()[i].name:inpt for i, inpt in enumerate(to_np(inputs))}\n \n /opt/conda/lib/python3.7/site-packages/onnxruntime/capi/session.py in __init__(self, path_or_bytes, sess_options, providers)\n      23         self._path_or_bytes = path_or_bytes\n      24         self._sess_options = sess_options\n ---> 25         self._load_model(providers)\n      26         self._enable_fallback = True\n      27 \n \n /opt/conda/lib/python3.7/site-packages/onnxruntime/capi/session.py in _load_model(self, providers)\n      41             raise TypeError(\"Unable to load from type '{0}'\".format(type(self._path_or_bytes)))\n      42 \n ---> 43         self._sess.load_model(providers)\n      44 \n      45         self._session_options = self._sess.session_options\n \n Fail: [ONNXRuntimeError] : 1 : FAIL : Exception during loading: /onnxruntime_src/onnxruntime/core/graph/graph.cc:2485 onnxruntime::common::Status onnxruntime::Graph::SetGraphInputsOutputs() node_arg was false. Graph ctor should have created NodeArg for initializer.\n </denchmark-code>\n \n Urgency\n Moderate.\n System Information\n OS: Linux Ubuntu 16.04\n ONNX runtime version 1.1.0 installed from binary\n Python version: 3.7.4\n Expected Behavior\n Successful creation of onnxruntime session.\n Could someone please help me resolve this error?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "alinaSarwar", "commentT": "2020-05-04T20:29:49Z", "comment_text": "\n \t\tCan you please share the ONNX model file?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "alinaSarwar", "commentT": "2020-05-05T08:45:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hariharans29>@hariharans29</denchmark-link>\n  the code runs as is -- I reproduced in a <denchmark-link:https://colab.research.google.com/drive/1YG-lcyW_ZK5IFMkGqllbqRr8rEEdzh7O?usp=sharing>colab notebook</denchmark-link>\n \n Running the notebook gives you this model: <denchmark-link:https://github.com/microsoft/onnxruntime/files/4579717/onnxruntime_3755.zip>onnxruntime_3755.zip</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "alinaSarwar", "commentT": "2020-05-20T05:01:31Z", "comment_text": "\n \t\tThere are actually two edge case bugs involved here.\n \n The 'count' input to the Loop subgraph has no type information (which is valid in an ONNX model) and is not used directly in the Loop. It's used inside an If node in the Loop which is a separate nested subgraph. That creates a subtle interaction where we don't create a necessary piece in the Loop subgraph.\n \n After fixing that I hit another issue related to the combination of Loop and If in a trivial graph.\n \n The loop state variable for 'count' is being passed directly to an If node and used directly as the output in one of the branches there. As the loop state variable may change shape across iterations we can't infer a shape, which means we couldn't infer the output shape from the If branch. We do have a way to handle this sort of unknown output shape so a fix was also required to use that.\n \n Once I create some unit tests I'll put the changes in a PR.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "alinaSarwar", "commentT": "2020-05-21T10:55:36Z", "comment_text": "\n \t\t\n There are actually two edge case bugs involved here.\n \n The 'count' input to the Loop subgraph has no type information (which is valid in an ONNX model) and is not used directly in the Loop. It's used inside an If node in the Loop which is a separate nested subgraph. That creates a subtle interaction where we don't create a necessary piece in the Loop subgraph.\n \n After fixing that I hit another issue related to the combination of Loop and If in a trivial graph.\n \n The loop state variable for 'count' is being passed directly to an If node and used directly as the output in one of the branches there. As the loop state variable may change shape across iterations we can't infer a shape, which means we couldn't infer the output shape from the If branch. We do have a way to handle this sort of unknown output shape so a fix was also required to use that.\n \n Once I create some unit tests I'll put the changes in a PR.\n \n Hi,\n When I run the above code, I met a different error:\n \n onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /onnxruntime_src/onnxruntime/core/graph/graph.cc:912 void onnxruntime::Graph::InitializeStateFromModelFileGraphProto() This is an invalid model. Graph output (count.10) does not exist in the graph.\n \n I want to know if this error can be fixed in your new merge request <denchmark-link:https://github.com/microsoft/onnxruntime/pull/4004>#4004</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "alinaSarwar", "commentT": "2020-05-26T06:12:08Z", "comment_text": "\n \t\t\n Running the notebook gives you this model: onnxruntime_3755.zip\n @snsun I tested with this model and see no errors. How are you creating the model that fails?\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "alinaSarwar", "commentT": "2020-05-26T07:23:27Z", "comment_text": "\n \t\t\n \n Running the notebook gives you this model: onnxruntime_3755.zip\n @snsun I tested with this model and see no errors. How are you creating the model that fails?\n \n \n Tracked it down to being due to the pytorch version. Version 1.4 emits a bad subgraph where a\n graph output is not produced by any nodes in the subgraph. Version 1.5 works fine (which is what produced the graph I tested with).\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "alinaSarwar", "commentT": "2020-05-26T08:38:11Z", "comment_text": "\n \t\tOh, I am using pytorch 1.4. I will test with pytorch 1.5. Thank you!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "alinaSarwar", "commentT": "2020-07-25T08:39:52Z", "comment_text": "\n \t\tThis issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.\n \t\t"}}}, "commit": {"commit_id": "2a96be83f629bfbf2aa5c3937e9c6e86e050bdad", "commit_author": "Scott McKay", "commitT": "2020-05-29 14:48:07+10:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "onnxruntime\\core\\graph\\graph.cc", "file_new_name": "onnxruntime\\core\\graph\\graph.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "211,212", "deleted_lines": "211,212", "method_info": {"method_name": "onnxruntime::NodeArg::UpdateTypeAndShape", "method_params": "input_type,strict,override_types,logger", "method_startline": "211", "method_endline": "304"}}, "hunk_1": {"Ismethod": 1, "added_lines": "891", "deleted_lines": "883", "method_info": {"method_name": "onnxruntime::Graph::InitializeStateFromModelFileGraphProto", "method_params": "", "method_startline": "864", "method_endline": "941"}}, "hunk_2": {"Ismethod": 1, "added_lines": "787,788,789,790,791,792,793,794,795,796,797,824", "deleted_lines": "787,788,789,816", "method_info": {"method_name": "onnxruntime::Graph::Graph", "method_params": "owning_model,graph_proto,domain_to_version,ir_version,schema_registry,parent_graph,parent_node,logger,model_functions", "method_startline": "739", "method_endline": "854"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cpu\\controlflow\\if.cc", "file_new_name": "onnxruntime\\core\\providers\\cpu\\controlflow\\if.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "251,253,254,256,257,258,259,260,262,263,265,266,267,268,269,270,271", "deleted_lines": "251,252,253,254,256,258,259,260,261,262,263,265,266,268", "method_info": {"method_name": "onnxruntime::IfImpl::AllocateOutputTensors", "method_params": "", "method_startline": "245", "method_endline": "278"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\test\\providers\\cpu\\controlflow\\loop_test.cc", "file_new_name": "onnxruntime\\test\\providers\\cpu\\controlflow\\loop_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "Loop,PassThroughSubgraphInputNoTypeOrShape", "method_startline": "831", "method_endline": "930"}}}}}}}