{"BR": {"BR_id": "729", "BR_author": "lucienwang1009", "BRopenT": "2019-03-28T09:40:59Z", "BRcloseT": "2019-06-03T21:23:52Z", "BR_text": {"BRsummary": "Can't use parent graph's initializer in subgraphs", "BRdescription": "\n \n ONNX IR version 4 relaxes the constraint that initializers must be a subset of graph's inputs: <denchmark-link:https://github.com/onnx/onnx/pull/1718>onnx/onnx#1718</denchmark-link>\n .\n While in current onnxruntime, if a subgraph, e.g., the body-graph of If op, refers to an initializer of its parent graph and the initializer isn't amid graph's inputs, the runtime will screw up with a message:\n INVALID_GRAPH : Load model from model.onnx failed:At top level graph without matching NodeArg that subgraph consumes. Name=const_1 Graph may not conform to the ONNX spec and contain initializers that are not graph inputs.\n But if the initializer is in parent graph's inputs, everything will be fine.\n Here is a sample model for your reference:\n import onnx\n from onnx import helper, numpy_helper\n from onnx import AttributeProto, TensorProto, GraphProto\n import numpy as np\n import onnxruntime\n \n # The protobuf definition can be found here:\n # https://github.com/onnx/onnx/blob/master/onnx/onnx.proto\n \n \n # Create one input (ValueInfoProto)\n initializers = [\n     onnx.helper.make_tensor(\n      name='cond_1',\n      data_type=onnx.TensorProto.BOOL,\n      dims=[1],\n      vals=[0]\n     ),\n     onnx.helper.make_tensor(\n      name='const_1',\n      data_type=onnx.TensorProto.FLOAT,\n      dims=[2],\n      vals=[1,2]\n     ),\n     onnx.helper.make_tensor(\n      name='const_2',\n      data_type=onnx.TensorProto.FLOAT,\n      dims=[2],\n      vals=[1,2]\n     ),\n ]\n \n # Create one output (ValueInfoProto)\n Y = helper.make_tensor_value_info('output', TensorProto.FLOAT, [2])\n \n # then graph\n then_graph = helper.make_graph(\n     [helper.make_node(\"Identity\", inputs=[\"const_1\"], outputs=[\"then_graph_out\"], name=\"iden_1\")],\n     \"then_graph\",\n     [],\n     [helper.make_tensor_value_info(\"then_graph_out\", TensorProto.FLOAT, [2])],\n     []\n )\n else_graph = helper.make_graph(\n     [helper.make_node(\"Identity\", inputs=[\"const_2\"], outputs=[\"else_graph_out\"], name=\"iden_2\")],\n     \"else_graph\",\n     [],\n     [helper.make_tensor_value_info(\"else_graph_out\", TensorProto.FLOAT, [2])],\n     []\n )\n # If node\n if_node = helper.make_node(\n     \"If\",\n     inputs=[\"cond_1\"],\n     outputs=[\"output\"],\n     else_branch=else_graph,\n     then_branch=then_graph,\n     name=\"if\"\n )\n \n # Create the graph (GraphProto)\n graph_def = helper.make_graph(\n     [if_node],\n     \"test-model\",\n     [],\n     [Y],\n     initializers\n )\n \n # Create the model (ModelProto)\n model_def = helper.make_model(graph_def,\n                               producer_name='onnx-example')\n onnx.save(model_def, \"model.onnx\")\n \n sess = onnxruntime.InferenceSession(\"model.onnx\")\n result = sess.run(['output'], input_feed={})\n print(result)\n System information\n \n ONNX Runtime version (you are using): 0.3.0\n \n Describe the solution you'd like\n According to ONNX document, subgraphs should be able to use initializers in its parent graph, no matter whether the initializer is the part of inputs.\n Describe alternatives you've considered\n Additional context\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lucienwang1009", "commentT": "2019-03-28T09:51:38Z", "comment_text": "\n \t\tThe Graph constructor need to create a NodeArg for values in the initializer list. The current code assumes that just doing that for graph inputs is sufficient. We could call GetOrCreateNodeArg when iterating the initialziers and adding to the name_to_initial_tensor_ map.\n <denchmark-code>// Copy initial tensors to a map.\n for (auto& tensor : graph_proto_->initializer()) {\n   name_to_initial_tensor_[tensor.name()] = &tensor;\n }\n \n // Collect all node arg name, type, shape information in the graph.\n // type/shape information will be assigned to each node arg when going\n // thru all nodes later.\n for (auto& graph_input : graph_proto_->input()) {\n   if (graph_input.has_name() && graph_input.has_type()) {\n     name_to_type_map[graph_input.name()] = graph_input.type();\n     // always create a NodeArg for graph input in case its from an initializer\n     GetOrCreateNodeArg(graph_input.name(), &graph_input.type());\n   }\n }\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lucienwang1009", "commentT": "2019-03-28T10:04:44Z", "comment_text": "\n \t\t\n The Graph constructor need to create a NodeArg for values in the initializer list. The current code assumes that just doing that for graph inputs is sufficient. We could call GetOrCreateNodeArg when iterating the initialziers and adding to the name_to_initial_tensor_ map.\n // Copy initial tensors to a map.\n for (auto& tensor : graph_proto_->initializer()) {\n   name_to_initial_tensor_[tensor.name()] = &tensor;\n }\n \n // Collect all node arg name, type, shape information in the graph.\n // type/shape information will be assigned to each node arg when going\n // thru all nodes later.\n for (auto& graph_input : graph_proto_->input()) {\n   if (graph_input.has_name() && graph_input.has_type()) {\n     name_to_type_map[graph_input.name()] = graph_input.type();\n     // always create a NodeArg for graph input in case its from an initializer\n     GetOrCreateNodeArg(graph_input.name(), &graph_input.type());\n   }\n }\n \n \n Thanks for quick action! This bug blocked tf2onnx upgrade to IR version 4.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lucienwang1009", "commentT": "2019-04-08T02:56:18Z", "comment_text": "\n \t\tNew error:\n RuntimeError: [ONNXRuntimeError] : 1 : GENERAL ERROR : Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/node_index_info.cc:82 onnxruntime::NodeIndexInfo::Init(const TValidNodes&, onnxruntime::NodeIndex, const onnxruntime::MLValueNameIdxMap&)::<lambda(const onnxruntime::NodeArg&, bool)> [with TValidNodes = onnxruntime::ValidNodes<const std::vector<const onnxruntime::Node> >] status.IsOK() was false. Could not find MLValue with name 'const_1'*\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lucienwang1009", "commentT": "2019-04-18T07:30:20Z", "comment_text": "\n \t\tThis should be related with two issues:\n \n One should be known issue in https://github.com/Microsoft/onnxruntime/blob/master/onnxruntime/core/optimizer/optimizer_execution_frame.cc#L53. It does not support control flow ops by design, @weixingzhang\n One may be a bug in constant folding https://github.com/Microsoft/onnxruntime/blob/master/onnxruntime/core/optimizer/constant_folding.cc#L26. I believe that for constant folding it should use OpKernelContextInternal. @weixingzhang @kkaranasos\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lucienwang1009", "commentT": "2019-04-22T18:49:06Z", "comment_text": "\n \t\tConstant folding uses OpKernelContext (and not OpKernelContextInternal) on purpose, so that we don't rely on the InferenceSession in the optimizer.\n For now we disabled constant folding for subgraphs (<denchmark-link:https://github.com/microsoft/onnxruntime/pull/858>#858</denchmark-link>\n ) until we provide a proper support for it.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "lucienwang1009", "commentT": "2019-05-15T01:20:17Z", "comment_text": "\n \t\tI ran the test code with the latest onnxruntime code and it was successful. <denchmark-link:https://github.com/lucienwang1009>@lucienwang1009</denchmark-link>\n  can you please check to see if it works for you?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "lucienwang1009", "commentT": "2019-06-03T21:23:52Z", "comment_text": "\n \t\tClosing as I can't reproduce. Please re-open if this is still an issue.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "lucienwang1009", "commentT": "2019-06-04T01:28:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/skottmckay>@skottmckay</denchmark-link>\n  It works fine now, Thanks!!\n \t\t"}}}, "commit": {"commit_id": "65c50bb25b2f5a77572fae7a3142ecc07088c84a", "commit_author": "Scott McKay", "commitT": "2019-04-05 14:09:27+10:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\graph\\graph.cc", "file_new_name": "onnxruntime\\core\\graph\\graph.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "661,662,663,664,665,666,667,668,669,670,671,672", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::Graph::Graph", "method_params": "graph_proto,domain_to_version,ir_version,schema_registry,parent_graph,model_functions", "method_startline": "604", "method_endline": "705"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\test\\ir\\onnx_model_test.cc", "file_new_name": "onnxruntime\\test\\ir\\onnx_model_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "154,155,156,157,158", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "ONNXModelsTest,TestIRv4NonInputInitializers", "method_startline": "154", "method_endline": "158"}}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "onnxruntime\\test\\testdata\\subgraph_implicit_input_from_initializer.onnx", "file_new_name": "onnxruntime\\test\\testdata\\subgraph_implicit_input_from_initializer.onnx"}}}}