{"BR": {"BR_id": "6742", "BR_author": "serycjon", "BRopenT": "2016-06-29T09:46:12Z", "BRcloseT": "2019-03-14T10:51:16Z", "BR_text": {"BRsummary": "cv::cuda::Filter thread safety", "BRdescription": "\n cv::cuda::Filter created by cv::cuda::createGaussianFilter(...) is not thread safe. And I don't mean the classical thread-safety of accessing one instance from multiple cpu threads. It is not safe to create separate instances of the Filter in multiple threads.\n <denchmark-h:h3>system info</denchmark-h>\n \n \n OpenCV version: 3.1.0\n Host OS: Linux (Ubuntu 14.04)\n GPU: GeForce GTX TITAN X\n compiler: g++ 4.8.4\n \n <denchmark-h:h3>Expected behaviour</denchmark-h>\n \n <denchmark-link:https://cloud.githubusercontent.com/assets/3328599/16447658/dfa96ad0-3ded-11e6-8e4c-ba4484440cc2.png></denchmark-link>\n \n <denchmark-link:https://cloud.githubusercontent.com/assets/3328599/16447657/dfa82aee-3ded-11e6-9710-a485cd1866cb.png></denchmark-link>\n \n <denchmark-h:h3>Actual behaviour</denchmark-h>\n \n <denchmark-link:https://cloud.githubusercontent.com/assets/3328599/16447632/b9ef3716-3ded-11e6-857c-ed0673e36861.png></denchmark-link>\n \n <denchmark-link:https://cloud.githubusercontent.com/assets/3328599/16447624/b24974f4-3ded-11e6-93ee-18aa667fd2b9.png></denchmark-link>\n \n (not deterministic... each execution outputs slightly different results, but the first image is almost always badly distorted)\n <denchmark-h:h3>Code example to reproduce the issue</denchmark-h>\n \n <denchmark-code>#include <iostream>\n #include <string>\n #include <thread>\n #include <opencv2/core/core.hpp>\n #include <opencv2/cudafilters.hpp>\n #include <opencv2/highgui/highgui.hpp>\n \n void show(const std::string& win_name, const cv::Mat& im) {\n   cv::Mat to_show;\n   double min, max;\n   cv::minMaxIdx(im, &min, &max);\n   im.convertTo(to_show, CV_8UC1, 255 / (max-min), -min);\n   cv::namedWindow(win_name, CV_WINDOW_NORMAL);\n   std::cout << win_name << \" - min: \" << min << \"; max: \" << max << std::endl;\n   cv::imshow(win_name, to_show);\n   // cv::imwrite(std::string(\"/tmp/\")+win_name+\".png\", to_show);\n }\n \n void filter_it(const cv::cuda::GpuMat& d_src, int ksize, cv::Mat& h_dst) {\n   cv::cuda::GpuMat d_dst(d_src.rows, d_src.cols, CV_32FC1);\n   cv::Ptr<cv::cuda::Filter> filter =\n       cv::cuda::createGaussianFilter(CV_32FC1, CV_32FC1, cv::Size(ksize,ksize), 0, 0);\n   filter->apply(d_src, d_dst);\n   h_dst = cv::Mat(d_dst);\n }\n \n int main(int argc, char *argv[])\n {\n   cv::Mat dot1;\n   const int size = 65;\n   dot1.create(size, size, CV_32FC1);\n   dot1.at<float>(size/2, size/2) = 1.0f;\n \n   cv::Mat dot2;\n   dot1.copyTo(dot2);\n \n   cv::cuda::GpuMat d_src1(dot1);\n   cv::cuda::GpuMat d_src2(dot2);\n \n   cv::Mat h_dst1;\n   h_dst1.create(size, size, CV_32FC1);\n   cv::Mat h_dst2;\n   h_dst2.create(size, size, CV_32FC1);\n \n   std::thread thr1(&filter_it, std::ref(d_src1), 31, std::ref(h_dst1));\n   std::thread thr2(&filter_it, std::ref(d_src2), 3,  std::ref(h_dst2));\n   thr1.join();\n   thr2.join();\n \n   show(\"filter_1\", h_dst1);\n   show(\"filter_2\", h_dst2);\n   cv::waitKey(0);\n   return 0;\n }\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "serycjon", "commentT": "2016-07-02T20:24:19Z", "comment_text": "\n \t\tThe issue is caused by cuda constant memory being used for the filter (modules/cudafilters/src/cuda/row_filter.hpp:55 -  constant float c_kernel[MAX_KERNEL_SIZE];)\n I see two possible fixes for this bug:\n \n synchronize filtering by mutex -> awful performance loss when multithreading\n use cuda global memory instead of the constant memory -> probably minor (?) performance loss when singlethreading\n \n Is there any other possibility? Or which of these two solutions is prefered for OpenCV? (So that I could try and create a fix and a pull request)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "serycjon", "commentT": "2017-12-21T18:24:58Z", "comment_text": "\n \t\tIt looks like the proposed/attached fix of changing from constant to global was only made in the row_filter - seems like it would have been needed to be made in the col_filter as well. I've run into this thread safety issue in the 2.4.x release as well.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "serycjon", "commentT": "2019-01-25T10:14:54Z", "comment_text": "\n \t\tAfter some examination of the source code I think I figured out the cause of this issue.\n Specifically the code in opencv_contrib/modules/cudafilters/src/cuda/filter2d.cu is the cause.\n In the macro\n #define IMPLEMENT_FILTER2D_TEX_READER(type)\n the variable\n texture< type, cudaTextureType2D, cudaReadModeElementType> tex_filter2D_ ## type \n is declared at global scope.\n Then, in the call function, this same global scope variable is bound to the input matrix\n bindTexture(&tex_filter2D_ ## type, srcWhole);\n In this of course means that if this call() function is simultaneously called from multiple threads, it is possible that the global tex_filter2D_ ## type would be bound to a different area of global memory by two or more threads (and specifically it could be changed partway through a computation), leading to the observed instability.\n Probably the reason it was written this way is that texture can not be easily put as a member of a struct, so a \"quick and dirty\" fix that I wrote simply used the PtrStepSz of srcWhole directly and accessed via operator(y,x), instead of binding to a texture and then using the tex2D function to access things. This probably leads to not-great memory performance, but I don't have the time to figure out how to hack a texture (pointer?) into a struct member right now.\n As for the reason this happened, I guess it is the separation between filter object creation and then calling. In other code in opencv::cuda, where filters/functions are called directly with streams, the code checks whether the passed stream reference is NULL, and if it is, it uses optimized single thread versions e.g. using const memory, otherwise it uses less optimized but thread-safe examples.\n With filter pointers, at creation of the filter there is no way to know whether it is going to be called with non-default stream or not. So there is not really a good way to indicate to the creation function whether it should encapsulate memory or not.\n Anyways, if anyone has input on this please say so.\n I will try to make a pull request but I don't know if it is appropriate since as I said I am not sure what the impact this will have on performance. As it is the current function works fine in single thread, it is only when there are two of the same function called at the same time that it causes corrupted output.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "serycjon", "commentT": "2019-01-25T10:42:41Z", "comment_text": "\n \t\tI've prepared a patch and verified that it works, the corruption issues are gone when linking to the patched code:\n In the file modules/cudafilters/src/cuda/filter2d.cu (in opencv_contrib in >=4.0, and in opencv in <4.0)\n Replace:\n <denchmark-code>    #define IMPLEMENT_FILTER2D_TEX_READER(type) \\\n         texture< type , cudaTextureType2D, cudaReadModeElementType> tex_filter2D_ ## type (0, cudaFilterModePoint, cudaAddressModeClamp); \\\n         struct tex_filter2D_ ## type ## _reader \\\n         { \\\n             typedef type elem_type; \\\n             typedef int index_type; \\\n             const int xoff; \\\n             const int yoff; \\\n             tex_filter2D_ ## type ## _reader (int xoff_, int yoff_) : xoff(xoff_), yoff(yoff_) {} \\\n             __device__ __forceinline__ elem_type operator ()(index_type y, index_type x) const \\\n             { \\\n                 return tex2D(tex_filter2D_ ## type , x + xoff, y + yoff); \\\n             } \\\n         }; \\\n         template <typename D, template <typename> class Brd> struct Filter2DCaller< type , D, Brd> \\\n         { \\\n             static void call(const PtrStepSz< type > srcWhole, int xoff, int yoff, PtrStepSz<D> dst, const float* kernel, \\\n                 int kWidth, int kHeight, int anchorX, int anchorY, const float* borderValue, cudaStream_t stream) \\\n             { \\\n                 typedef typename TypeVec<float, VecTraits< type >::cn>::vec_type work_type; \\\n                 dim3 block(16, 16); \\\n                 dim3 grid(divUp(dst.cols, block.x), divUp(dst.rows, block.y)); \\\n                 bindTexture(&tex_filter2D_ ## type , srcWhole); \\\n                 tex_filter2D_ ## type ##_reader texSrc(xoff, yoff); \\\n                 Brd<work_type> brd(dst.rows, dst.cols, VecTraits<work_type>::make(borderValue)); \\\n                 BorderReader< tex_filter2D_ ## type ##_reader, Brd<work_type> > brdSrc(texSrc, brd); \\\n                 filter2D<<<grid, block, 0, stream>>>(brdSrc, dst, kernel, kWidth, kHeight, anchorX, anchorY); \\\n                 cudaSafeCall( cudaGetLastError() ); \\\n                 if (stream == 0) \\\n                     cudaSafeCall( cudaDeviceSynchronize() ); \\\n             } \\\n         };\n \n </denchmark-code>\n \n with this:\n <denchmark-code>    #define IMPLEMENT_FILTER2D_TEX_READER(type) \\\n         struct tex_filter2D_ ## type ## _reader \\\n         { \\\n \t    PtrStepSz<type> dat; \\\n             typedef type elem_type; \\\n             typedef int index_type; \\\n             const int xoff; \\\n             const int yoff; \\\n             tex_filter2D_ ## type ## _reader (PtrStepSz<type> dat_, int xoff_, int yoff_) : dat(dat_), xoff(xoff_), yoff(yoff_) {} \\\n             __device__ __forceinline__ elem_type operator ()(index_type y, index_type x) const \\\n             { \\\n                 return dat(y + yoff, x + xoff ); \\\n             } \\\n         }; \\\n         template <typename D, template <typename> class Brd> struct Filter2DCaller< type , D, Brd> \\\n         { \\\n             static void call(const PtrStepSz< type > srcWhole, int xoff, int yoff, PtrStepSz<D> dst, const float* kernel, \\\n                 int kWidth, int kHeight, int anchorX, int anchorY, const float* borderValue, cudaStream_t stream) \\\n             { \\\n                 typedef typename TypeVec<float, VecTraits< type >::cn>::vec_type work_type; \\\n                 dim3 block(16, 16); \\\n                 dim3 grid(divUp(dst.cols, block.x), divUp(dst.rows, block.y)); \\\n                 tex_filter2D_ ## type ##_reader texSrc(srcWhole, xoff, yoff); \\\n                 Brd<work_type> brd(dst.rows, dst.cols, VecTraits<work_type>::make(borderValue)); \\\n                 BorderReader< tex_filter2D_ ## type ##_reader, Brd<work_type> > brdSrc(texSrc, brd); \\\n                 filter2D<<<grid, block, 0, stream>>>(brdSrc, dst, kernel, kWidth, kHeight, anchorX, anchorY); \\\n                 cudaSafeCall( cudaGetLastError() ); \\\n                 if (stream == 0) \\\n                     cudaSafeCall( cudaDeviceSynchronize() ); \\\n             } \\\n         };\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "serycjon", "commentT": "2019-01-25T10:47:24Z", "comment_text": "\n \t\tFeel free to prepare a patch into 3.4 branch of \"opencv\" repository.\n We will merge changes into \"opencv_contrib\" during regular merging of 3.4 branch into master (weekly / bi-weekly).\n <denchmark-link:https://github.com/nglee>@nglee</denchmark-link>\n  <denchmark-link:https://github.com/tomoaki0705>@tomoaki0705</denchmark-link>\n  Could you take a look on this fix?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "serycjon", "commentT": "2019-01-25T10:47:40Z", "comment_text": "\n \t\tThis patch should work on 3.4 and 4.0, and in 2.4 the corresponding code is in modules/gpu/src/cuda/imgproc.cu\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "serycjon", "commentT": "2019-01-25T10:48:29Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/alalek>@alalek</denchmark-link>\n , I tested on both 3.4 and 4.0, I don't feel like downloading and compiling 2.4 since I don't use it anymore but the code is identical, it looks like it was just rearranged at some point.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "serycjon", "commentT": "2019-01-25T10:51:50Z", "comment_text": "\n \t\tWe can backport patch onto 2.4 later after validation onto 3.4/master branches.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "serycjon", "commentT": "2019-01-25T10:55:40Z", "comment_text": "\n \t\tI think we should test the performance of the patched version thoroughly before merging it. Since it has been over two years of nobody really caring about this bug, I think that any significant performance drop for single-threaded use would not be worth it.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "serycjon", "commentT": "2019-01-25T12:00:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/serycjon>@serycjon</denchmark-link>\n  Sounds good, do you have performance metrics? In my code, I run linear filters about 1000 times per second, and I don't see any performance drop that is noticable.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "serycjon", "commentT": "2019-01-25T12:02:38Z", "comment_text": "\n \t\tAnd the bug has existed since at least 2013 (when the initial commit happened). Note that in all the rest of the code, performance sacrifices have been made (e.g. changing constant to global memory) in order to make the functions \"thread safe\". In general using secret global variables seems bad practice, but then opencv does all sorts of weird stuff with multithreading so I guess it depends on what you think the main purpose is. It should clearly say that the function is not thread-safe, at any rate.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "serycjon", "commentT": "2019-01-25T12:12:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/flyingfalling>@flyingfalling</denchmark-link>\n  I don't have the metrics and I don't work with the code causing the bug anymore, so I won't be able to help with that now.  Sure, if it seems that it runs ok and if this \"thread safe\" strategy is common in opencv, then I have no objections to this patch.  Thank you for solving this.\n I was just off-put by complete lack of any feedback on the proposed fixes back in 2016, so I wanted to discuss the correctness for multi-threading vs speed for single-threading sacrifices now.\n Agree that the documentation should be improved anyway.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "serycjon", "commentT": "2019-01-25T12:20:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/serycjon>@serycjon</denchmark-link>\n , Thank you for the feedback. Yea, the fixes proposed on 2016 didn't fix this bug (as you can see from the comments above). The fixes were aimed at making the functions thread-safe but they seem to have missed this important part of the code.\n In general, if I am to do a pull request, should I do it off of master, or the 3.4 branch...? Or do one for each? Seems like a lot.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "serycjon", "commentT": "2019-01-25T12:52:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/flyingfalling>@flyingfalling</denchmark-link>\n  Your PR should go to the 3.4 branch. Reading <denchmark-link:https://github.com/opencv/opencv/wiki/Branches>this document</denchmark-link>\n  written by the maintainers may be helpful when deciding which branch a PR should be merged to.\n There was a PR (<denchmark-link:https://github.com/opencv/opencv/pull/11483>#11483</denchmark-link>\n ) that tried to solve similar issue about Canny filter being not thread-safe. It might be helpful to look over it. Beside from fixing the filter code itself, the PR also tried to add some test cases to check for multi thread safety.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "serycjon", "commentT": "2019-01-25T13:21:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/nglee>@nglee</denchmark-link>\n , OK thank you. I don't think I have the time to make test cases right now but perhaps later.\n A note to posterity: the old code used (global) textures which could only be declared at static global scope, which is the reason for the old implementation I assume. After kepler (cuda 5.0), they introduced ability to make \"texture objects\" (<denchmark-link:https://devblogs.nvidia.com/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility/>https://devblogs.nvidia.com/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility/</denchmark-link>\n ) which would enable the use of textures as in the original implementation but with thread safety as well. Since this would require a version check and some code complexities, I just abandoned the texture access and directly access the global device memory holding the input image. This may have some downsides when it comes to 2D locality since it is accessing in 16x16 chunks, but on newer devices this downside will probably be negligible. In the future someone with more interest in optimization may change to using texture objects and compare performance more carefully.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "serycjon", "commentT": "2019-01-25T13:29:34Z", "comment_text": "\n \t\tThere are some performance tests for CUDA filters module. At least they should cover \"single thread\" case (check regressions on CUDA device which you have).\n \t\t"}}}, "commit": {"commit_id": "8158e5b7a0000ff1a6f7d65763d2b1605844ba46", "commit_author": "Richard Veale", "commitT": "2019-03-13 20:53:59+03:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "modules\\cudafilters\\src\\cuda\\filter2d.cu", "file_new_name": "modules\\cudafilters\\src\\cuda\\filter2d.cu", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "82,87,90,101", "deleted_lines": "80,87,90,101,102"}}}}}}