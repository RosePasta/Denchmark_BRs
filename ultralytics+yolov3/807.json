{"BR": {"BR_id": "807", "BR_author": "LinCoce", "BRopenT": "2020-01-26T03:32:12Z", "BRcloseT": "2020-01-31T19:20:43Z", "BR_text": {"BRsummary": "fuse_conv_and_bn", "BRdescription": "\n <denchmark-h:h3>File \"torch_utils.py\", line 44, fuse_conv_and_bn() funtion</denchmark-h>\n \n This function is from <denchmark-link:https://tehnokv.com/posts/fusing-batchnorm-and-conv/>https://tehnokv.com/posts/fusing-batchnorm-and-conv/</denchmark-link>\n , but i think the original implementation is flawed. The fuse formula is\n <denchmark-link:https://user-images.githubusercontent.com/16974482/73130188-86a73600-402e-11ea-93fc-afdf86dc29f4.png></denchmark-link>\n , but the code for fusedconv.bias calculation is: . I think  has been forgot.\n The test result is still right cause when BN is used, the bias in conv is usually canceled. So the inference calculation is not affected. But I think the code is till not rigorous enough.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "LinCoce", "commentT": "2020-01-27T17:59:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LinCoce>@LinCoce</denchmark-link>\n  this is very interesting, I did not realize this. Can you try to implement your proposed fix and compare outputs?\n I believe when I implemented this originally I compared output values from fused and unfused and received nearly identical results, but I could be wrong.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "LinCoce", "commentT": "2020-01-27T18:13:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LinCoce>@LinCoce</denchmark-link>\n  as a quick test if I run detect.py with default settings:\n         # Get detections\n         img = torch.from_numpy(img).to(device)\n         if img.ndimension() == 3:\n             img = img.unsqueeze(0)\n         pred = model(img)[0]\n \n         torch_utils.model_info(model, report='summary')  # 'full' or 'summary'\n         print(pred[0])\n Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients\n \n tensor([[1.39544e+01, 1.49381e+01, 1.57842e+02,  ..., 1.65460e-02, 6.70419e-03, 3.36679e-03],\n         [5.27974e+01, 2.14898e+01, 1.32991e+02,  ..., 1.52713e-02, 6.06968e-03, 3.10304e-03],\n         [8.22322e+01, 1.97613e+01, 1.85406e+02,  ..., 1.17630e-02, 4.69894e-03, 2.74525e-03],\n         ...,\n         [2.99128e+02, 4.13183e+02, 9.45507e+01,  ..., 5.09370e-03, 2.40942e-03, 6.71110e-03],\n         [3.06646e+02, 4.13326e+02, 8.40165e+01,  ..., 5.85396e-03, 2.03835e-03, 4.50750e-03],\n         [3.16871e+02, 4.12817e+02, 8.86255e+01,  ..., 4.85221e-03, 2.79183e-03, 3.80355e-03]])\n And if I use model.fuse():\n Model Summary: 152 layers, 6.29719e+07 parameters, 6.29719e+07 gradients\n \n tensor([[1.39544e+01, 1.49381e+01, 1.57842e+02,  ..., 1.65461e-02, 6.70421e-03, 3.36681e-03],\n         [5.27974e+01, 2.14898e+01, 1.32991e+02,  ..., 1.52714e-02, 6.06968e-03, 3.10305e-03],\n         [8.22322e+01, 1.97613e+01, 1.85405e+02,  ..., 1.17630e-02, 4.69893e-03, 2.74525e-03],\n         ...,\n         [2.99128e+02, 4.13183e+02, 9.45507e+01,  ..., 5.09367e-03, 2.40942e-03, 6.71111e-03],\n         [3.06646e+02, 4.13326e+02, 8.40166e+01,  ..., 5.85390e-03, 2.03834e-03, 4.50747e-03],\n         [3.16871e+02, 4.12817e+02, 8.86256e+01,  ..., 4.85219e-03, 2.79182e-03, 3.80353e-03]])\n The differences all seem to be in the 1E-6 range, which I think it consistent with single precision (FP32) from here for example <denchmark-link:https://en.wikipedia.org/wiki/Machine_epsilon>https://en.wikipedia.org/wiki/Machine_epsilon</denchmark-link>\n \n That's strange that the gradients are appearing turned on though...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "LinCoce", "commentT": "2020-01-28T12:06:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n \n I reproduced your detect.py test, with \n Detect.py was modifed for conv-with-bias checking:\n <denchmark-code>...\n def check_bias_in_conv(self, is_print=True):\n     # check Conv2d + BatchNorm2d layers or fuseConv layers\n     conv_with_bias_before_bn, conv_with_bias_no_bn, conv_no_bias_before_bn, conv_no_bias_no_bn = 0, 0, 0, 0\n     for a in list(self.children())[0]:\n         if isinstance(a, nn.Sequential):\n             for i, b in enumerate(a):\n                 if isinstance(b, nn.modules.conv.Conv2d):\n                     try:\n                         bn = a[i+1]\n                         if isinstance(bn, nn.modules.batchnorm.BatchNorm2d):\n                             if b.bias is not None: conv_with_bias_before_bn += 1\n                             else: conv_no_bias_before_bn += 1\n                         else:\n                             if b.bias is not None: conv_with_bias_no_bn += 1\n                             else: conv_no_bias_no_bn += 1\n                     except IndexError:\n                         if b.bias is not None: conv_with_bias_no_bn += 1\n                         else: conv_no_bias_no_bn += 1\n                         continue\n     print('If followed by bn, there are %d conv has bias and %d NOT' % (conv_with_bias_before_bn, conv_no_bias_before_bn))\n     print('If NOT followed by bn, there are %d conv has bias and %d NOT' % (conv_with_bias_no_bn, conv_no_bias_no_bn))\n ...\n def detect(save_txt=False, save_img=False):\n ...\n         # Get detections\n         img = torch.from_numpy(img).to(device)\n         if img.ndimension() == 3:\n             img = img.unsqueeze(0)\n         pred = model(img)[0]\n \n         # ########## fuse conv and bn test ##########\n         torch_utils.model_info(model, report='summary')  # 'full' or 'summary'\n         print(pred[0])\n         check_bias_in_conv(model, is_print=False)\n \n         model.fuse()\n         pred_fuse = model(img.cpu())[0]\n         torch_utils.model_info(model, report='summary')  # 'full' or 'summary'\n         print(pred_fuse[0])\n         check_bias_in_conv(model, is_print=False)\n \n         d = (pred[0] - pred_fuse[0]).norm().div(pred[0].norm()).item()\n         print(\"Error: %.8f\" % d)\n \n         # ########## End ##########\n ...\n </denchmark-code>\n \n And the results are:\n <denchmark-code>Model Summary: 37 layers, 8.85237e+06 parameters, 8.85237e+06 gradients\n tensor([[2.40001e+01, 2.08713e+01, 8.70725e+01,  ..., 1.83841e-02, 4.68162e-04, 1.49674e-04],\n         [4.90474e+01, 2.08060e+01, 1.20938e+02,  ..., 2.79196e-02, 9.30916e-04, 1.17514e-04],\n         [8.19246e+01, 1.86440e+01, 1.62108e+02,  ..., 3.81493e-02, 1.59860e-03, 4.53681e-04],\n         ...,\n         [2.79226e+02, 4.06461e+02, 4.92743e+01,  ..., 1.04082e-03, 3.57477e-04, 4.14086e-04],\n         [2.93216e+02, 4.07816e+02, 4.28459e+01,  ..., 8.41400e-04, 4.19613e-04, 4.04124e-04],\n         [3.11913e+02, 4.09623e+02, 4.62420e+01,  ..., 1.51729e-03, 6.55530e-04, 6.08192e-04]])\n If followed by bn, there are 0 conv has bias and 11 NOT\n If NOT followed by bn, there are 2 conv has bias and 0 NOT\n Model Summary: 26 layers, 8.84918e+06 parameters, 8.84918e+06 gradients\n tensor([[2.40001e+01, 2.08713e+01, 8.70725e+01,  ..., 1.83841e-02, 4.68161e-04, 1.49674e-04],\n         [4.90474e+01, 2.08060e+01, 1.20938e+02,  ..., 2.79196e-02, 9.30916e-04, 1.17514e-04],\n         [8.19246e+01, 1.86440e+01, 1.62108e+02,  ..., 3.81494e-02, 1.59860e-03, 4.53681e-04],\n         ...,\n         [2.79226e+02, 4.06461e+02, 4.92743e+01,  ..., 1.04082e-03, 3.57477e-04, 4.14087e-04],\n         [2.93216e+02, 4.07816e+02, 4.28459e+01,  ..., 8.41400e-04, 4.19613e-04, 4.04126e-04],\n         [3.11913e+02, 4.09623e+02, 4.62420e+01,  ..., 1.51729e-03, 6.55529e-04, 6.08189e-04]])\n If followed by bn, there are 0 conv has bias and 0 NOT\n If NOT followed by bn, there are 13 conv has bias and 0 NOT\n Error: 0.00000008\n </denchmark-code>\n \n I got the same results. Fused and unfused convs receive nearly identical results.\n But this is because when BN is used, the bias in conv is usually canceled, like in models.py:\n <denchmark-code>            modules.add_module('Conv2d', nn.Conv2d(in_channels=output_filters[-1],\n                                                    out_channels=filters,\n                                                    kernel_size=size,\n                                                    stride=stride,\n                                                    padding=pad,\n                                                    groups=int(mdef['groups']) if 'groups' in mdef else 1,\n                                                    bias=not bn))\n             if bn:\n                 modules.add_module('BatchNorm2d', nn.BatchNorm2d(filters, momentum=0.1))\n </denchmark-code>\n \n The fuse code only fail when conv_with_bias_before_bn, which normally can be avoided. So I just think the code is not rigorous enough.\n There are the errors in purpose. Insert random bias in model's first conv:\n <denchmark-code>...\n         # Get detections\n         img = torch.from_numpy(img).to(device)\n         if img.ndimension() == 3:\n             img = img.unsqueeze(0)\n \n         # ########## insert conv_with_bias_before_bn ##########\n         target = list(model.children())[0][0][0]\n         modified = torch.nn.Conv2d(target.in_channels,\n                                    target.out_channels,\n                                    kernel_size=target.kernel_size,\n                                    stride=target.stride,\n                                    padding=target.padding,\n                                    bias=True)\n         modified.weight.copy_(target.weight)\n         list(model.children())[0][0][0] = modified\n         # ########## End ##########\n         pred = model(img)[0]\n ...\n </denchmark-code>\n \n and the results:\n <denchmark-code>Model Summary: 38 layers, 8.85238e+06 parameters, 8.85238e+06 gradients\n tensor([[2.40035e+01, 2.01134e+01, 8.97949e+01,  ..., 5.17192e-02, 4.99909e-04, 3.00301e-04],\n         [5.01554e+01, 2.19869e+01, 1.14426e+02,  ..., 8.63517e-02, 1.17998e-03, 2.65839e-04],\n         [8.16482e+01, 2.06688e+01, 1.53743e+02,  ..., 6.54906e-02, 1.63738e-03, 4.91107e-04],\n         ...,\n         [2.79235e+02, 4.06116e+02, 5.00214e+01,  ..., 1.12911e-03, 2.80363e-04, 2.76489e-04],\n         [2.93324e+02, 4.07370e+02, 4.42561e+01,  ..., 5.48370e-04, 2.99978e-04, 3.80757e-04],\n         [3.11824e+02, 4.09439e+02, 4.67531e+01,  ..., 1.55280e-03, 5.35421e-04, 6.56669e-04]])\n If followed by bn, there are 1 conv has bias and 10 NOT\n If NOT followed by bn, there are 2 conv has bias and 0 NOT\n Model Summary: 26 layers, 8.84918e+06 parameters, 8.84918e+06 gradients\n tensor([[2.40331e+01, 2.07188e+01, 8.80087e+01,  ..., 2.36877e-02, 4.53512e-04, 1.77438e-04],\n         [4.93723e+01, 2.12234e+01, 1.19377e+02,  ..., 3.68400e-02, 9.47023e-04, 1.40918e-04],\n         [8.16568e+01, 1.92198e+01, 1.59739e+02,  ..., 4.37197e-02, 1.53967e-03, 4.77384e-04],\n         ...,\n         [2.79245e+02, 4.06299e+02, 4.89226e+01,  ..., 1.02152e-03, 3.31561e-04, 3.56736e-04],\n         [2.93202e+02, 4.07689e+02, 4.28357e+01,  ..., 7.43237e-04, 3.77242e-04, 3.69071e-04],\n         [3.11930e+02, 4.09628e+02, 4.64648e+01,  ..., 1.47649e-03, 6.05659e-04, 5.74691e-04]])\n If followed by bn, there are 0 conv has bias and 0 NOT\n If NOT followed by bn, there are 13 conv has bias and 0 NOT\n Error: 0.01386723\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "LinCoce", "commentT": "2020-01-29T19:05:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LinCoce>@LinCoce</denchmark-link>\n  ah yes I understand now. The Conv2d() layers always have  since they are all followed by batchnorm layers. So we want to change this line\n \n \n to this?\n fusedconv.bias.copy_(torch.mm(w_bn, b_conv) + b_bn)\n Does this change pass your test code?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "LinCoce", "commentT": "2020-01-30T07:46:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n \n Yeah, in my test,  behave correctly in any conv-bias-bn situation.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "LinCoce", "commentT": "2020-01-30T18:02:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LinCoce>@LinCoce</denchmark-link>\n  perfect. Do you want to submit a PR so you can be added to the repo authorlist or should I just make the change myself?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "LinCoce", "commentT": "2020-01-31T04:07:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n \n Thanks. I have submitted a PR for you.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "LinCoce", "commentT": "2020-01-31T19:20:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LinCoce>@LinCoce</denchmark-link>\n  great, it's all done! Thanks for spotting the bug. I'll close the issue now.\n \t\t"}}}, "commit": {"commit_id": "0c7af1a4d293b40de08613468369e2f5d9b143fa", "commit_author": "LinCoce", "commitT": "2020-01-30 21:58:26-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "utils\\torch_utils.py", "file_new_name": "utils\\torch_utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "64", "deleted_lines": "64", "method_info": {"method_name": "fuse_conv_and_bn", "method_params": "conv,bn", "method_startline": "42", "method_endline": "66"}}}}}}}