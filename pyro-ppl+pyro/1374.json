{"BR": {"BR_id": "1374", "BR_author": "thematrixduo", "BRopenT": "2018-09-10T13:17:49Z", "BRcloseT": "2018-09-14T18:02:14Z", "BR_text": {"BRsummary": "Low count accuracy of AIR model", "BRdescription": "\n Hi,\n I have been experimenting with the Attend Infer Repeat model. I downloaded the code from examples and run the code with the args provided in the tutorial (<denchmark-link:http://pyro.ai/examples/air.html>http://pyro.ai/examples/air.html</denchmark-link>\n ), which is:\n python main.py -n 200000 -blr 0.1 --z-pres-prior 0.01 --scale-prior-sd 0.2 --predict-net 200 --bl-predict-net 200 --decoder-output-use-sigmoid --decoder-output-bias -2 --seed 287710\n But the highest count accuracy I get is around 76%. Could someone let me know if the args are not correct? I did not change any part of the code.\n Also I am using pyro version 0.2.1 and pytorch version 0.4.0\n Many thanks\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "thematrixduo", "commentT": "2018-09-10T18:21:34Z", "comment_text": "\n \t\tHi, did you let inference run to completion or are you stopping it after a few epochs?  Are you talking about count accuracy on the training images (as in the tutorial) or some unseen test images?\n cc <denchmark-link:https://github.com/null-a>@null-a</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "thematrixduo", "commentT": "2018-09-10T18:37:40Z", "comment_text": "\n \t\t\n Could someone let me know if the args are not correct?\n \n The command line args in the tutorial are the ones I used, and it looks like you're using the same.\n As well as answers to eb8680's questions, I'd be interested in hearing what other values for count accuracy you obtained, if that's possible. Thanks.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "thematrixduo", "commentT": "2018-09-11T11:03:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/eb8680>@eb8680</denchmark-link>\n  <denchmark-link:https://github.com/null-a>@null-a</denchmark-link>\n \n I let the inference run till the end (I believe it is specified by -n 200000). I am using the count accuracy function provided in 'main.py'. I just specified the args '--eval-every' to 1000. I have attached a plot of the count accuracy obtained. This plot looks very different from the plot given in the tutorial.\n <denchmark-link:https://user-images.githubusercontent.com/8089252/45356239-a5806600-b5ba-11e8-9bb2-e2b819eaf611.jpg></denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "thematrixduo", "commentT": "2018-09-11T12:44:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/thematrixduo>@thematrixduo</denchmark-link>\n  How many times have you run inference? What other values for final count accuracy have you obtained? Setting the random seed doesn't make this deterministic, so some variance is expected. If you're seeing consistently poor results, I'll try running it myself and see if I can spot anything odd going on. Thanks.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "thematrixduo", "commentT": "2018-09-11T16:09:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/null-a>@null-a</denchmark-link>\n  I have tried at least 5 times but for none of them did the count accuracy go above 80%. I have attached another plot.\n <denchmark-link:https://user-images.githubusercontent.com/8089252/45372635-6a475c80-b5e5-11e8-8d74-7c4a8d3b9cf6.jpg></denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "thematrixduo", "commentT": "2018-09-12T09:24:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/thematrixduo>@thematrixduo</denchmark-link>\n  Great, thanks for the info. That sounds worse than I would expect, so I'll take a look. To begin, I guess I'll confirm I'm getting similar results to you, and then I'll go back and try running against the Pyro commit recorded in the tutorial.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "thematrixduo", "commentT": "2018-09-12T15:10:07Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/thematrixduo>@thematrixduo</denchmark-link>\n  It looks like performance may have degraded after <denchmark-link:https://github.com/pyro-ppl/pyro/commit/c99ea6732b342ef9a52430c8a2567b0387912236>c99ea67</denchmark-link>\n . If you apply the following patch (to v0.2.1 say) and then re-try I think you will see performance similar to that reported in the tutorial.\n diff --git a/examples/air/main.py b/examples/air/main.py\n index df89df2..09bdf25 100644\n --- a/examples/air/main.py\n +++ b/examples/air/main.py\n @@ -195,7 +195,7 @@ def main(**kwargs):\n          vis.images(draw_many(x, tensor_to_objs(latents_to_tensor(z))))\n  \n      def per_param_optim_args(module_name, param_name):\n -        lr = args.baseline_learning_rate if 'bl_' in param_name else args.learning_rate\n +        lr = args.baseline_learning_rate if 'bl_' in param_name or 'bl_' in module_name else args.learning_rate\n          return {'lr': lr}\n  \n      svi = SVI(air.model, air.guide,\n If we confirm this fixes the problem, then I'll open a PR to apply the patch to dev. (BTW, I've not been using a fixed random seed when I run this locally, and I seem to achieve OK results consistently.)\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "thematrixduo", "commentT": "2018-09-12T17:47:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/null-a>@null-a</denchmark-link>\n  good catch!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "thematrixduo", "commentT": "2018-09-13T14:05:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/null-a>@null-a</denchmark-link>\n  Thanks for the help!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "thematrixduo", "commentT": "2018-09-14T19:18:11Z", "comment_text": "\n \t\t\n @null-a Thanks for the help!\n \n <denchmark-link:https://github.com/thematrixduo>@thematrixduo</denchmark-link>\n  No problem, thanks for opening the issue.\n \t\t"}}}, "commit": {"commit_id": "b03be07f31aeaf11f3526453874b396b69c7e854", "commit_author": "null-a", "commitT": "2018-09-14 11:02:09-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\air\\air.py", "file_new_name": "examples\\air\\air.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "285", "deleted_lines": "285", "method_info": {"method_name": "baseline_step", "method_params": "self,prev,inputs", "method_startline": "260", "method_endline": "286"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "examples\\air\\main.py", "file_new_name": "examples\\air\\main.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "197,198,199,201", "deleted_lines": "198", "method_info": {"method_name": "main", "method_params": "kwargs", "method_startline": "124", "method_endline": "243"}}, "hunk_1": {"Ismethod": 1, "added_lines": "197,198", "deleted_lines": "198", "method_info": {"method_name": "main.isBaselineParam", "method_params": "module_name,param_name", "method_startline": "197", "method_endline": "198"}}, "hunk_2": {"Ismethod": 1, "added_lines": "201", "deleted_lines": null, "method_info": {"method_name": "main.per_param_optim_args", "method_params": "module_name,param_name", "method_startline": "200", "method_endline": "202"}}}}}}}