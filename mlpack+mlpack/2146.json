{"BR": {"BR_id": "2146", "BR_author": "adithya-tp", "BRopenT": "2020-01-09T12:11:20Z", "BRcloseT": "2020-03-23T17:29:13Z", "BR_text": {"BRsummary": "Can't Train a Normal CNN.", "BRdescription": "\n <denchmark-h:h4>Issue description</denchmark-h>\n \n When I try to train the following cnn, it only runs when I include the line\n arma::conv_to<arma::mat>::from(matrix).print(\"Print Image: \\n\");\n However, if I comment out this line, the code no longer runs (both cases compile. but in this second case doesn't run - \"Segmentation Fault\"). It doesn't make sense to me why this happens.\n <denchmark-h:h4>Your environment</denchmark-h>\n \n \n version of mlpack: 3.2.2\n operating system: ubuntu 18.04\n compiler: g++ 7.4.0\n version of dependencies (Boost/Armadillo): libboost-1.65.1, armadillo-9.800.3\n \n <denchmark-h:h4>Steps to reproduce</denchmark-h>\n \n Here's the code to reproduce the issue:\n <denchmark-code>#include <mlpack/core.hpp>\n #include <mlpack/core/data/split_data.hpp>\n #include <mlpack/methods/ann/layer/layer.hpp>\n #include <mlpack/methods/ann/ffn.hpp>\n #include <ensmallen.hpp>\n \n using namespace mlpack;\n using namespace mlpack::ann;\n using namespace arma;\n using namespace std;\n using namespace ens;\n \n int main()\n {\n     double RATIO = 0.1;\n     int CYCLES = 10;\n     const int ITERATIONS_PER_CYCLE = 1;\n     double STEP_SIZE = 10e-3;\n     int BATCH_SIZE = 1;\n \n     // Reading in image.\n     cout << \"Reading data ...\" << endl;\n     arma::Mat<unsigned char> matrix;\n     data::ImageInfo info;\n     data::Load(\"saitama.png\", matrix, info, false, true);\n \n     FFN<NegativeLogLikelihood<>, RandomInitialization> model;\n \n     model.Add<Convolution<> >(info.Channels(), 6, 5, 5, 1, 1, 0, 0, info.Width(), info.Height());\n     model.Add<ReLULayer<> >();\n     model.Add<MaxPooling<> >(2, 2, 2, 2, true);\n     model.Add<Linear<> >(62*62*6, 2);\n     model.Add<LogSoftMax<> >();\n     cout << \"Training ...\" << endl;\n \n     // Setting parameters Stochastic Gradient Descent (SGD) optimizer.\n     SGD<AdamUpdate> optimizer(\n       STEP_SIZE,\n       BATCH_SIZE,\n       ITERATIONS_PER_CYCLE,\n       1e-8,\n       true,\n       AdamUpdate(1e-8, 0.9, 0.999));\n \n     arma::mat trainY;\n     trainY << 1;// << 0;\n     arma::conv_to<arma::mat>::from(matrix).print(\"Print Image: \\n\");\n     trainY.print(\"Train label: \\n\");\n     cout << trainY.n_rows << \" \" << trainY.n_cols;\n \n     for (int i = 1; i <= CYCLES; i++)\n     {\n       model.Train(arma::conv_to<arma::mat>::from(matrix), trainY, optimizer, PrintLoss(), ProgressBar());\n     }\n     return 0;\n }\n </denchmark-code>\n \n (The image size I used is 128 x 128).\n compiled using:\n g++ cnn.cpp -o cnn `pkg-config --cflags --libs mlpack armadillo` -fopenmp -DHAS_STB\n <denchmark-h:h4>Expected behavior</denchmark-h>\n \n When the line is commented out, the network should still train. The code should run irrespective of whether the line in question is commented out or not.\n <denchmark-h:h4>Actual behavior</denchmark-h>\n \n The code runs only when the line is NOT commented out.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "adithya-tp", "commentT": "2020-01-09T16:49:05Z", "comment_text": "\n \t\tSame behaviour if you convert the matrix before you pass it?\n arma::mat input = arma::conv_to<arma::mat>::from(matrix);\n model.Train(input, trainY, optimizer, PrintLoss(), ProgressBar());\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "adithya-tp", "commentT": "2020-01-09T17:02:01Z", "comment_text": "\n \t\tYes. I just tried it out with this change. Same behaviour.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "adithya-tp", "commentT": "2020-01-09T18:19:53Z", "comment_text": "\n \t\tOkay, thanks for the quick test, I thought maybe the matrix is going to be deconstructed before we call Train().\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "adithya-tp", "commentT": "2020-01-09T19:04:18Z", "comment_text": "\n \t\tvalgrind seems to indicate that this is actually coming from the STB loading code:\n <denchmark-code>==535800== Invalid read of size 8\n ==535800==    at 0x483A1BC: memcpy@GLIBC_2.2.5 (vg_replace_strmem.c:1034)\n ==535800==    by 0x13824B: copy<unsigned char> (arrayops_meat.hpp:36)\n ==535800==    by 0x13824B: arma::Mat<unsigned char>::Mat(unsigned char*, unsigned long long, unsigned long long, bool, bool) (Mat_meat.hpp:1231)\n ==535800==    by 0x13289E: bool mlpack::data::Load<unsigned char>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, arma::Mat<unsigned char>&, mlpack::data::ImageInfo&, bool, bool) (load_image_impl.hpp:79)\n ==535800==    by 0x12C04C: main (cnn_test.cpp:27)\n ==535800==  Address 0x8f47538 is 8 bytes before a block of size 65,536 alloc'd\n ==535800==    at 0x4837EC3: memalign (vg_replace_malloc.c:908)\n ==535800==    by 0x4837FF0: posix_memalign (vg_replace_malloc.c:1072)\n ==535800==    by 0x13A2BA: unsigned char* arma::memory::acquire<unsigned char>(unsigned long long) (memory.hpp:82)\n ==535800==    by 0x13D6AD: arma::Mat<unsigned char>::init_cold() (Mat_meat.hpp:220)\n ==535800==    by 0x1381EC: arma::Mat<unsigned char>::Mat(unsigned char*, unsigned long long, unsigned long long, bool, bool) (Mat_meat.hpp:1229)\n ==535800==    by 0x13289E: bool mlpack::data::Load<unsigned char>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, arma::Mat<unsigned char>&, mlpack::data::ImageInfo&, bool, bool) (load_image_impl.hpp:79)\n ==535800==    by 0x12C04C: main (cnn_test.cpp:27)\n </denchmark-code>\n \n I'm not sure why that is... the code looks correct to me!  I am still digging and trying to figure out what is going on here though.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "adithya-tp", "commentT": "2020-01-09T21:46:24Z", "comment_text": "\n \t\tI think I know what happens here, we are loading an image with an alpha channel, but we tell stb to only load 3 channels (r,g,b):\n \n \n \n mlpack/src/mlpack/core/data/load_image_impl.hpp\n \n \n         Lines 60 to 61\n       in\n       7f520f0\n \n \n \n \n \n \n  image = stbi_load(filename.c_str(), &tempWidth, &tempHeight, &tempChannels, \n \n \n \n      STBI_rgb); \n \n \n \n \n \n Here STBI_rgb is the number of desired channels to load, which is defined as 3 not 4, looks like there is an auto which loads all channels, if we set it to 0.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "adithya-tp", "commentT": "2020-01-09T21:49:25Z", "comment_text": "\n \t\tAgreed, I think we need to revise the handling of the number of channels to load---maybe if info.Channels() is 0, we auto-detect based on what STB finds, and otherwise we use the given number of channels.\n I made a change to load the correct number of channels for the test image I had, but then there was a later segfault in the CNN code.  I'm not sure yet if this is because of the parameters of the CNN or some other issue.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "adithya-tp", "commentT": "2020-01-09T21:57:12Z", "comment_text": "\n \t\tThe only CNN parameter change that I could suggest, off the top of my head, is changing the output number from 2 to 1 in the linear layer. Maybe that helps.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "adithya-tp", "commentT": "2020-01-15T03:03:49Z", "comment_text": "\n \t\tI believe the number of input maps needs to be 1, not info.Channels().\n After making that change, I was still encountering memory issues, so I dug deeper and the issue that I am finding is that memory is inadvertently freed, apparently by the Convolution layer, during the call to Forward():\n <denchmark-code>$ valgrind --leak-check=full --track-origins=yes ./cnn_test 2>&1\n ...\n ==792463== Invalid read of size 8\n ==792463==    at 0x483A024: memcpy@GLIBC_2.2.5 (vg_replace_strmem.c:1034)\n ==792463==    by 0x131944: copy<double> (arrayops_meat.hpp:36)\n ==792463==    by 0x131944: arma::Mat<double>::operator=(arma::Mat<double> const&) (Mat_meat.hpp:813)\n ==792463==    by 0x231EEA: void mlpack::ann::Convolution<mlpack::ann::NaiveConvolution<mlpack::ann::ValidConvolution>, mlpack::ann::NaiveConvolution<mlpack::ann::FullConvolution>, mlpack::ann::NaiveConvolution<mlpack::ann::ValidConvolution>, arma::Mat<double>, arma::Mat<double> >::Gradient<double>(arma::Mat<double> const&&, arma::Mat<double>&&, arma::Mat<double>&&) (convolution_impl.hpp:276)\n ...\n ==792463==  Address 0x9161300 is 0 bytes inside a block of size 524,288 free'd\n ==792463==    at 0x48369AB: free (vg_replace_malloc.c:540)\n ==792463==    by 0x1300BC: release<double const> (memory.hpp:141)\n ==792463==    by 0x1300BC: arma::Mat<double>::~Mat() (Mat_meat.hpp:29)\n ==792463==    by 0x154BF0: double mlpack::ann::FFN<mlpack::ann::NegativeLogLikelihood<arma::Mat<double>, arma::Mat<double> >, mlpack::ann::RandomInitialization>::Eval\n uateWithGradient<arma::Mat<double> >(arma::Mat<double> const&, unsigned long, arma::Mat<double>&, unsigned long) (ffn_impl.hpp:325)\n ...\n </denchmark-code>\n \n If I remove the  layer and replace it with a  layer (of the correct size), there's no memory leak.  I'm not particularly confident with this part of the code--- <denchmark-link:https://github.com/zoq>@zoq</denchmark-link>\n , any ideas?  I can dig deeper, but I think it might take a while before I figured out what was going on.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "adithya-tp", "commentT": "2020-01-15T05:26:14Z", "comment_text": "\n \t\t\n I believe the number of input maps needs to be 1, not info.Channels().\n \n Could you explain the rationale behind this?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "adithya-tp", "commentT": "2020-01-20T04:49:16Z", "comment_text": "\n \t\t\n \n I believe the number of input maps needs to be 1, not info.Channels().\n \n Could you explain the rationale behind this?\n \n The rationale was that I misunderstood the code. :)  Anyway, you're right, it should be info.Channels().\n I found the real issue here.  Convolution creates an alias to its input during the forward pass, on line 119 of convolution_impl.hpp:\n <denchmark-code>  inputTemp = arma::cube(const_cast<arma::Mat<eT>&&>(input).memptr(),\n       inputWidth, inputHeight, inSize * batchSize, false, false);\n </denchmark-code>\n \n But FFN::Forward() is called using std::move(), which means that memory will be freed after the forward pass.  See ffn_impl.hpp, line 325:\n <denchmark-code>  Forward(std::move(predictors.cols(begin, begin + batchSize - 1)));\n </denchmark-code>\n \n As a result, when inputTemp is used later in Convolution::Gradient(), for instance, on line 276, the access is invalid.  The bug can be worked around by changing the creation of inputTemp to force a copy (note that this causes slowdown):\n <denchmark-code>  inputTemp = arma::cube(const_cast<arma::Mat<eT>&&>(input).memptr(),\n       inputWidth, inputHeight, inSize * batchSize, true, false);\n </denchmark-code>\n \n I think that Convolution is not the only layer that does this---it looks like MaxPooling and MeanPooling might also.\n <denchmark-link:https://github.com/zoq>@zoq</denchmark-link>\n , <denchmark-link:https://github.com/ShikharJ>@ShikharJ</denchmark-link>\n , anyone else--- any thoughts on this?  I didn't write this code so I'm not sure exactly what the best solution is.  Since  simply calls the  with each previous layer's output, it seems like  (and other layers) could just take ownership of the input, instead of creating an alias that later becomes unsafe.  Let me know what you think. \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "adithya-tp", "commentT": "2020-01-20T05:10:46Z", "comment_text": "\n \t\tmaybe the error in my   example(<denchmark-link:https://github.com/mlpack/models/pull/35>mlpack/models#35</denchmark-link>\n ) was caused by the same memory error, because the error pointed to the system file (I can be wrong)\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "adithya-tp", "commentT": "2020-01-20T05:37:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/unnamed082>@unnamed082</denchmark-link>\n  yes, I think this is correct.  A workaround similar to the one I posted for  for the other layers in your network could help, I think.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "adithya-tp", "commentT": "2020-01-20T07:02:59Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/rcurtin>@rcurtin</denchmark-link>\n ,\n I am trying to figure out the error, I have a doubt, which version are you using for generating the  result. Because currently in 3.2.2 line 325 in  is slightly different. Also line 276 in  is slightly looking different.\n Also I saw 3.1.x branch code it is also looks slightly different.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "adithya-tp", "commentT": "2020-01-20T11:12:31Z", "comment_text": "\n \t\tHi,\n I implemented similar thing what is done convolution.hpp. Below is the sample code.\n <denchmark-code># include <bits/stdc++.h>\n # include <mlpack/core.hpp>\n using namespace std;\n # define ll long long int\n void make(arma::mat num)\n {\n \tfor(auto i = 0 ;i < num.n_cols; i++)\n \t{\n \t\tcout << num(0, i) << \" \";\n \t}\n \tcout << endl;\n }\n void call(const arma::mat&& arr)\n {\n \tarma::cube x;\n \tx = arma::cube(const_cast<arma::mat&&>(arr).memptr(),\n                1, 5, 5, false, false);\t\n \n \n \tfor(auto u : x)\n \t{\n \t\tcout << u << \" \";\n \t}\n \tcout << endl;\n \t//make(arr);\n }\n int main()\n {\n \tint var = 3;\n \tarma::mat pred = arma::ones(1, 25);\n \t\t\n \n \t/*vector <ll> arr;\n \tarr.push_back(8);\n \tarr.push_back(7);\n \tarr.push_back(5);\n \tarr.push_back(3);*/\n \tcall(move(pred));\n \tfor(auto i = 0; i < pred.n_cols; i++)\n \t{\n \t\tcout << pred(0, i) << \" \";\n \t}\n \tcout << endl;\n }\n </denchmark-code>\n \n You can compile it using g++ file.cpp -fopenmp provided you have installed mlpack after building. Looks like the memory doesn't get erased because of an alias. There might be another reason for it.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "adithya-tp", "commentT": "2020-01-20T14:47:46Z", "comment_text": "\n \t\tOh, sorry, I thought I was looking on the master branch but I was actually on another checked out branch.  The line in ffn_impl.hpp is line 328, and in convolution_impl.hpp is line 203.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "adithya-tp", "commentT": "2020-01-21T20:02:41Z", "comment_text": "\n \t\t\n @zoq, @ShikharJ, anyone else--- any thoughts on this? I didn't write this code so I'm not sure exactly what the best solution is. Since FFN::Forward() simply calls the ForwardVisitor with each previous layer's output, it seems like Convolution (and other layers) could just take ownership of the input, instead of creating an alias that later becomes unsafe. Let me know what you think. \ud83d\udc4d\n \n Nice catch, taking ownership of the input is probably the best option to solve the issue, no need to create an extra copy that slows everything down. So I guess we could either open an issue, or I could write a patch for the issue, what do you think?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "adithya-tp", "commentT": "2020-02-17T23:36:55Z", "comment_text": "\n \t\tWhile working through the patch this turns out to be more complex than I thought: the FFN class will set outputParameter to be equal to the previous layer's output, in Forward():\n <denchmark-code>  for (size_t i = 1; i < end - begin + 1; ++i)\n   {\n     boost::apply_visitor(ForwardVisitor(std::move(boost::apply_visitor(\n         outputParameterVisitor, network[begin + i - 1])), std::move(\n         boost::apply_visitor(outputParameterVisitor, network[begin + i]))),\n         network[begin + i]);\n   }\n </denchmark-code>\n \n But then the output is used again in Backward():\n <denchmark-code>  for (size_t i = 2; i < network.size(); ++i)\n   {\n     boost::apply_visitor(BackwardVisitor(std::move(boost::apply_visitor(\n         outputParameterVisitor, network[network.size() - i])), std::move(\n         boost::apply_visitor(deltaVisitor, network[network.size() - i + 1])),\n         std::move(boost::apply_visitor(deltaVisitor,\n         network[network.size() - i]))), network[network.size() - i]);\n   }\n </denchmark-code>\n \n So if a layer that takes ownership of the input is used as a layer that's not the first layer, this will cause problems.\n It seems like the only way around for now would be to make a copy, and in the longer-term a more significant redesign might be needed; I'm not sure.  Let me know what you think the best way forward is...\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "adithya-tp", "commentT": "2020-02-24T00:07:32Z", "comment_text": "\n \t\tI'll take a closer look into the issue tomorrow, but making a copy is a solution at least for now.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "adithya-tp", "commentT": "2020-02-24T01:26:04Z", "comment_text": "\n \t\tI've already got most of a PR (it's quite simple); I'll try and open it tonight.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "adithya-tp", "commentT": "2020-02-24T15:56:40Z", "comment_text": "\n \t\tPR <denchmark-link:https://github.com/mlpack/mlpack/pull/2234>#2234</denchmark-link>\n  should fix this, although perhaps it is not the best possible solution. \n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "adithya-tp", "commentT": "2020-03-15T15:47:15Z", "comment_text": "\n \t\tSince <denchmark-link:https://github.com/rcurtin>@rcurtin</denchmark-link>\n  has resolved the issue, shall I close this?\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "adithya-tp", "commentT": "2020-03-17T00:13:39Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mlpack/mlpack/pull/2234>#2234</denchmark-link>\n  isn't merged yet, so let's wait on that first. \n \t\t"}}}, "commit": {"commit_id": "29422c6066d81e3200bbf1961124183340e6f77e", "commit_author": "Ryan Curtin", "commitT": "2020-03-22 16:40:02-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\mlpack\\methods\\ann\\layer\\atrous_convolution.hpp", "file_new_name": "src\\mlpack\\methods\\ann\\layer\\atrous_convolution.hpp", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "368,369,370"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "src\\mlpack\\methods\\ann\\layer\\atrous_convolution_impl.hpp", "file_new_name": "src\\mlpack\\methods\\ann\\layer\\atrous_convolution_impl.hpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "328,334,335", "deleted_lines": "328", "method_info": {"method_name": "mlpack::ann::AtrousConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Gradient", "method_params": "input,error,gradient", "method_startline": "321", "method_endline": "404"}}, "hunk_1": {"Ismethod": 1, "added_lines": "190", "deleted_lines": "190", "method_info": {"method_name": "mlpack::ann::AtrousConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Forward", "method_params": "input,output", "method_startline": "181", "method_endline": "252"}}, "hunk_2": {"Ismethod": 1, "added_lines": "274,275,276", "deleted_lines": "274,275,276", "method_info": {"method_name": "mlpack::ann::AtrousConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Backward", "method_params": "gy,g", "method_startline": "262", "method_endline": "311"}}, "hunk_3": {"Ismethod": 1, "added_lines": "328,334,335", "deleted_lines": "328", "method_info": {"method_name": "mlpack::ann::AtrousConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Gradient", "method_params": "error,gradient", "method_startline": "321", "method_endline": "402"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\mlpack\\methods\\ann\\layer\\convolution.hpp", "file_new_name": "src\\mlpack\\methods\\ann\\layer\\convolution.hpp", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "365,366,367"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "src\\mlpack\\methods\\ann\\layer\\convolution_impl.hpp", "file_new_name": "src\\mlpack\\methods\\ann\\layer\\convolution_impl.hpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "311,317,318", "deleted_lines": "311", "method_info": {"method_name": "mlpack::ann::Convolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Gradient", "method_params": "input,error,gradient", "method_startline": "304", "method_endline": "373"}}, "hunk_1": {"Ismethod": 1, "added_lines": "181", "deleted_lines": "181", "method_info": {"method_name": "mlpack::ann::Convolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Forward", "method_params": "input,output", "method_startline": "172", "method_endline": "239"}}, "hunk_2": {"Ismethod": 1, "added_lines": "311,317,318", "deleted_lines": "311", "method_info": {"method_name": "mlpack::ann::Convolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Gradient", "method_params": "error,gradient", "method_startline": "304", "method_endline": "371"}}, "hunk_3": {"Ismethod": 1, "added_lines": "261,262,263", "deleted_lines": "261,262,263", "method_info": {"method_name": "mlpack::ann::Convolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Backward", "method_params": "gy,g", "method_startline": "249", "method_endline": "294"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\mlpack\\methods\\ann\\layer\\transposed_convolution.hpp", "file_new_name": "src\\mlpack\\methods\\ann\\layer\\transposed_convolution.hpp", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "432,433,434"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "src\\mlpack\\methods\\ann\\layer\\transposed_convolution_impl.hpp", "file_new_name": "src\\mlpack\\methods\\ann\\layer\\transposed_convolution_impl.hpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "333,334,335", "deleted_lines": "333,334,335", "method_info": {"method_name": "mlpack::ann::TransposedConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Backward", "method_params": "gy,g", "method_startline": "307", "method_endline": "367"}}, "hunk_1": {"Ismethod": 1, "added_lines": "384,390,391", "deleted_lines": "384", "method_info": {"method_name": "mlpack::ann::TransposedConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Gradient", "method_params": "input,error,gradient", "method_startline": "377", "method_endline": "436"}}, "hunk_2": {"Ismethod": 1, "added_lines": "384,390,391", "deleted_lines": "384", "method_info": {"method_name": "mlpack::ann::TransposedConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Gradient", "method_params": "error,gradient", "method_startline": "377", "method_endline": "434"}}, "hunk_3": {"Ismethod": 1, "added_lines": "213", "deleted_lines": "213", "method_info": {"method_name": "mlpack::ann::TransposedConvolution<ForwardConvolutionRule,BackwardConvolutionRule,GradientConvolutionRule,InputDataType,OutputDataType>::Forward", "method_params": "input,output", "method_startline": "204", "method_endline": "297"}}}}}}}