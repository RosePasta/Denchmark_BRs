{"BR": {"BR_id": "605", "BR_author": "jotterbach", "BRopenT": "2018-11-01T22:51:26Z", "BRcloseT": "2018-11-03T01:28:25Z", "BR_text": {"BRsummary": "pytorch + horovod 0.15.1 distributed optimizer not working anymore", "BRdescription": "\n I just upgraded horovod 0.15.0 -> 0.15.1 on a ubuntu image 4.4.0-137-generic #163-Ubuntu SMP Mon Sep 24 13:14:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. When using the DistributedOptimizer from horovod.torch I now encounter the error\n <denchmark-code>Traceback (most recent call last):\n   File \"train.py\", line 641, in <module>\n     train_images(hps)\n   File \"train.py\", line 444, in train_images\n     train_step(batch, batch_idx, epoch, hps, model, opt, train_logger)\n   File \"train.py\", line 457, in train_step\n     opt.step()\n   File \"/opt/conda/lib/python3.6/site-packages/horovod/torch/__init__.py\", line 97, in step\n     return super(self.__class__, self).step(closure)\n   File \"/opt/conda/lib/python3.6/site-packages/torch/optim/adamax.py\", line 75, in step\n     exp_avg.mul_(beta1).add_(1 - beta1, grad)\n TypeError: mul_() received an invalid combination of arguments - got (numpy.float32), but expected one of:\n  * (Tensor other)\n       didn't match because some of the arguments have invalid types: (numpy.float32)\n  * (float other)\n       didn't match because some of the arguments have invalid types: (numpy.float32)\n </denchmark-code>\n \n Downgrading to 0.15.0 fixes the issue. The behavior is independent of CPU, GPU or MultipleGPU training.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jotterbach", "commentT": "2018-11-01T23:48:12Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/jotterbach>@jotterbach</denchmark-link>\n , what version of PyTorch are you using?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jotterbach", "commentT": "2018-11-02T00:27:49Z", "comment_text": "\n \t\tSorry, I forgot to specify that. It's 0.4.0.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jotterbach", "commentT": "2018-11-02T16:20:06Z", "comment_text": "\n \t\tThat's curious.  I haven't been able to repro yet, but it looks like the value of beta1 is being set to a numpy float32 when a pure Python float is expected.  Do you have a snippet of code that initializes the Adamax optimizer?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jotterbach", "commentT": "2018-11-02T19:51:14Z", "comment_text": "\n \t\tSure thing <denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>\n  . The code is fairly straight forward:\n <denchmark-code>if hps.cuda:\n     model = model.cuda()\n opt_s = torch.optim.Adamax(model.parameters(), hps.lr)\n opt = hvd.DistributedOptimizer(opt_s,\n                                named_parameters=model.named_parameters())\n \n hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n hvd.broadcast_optimizer_state(opt, root_rank=0)\n </denchmark-code>\n \n The relevant code in the training loop looks like:\n <denchmark-code>model.train()\n opt.zero_grad()\n if hps.cuda:\n     batch = batch.cuda()\n loss = model(batch)\n \n loss.backward()\n opt.synchronize()\n opt.step()\n </denchmark-code>\n \n The optimizer is called nowhere else.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jotterbach", "commentT": "2018-11-02T20:45:18Z", "comment_text": "\n \t\tAh, okay, I see what's going on here.  This is the result of a change we made to hvd.broadcast_optimizer_state() to broadcast the scalar options.  Looks like beta1 is getting coerced into a numpy.float32, which is causing your issue.\n I'll start working on a fix for this.  Thanks for reporting!\n \t\t"}}}, "commit": {"commit_id": "62d2869047ee8ccab3d559bee35b8f5e392936fb", "commit_author": "Travis Addair", "commitT": "2018-11-02 18:28:24-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "horovod\\torch\\__init__.py", "file_new_name": "horovod\\torch\\__init__.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "238,239,240,241,242,243,244", "deleted_lines": "239,241", "method_info": {"method_name": "broadcast_optimizer_state._recursive_cast", "method_params": "x,dtype", "method_startline": "238", "method_endline": "244"}}, "hunk_1": {"Ismethod": 1, "added_lines": "257", "deleted_lines": "257", "method_info": {"method_name": "broadcast_optimizer_state.broadcast_optimizer_state._create_option_callback._from_tensor", "method_params": "", "method_startline": "256", "method_endline": "257"}}, "hunk_2": {"Ismethod": 1, "added_lines": "255,257", "deleted_lines": "255,257", "method_info": {"method_name": "broadcast_optimizer_state._create_option_callback", "method_params": "index,option_key,option_tensor,dtypes", "method_startline": "255", "method_endline": "258"}}, "hunk_3": {"Ismethod": 1, "added_lines": "231,232,233,234,235", "deleted_lines": null, "method_info": {"method_name": "broadcast_optimizer_state._get_types", "method_params": "x", "method_startline": "231", "method_endline": "235"}}, "hunk_4": {"Ismethod": 1, "added_lines": "230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,255,257,271,273", "deleted_lines": "239,241,255,257", "method_info": {"method_name": "broadcast_optimizer_state", "method_params": "optimizer,root_rank", "method_startline": "185", "method_endline": "301"}}, "hunk_5": {"Ismethod": 1, "added_lines": "239,240,241,242", "deleted_lines": "239,241", "method_info": {"method_name": "broadcast_optimizer_state._create_option_callback", "method_params": "index,option_key,option_tensor,dtype", "method_startline": "239", "method_endline": "242"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\test_torch.py", "file_new_name": "test\\test_torch.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "876,878,927,930,931,932,933,934,935", "deleted_lines": "876,878", "method_info": {"method_name": "test_broadcast_state_options", "method_params": "self", "method_startline": "868", "method_endline": "935"}}}}}}}