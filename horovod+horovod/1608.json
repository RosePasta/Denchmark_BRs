{"BR": {"BR_id": "1608", "BR_author": "una-dinosauria", "BRopenT": "2019-12-23T20:45:44Z", "BRcloseT": "2020-01-09T21:03:49Z", "BR_text": {"BRsummary": "RuntimeError: For integral input tensors, argument alpha must not be a floating point number", "BRdescription": "\n Environment:\n \n Framework: (TensorFlow, Keras, PyTorch, MXNet): Pytorch\n Framework version: 1.3.0\n Horovod version: 0.16.1\n MPI version: 3.1.2 (?)\n CUDA version: 10.2\n NCCL version: ? (probably not important)\n Python version: 2.7\n OS and version: Ubuntu 14.04.4 (probably not important)\n GCC version: 4.8.4 (probably not important)\n \n Bug report:\n Horovod does not like it when my module has integer parameters, even if they do not require a gradient. It says\n Traceback (most recent call last):\n   File \"/.../mwe.py\", line 30, in <module>\n     hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n   File \"/.../python2.7/dist-packages/horovod-0.16.1-py2.7-linux-x86_64.egg/horovod/torch/__init__.py\", line 261, in broadcast_optimizer_state\n     super(optimizer.__class__, optimizer).step()\n   File \"/.../torch/optim/sgd.py\", line 106, in step\n     p.data.add_(-group['lr'], d_p)\n RuntimeError: For integral input tensors, argument alpha must not be a floating point number.\n MWE\n import torch\n import torch.nn as nn\n import torch.optim as optim\n \n import horovod.torch as hvd\n \n \n class A(nn.Module):\n \n     def __init__(self, a, b):\n         super(A, self).__init__()\n         self.a = nn.Parameter(a.int(), requires_grad=False)   # change to a.float() and hvd is happy\n         self.b = nn.Parameter(b)\n \n     def forward(self, x):\n         return torch.index_select(self.b, 0, self.a.long()) * x\n \n \n hvd.init()\n \n a = torch.Tensor([1, 3])\n b = torch.rand(4)\n \n model = A(a, b).cuda()\n \n optimizer = optim.SGD(model.parameters(), lr=0.01)\n optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n \n hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n hvd.broadcast_optimizer_state(optimizer, root_rank=0)  # <-- hvd is sad :(\n \n model.train()\n \n for i in range(1000):\n \n     x = torch.Tensor(torch.rand(2)).cuda()\n \n     optimizer.zero_grad()\n \n     loss = torch.mean((model(x) - 0) ** 2)\n \n     loss.backward()\n     optimizer.step()\n \n     if hvd.rank() == 0:\n         print(i, loss.detach())\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "una-dinosauria", "commentT": "2019-12-23T23:19:51Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/una-dinosauria>@una-dinosauria</denchmark-link>\n , thanks for reporting this.  And thanks even more for the reproducible example.  I'll take a look at this today and get back to you as soon as I have a fix.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "una-dinosauria", "commentT": "2019-12-24T00:35:51Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/una-dinosauria>@una-dinosauria</denchmark-link>\n , please take a look at <denchmark-link:https://github.com/horovod/horovod/pull/1609>#1609</denchmark-link>\n  if you get a chance, it should address your issue.  Going forward, it will also provide a more generic  function that can be used to broadcast anything, for example, if  doesn't work in the future.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "una-dinosauria", "commentT": "2019-12-24T05:40:40Z", "comment_text": "\n \t\tLeft a comment in the PR. Looks awesome -- thanks.\n \t\t"}}}, "commit": {"commit_id": "b505f149b4cb1c1e8f61e8d3f19aeba15383be71", "commit_author": "Travis Addair", "commitT": "2020-01-09 13:03:48-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "horovod\\torch\\__init__.py", "file_new_name": "horovod\\torch\\__init__.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "500,501,592", "deleted_lines": "494,585", "method_info": {"method_name": "broadcast_optimizer_state", "method_params": "optimizer,root_rank", "method_startline": "478", "method_endline": "595"}}, "hunk_1": {"Ismethod": 1, "added_lines": "598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638", "deleted_lines": null, "method_info": {"method_name": "broadcast_object", "method_params": "obj,root_rank,name", "method_startline": "598", "method_endline": "638"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "test\\test_torch.py", "file_new_name": "test\\test_torch.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1117,1118", "deleted_lines": null, "method_info": {"method_name": "test_broadcast_state_no_grad.forward", "method_params": "self,x", "method_startline": "1117", "method_endline": "1118"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134", "deleted_lines": null, "method_info": {"method_name": "test_broadcast_state_no_grad", "method_params": "self", "method_startline": "1110", "method_endline": "1134"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1112,1113,1114,1115", "deleted_lines": null, "method_info": {"method_name": "test_broadcast_state_no_grad.__init__", "method_params": "self,a,b", "method_startline": "1112", "method_endline": "1115"}}, "hunk_3": {"Ismethod": 1, "added_lines": "1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146", "deleted_lines": null, "method_info": {"method_name": "test_broadcast_object", "method_params": "self", "method_startline": "1136", "method_endline": "1146"}}}}}}}