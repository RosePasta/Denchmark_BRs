{"BR": {"BR_id": "1552", "BR_author": "NanoCode012", "BRopenT": "2020-11-29T16:02:00Z", "BRcloseT": "2020-11-29T16:47:52Z", "BR_text": {"BRsummary": "RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation when running on Docker", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n I got the below error message when I try to test out the latest commit <denchmark-link:https://github.com/ultralytics/yolov5/commit/cff9263490fbf4b80dcc2d87914e087e6c07b6a0>cff9263</denchmark-link>\n  on a new docker image. I haven't pulled recently, so I'm not sure which commit made this error.\n RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n <denchmark-h:h2>To Reproduce (REQUIRED)</denchmark-h>\n \n \n Pull docker and run it\n Run python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache\n \n Output:\n <denchmark-code> 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n  23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n  24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n Traceback (most recent call last):\n   File \"train.py\", line 492, in <module>\n     train(hyp, opt, device, tb_writer, wandb)\n   File \"train.py\", line 83, in train\n     model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  # create\n   File \"/usr/src/app/models/yolo.py\", line 95, in __init__\n     self._initialize_biases()  # only run once\n   File \"/usr/src/app/models/yolo.py\", line 150, in _initialize_biases\n     b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n </denchmark-code>\n \n <denchmark-h:h2>Expected behavior</denchmark-h>\n \n Run normally\n <denchmark-h:h2>Environment</denchmark-h>\n \n \n Docker + JupyterLab (from my repo)\n CPU, 1 GPU, Multi-GPU\n \n <denchmark-h:h2>Additional context</denchmark-h>\n \n It seems to run fine when I'm running from an old conda py37 environment with torch 1.6.\n I cannot reproduce this error on Google Colab.\n Could there be something wrong with Docker dependencies?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "NanoCode012", "commentT": "2020-11-29T16:15:07Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/NanoCode012>@NanoCode012</denchmark-link>\n  thanks for the bug report. I'll try to reproduce with yolov5:latest on a GCP instance.\n I've seen this error in the past when running in-place ops like L150 in your error message with autograd on, but that line has not changed in a long time. PyTorch versions are changing though, so perhaps this is handled differently now.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "NanoCode012", "commentT": "2020-11-29T16:23:59Z", "comment_text": "\n \t\tYeah I get the same result. I think the issue is that nvidia seems to prefer pytorch nightly for their FROM images rather than the last stable release, so I can't tell if this is a nightly instability or there's some 1.8 update set to cause errors on this in the future.\n If I pull latest and then run this line, everything trains fine.\n <denchmark-code>pip install torch==1.7.0+cu110 torchvision==0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n </denchmark-code>\n \n I guess for now I'll simply reset the image FROM tag to 20.10, which I think was working well.\n <denchmark-link:https://user-images.githubusercontent.com/26833433/100547405-d5cd8100-3266-11eb-8ad3-6781c7797e00.png></denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "NanoCode012", "commentT": "2020-11-29T16:27:48Z", "comment_text": "\n \t\tOr wait, I just had a great idea! I think if I start from a different base image, such as pytorch/pytorch:latest, then this seems to point to the last stable release, and perhaps eliminates maintenance also as the tag never changes. I will try an experiment and see if it works.\n <denchmark-code>FROM nvcr.io/nvidia/pytorch:20.11-py3\n FROM pytorch/pytorch:latest\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "NanoCode012", "commentT": "2020-11-29T16:43:35Z", "comment_text": "\n \t\tI tried to create a pytorch:latest image here with this Dockerfile, but the image lacks some dependencies like cv2, which are causing problems on pip install, so I gave up on it. The Dockerfile is here in case anyone can debug this. In the meantime I think a rollback to 20.10 will fix this, I'll get that done.\n docker pull ultralytics/yolov5:pytorch_latest\n <denchmark-code>FROM pytorch/pytorch:latest\n \n # Install dependencies\n RUN pip install --upgrade pip\n # COPY requirements.txt .\n # RUN pip install -r requirements.txt\n RUN pip install gsutil\n \n # Create working directory\n RUN mkdir -p /usr/src/app\n WORKDIR /usr/src/app\n \n # Copy contents\n COPY . /usr/src/app\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "NanoCode012", "commentT": "2020-11-29T16:51:13Z", "comment_text": "\n \t\tVerified new image works, problem should be resolved now in PR <denchmark-link:https://github.com/ultralytics/yolov5/pull/1553>#1553</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "NanoCode012", "commentT": "2020-11-29T16:53:09Z", "comment_text": "\n \t\tThanks glenn! I will wait for image to build from dockerhub and test it!\n Regarding pytorch:latest, I think it could be dangerous to use it in DockerFile because if there is some breaking change, you may not know till someone reports it.\n Edit: This would mean that this repo will not be able to use later versions of nvidia's package until this bug is fixed somehow..\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "NanoCode012", "commentT": "2020-11-29T16:57:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/NanoCode012>@NanoCode012</denchmark-link>\n  yes that's true. The docker images don't actually have any CI tests, they just build on every commit under the assumption that the github CI tests would mostly apply to docker as well, but it is true that they often may use different PyTorch versions. GitHub also updates their dependencies on their own schedule, so when 1.6 came out for example the next day we had the daily CI test failing.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "NanoCode012", "commentT": "2020-11-30T04:13:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  correct me if I am wrong, but both nvcr.io/nvidia/pytorch:20.11-py3 and nvcr.io/nvidia/pytorch:20.10-py3 seems to use python 3.6\n This project requests 3.8 or above.\n Will this be a problem?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "NanoCode012", "commentT": "2020-11-30T11:37:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cesarandreslopez>@cesarandreslopez</denchmark-link>\n  yes I noticed that as well. I'm not sure if 3.6.0 is compatible with this repo, I think the last one I checked was using 3.6.9. I'm doing all development in 3.8.0, but in general backwards compatibility is something I don't have lots of time to maintain and verify, which is the reason I've simply put 3.8 down as the requirement.\n But as you're seeing 3.7 appears compatible, as well as possibly much of the 3.6.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "NanoCode012", "commentT": "2020-12-02T10:13:00Z", "comment_text": "\n \t\tHi, guys. <denchmark-link:https://github.com/NanoCode012>@NanoCode012</denchmark-link>\n  <denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  The following code works for me:\n with torch.no_grad():\n b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "NanoCode012", "commentT": "2020-12-02T10:23:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/MingcongCao>@MingcongCao</denchmark-link>\n  ah, I've resolved the original issue by resetting the base image to Nvidia 20.10, so all docker operations should be operating correctly now.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "NanoCode012", "commentT": "2020-12-07T14:50:17Z", "comment_text": "\n \t\tI have met this issue with RTX3090 & Cuda 11.1.0. Is there any solution for this configuration?\n \n python train.py --batch-size 64 --data ./data/coco128.yaml --cfg ./models/yolov5s.yaml --weights ''\n \n Using torch 1.8.0.dev20201117 CUDA:0 (GeForce RTX 3090, 24265MB)\n Traceback (most recent call last):\n File \"train.py\", line 492, in \n train(hyp, opt, device, tb_writer, wandb)\n File \"train.py\", line 91, in train\n model = Model(opt.cfg, ch=3, nc=nc).to(device)  # create\n File \"/home/yons/work/yolov5/models/yolo.py\", line 95, in init\n self._initialize_biases()  # only run once\n File \"/home/yons/work/yolov5/models/yolo.py\", line 150, in _initialize_biases\n b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "NanoCode012", "commentT": "2020-12-07T14:58:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hcodee>@hcodee</denchmark-link>\n  , could you try stable torch 1.7?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "NanoCode012", "commentT": "2020-12-07T15:03:06Z", "comment_text": "\n \t\tThe torch 1.7 does not work with RTX3090. Takes long time to figure out to run on nightly build Torch 1.8.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "NanoCode012", "commentT": "2020-12-07T15:31:07Z", "comment_text": "\n \t\t\n The torch 1.7 does not work with RTX3090. Takes long time to figure out to run on nightly build Torch 1.8.\n \n You need to compile pytorch yourself witch cuda 11.1 installed. It is doable, I did it without any hassle ( surprisingly ) from master. Unfortunately I need to do it again for 1.7\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "NanoCode012", "commentT": "2020-12-07T15:56:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/batrlatom>@batrlatom</denchmark-link>\n  Cool, Thanks remind. I will try it out.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "NanoCode012", "commentT": "2020-12-17T11:01:33Z", "comment_text": "\n \t\t\n I have met this issue with RTX3090 & Cuda 11.1.0. Is there any solution for this configuration?\n \n python train.py --batch-size 64 --data ./data/coco128.yaml --cfg ./models/yolov5s.yaml --weights ''\n \n Using torch 1.8.0.dev20201117 CUDA:0 (GeForce RTX 3090, 24265MB)\n Traceback (most recent call last):\n File \"train.py\", line 492, in\n train(hyp, opt, device, tb_writer, wandb)\n File \"train.py\", line 91, in train\n model = Model(opt.cfg, ch=3, nc=nc).to(device) # create\n File \"/home/yons/work/yolov5/models/yolo.py\", line 95, in init\n self._initialize_biases() # only run once\n File \"/home/yons/work/yolov5/models/yolo.py\", line 150, in _initialize_biases\n b[:, 4] += math.log(8 / (640 / s) ** 2) # obj (8 objects per 640 image)\n RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n \n Same problem with nighly pytorch version here. Any luck with using the self compiled pytorch 1.8?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "NanoCode012", "commentT": "2020-12-18T02:44:49Z", "comment_text": "\n \t\tI met the same issue with pytorch 1.8, and the following code works for me:\n <denchmark-code>b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\n </denchmark-code>\n \n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "NanoCode012", "commentT": "2020-12-23T01:17:28Z", "comment_text": "\n \t\tI just ran into this issue myself, so it's time for a fix :) Will add a TODO and prioritize this for a fix ASAP.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "NanoCode012", "commentT": "2020-12-23T01:20:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/DoctorKey>@DoctorKey</denchmark-link>\n  can confirm your solution works correctly. I will submit a PR for this to master.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "NanoCode012", "commentT": "2020-12-23T01:29:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/NanoCode012>@NanoCode012</denchmark-link>\n  <denchmark-link:https://github.com/DoctorKey>@DoctorKey</denchmark-link>\n  <denchmark-link:https://github.com/batrlatom>@batrlatom</denchmark-link>\n  <denchmark-link:https://github.com/hcodee>@hcodee</denchmark-link>\n  this problems should be resolved now by implementing <denchmark-link:https://github.com/DoctorKey>@DoctorKey</denchmark-link>\n  fix in PR <denchmark-link:https://github.com/ultralytics/yolov5/pull/1759>#1759</denchmark-link>\n . Docker image for ultralytics/yolov5:latest should be updated in a few minutes with this fix.\n Let me know if any other issues pop up, and thank you for your contributions!\n \t\t"}}}, "commit": {"commit_id": "68211f72c99915a15855f7b99bf5d93f5631330f", "commit_author": "Glenn Jocher", "commitT": "2020-11-29 17:47:51+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "Dockerfile", "file_new_name": "Dockerfile", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "2,31", "deleted_lines": "2,31"}}}}}}