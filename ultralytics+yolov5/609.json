{"BR": {"BR_id": "609", "BR_author": "ZeKunZhang1998", "BRopenT": "2020-08-03T02:09:06Z", "BRcloseT": "2020-08-05T20:41:43Z", "BR_text": {"BRsummary": "Possible Bug Training on Empty Batch?", "BRdescription": "\n <denchmark-h:h2>\u2754Question</denchmark-h>\n \n Traceback (most recent call last):\n File \"train.py\", line 463, in \n train(hyp, tb_writer, opt, device)\n File \"train.py\", line 286, in train\n loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size\n File \"/content/drive/My Drive/yolov5/utils/utils.py\", line 443, in compute_loss\n tcls, tbox, indices, anchors = build_targets(p, targets, model)  # targets\n File \"/content/drive/My Drive/yolov5/utils/utils.py\", line 542, in build_targets\n b, c = t[:, :2].long().T  # image, class\n ValueError: too many values to unpack (expected 2)\n <denchmark-h:h2>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T18:34:22Z", "comment_text": "\n \t\tHello, thank you for your interest in our work! This issue seems to lack the minimum requirements for a proper response, or is insufficiently detailed for us to help you. Please note that most technical problems are due to:\n \n Your changes to the default repository. If your issue is not reproducible in a new git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:\n \n sudo rm -rf yolov5  # remove existing\n git clone https://github.com/ultralytics/yolov5 && cd yolov5 # clone latest\n python detect.py  # verify detection\n # CODE TO REPRODUCE YOUR ISSUE HERE\n \n \n Your custom data. If your issue is not reproducible with COCO or COCO128 data we can not debug it. Visit our Custom Training Tutorial for guidelines on training your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.\n \n \n Your environment. If your issue is not reproducible in one of the verified environments below we can not debug it. If you are running YOLOv5 locally, ensure your environment meets all of the requirements.txt dependencies specified below.\n \n \n If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!\n <denchmark-h:h2>Requirements</denchmark-h>\n \n Python 3.8 or later with all <denchmark-link:https://github.com/ultralytics/yolov5/blob/master/requirements.txt>requirements.txt</denchmark-link>\n  dependencies installed, including . To install run:\n $ pip install -U -r requirements.txt\n <denchmark-h:h2>Environments</denchmark-h>\n \n YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including <denchmark-link:https://developer.nvidia.com/cuda>CUDA</denchmark-link>\n /<denchmark-link:https://developer.nvidia.com/cudnn>CUDNN</denchmark-link>\n , <denchmark-link:https://www.python.org/>Python</denchmark-link>\n  and <denchmark-link:https://pytorch.org/>PyTorch</denchmark-link>\n  preinstalled):\n \n Google Colab Notebook with free GPU: \n Kaggle Notebook with free GPU: https://www.kaggle.com/ultralytics/yolov5\n Google Cloud Deep Learning VM. See GCP Quickstart Guide\n Docker Image https://hub.docker.com/r/ultralytics/yolov5. See Docker Quickstart Guide \n \n <denchmark-h:h2>Current Status</denchmark-h>\n \n <denchmark-link:https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg></denchmark-link>\n \n If this badge is green, all <denchmark-link:https://github.com/ultralytics/yolov5/actions>YOLOv5 GitHub Actions</denchmark-link>\n  Continuous Integration (CI) test are passing. These tests evaluate proper operation of basic YOLOv5 functionality, including training (<denchmark-link:https://github.com/ultralytics/yolov5/blob/master/train.py>train.py</denchmark-link>\n ), testing (<denchmark-link:https://github.com/ultralytics/yolov5/blob/master/test.py>test.py</denchmark-link>\n ), inference (<denchmark-link:https://github.com/ultralytics/yolov5/blob/master/detect.py>detect.py</denchmark-link>\n ) and export (<denchmark-link:https://github.com/ultralytics/yolov5/blob/master/models/export.py>export.py</denchmark-link>\n ) on MacOS, Windows, and Ubuntu.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T21:23:06Z", "comment_text": "\n \t\tChanging batchsize will solve this issue, it will occur when a batch of images contain no object.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T22:06:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  ah I see. I remember a similar issue about testing with no targets, but I believed this was resolved. Does this occur when training or testing? Can you supply code to reproduce?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T22:12:49Z", "comment_text": "\n \t\tWhile changing the batchsize helped prolong the learning process, this issue still occurs for me. By printing the paths of the images in each batch i can check to see if they have an object, and there's definitely at least one object in each occasion (my dataset doesn't have images with empty label files) of the crash.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T22:31:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/MiiaBestLamia>@MiiaBestLamia</denchmark-link>\n  can you supply exact steps and code to reproduce this issue following the steps outlined before (current repo, valid environment, common dataset?)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T22:39:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  i'm using the most recent repository, all of the requirements are satisfied except the Python version, which might be the reason for the issue, though it seems strange that the learning process works for a bit, then crashes (I'm using 3.6.9). I have altered only one line of code in the repository, the one that prints paths of the files used in the batch. Using a dataset that I may not be allowed to share, so I cannot provide you with the files.\n I'm using the .txt option of providing the train and val sets, i have only one class, using image size 800 with batch size 4, providing yolov5l.pt weights (that I downloaded from the google drive), would be nice to see what <denchmark-link:https://github.com/ZeKunZhang1998>@ZeKunZhang1998</denchmark-link>\n  is working with, maybe that can narrow down the problem.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T22:49:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/MiiaBestLamia>@MiiaBestLamia</denchmark-link>\n  I would verify your issue is reproducible in one of the environments above. That's what they're there for.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-03T23:58:45Z", "comment_text": "\n \t\t\n Changing batchsize will solve this issue, it will occur when a batch of images contain no object.\n \n when I change to batch size = 1,another picture is something wrong.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T00:05:07Z", "comment_text": "\n \t\t\n @glenn-jocher i'm using the most recent repository, all of the requirements are satisfied except the Python version, which might be the reason for the issue, though it seems strange that the learning process works for a bit, then crashes (I'm using 3.6.9). I have altered only one line of code in the repository, the one that prints paths of the files used in the batch. Using a dataset that I may not be allowed to share, so I cannot provide you with the files.\n I'm using the .txt option of providing the train and val sets, i have only one class, using image size 800 with batch size 4, providing yolov5l.pt weights (that I downloaded from the google drive), would be nice to see what @ZeKunZhang1998 is working with, maybe that can narrow down the problem.\n \n I use a private dataset.When I jump the wrong batch , the problem will be solved.The wrong batch has object, it is not an empty picture.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T03:30:49Z", "comment_text": "\n \t\tMaybe i am wrong about this issue, but this issue disappeared when i changed to another batchsize.\n I've added print(targets.shape) in build_targets, i got this Tensor.Size([0, 6]) when ValueError: too many values to unpack (expected 2)\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T04:14:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  the only thing we can act on is a reproducible example in one of the verified environments.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T04:40:57Z", "comment_text": "\n \t\t\n @acai66 the only thing we can act on is a reproducible example in one of the verified environments.\n \n I will reinstall pytorch from source code and fetch latest yolov5, i will upload my datasets if this issue occurs again.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T07:33:03Z", "comment_text": "\n \t\tI meet the same problem too.\n I think the reason is that after box_candidates function there are no targets.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T08:32:15Z", "comment_text": "\n \t\tAfter changing some of the hyperparameters in train.py (lr0:0.001, scale:0.2), moving the project to a computer with a beefier GPU (went from 2060S to 2080Ti, using Python 3.6.9) and increasing the batch size to 8, training is functioning properly for 4 epochs now. I can still reproduce the issue with my data on the 2080Ti by launching train.py with a batch size of 4, so I suppose this issue is caused by peculiarities in data, not problems with the network/code.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T09:52:45Z", "comment_text": "\n \t\t\n @acai66 the only thing we can act on is a reproducible example in one of the verified environments.\n \n here is my datasets,\n <denchmark-link:https://drive.google.com/file/d/12epqSyYELm7c4mXXIIJos33KDcj1l67f/view?usp=sharing>https://drive.google.com/file/d/12epqSyYELm7c4mXXIIJos33KDcj1l67f/view?usp=sharing</denchmark-link>\n \n data yaml:\n <denchmark-link:https://github.com/ultralytics/yolov5/files/5021341/2020.yaml.txt>2020.yaml.txt</denchmark-link>\n \n models yaml:\n <denchmark-link:https://github.com/ultralytics/yolov5/files/5021344/yolov5x_2020.yaml.txt>yolov5x_2020.yaml.txt</denchmark-link>\n \n train command:\n python train.py --cfg models/yolov5x_2020.yaml --data data/2020.yaml --epochs 300 --batch-size 8 --img-size 512 512 --cache-images --weights '' --name \"yolov5x_2020_default\" --single-cls\n This issue disappeared when changed batchsize to 12.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T13:37:33Z", "comment_text": "\n \t\tWhen I use batch size = 4, it works.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T13:38:37Z", "comment_text": "\n \t\t\n @acai66 the only thing we can act on is a reproducible example in one of the verified environments.\n \n Hi,I think some data augmentation make the boxes disappear, right? If you use bigger batch size , this issue will disappear.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-04T18:59:38Z", "comment_text": "\n \t\tOn a private dataset I have also had this issue with batch size = 16. Haven't fully tested further, but the dataset does include a fair amount of images without objects. For what that's worth.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T03:08:02Z", "comment_text": "\n \t\tIt means that whole dataset must have object, if an image isnt labelled then wrong, isnt it?\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T04:20:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/buimanhlinh96>@buimanhlinh96</denchmark-link>\n  that is not correct. COCO has over a thousand images without labels.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T04:24:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  so what happended with this issue?  It might in a batch, we must have at least one image is labelled?\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T04:56:49Z", "comment_text": "\n \t\t\n \n @acai66 the only thing we can act on is a reproducible example in one of the verified environments.\n \n here is my datasets,\n https://drive.google.com/file/d/12epqSyYELm7c4mXXIIJos33KDcj1l67f/view?usp=sharing\n data yaml:\n 2020.yaml.txt\n models yaml:\n yolov5x_2020.yaml.txt\n train command:\n python train.py --cfg models/yolov5x_2020.yaml --data data/2020.yaml --epochs 300 --batch-size 8 --img-size 512 512 --cache-images --weights '' --name \"yolov5x_2020_default\" --single-cls\n This issue disappeared when changed batchsize to 12.\n \n <denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  thank you! I think I can work with this. I can only debug official models though, so I will use yolov5x.yaml in place of yours. Do you yourself see the error when running on the default models?\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T05:00:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/buimanhlinh96>@buimanhlinh96</denchmark-link>\n  I don't know, I have not tried to reproduce yet. I know test.py operates correctly on datasets without labels, I don't know about train.py. Can you provide minimum viable code to reproduce your specific issue?\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T05:01:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  also are you able to reproduce in one of the verified environments?\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T06:32:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  I try some experiments and come up with the batch-size should be more than or equal 8\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T06:34:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/buimanhlinh96>@buimanhlinh96</denchmark-link>\n  there is no constraint on batch size, so you should be able to use batch size 1 to batch size x, whatever your hardware can handle. If this is not the case then there is a bug.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T09:47:50Z", "comment_text": "\n \t\t\n @acai66 also are you able to reproduce in one of the verified environments?\n \n you can try default yolov5x.yaml. I just change nc to 1 in yolov5x_2020_default.yaml actually.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T10:24:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  Yes. Hopefully we can fix it ASAP. Love yolov5\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T17:07:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  ah I see, of course. We actually updated train.py a few weeks back to inherit  from the data.yaml in case of a mismatch with the model yaml , so you should be able to use your command with the default 80 class yolov5x.yaml as well, and it will still operate correctly.\n Ok, I will try to reproduce this in a colab notebook today if I have time.\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T17:53:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  I'm in the same boat.\n \n Colab notebook environment with torch 1.6.0\n yolov5x yaml changing NC\n private dataset\n \n For me the bug hits right after the first epoch (which successfully completes), when moving to the second epoch.\n It seems fixed by moving the batch size from 4 to 12 as suggested above (Colab runs out of memory on this dataset at 16).\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T18:02:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Jacobsolawetz>@Jacobsolawetz</denchmark-link>\n  hmm ok. Do you have a pretty sparse dataset, do you think it's possible a whole batch of 4 images might have no labels? Does the bug happen during training or testing?\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T20:11:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  I'm able to reproduce this in a colab notebook:\n <denchmark-link:https://colab.research.google.com/drive/1bCFd_1fyFG8pkXkQ8MubvRSgFsb9ZPhu#scrollTo=-AVqcyhjO89V>https://colab.research.google.com/drive/1bCFd_1fyFG8pkXkQ8MubvRSgFsb9ZPhu#scrollTo=-AVqcyhjO89V</denchmark-link>\n \n I see this midway through the first epoch:\n <denchmark-code>     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n      0/299     4.78G   0.07214   0.01437         0   0.08651         3       512:  70% 105/150 [00:58<00:22,  2.02it/s]Traceback (most recent call last):\n   File \"train.py\", line 477, in <module>\n     train(hyp, opt, device, tb_writer)\n   File \"train.py\", line 300, in train\n     loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size\n   File \"/content/yolov5/utils/general.py\", line 446, in compute_loss\n     tcls, tbox, indices, anchors = build_targets(p, targets, model)  # targets\n   File \"/content/yolov5/utils/general.py\", line 545, in build_targets\n     b, c = t[:, :2].long().T  # image, class\n ValueError: too many values to unpack (expected 2)\n      0/299     4.78G   0.07214   0.01437         0   0.08651         3       512:  70% 105/150 [00:58<00:25,  1.79it/s]\n </denchmark-code>\n \n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T20:38:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ZeKunZhang1998>@ZeKunZhang1998</denchmark-link>\n  <denchmark-link:https://github.com/mrk230>@mrk230</denchmark-link>\n  <denchmark-link:https://github.com/Jacobsolawetz>@Jacobsolawetz</denchmark-link>\n  <denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  <denchmark-link:https://github.com/buimanhlinh96>@buimanhlinh96</denchmark-link>\n  this issue should be resolved now in <denchmark-link:https://github.com/ultralytics/yolov5/commit/7eaf225d558c6495190e0c79a56553633a065c49>7eaf225</denchmark-link>\n . Please  to receive the latest updates and try again.\n Let us know if you run into anymore problems, and good luck!\n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T20:40:55Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/acai66>@acai66</denchmark-link>\n  for your dataset I would recommend several changes:\n \n You have very small objects, you need to train at the highest viable resolution, even if it means using a smaller model.\n Start from pretrained weights for best results, but also try training from scratch to compare.\n Use the largest batch size that will fit into RAM.\n Your dataset is different enough from COCO that it may benefit from substantially different hyperparameters. See hyperparameter evolution tutorial: https://github.com/ultralytics/yolov5#tutorials\n \n \t\t"}, "comments_34": {"comment_id": 35, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T21:20:36Z", "comment_text": "\n \t\t\n @acai66 for your dataset I would recommend several changes:\n \n You have very small objects, you need to train at the highest viable resolution, even if it means using a smaller model.\n Start from pretrained weights for best results, but also try training from scratch to compare.\n Use the largest batch size that will fit into RAM.\n Your dataset is different enough from COCO that it may benefit from substantially different hyperparameters. See hyperparameter evolution tutorial: https://github.com/ultralytics/yolov5#tutorials\n \n \n Thank you very much for your recommendation, and i will try to do that,. This issue was solved after git pull latest commits.\n \t\t"}, "comments_35": {"comment_id": 36, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T22:41:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  yes... after introspection, there are maybe 6 or so images in the dataset of 500 that do not have annotations. A random grouping of those may have caused the cough.\n Thanks for fixing this bug so quickly!\n \t\t"}, "comments_36": {"comment_id": 37, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-05T23:48:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  Thank you very much!!!!!!!\n \t\t"}, "comments_37": {"comment_id": 38, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-06T02:37:56Z", "comment_text": "\n \t\t[<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n ] I am also facing the same issue (cloned latest code). Maybe the bug still remain, it quite strange because it can train to final epoch before the error happens. I trained with yolov5-s.yaml, batch-size=100 (maybe it is too large ?) on 2 GPU RTX 2080Ti. Every image contain at least one object\n <denchmark-link:https://user-images.githubusercontent.com/30823943/89484395-a16e1a80-d7c8-11ea-9b9c-15014309e329.png></denchmark-link>\n \n \t\t"}, "comments_38": {"comment_id": 39, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-06T04:35:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/anhnktp>@anhnktp</denchmark-link>\n  no, you are incorrect, you are not using the latest code. L545 no longer contains the same code, so the error message you see is not possible to produce in origin/master.\n \t\t"}, "comments_39": {"comment_id": 40, "comment_author": "ZeKunZhang1998", "commentT": "2020-08-06T04:49:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  oh, I see. It is yolov5 version 2 days ago. You added some code. I'll recheck again. Thank you\n \t\t"}}}, "commit": {"commit_id": "7eaf225d558c6495190e0c79a56553633a065c49", "commit_author": "Glenn Jocher", "commitT": "2020-08-05 13:35:31-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "utils\\general.py", "file_new_name": "utils\\general.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "526,542,543,544", "deleted_lines": "527", "method_info": {"method_name": "build_targets", "method_params": "p,targets,model", "method_startline": "506", "method_endline": "560"}}, "hunk_1": {"Ismethod": 1, "added_lines": "499", "deleted_lines": "499,500", "method_info": {"method_name": "compute_loss", "method_params": "p,targets,model", "method_startline": "443", "method_endline": "503"}}}}}}}