{"BR": {"BR_id": "375", "BR_author": "elyase", "BRopenT": "2016-05-13T11:54:35Z", "BRcloseT": "2016-09-27T17:24:18Z", "BR_text": {"BRsummary": "Noun phrase merge is failing", "BRdescription": "\n This is now failing:\n <denchmark-code>>>> doc = nlp('The cat sat on the mat')\n >>> for np in doc.noun_chunks:\n         np.merge(np.root.tag_, np.text, np.root.ent_type_)\n \n ---------------------------------------------------------------------------\n IndexError                                Traceback (most recent call last)\n <ipython-input-409-f6294d1a1cf8> in <module>()\n       1 doc = nlp('The cat sat on the mat')\n ----> 2 for np in doc.noun_chunks:\n       3     np.merge(np.root.tag_, np.text, np.root.ent_type_)\n \n /Users/yaser/miniconda3/envs/spacy/lib/python3.5/site-packages/spacy/tokens/doc.pyx in noun_chunks (spacy/tokens/doc.cpp:7745)()\n \n /Users/yaser/miniconda3/envs/spacy/python3.5/site-packages/spacy/syntax/iterators.pyx in english_noun_chunks (spacy/syntax/iterators.cpp:1559)()\n \n /Users/yaser/miniconda3/envs/spacy/lib/python3.5/site-packages/spacy/tokens/doc.pyx in spacy.tokens.doc.Doc.__getitem__ (spacy/tokens/doc.cpp:4853)()\n \n IndexError: list index out of range\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "elyase", "commentT": "2016-05-16T05:32:09Z", "comment_text": "\n \t\tThanks. v0.101.0?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "elyase", "commentT": "2016-05-19T18:48:52Z", "comment_text": "\n \t\tI am still on 0.100.7.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "elyase", "commentT": "2016-05-19T19:01:44Z", "comment_text": "\n \t\tCould you try 0.101.0?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "elyase", "commentT": "2016-05-19T22:00:39Z", "comment_text": "\n \t\tThanks guys for the amazing work. Although, Same issue while running v0.101.0. Hope you can fix it soon ;)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "elyase", "commentT": "2016-05-20T08:05:51Z", "comment_text": "\n \t\tAh, this was dumb, sorry --- I didn't have time to really look at this, now that I have it's obvious there's a problem. Actually I'm not sure how the code was working before. I think there was always a bug here.\n Please work around this for now by doing for np in list(doc.noun_chunks). The problem is that we're changing the tokenisation out from underneath the iterator we're yielding from, and this is causing problems.\n I think this is always going to be hard to get right, and I'm going to change the noun chunks code to accumulate the spans before it yields them.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "elyase", "commentT": "2016-05-22T00:29:29Z", "comment_text": "\n \t\tSorry for being late to get back to you but your tip works. Thanks a lot!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "elyase", "commentT": "2016-09-05T16:11:46Z", "comment_text": "\n \t\tThis is still failing for me on 0.101.0. The workaround with for np in list(doc.noun_chunks) works.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "elyase", "commentT": "2016-09-27T17:24:18Z", "comment_text": "\n \t\tFixed on the master branch, and the fix will be released in 1.0.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "elyase", "commentT": "2017-01-29T21:00:33Z", "comment_text": "\n \t\tHi, I'm not sure whether this is a related issue, but I'm using the merge functionality to merge tokens from certain noun phrases. I've set two rules to capture tokens that match \"unemployment\" and \"unemployment rate\". In a sentence like \"Brazil's unemployment rate is 13%.\" I get two matches: [unemployment] and [unemployment, rate]. I pretty much copied the code from the lightning tour stated below:\n def merge_phrases(matcher, doc, match_idx, matches):\n     \"\"\"\n     Merge a phrase: ['unemployment', 'rate'] => ['unemployment rate'].\n     Be careful: token indices will be changed.\n     To avoid problems, merge all the phrases once we're called on the last match.\n \n     :param matcher: spacy.matcher.Matcher: token matcher\n     :param doc: spacy.tokens.Document: parsed doc\n     :param match_idx: int: token index of current match\n     :param matches: list: of spacy.tokens.Token\n     :return: list\n     \"\"\"\n     if match_idx != len(matches)-1:\n         return None\n     # Get Span objects\n     spans = [(ent_id, label, doc[start: end]) for ent_id, label, start, end in matches]\n     logger.info(\"Got Spans %s\", spans)\n     for ent_id, label, span in spans:\n         logger.info(\"Merging ent_id %s label %s span %s\", ent_id, label, span)\n         span.merge(label=label, tag='NNP' if label else span.root.tag_)\n     logger.info(\"Merged Spans %s\", spans)```\n Now, if i leave the logger.info(\"Merged Spans %s\", spans) after the for loop in the sentence above, I'd get something like:\n <denchmark-code>-- Logging error ---\n Traceback (most recent call last):\n   File \"/Users/thiago/miniconda2/envs/factutils/lib/python3.5/logging/__init__.py\", line 980, in emit\n     msg = self.format(record)\n   File \"/Users/thiago/miniconda2/envs/factutils/lib/python3.5/logging/__init__.py\", line 830, in format\n     return fmt.format(record)\n   File \"/Users/thiago/miniconda2/envs/factutils/lib/python3.5/logging/__init__.py\", line 567, in format\n     record.message = record.getMessage()\n   File \"/Users/thiago/miniconda2/envs/factutils/lib/python3.5/logging/__init__.py\", line 330, in getMessage\n     msg = msg % self.args\n   File \"spacy/tokens/span.pyx\", line 72, in spacy.tokens.span.Span.__repr__ (spacy/tokens/span.cpp:4024)\n   File \"spacy/tokens/span.pyx\", line 191, in spacy.tokens.span.Span.text.__get__ (spacy/tokens/span.cpp:6574)\n   File \"spacy/tokens/span.pyx\", line 198, in spacy.tokens.span.Span.text_with_ws.__get__ (spacy/tokens/span.cpp:6727)\n   File \"spacy/tokens/span.pyx\", line 87, in __iter__ (spacy/tokens/span.cpp:4472)\n   File \"spacy/tokens/span.pyx\", line 130, in spacy.tokens.span.Span._recalculate_indices (spacy/tokens/span.cpp:5060)\n IndexError: Error calculating span: Can't find end\n </denchmark-code>\n \n Note that if I comment out the last logging statement, not exception is thrown (there's more stack trace if needed). The funny thing is that the first logging statement gives me this 2017-01-29 20:28:56,240 | INFO | text.py-0043 | Got Spans [(1510408, 0, unemployment), (1510409, 0, unemployment rate)] (which are the tokens I want to merge).  I'm raising this here because on the example I posted,  the results of matcher.match becomes  [unemployment rate] and [unemployment rate, is] (where as I expected only the former).\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "elyase", "commentT": "2017-04-24T19:31:27Z", "comment_text": "\n \t\tHey does anyone still have a problem with this?  I am running Spacy 1.8 and still run into this error with and without the workaround of casting it to a list\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "elyase", "commentT": "2017-04-25T10:27:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/patcollis34>@patcollis34</denchmark-link>\n  the safer way I approached the issue was to leave the tokens unmerged, so the matcher just returns a list of matches and then write a function that removes overlapping matches contained in bigger matches.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "elyase", "commentT": "2017-09-07T21:44:41Z", "comment_text": "\n \t\tThis issue should not have been closed because it is still present in Spacy 2.0 alpha. Merging tokens (compounds, entities, matches, etc.) often results in this IndexError: Error calculating span: Can't find start.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "elyase", "commentT": "2017-11-28T18:56:59Z", "comment_text": "\n \t\tIn my experience this is cause by multiple merge calls in one loop.\n For example, I could call this:\n for np in doc.noun_chunks:\n     np.merge(tag=np.root.tag_, lemma=np.lemma_, ent_type=np.root.ent_type_)\n Now, say in the same loop I wanted to call a different merge whereby if all the POS tags weren't of the same type, I sub out the mismatched tags and merge only those that have a homogeneous structure. My code would look something like this:\n for np in doc.noun_chunks:\n     _tags = []\n \n     current_segment = []\n     for idx, _part in enumerate(np):\n          _tags.append(_part.tag_)\n                 \n          if _part.tag_ in ['NNP', 'NNS','NN']:\n              current_segment.append(_part)\n          elif len(current_segment) > 1:\n              segments.append(current_segment)\n              current_segment = []\n \n         if idx == (len(np) - 1) and len(current_segment) > 1:\n             segments.append(current_segment)\n \n      if len(set(_tags)) == 1:\n          np.merge(tag=np.root.tag_, lemma=np.lemma_, ent_type=np.root.ent_type_)\n      elif len(segments) > 0:\n          for _segment in segments:\n                 if len(_segment) > 1:\n                     doc.merge(start_idx=_segment[0].idx, end_idx=_segment[len(_segment) - 1].idx + len(_segment[len(_segment) - 1]),\n                               tag=_segment[0].tag_,\n                               lemma=' '.join([token.text for token in _segment]),\n                               ent_type=_segment[0].ent_type_)\n This will fail. It will probably succeed in some cases solely because nothing was merged in the wrong place :) But it will fail eventually since you will be modifying the state of the actual document whilst iterating over it.\n It's safer to extract one of the merges and do the merge outside of the loop.\n e.g.\n segments = []\n \n for np in doc.noun_chunks:\n     _tags = []\n \n     current_segment = []\n     for idx, _part in enumerate(np):\n         _tags.append(_part.tag_)\n \n         if _part.tag_ in AspectBasedSentimentAnalyser.T_TYPE or _part.lemma in ['south', 'north', 'east',\n                                                                                 'west']:\n             current_segment.append(_part)\n         elif len(current_segment) > 1:\n             segments.append(current_segment)\n             current_segment = []\n \n         if idx == (len(np) - 1) and len(current_segment) > 1:\n             segments.append(current_segment)\n \n     if len(set(_tags)) == 1:\n         np.merge(tag=np.root.tag_, lemma=np.lemma_, ent_type=np.root.ent_type_)\n \n if len(segments) > 0:\n     for _segment in segments:\n         if len(_segment) > 1:\n             doc.merge(start_idx=_segment[0].idx,\n                       end_idx=_segment[len(_segment) - 1].idx + len(_segment[len(_segment) - 1]),\n                       tag=_segment[0].tag_,\n                       lemma=' '.join([token.text for token in _segment]),\n                       ent_type=_segment[0].ent_type_)\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "elyase", "commentT": "2018-03-08T05:11:30Z", "comment_text": "\n \t\ttry this merge_file\n <denchmark-link:https://github.com/explosion/spaCy/files/1791904/merge_file.txt>merge_file.txt</denchmark-link>\n \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "elyase", "commentT": "2018-05-07T21:53:21Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "cdc10e9a1ca68e29c6c97e57325962d60c86e103", "commit_author": "Matthew Honnibal", "commitT": "2016-05-20 10:14:06+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\doc.pyx", "file_new_name": "spacy\\tokens\\doc.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "250,251,252,253,254,256,257,258", "deleted_lines": "251"}}}}}}