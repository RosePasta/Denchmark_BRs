{"BR": {"BR_id": "6060", "BR_author": "kinghuang", "BRopenT": "2020-09-13T02:30:33Z", "BRcloseT": "2020-09-22T19:53:34Z", "BR_text": {"BRsummary": "Calling retokenize after setting custom token norm applies norm to wrong token", "BRdescription": "\n Setting a custom norm on a token then retokenizing causes the custom norm to be applied to both the original token (which has now moved) and whatever token is now at the original token's index (due to retokenization).\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n import spacy\n \n nlp = spacy.load(\"en\")\n doc = nlp(\"The quick brownfoxjumpsoverthe lazy dog w/ white spots.\")\n \n # Set custom norm on the w/ token.\n doc[5].norm_ = \"with\"\n \n # Retokenize to split out the words in the token at doc[2].\n token = doc[2]\n with doc.retokenize() as retokenizer:\n   retokenizer.split(token, [\"brown\", \"fox\", \"jumps\", \"over\", \"the\"], heads=[(token, idx) for idx in range(5)])\n \n # The new token at index 5 is \"over\". But, its norm is incorrectly set to \"with\".\n # The previous \"w/\" token is now at index 9, and still has the custom norm set earlier.\n assert doc[9].text  == \"w/\"   # OK\n assert doc[9].norm_ == \"with\" # OK\n assert doc[5].text  == \"over\" # OK\n assert doc[5].norm_ == \"over\" # Error - The new \"over\" token has the \"w/\" token's custom norm.\n <denchmark-h:h2>Info about spaCy</denchmark-h>\n \n \n spaCy version: 2.3.2\n Platform: Linux-4.19.76-linuxkit-x86_64-with-glibc2.2.5\n Python version: 3.8.5\n Models: en\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kinghuang", "commentT": "2020-09-13T17:47:39Z", "comment_text": "\n \t\tThanks for the report! Definitely looks like a bug, we'll look into it.\n \t\t"}}}, "commit": {"commit_id": "e4acb286582477caaf5486833781c5802374d171", "commit_author": "Adriane Boyd", "commitT": "2020-09-22 21:53:33+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\tests\\doc\\test_retokenize_split.py", "file_new_name": "spacy\\tests\\doc\\test_retokenize_split.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219", "deleted_lines": null, "method_info": {"method_name": "test_doc_retokenizer_split_norm", "method_params": "en_vocab", "method_startline": "203", "method_endline": "219"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\_retokenize.pyx", "file_new_name": "spacy\\tokens\\_retokenize.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "358", "deleted_lines": null}}}}}}