{"BR": {"BR_id": "522", "BR_author": "xuanyiguang", "BRopenT": "2016-10-11T19:09:31Z", "BRcloseT": "2016-10-23T13:28:49Z", "BR_text": {"BRsummary": "vector_norm and similarity value incorrect", "BRdescription": "\n Somehow vector_norm is incorrectly calculated.\n <denchmark-code>import spacy\n import numpy as np\n nlp = spacy.load(\"en\")\n # using u\"apples\" just as an example\n apples = nlp.vocab[u\"apples\"]\n print apples.vector_norm\n # prints 1.4142135381698608, or sqrt(2)\n print np.sqrt(np.dot(apples.vector, apples.vector))\n # prints 1.0\n </denchmark-code>\n \n Then vector_norm is used in similarity, which always returns a value that is always half of the correct value.\n <denchmark-code>def similarity(self, other):\n     if self.vector_norm == 0 or other.vector_norm == 0:\n         return 0.0\n     return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)\n </denchmark-code>\n \n It is OK if the use case is to rank similarity scores for synonyms. But the cosine similarity score itself is incorrect.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xuanyiguang", "commentT": "2016-10-11T19:11:44Z", "comment_text": "\n \t\tThanks! Will figure this out.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xuanyiguang", "commentT": "2016-10-23T13:03:07Z", "comment_text": "\n \t\tI think this is fixed in 1.0, but this bug makes me uneasy because I don't feel like I really understand what was wrong. I haven't had time to test 0.101.0 yet, but: you say the cosine was always half? I can't figure out why that should be...\n What I've come up with is that this calculation looks unreliable:\n         for orth, lex_addr in self._by_orth.items():\n             lex = <LexemeC*>lex_addr\n             if lex.lower < vectors.size():\n                 lex.vector = vectors[lex.lower]\n                 for i in range(vec_len):\n                     lex.l2_norm += (lex.vector[i] * lex.vector[i])\n                 lex.l2_norm = math.sqrt(lex.l2_norm)\n             else:\n                 lex.vector = EMPTY_VEC\n The lex.l2_norm value is possibly uninitialised, and so there may be a problem there. Passing a 32 bit float to the Python function math.sqrt is also suspicious. But if this was the problem, the results should have been \"unreliable, always wrong\". Always half?? Unsettling!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "xuanyiguang", "commentT": "2016-10-23T13:28:49Z", "comment_text": "\n \t\tGot it now.\n The previous default vectors were already normalized. This led to a value of lex.l2_norm = 1 being stored in the lexemes.bin file. This was then read back out into the LexemeC struct when the vocabulary was deserialised.\n Later, I added the capability to load custom word vectors, which meant the L2 norm had to be calculated. However, I didn't initialised the value of lex.l2_norm to 0 before computing the new norm. Since the default vectors were normalised, the initial value was always 1, and the eventual norm was sqrt(1+1). This explains why the similarity was consistently half.\n No tests checked the exact value returned by the similarity function. They only sanity-checked relative values. This has since been addressed.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "xuanyiguang", "commentT": "2018-05-09T07:39:15Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "2c3a67b693da506ca4b523743969ee6757e98b22", "commit_author": "Matthew Honnibal", "commitT": "2016-10-23 14:49:31+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\doc.pyx", "file_new_name": "spacy\\tokens\\doc.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4,254,256,258,259", "deleted_lines": "9,255,257,258"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\span.pyx", "file_new_name": "spacy\\tokens\\span.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "6,139,141,143,144", "deleted_lines": "6,140,142,143"}}}}}}