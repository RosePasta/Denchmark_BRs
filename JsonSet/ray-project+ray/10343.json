{"BR": {"BR_id": "10343", "BR_author": "richardliaw", "BRopenT": "2020-08-26T21:15:48Z", "BRcloseT": "2020-09-05T20:14:08Z", "BR_text": {"BRsummary": "[core] Raylet does not start up properly on remote instance", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n I've started a 2 node cluster, but the remote node has to retry 10 times in order to startup.\n <denchmark-code>rliaw@ray-gpu-docker-worker-97ecf362:/tmp/ray/session_latest/logs$ ls\n log_monitor.10.err   plasma_store.11.out                                                                    raylet.11.out    reporter.1.out\n log_monitor.10.out   plasma_store.12.err                                                                    raylet.12.err    reporter.2.err\n log_monitor.11.err   plasma_store.12.out                                                                    raylet.12.out    reporter.2.out\n log_monitor.11.out   plasma_store.1.err                                                                     raylet.1.err     reporter.3.err\n log_monitor.12.err   plasma_store.1.out                                                                     raylet.1.out     reporter.3.out\n log_monitor.12.out   plasma_store.2.err                                                                     raylet.2.err     reporter.4.err\n log_monitor.1.err    plasma_store.2.out                                                                     raylet.2.out     reporter.4.out\n log_monitor.1.out    plasma_store.3.err                                                                     raylet.3.err     reporter.5.err\n log_monitor.2.err    plasma_store.3.out                                                                     raylet.3.out     reporter.5.out\n log_monitor.2.out    plasma_store.4.err                                                                     raylet.4.err     reporter.6.err\n log_monitor.3.err    plasma_store.4.out                                                                     raylet.4.out     reporter.6.out\n log_monitor.3.out    plasma_store.5.err                                                                     raylet.5.err     reporter.7.err\n log_monitor.4.err    plasma_store.5.out                                                                     raylet.5.out     reporter.7.out\n log_monitor.4.out    plasma_store.6.err                                                                     raylet.6.err     reporter.8.err\n log_monitor.5.err    plasma_store.6.out                                                                     raylet.6.out     reporter.8.out\n log_monitor.5.out    plasma_store.7.err                                                                     raylet.7.err     reporter.9.err\n log_monitor.6.err    plasma_store.7.out                                                                     raylet.7.out     reporter.9.out\n log_monitor.6.out    plasma_store.8.err                                                                     raylet.8.err     reporter.err\n log_monitor.7.err    plasma_store.8.out                                                                     raylet.8.out     reporter.out\n log_monitor.7.out    plasma_store.9.err                                                                     raylet.9.err     worker-308de9ed75f814ffdea62d08393b3330a48e16b9-13529.err\n log_monitor.8.err    plasma_store.9.out                                                                     raylet.9.out     worker-308de9ed75f814ffdea62d08393b3330a48e16b9-13529.out\n log_monitor.8.out    plasma_store.err                                                                       raylet.err       worker-35d505598ecd15b326ea158ca3597109e8b84fec-13530.err\n log_monitor.9.err    plasma_store.out                                                                       raylet.out       worker-35d505598ecd15b326ea158ca3597109e8b84fec-13530.out\n log_monitor.9.out    python-core-worker-336a05fc3287e69c95972742c25fff720d52d688.20200826-200358.13529.log  reporter.10.err  worker-3877e3561edd1e1db1576dbbdd6f091412dc4346-0100-13528.err\n log_monitor.err      python-core-worker-49f675a34a9fc5484085e1192f001ffc8e51720d.20200826-200358.13530.log  reporter.10.out  worker-3877e3561edd1e1db1576dbbdd6f091412dc4346-0100-13528.out\n log_monitor.out      python-core-worker-b715d4ed0005beaa6a77909b20bb6d1fc1576ecc.20200826-200358.13528.log  reporter.11.err  worker-3877e3561edd1e1db1576dbbdd6f091412dc4346-13528.err\n old                  python-core-worker-cf9cd0d5f987e7bddd7b1fd6ff0f95955c3d3779.20200826-200358.13527.log  reporter.11.out  worker-3877e3561edd1e1db1576dbbdd6f091412dc4346-13528.out\n plasma_store.10.err  raylet.10.err                                                                          reporter.12.err  worker-4456bccd1793ce975b7f1144fb29665e1002f101-13527.err\n plasma_store.10.out  raylet.10.out                                                                          reporter.12.out  worker-4456bccd1793ce975b7f1144fb29665e1002f101-13527.out\n plasma_store.11.err  raylet.11.err                                                                          reporter.1.err\n </denchmark-code>\n \n All of the raylet error files look like:\n <denchmark-code>E0826 20:02:15.725277270   12954 server_chttp2.cc:40]        {\"created\":\"@1598472135.725176780\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/com_github_grpc_grpc/src/c\n ore/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":394,\"referenced_errors\":[{\"created\":\"@1598472135.725174514\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/com_\n github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":341,\"referenced_errors\":[{\"created\":\"@1598472135.725159248\",\"description\":\"Unable to configure socket\",\"fd\":32,\"file\":\"external\n /com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":208,\"referenced_errors\":[{\"created\":\"@1598472135.725152253\",\"description\":\"Address already in use\",\"errno\":98\n ,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":181,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1598472135.72517376\n 0\",\"description\":\"Unable to configure socket\",\"fd\":32,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":208,\"referenced_errors\":[{\"created\":\"@1\n 598472135.725170986\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":181,\"os_error\":\"Addres\n s already in use\",\"syscall\":\"bind\"}]}]}]}\n *** Aborted at 1598472135 (unix time) try \"date -d @1598472135\" if you are using GNU date ***\n PC: @                0x0 (unknown)\n *** SIGSEGV (@0x55d400000058) received by PID 12954 (TID 0x7ff6e633b7c0) from PID 88; stack trace: ***\n     @     0x7ff6e58990e0 (unknown)\n     @     0x55d44e85c782 grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()\n     @     0x55d44e4fc199 ray::rpc::ObjectManagerService::WithAsyncMethod_Push<>::RequestPush()\n     @     0x55d44e50bdfb ray::rpc::ServerCallFactoryImpl<>::CreateCall()\n     @     0x55d44e785c69 ray::rpc::GrpcServer::Run()\n     @     0x55d44e50045e ray::ObjectManager::StartRpcService()\n     @     0x55d44e510f1c ray::ObjectManager::ObjectManager()\n     @     0x55d44e466162 ray::raylet::Raylet::Raylet()\n     @     0x55d44e43fc3d _ZZ4mainENKUlN3ray6StatusEN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEE_clES0_SD_\n     @     0x55d44e440c41 _ZNSt17_Function_handlerIFvN3ray6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEZ4mainEUlS1_SE_E_E9_M_invokeERKSt9_Any_dat\n aS1_SG_\n     @     0x55d44e5bd6ac _ZZN3ray3gcs28ServiceBasedNodeInfoAccessor22AsyncGetInternalConfigERKSt8functionIFvNS_6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pair\n IKSsSsEEEEEEEENKUlRKS3_RKNS_3rpc22GetInternalConfigReplyEE_clESO_SS_\n     @     0x55d44e56f39f _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc22GetInternalConfigReplyEEZNS4_12GcsRpcClient17GetInternalConfigERKNS4_24GetInternalConfigRequestERKSt8functionIS8_EEUl\n S3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_\n     @     0x55d44e56f49d ray::rpc::ClientCallImpl<>::OnReplyReceived()\n     @     0x55d44e49d690 _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10err\n or_codeEm\n     @     0x55d44eaef54f boost::asio::detail::scheduler::do_run_one()\n     @     0x55d44eaf0a51 boost::asio::detail::scheduler::run()\n     @     0x55d44eaf1a82 boost::asio::io_context::run()\n     @     0x55d44e421730 main\n     @     0x7ff6e50ea2e1 __libc_start_main\n     @     0x55d44e4328b1 (unknown)\n </denchmark-code>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n Latest. python=3.7.\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n <denchmark-code># An unique identifier for the head node and workers of this cluster.\n cluster_name: gpu-docker\n \n # The minimum number of workers nodes to launch in addition to the head\n # node. This number should be >= 0.\n min_workers: 1\n \n # The maximum number of workers nodes to launch in addition to the head\n # node. This takes precedence over min_workers.\n max_workers: 1\n \n # The initial number of worker nodes to launch in addition to the head\n # node. When the cluster is first brought up (or when it is refreshed with a\n # subsequent `ray up`) this number of nodes will be started.\n initial_workers: 0\n \n # Whether or not to autoscale aggressively. If this is enabled, if at any point\n #   we would start more workers, we start at least enough to bring us to\n #   initial_workers.\n autoscaling_mode: default\n \n # This executes all commands on all nodes in the docker container,\n # and opens all the necessary ports to support the Ray cluster.\n # Empty string means disabled.\n # docker:\n #     image: \"tensorflow/tensorflow:1.13.1-gpu-py3\"\n #     container_name: \"ray-nvidia-docker-test\" # e.g. ray_docker\n #     run_options:\n #       - --runtime=nvidia\n \n     # # Example of running a GPU head with CPU workers\n     # head_image: \"tensorflow/tensorflow:1.13.1-gpu-py3\"\n     # head_run_options:\n     #     - --runtime=nvidia\n \n     # worker_image: \"ubuntu:18.04\"\n     # worker_run_options: []\n \n \n # The autoscaler will scale up the cluster to this target fraction of resource\n # usage. For example, if a cluster of 10 nodes is 100% busy and\n # target_utilization is 0.8, it would resize the cluster to 13. This fraction\n # can be decreased to increase the aggressiveness of upscaling.\n # This value must be less than 1.0 for scaling to happen.\n target_utilization_fraction: 0.8\n \n # If a node is idle for this many minutes, it will be removed.\n idle_timeout_minutes: 5\n \n # Cloud-provider specific configuration.\n provider:\n     type: gcp\n     region: us-central1\n     availability_zone: us-central1-a\n     project_id: ~~~~~~~~~~~~~~ # Globally unique project id\n \n # How Ray will authenticate with newly launched nodes.\n auth:\n     ssh_user: ubuntu\n # By default Ray creates a new private keypair, but you can also use your own.\n # If you do so, make sure to also set \"KeyName\" in the head and worker node\n # configurations below. This requires that you have added the key into the\n # project wide meta-data.\n #    ssh_private_key: /path/to/your/key.pem\n \n # Provider-specific config for the head node, e.g. instance type. By default\n # Ray will auto-configure unspecified fields such as subnets and ssh-keys.\n # For more documentation on available fields, see:\n # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert\n head_node:\n     machineType: n1-standard-4\n     disks:\n       - boot: true\n         autoDelete: true\n         type: PERSISTENT\n         initializeParams:\n           diskSizeGb: 100\n           # See https://cloud.google.com/compute/docs/images for more images\n           sourceImage: projects/ml-images/global/images/c5-deeplearning-tf2-2-2-cu101-v20200701\n     guestAccelerators:\n       - acceleratorType: projects/~~~~~~~~~~~~~~/zones/us-central1-a/acceleratorTypes/nvidia-tesla-p4\n         acceleratorCount: 1\n     scheduling:\n       - onHostMaintenance: TERMINATE\n         preemptible: false\n         automaticRestart: true\n     metadata:\n       - kind: compute#metadata\n         items:\n         - { \"key\": \"install-nvidia-driver\", \"value\": \"True\" }\n \n     # Additional options can be found in in the compute docs at\n     # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert\n \n worker_nodes:\n     machineType: n1-standard-4\n     disks:\n       - boot: true\n         autoDelete: true\n         type: PERSISTENT\n         initializeParams:\n           diskSizeGb: 100\n           # See https://cloud.google.com/compute/docs/images for more images\n           sourceImage: projects/ml-images/global/images/c5-deeplearning-tf2-2-2-cu101-v20200701\n     guestAccelerators:\n       - acceleratorType: projects/~~~~~~~~~~~~~~/zones/us-central1-a/acceleratorTypes/nvidia-tesla-p4\n         acceleratorCount: 1\n     scheduling:\n       - onHostMaintenance: TERMINATE\n         preemptible: false\n         automaticRestart: true\n     metadata:\n       - kind: compute#metadata\n         items:\n         - { \"key\": \"install-nvidia-driver\", \"value\": \"True\" }\n \n \n # Files or directories to copy to the head and worker nodes. The format is a\n # dictionary from REMOTE_PATH: LOCAL_PATH, e.g.\n file_mounts: {\n     /home/ubuntu/train_data.txt: /Users/rliaw/dev/summit-tune-demo/train_data.txt\n }\n \n \n # initialization_commands:\n #     # Wait until nvidia drivers are installed\n #     - >-\n #       timeout 300 bash -c \"\n #           command -v nvidia-smi && nvidia-smi\n #           until [ \\$? -eq 0 ]; do\n #               command -v nvidia-smi && nvidia-smi\n #           done\"\n \n # List of shell commands to run to set up nodes.\n setup_commands:\n     - source /opt/conda/bin/activate && pip install -U pip\n     - source /opt/conda/bin/activate && pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl\n     - source /opt/conda/bin/activate && pip install torch==1.4.0 torchvision==0.5.0\n     - source /opt/conda/bin/activate && pip install transformers\n     - source /opt/conda/bin/activate && pip install wandb\n \n # Custom commands that will be run on the head node after common setup.\n head_setup_commands:\n   - source /opt/conda/bin/activate && pip install google-api-python-client==1.7.8\n \n # Custom commands that will be run on worker nodes after common setup.\n worker_setup_commands: []\n \n # Command to start ray on the head node. You don't need to change this.\n head_start_ray_commands:\n     - source /opt/conda/bin/activate && ray stop\n     - >-\n       ulimit -n 65536;\n       source /opt/conda/bin/activate && ray start\n       --head\n       --port=6379\n       --object-manager-port=8076\n       --autoscaling-config=~/ray_bootstrap_config.yaml\n \n # Command to start source /opt/conda/bin/activate && ray on worker nodes. You don't need to change this.\n worker_start_ray_commands:\n     - source /opt/conda/bin/activate && ray stop\n     - >-\n       ulimit -n 65536;\n       source /opt/conda/bin/activate && ray start\n       --address=$RAY_HEAD_IP:6379\n       --object-manager-port=8076\n </denchmark-code>\n \n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "richardliaw", "commentT": "2020-08-26T21:16:05Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/orgs/ray-project/teams/ray-core>@ray-project/ray-core</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "richardliaw", "commentT": "2020-08-27T18:09:58Z", "comment_text": "\n \t\tMore information on similar issue.\n Setup : autoscaler/kubernetes\n initial_workers: 2\n min_workers: 2\n max_workers: 4\n Observation:\n \n Workers start running in the pod.\n But no Ray-related processes running on them.\n Running : ps -aux| grep ray inside worker-node (kubectl exec)\n Out: root         188  0.0  0.0   5196   740 pts/0    S+   16:14   0:00 grep --color=auto ray\n \n Thus, workers wont  run worker_start_ray_commands OR  ray start successfully ? I was able to reproduce this issue in my minikube env too, which rules out anything wrong with our K8s provider (Google).\n The manual workaround that I am using at the moment  :\n \n kubectl -n ray get pods. Copy worker-pod-id\n kubectl exec -it  <worker-id> -- bin/bash\n ray start --num-cpus=1 --address=$RAY_HEAD_SERVICE_HOST:6379 --object-manager-port=8076 . To get value for $RAY_HEAD_SERVICE_HOST you can either printenv  in worker OR kubectl -n ray get svc and copy the IP of the ray-head svc.\n Do 1-3 in all the worker.\n \n To test what error the StandardAutoscaler is running into, I killed one worker. Thus triggering the Autoscaler to re-span a worker to maintain the min_worker count. Here is the log from the head node.\n <denchmark-code>2020-08-26 12:03:15,238\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.17289352416992188}\n  - NodeIdleSeconds: Min=227 Mean=227 Max=227\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n \n ==> /tmp/ray/session_latest/logs/monitor.err <==\n 2020-08-26 12:03:15,238\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.17289352416992188}\n  - NodeIdleSeconds: Min=227 Mean=227 Max=227\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n \n ==> /tmp/ray/session_2020-08-26_11-59-24_547664_135/logs/monitor.err <==\n 2020-08-26 12:03:20,269\tINFO autoscaler.py:467 -- Cluster status: 1/2 target nodes (0 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.15368938446044922}\n  - NodeIdleSeconds: Min=233 Mean=233 Max=233\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n 2020-08-26 12:03:20,280\tINFO autoscaler.py:447 -- StandardAutoscaler: Queue 1 new nodes for launch\n 2020-08-26 12:03:20,282\tINFO node_launcher.py:74 -- NodeLauncher1: Got 1 nodes to launch.\n 2020-08-26 12:03:20,292\tINFO node_launcher.py:74 -- NodeLauncher1: Launching 1 nodes, type None.\n 2020-08-26 12:03:20,293\tINFO node_provider.py:87 -- KubernetesNodeProvider: calling create_namespaced_pod (count=1).\n 2020-08-26 12:03:20,298\tINFO autoscaler.py:467 -- Cluster status: 1/2 target nodes (1 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.18278002738952637}\n  - NodeIdleSeconds: Min=233 Mean=233 Max=233\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n \n ==> /tmp/ray/session_latest/logs/monitor.err <==\n 2020-08-26 12:03:20,269\tINFO autoscaler.py:467 -- Cluster status: 1/2 target nodes (0 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.15368938446044922}\n  - NodeIdleSeconds: Min=233 Mean=233 Max=233\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n 2020-08-26 12:03:20,280\tINFO autoscaler.py:447 -- StandardAutoscaler: Queue 1 new nodes for launch\n 2020-08-26 12:03:20,282\tINFO node_launcher.py:74 -- NodeLauncher1: Got 1 nodes to launch.\n 2020-08-26 12:03:20,292\tINFO node_launcher.py:74 -- NodeLauncher1: Launching 1 nodes, type None.\n 2020-08-26 12:03:20,293\tINFO node_provider.py:87 -- KubernetesNodeProvider: calling create_namespaced_pod (count=1).\n 2020-08-26 12:03:20,298\tINFO autoscaler.py:467 -- Cluster status: 1/2 target nodes (1 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.18278002738952637}\n  - NodeIdleSeconds: Min=233 Mean=233 Max=233\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n \n ==> /tmp/ray/session_2020-08-26_11-59-24_547664_135/logs/monitor.err <==\n 2020-08-26 12:03:23,211\tINFO log_timer.py:27 -- NodeUpdater: ray-worker-4gc2c: Got remote shell  [LogTimer=230969ms]\n 2020-08-26 12:03:23,211\tINFO log_timer.py:27 -- NodeUpdater: ray-worker-4gc2c: Applied config 6a1cdc5f85138bb386579a27532ac2cc0bacd521  [LogTimer=231063ms]\n 2020-08-26 12:03:23,240\tERROR updater.py:101 -- NodeUpdater: ray-worker-4gc2c: Error executing: Unable to connect to node\n \n Exception in thread Thread-6:\n Traceback (most recent call last):\n   File \"/root/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n     self.run()\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/updater.py\", line 89, in run\n     self.do_update()\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/updater.py\", line 246, in do_update\n     self.wait_ready(deadline)\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/updater.py\", line 238, in wait_ready\n     assert False, \"Unable to connect to node\"\n AssertionError: Unable to connect to node\n \n \n ==> /tmp/ray/session_latest/logs/monitor.err <==\n 2020-08-26 12:03:23,211\tINFO log_timer.py:27 -- NodeUpdater: ray-worker-4gc2c: Got remote shell  [LogTimer=230969ms]\n 2020-08-26 12:03:23,211\tINFO log_timer.py:27 -- NodeUpdater: ray-worker-4gc2c: Applied config 6a1cdc5f85138bb386579a27532ac2cc0bacd521  [LogTimer=231063ms]\n 2020-08-26 12:03:23,240\tERROR updater.py:101 -- NodeUpdater: ray-worker-4gc2c: Error executing: Unable to connect to node\n \n Exception in thread Thread-6:\n Traceback (most recent call last):\n   File \"/root/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n     self.run()\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/updater.py\", line 89, in run\n     self.do_update()\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/updater.py\", line 246, in do_update\n     self.wait_ready(deadline)\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/updater.py\", line 238, in wait_ready\n     assert False, \"Unable to connect to node\"\n AssertionError: Unable to connect to node\n \n \n ==> /tmp/ray/session_2020-08-26_11-59-24_547664_135/logs/monitor.err <==\n 2020-08-26 12:03:25,383\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.15513324737548828}\n  - NodeIdleSeconds: Min=238 Mean=238 Max=238\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n 2020-08-26 12:03:25,406\tERROR autoscaler.py:129 -- StandardAutoscaler: Error during autoscaling.\n Traceback (most recent call last):\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/autoscaler.py\", line 127, in update\n     self._update()\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/autoscaler.py\", line 238, in _update\n     self.load_metrics.mark_active(self.provider.internal_ip(node_id))\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/kubernetes/node_provider.py\", line 65, in internal_ip\n     pod = core_api().read_namespaced_pod_status(node_id, self.namespace)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 19328, in read_namespaced_pod_status\n     (data) = self.read_namespaced_pod_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 19413, in read_namespaced_pod_status_with_http_info\n     collection_formats=collection_formats)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 345, in call_api\n     _preload_content, _request_timeout)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 176, in __call_api\n     _request_timeout=_request_timeout)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 366, in request\n     headers=headers)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 241, in GET\n     query_params=query_params)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 231, in request\n     raise ApiException(http_resp=r)\n kubernetes.client.rest.ApiException: (404)\n Reason: Not Found\n HTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Wed, 26 Aug 2020 19:03:25 GMT', 'Content-Length': '200'})\n HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"pods \\\"ray-worker-4gc2c\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"ray-worker-4gc2c\",\"kind\":\"pods\"},\"code\":404}\n \n \n \n ==> /tmp/ray/session_latest/logs/monitor.err <==\n 2020-08-26 12:03:25,383\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (2 updating) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.15513324737548828}\n  - NodeIdleSeconds: Min=238 Mean=238 Max=238\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n 2020-08-26 12:03:25,406\tERROR autoscaler.py:129 -- StandardAutoscaler: Error during autoscaling.\n Traceback (most recent call last):\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/autoscaler.py\", line 127, in update\n     self._update()\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/autoscaler.py\", line 238, in _update\n     self.load_metrics.mark_active(self.provider.internal_ip(node_id))\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/kubernetes/node_provider.py\", line 65, in internal_ip\n     pod = core_api().read_namespaced_pod_status(node_id, self.namespace)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 19328, in read_namespaced_pod_status\n     (data) = self.read_namespaced_pod_status_with_http_info(name, namespace, **kwargs)  # noqa: E501\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 19413, in read_namespaced_pod_status_with_http_info\n     collection_formats=collection_formats)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 345, in call_api\n     _preload_content, _request_timeout)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 176, in __call_api\n     _request_timeout=_request_timeout)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 366, in request\n     headers=headers)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 241, in GET\n     query_params=query_params)\n   File \"/root/anaconda3/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 231, in request\n     raise ApiException(http_resp=r)\n kubernetes.client.rest.ApiException: (404)\n Reason: Not Found\n HTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Wed, 26 Aug 2020 19:03:25 GMT', 'Content-Length': '200'})\n HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"pods \\\"ray-worker-4gc2c\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"ray-worker-4gc2c\",\"kind\":\"pods\"},\"code\":404}\n \n \n \n ==> /tmp/ray/session_2020-08-26_11-59-24_547664_135/logs/monitor.err <==\n 2020-08-26 12:03:30,422\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (1 updating) (1 failed to update) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.173508882522583}\n  - NodeIdleSeconds: Min=243 Mean=243 Max=243\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n 2020-08-26 12:03:30,462\tINFO updater.py:74 -- NodeUpdater: ray-worker-6k5hj: Updating to 6a1cdc5f85138bb386579a27532ac2cc0bacd521\n 2020-08-26 12:03:30,500\tINFO updater.py:194 -- NodeUpdater: ray-worker-6k5hj: Waiting for remote shell...\n \n ==> /tmp/ray/session_latest/logs/monitor.err <==\n 2020-08-26 12:03:30,422\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (1 updating) (1 failed to update) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.173508882522583}\n  - NodeIdleSeconds: Min=243 Mean=243 Max=243\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n 2020-08-26 12:03:30,462\tINFO updater.py:74 -- NodeUpdater: ray-worker-6k5hj: Updating to 6a1cdc5f85138bb386579a27532ac2cc0bacd521\n 2020-08-26 12:03:30,500\tINFO updater.py:194 -- NodeUpdater: ray-worker-6k5hj: Waiting for remote shell...\n \n ==> /tmp/ray/session_2020-08-26_11-59-24_547664_135/logs/monitor.err <==\n 2020-08-26 12:03:35,459\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (2 updating) (1 failed to update) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.15118980407714844}\n  - NodeIdleSeconds: Min=248 Mean=248 Max=248\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n \n ==> /tmp/ray/session_latest/logs/monitor.err <==\n 2020-08-26 12:03:35,459\tINFO autoscaler.py:467 -- Cluster status: 2/2 target nodes (0 pending) (2 updating) (1 failed to update) (bringup=True)\n  - MostDelayedHeartbeats: {'172.17.0.6': 0.15118980407714844}\n  - NodeIdleSeconds: Min=248 Mean=248 Max=248\n  - NumNodesConnected: 1\n  - NumNodesUsed: 0.0\n  - ResourceUsage: 0.0/1.0 CPU, 0.0 GiB/1.21 GiB memory, 0.0 GiB/0.42 GiB object_store_memory\n  - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0\n \n \n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "richardliaw", "commentT": "2020-08-27T18:17:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/raulchen>@raulchen</denchmark-link>\n  giving to you for triage; feel free to send back.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "richardliaw", "commentT": "2020-08-27T21:43:37Z", "comment_text": "\n \t\tI'm able to get the same stack trace as <denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  on a cluster of 4 machines started with the Ray cluster launcher. The error message appears at the beginning of the job, I think before any application code has run.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "richardliaw", "commentT": "2020-08-28T19:33:39Z", "comment_text": "\n \t\tDigging into this a bit more, it seems to have something to do with how ray processes are torn down and/or the autoscaler. The stacktrace appears to be caused by the object manager binding to a port that's already in use, probably from the previous raylet that ran on the same node. Sometimes it's able to resolve itself, but sometimes the autoscaler gets into a bad loop where it keeps trying to restart a worker raylet only to have it crash again.\n <denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n , any ideas?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "richardliaw", "commentT": "2020-08-28T19:43:17Z", "comment_text": "\n \t\tOkay, it does seem to have something to do with how ray processes get torn down. I ssh'ed into a worker node on a cluster that was experiencing the loop and noticed there was a leftover raylet process, probably from a previous version of the cluster. After running ray stop, I got the message saying that \"Stopped all n Ray processes\", but the raylet process was still alive.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "richardliaw", "commentT": "2020-08-28T19:44:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  <denchmark-link:https://github.com/edoakes>@edoakes</denchmark-link>\n  did we make any changes to teardown / port configuration that may affect this?\n <denchmark-link:https://github.com/stephanie-wang>@stephanie-wang</denchmark-link>\n  I can't think of any cause here, maybe bisect time?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "richardliaw", "commentT": "2020-08-28T19:46:11Z", "comment_text": "\n \t\tI've tried bisecting, but didn't get very far. It's pretty finicky to reproduce since it seems to be based on timing. I'm pretty sure that this was introduced after 0.8.7, though.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "richardliaw", "commentT": "2020-08-28T19:55:26Z", "comment_text": "\n \t\tCan you try reverting to 4f8fef134e1611e6b83a3156af8a26db762cb0d3? (or before then)?\n This was the last change to ray stop past 0.8.7.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "richardliaw", "commentT": "2020-08-28T19:56:29Z", "comment_text": "\n \t\tThis is:\n <denchmark-code>commit 40b8e35d61982b149e88fc09effd001879649bff (HEAD) <------------- suspect?\n Author: Maksim Smolin <maximsmol@gmail.com>\n Date:   Tue Aug 11 09:58:23 2020 -0700\n \n     [cli] New logging for the rest of the `ray` commands (#9984)\n \n     Co-authored-by: Richard Liaw <rliaw@berkeley.edu>\n \n commit 4f8fef134e1611e6b83a3156af8a26db762cb0d3\n Author: Amog Kamsetty <amogkam@users.noreply.github.com>\n Date:   Tue Aug 11 09:05:57 2020 -0700\n \n     [Tune] Remove `checkpoint_at_end` from tune+serve docs (#10034)\n \n commit 0400a88bf1b97a4e6b2021a3a2e9c25310b22170\n Author: Basasuya <basasuya@gmail.com>\n Date:   Tue Aug 11 17:36:07 2020 +0800\n \n     [EVENT] Basic Function and Definition (#9657)\n </denchmark-code>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "richardliaw", "commentT": "2020-08-28T22:58:02Z", "comment_text": "\n \t\tBoth of these commits seem to have the same issue :( Let me try bisecting again and see if I can figure it out.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "richardliaw", "commentT": "2020-08-29T00:00:46Z", "comment_text": "\n \t\tRealizing now that my previous attempts to bisect were probably getting messed up because of a zombie raylet process from who knows which commit :/ It's still turning out to be pretty difficult to consistently check whether a commit is OK or not, but for now using ray stop --force instead of ray stop in the cluster yaml seems to work.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "richardliaw", "commentT": "2020-09-01T03:51:57Z", "comment_text": "\n \t\tI couldn't find any clue from the error information. I suspect this is something related with the environment. Did you look into the core dump file?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "richardliaw", "commentT": "2020-09-02T00:29:31Z", "comment_text": "\n \t\tOkay, <denchmark-link:https://github.com/ray-project/ray/commit/1dc018d0bb209e7fec81587dee0ad637cc5f3c99>this</denchmark-link>\n  is the first bad commit. <denchmark-link:https://gist.github.com/stephanie-wang/1fb8290e7edb630c33fe9c1dc1507814>Here</denchmark-link>\n  is the test script that I used. It will hang and should continually print the stacktraces shown in the original issue if the commit is bad. You also have to make sure to first clean the cluster with , then restart again with the normal .\n Given that this PR is only needed for GCS restart (as I understand it), I think we should revert this PR ASAP. I tried to do it, but I wasn't totally clear on how to resolve the git conflicts. Can you help out, <denchmark-link:https://github.com/raulchen>@raulchen</denchmark-link>\n  <denchmark-link:https://github.com/ffbin>@ffbin</denchmark-link>\n ?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "richardliaw", "commentT": "2020-09-02T02:22:18Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/stephanie-wang>@stephanie-wang</denchmark-link>\n  , i prefer to add a configuration with detect turned off by default, so that the changes are minimal. What is your suggestion? If you agree with this proposal, I will submit a pr, thanks.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "richardliaw", "commentT": "2020-09-02T03:23:31Z", "comment_text": "\n \t\tSure, but let's make sure to keep the number of configuration parameters minimal. Ideally, we should only have one for any code related to GCS restart. Thanks!\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "richardliaw", "commentT": "2020-09-03T18:05:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ffbin>@ffbin</denchmark-link>\n , any update on this? This is high priority since the branch cutoff for 1.0 is very soon.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "richardliaw", "commentT": "2020-09-04T00:17:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/stephanie-wang>@stephanie-wang</denchmark-link>\n  I will submit a pr today. The company has outgoing these days, so it's a little delayed, Sorry.\n \t\t"}}}, "commit": {"commit_id": "4f02ad4ef9d4277d100fcff1c05578a99c5c71d2", "commit_author": "Stephanie Wang", "commitT": "2020-09-05 13:14:07-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\tests\\test_gcs_fault_tolerance.py", "file_new_name": "python\\ray\\tests\\test_gcs_fault_tolerance.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "24,25,26,27,49,50,51,52,70,71,72,73", "deleted_lines": "24,25,47,48,66,67"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\tests\\test_multi_node.py", "file_new_name": "python\\ray\\tests\\test_multi_node.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "15,16,17,18,19,20,21,22,23,24,25,26", "deleted_lines": null, "method_info": {"method_name": "test_remote_raylet_cleanup", "method_params": "ray_start_cluster", "method_startline": "15", "method_endline": "26"}}, "hunk_1": {"Ismethod": 1, "added_lines": "22,23", "deleted_lines": null, "method_info": {"method_name": "test_remote_raylet_cleanup.remote_raylets_dead", "method_params": "", "method_startline": "22", "method_endline": "23"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\ray\\common\\ray_config_def.h", "file_new_name": "src\\ray\\common\\ray_config_def.h", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "250", "deleted_lines": "250"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\gcs\\gcs_client\\test\\service_based_gcs_client_test.cc", "file_new_name": "src\\ray\\gcs\\gcs_client\\test\\service_based_gcs_client_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "28,29,30,31,32", "deleted_lines": "28", "method_info": {"method_name": "ray::ServiceBasedGcsClientTest::ServiceBasedGcsClientTest", "method_params": "", "method_startline": "28", "method_endline": "32"}}}}}}}