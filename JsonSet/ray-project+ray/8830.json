{"BR": {"BR_id": "8830", "BR_author": "richardrl", "BRopenT": "2020-06-08T05:22:21Z", "BRcloseT": "2020-06-12T23:38:39Z", "BR_text": {"BRsummary": "Docker + AWS hanging at \"Set tag...\" for autoscaler", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Using latest version of Ray (0.9.0.dev0)\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n If we take the example AWS configuration YAML, given <denchmark-link:https://docs.ray.io/en/master/autoscaling.html?highlight=docker>here</denchmark-link>\n  (under AWS header) and simply change line 26 and 27 to use an actual Docker image:\n <denchmark-link:https://github.com/ray-project/ray/files/4744002/example_full.zip>example_full.zip</denchmark-link>\n \n Running any commands with the autoscaler hangs and eventually fails:\n <denchmark-code>ray up config/example_full.yaml \n 2020-06-08 01:13:10,526 INFO config.py:143 -- _configure_iam_role: Role not specified for head node, using arn:aws:iam::<redacted>:instance-profile/ray-autoscaler-v1\n 2020-06-08 01:13:11,089 INFO config.py:194 -- _configure_key_pair: KeyName not specified for nodes, using ray-autoscaler_us-west-2\n 2020-06-08 01:13:11,365 INFO config.py:235 -- _configure_subnet: SubnetIds not specified for head node, using [('subnet-<redacted>', 'us-west-2b'), ('subnet-<redacted>', 'us-west-2a')]\n 2020-06-08 01:13:11,366 INFO config.py:241 -- _configure_subnet: SubnetId not specified for workers, using [('subnet-<redacted>', 'us-west-2b'), ('subnet-<redacted>', 'us-west-2a')]\n 2020-06-08 01:13:11,725 INFO config.py:261 -- _configure_security_group: SecurityGroupIds not specified for head node, using ray-autoscaler-default (sg-<redacted>)\n 2020-06-08 01:13:11,725 INFO config.py:268 -- _configure_security_group: SecurityGroupIds not specified for workers, using ray-autoscaler-default (sg-<redacted>)\n This will restart cluster services [y/N]: y\n 2020-06-08 01:13:15,214 INFO commands.py:238 -- get_or_create_head_node: Updating files on head node...\n 2020-06-08 01:13:15,215 INFO updater.py:379 -- NodeUpdater: i-<redacted>: Updating to <redacted>\n 2020-06-08 01:13:15,216 INFO updater.py:423 -- NodeUpdater: i-<redacted>: Waiting for remote shell...\n 2020-06-08 01:13:15,422 INFO log_timer.py:22 -- AWSNodeProvider: Set tag ray-node-status=waiting-for-ssh on ['i-<redacted>\n Exception in thread Thread-2:\n Traceback (most recent call last):\n   File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n     self.run()\n   File \"/home/richard/improbable/ray/python/ray/autoscaler/updater.py\", line 383, in run\n     self.do_update()\n   File \"/home/richard/improbable/ray/python/ray/autoscaler/updater.py\", line 450, in do_update\n     self.wait_ready(deadline)\n   File \"/home/richard/improbable/ray/python/ray/autoscaler/updater.py\", line 444, in wait_ready\n     assert False, \"Unable to connect to node\"\n AssertionError: Unable to connect to node\n \n 2020-06-08 01:11:59,679 ERROR commands.py:304 -- get_or_create_head_node: Updating <redacted> failed\n 2020-06-08 01:11:59,701 INFO log_timer.py:22 -- AWSNodeProvider: Set tag ray-node-status=update-failed on ['i-<redacted>']  [LogTimer=333ms]\n \n </denchmark-code>\n \n If I remove the Docker image and name on line 26 and line 27, the YAML file succeeds for any ray up / attach / submit etc. command.\n (Related, it would be nice if we piped out more verbose error messages about why errors like this occur when using the autoscaler.)\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "richardrl", "commentT": "2020-06-08T07:02:52Z", "comment_text": "\n \t\tAfter fixing the above issue by removing the \"allocate_tty\" flag, I am running into issues where the Docker container is not run with tty / not persistent.\n Where is the code that first starts the Docker container when doing \"ray up ...\"?\n I can manually add the run command in \"initialization_commands: []\" but I'm guessing the below error is a bug... If someone explains how Ray team wants the Docker setup to work (which file should contain run \"docker run ...\") I can do a PR.\n <denchmark-code>(pb_venv) richard@richard-desktop:~/improbable/<redacted>$ ray submit --docker config/example_full.yaml <redacted>/torch/train_ray.py \n 2020-06-08 02:03:06,388 INFO commands.py:54 -- Using cached config at /tmp/ray-config-e82fb02128a816538f2a0dc5e8abcfdf5dd594a5\n 2020-06-08 02:03:07,123 INFO updater.py:485 -- NodeUpdater: i-<redacted>: Syncing <redacted>/torch/train_ray.py to ~/train_ray.py...\n 2020-06-08 02:03:07,617 INFO updater.py:200 -- NodeUpdater: i-<redacted>: Waiting for IP...\n 2020-06-08 02:03:07,617 INFO log_timer.py:22 -- NodeUpdater: i-<redacted>: Got IP  [LogTimer=493ms]\n sending incremental file list\n train_ray.py\n \n sent 2,525 bytes  received 35 bytes  1,024.00 bytes/sec\n total size is 8,257  speedup is 3.23\n 2020-06-08 02:03:09,475 INFO updater.py:253 -- NodeUpdater: i-<redacted>: Running docker cp ~/train_ray.py ray_docker:`docker exec ray_docker env | grep HOME | cut -d'=' -f2`/train_ray.py on 34.208.67.2...\n 2020-06-08 02:03:09,476 INFO updater.py:254 -- Begin remote output from <redacted>\n Error: No such container: ray_docker\n Error: No such container:path: ray_docker:/\n Shared connection to <redacted> closed.\n Error: SSH command Failed. See above for the output from the failure.\n \n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "richardrl", "commentT": "2020-06-08T20:09:29Z", "comment_text": "\n \t\tJust to confirm, if you SSH into the node and run docker container ls it does not show anything?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "richardrl", "commentT": "2020-06-09T05:00:27Z", "comment_text": "\n \t\tCommit <denchmark-link:https://github.com/ray-project/ray/commit/daf1ee13da9da96d99fe112970ee01b1f372f360>daf1ee1</denchmark-link>\n   does not solve the issue.\n <denchmark-code>Shared connection to 127.0.0.1 closed.\n 2020-06-09 00:58:18,716 INFO updater.py:254 -- NodeUpdater: localhost: Running docker cp /home/richard/<code>:/home/richard/<code>/ on 127.0.0.1...\n 2020-06-09 00:58:18,716 INFO updater.py:255 -- Begin remote output from 127.0.0.1\n Error: No such container:path: <code>:/home/richard\n \n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "richardrl", "commentT": "2020-06-09T15:03:16Z", "comment_text": "\n \t\tWhat happens if you include the --start flag for ray submit.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "richardrl", "commentT": "2020-06-29T21:32:34Z", "comment_text": "\n \t\tI realize that this issue is closed, but the thread is the most recent and relevant that I could find to the problem that I am having, so I am asking for help.\n I was successfully running an EC2 cluster using Ray last year, and recently attempted to start one up again with the same configuration, but updated release.\n Firstly, I encountered the redis_address deprecation warning, so I changed my ray.init argument to (address=\"auto\"), and the head_ and worker_start_ray_commands to those in a recent version of the example-full.yaml config' (see attached).\n Now, when trying to bring ray up, I am getting an error very similar to the one above:-\n <denchmark-code>bash: no job control in this shell\n Error: no such option: --port\n 2020-06-29 17:24:37,025\tINFO log_timer.py:17 -- NodeUpdater: i-08ab35ed68fbb942a: Ray start commands completed [LogTimer=2355ms]\n 2020-06-29 17:24:37,026\tINFO log_timer.py:17 -- NodeUpdater: i-08ab35ed68fbb942a: Applied config ebfab67c94259a0ba4df6381d5cf2410fa5ef49b [LogTimer=38656ms]\n 2020-06-29 17:24:37,026\tERROR updater.py:348 -- NodeUpdater: i-08ab35ed68fbb942a: Error updating (Exit Status 2) ssh -i /home/haines/.ssh/ray-key2_us-east-1.pem -o ConnectTimeout=120s -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_98734ce2b6/2ebd7d2a8f/%C -o ControlPersist=10s -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 ubuntu@54.210.118.84 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && ulimit -n 65536; ray start --head --port=6379 --autoscaling-config=~/ray_bootstrap_config.yaml'\n Exception in thread Thread-2:\n Traceback (most recent call last):\n   File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n     self.run()\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 351, in run\n     raise e\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 341, in run\n     self.do_update()\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 434, in do_update\n     self.cmd_runner.run(cmd)\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 263, in run\n     self.process_runner.check_call(final_cmd)\n   File \"/usr/lib/python3.6/subprocess.py\", line 311, in check_call\n     raise CalledProcessError(retcode, cmd)\n subprocess.CalledProcessError: Command '['ssh', '-i', '/home/haines/.ssh/ray-key2_us-east-1.pem', '-o', 'ConnectTimeout=120s', '-o', 'StrictHostKeyChecking=no', '-o', 'ControlMaster=auto', '-o', 'ControlPath=/tmp/ray_ssh_98734ce2b6/2ebd7d2a8f/%C', '-o', 'ControlPersist=10s', '-o', 'IdentitiesOnly=yes', '-o', 'ExitOnForwardFailure=yes', '-o', 'ServerAliveInterval=5', '-o', 'ServerAliveCountMax=3', 'ubuntu@54.210.118.84', 'bash', '--login', '-c', '-i', \"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && ulimit -n 65536; ray start --head --port=6379 --autoscaling-config=~/ray_bootstrap_config.yaml'\"]' returned non-zero exit status 2.\n \n 2020-06-29 17:24:37,170\tERROR commands.py:285 -- get_or_create_head_node: Updating 54.210.118.84 failed\n 2020-06-29 17:24:37,184\tINFO log_timer.py:17 -- AWSNodeProvider: Set tag ray-node-status=update-failed on ['i-08ab35ed68fbb942a'] [LogTimer=157ms]\n </denchmark-code>\n \n The head node starts OK, but not the worker.  Is there something else that needs changing in order to substitute the redis address?\n <denchmark-link:https://github.com/ray-project/ray/files/4848130/ray_conf.txt>ray_conf.txt</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "richardrl", "commentT": "2020-06-30T00:02:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/snmhaines>@snmhaines</denchmark-link>\n  What if you try using  instead of ?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "richardrl", "commentT": "2020-06-30T21:41:55Z", "comment_text": "\n \t\tThanks for the quick reply Ian.  I reverted to my previous config, which has ulimit -n 65536; ray start --redis-address=$RAY_HEAD_IP:6379 --object-manager-port=8076 ... as the start_ray_commands, and the cluster started OK.\n I was going to check whether ray.init(redis_address=+\":6379\") still produces a warning for initialization in Python, but have run into a new problem on getting ray up:-\n <denchmark-code>Collecting py-spy>=0.2.0 (from ray)\n   Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n Collecting grpcio (from ray)\n   Downloading https://files.pythonhosted.org/packages/5e/29/1bd649737e427a6bb850174293b4f2b72ab80dd49462142db9b81e1e5c7b/grpcio-1.30.0.tar.gz (19.7MB)\n     Complete output from command python setup.py egg_info:\n     Traceback (most recent call last):\n       File \"<string>\", line 1, in <module>\n       File \"/tmp/pip-build-9x_z6x8v/grpcio/setup.py\", line 196, in <module>\n         if check_linker_need_libatomic():\n       File \"/tmp/pip-build-9x_z6x8v/grpcio/setup.py\", line 156, in check_linker_need_libatomic\n         stderr=PIPE)\n       File \"/home/ubuntu/anaconda3/lib/python3.6/subprocess.py\", line 709, in __init__\n         restore_signals, start_new_session)\n       File \"/home/ubuntu/anaconda3/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n         raise child_exception_type(errno_num, err_msg, err_filename)\n     FileNotFoundError: [Errno 2] No such file or directory: 'cc': 'cc'\n     \n     ----------------------------------------\n Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-9x_z6x8v/grpcio/\n You are using pip version 9.0.1, however version 20.1.1 is available.\n You should consider upgrading via the 'pip install --upgrade pip' command.\n 2020-06-30 16:52:10,251\tINFO log_timer.py:17 -- NodeUpdater: i-0dcaa2c55fbf456bd: Setup commands completed [LogTimer=97821ms]\n 2020-06-30 16:52:10,251\tINFO log_timer.py:17 -- NodeUpdater: i-0dcaa2c55fbf456bd: Applied config 3c0845120a75432975eabe3c3f52bc4599173c4f [LogTimer=166507ms]\n 2020-06-30 16:52:10,252\tERROR updater.py:348 -- NodeUpdater: i-0dcaa2c55fbf456bd: Error updating (Exit Status 1) ssh -i /home/haines/.ssh/ray-key2_us-east-1.pem -o ConnectTimeout=120s -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_98734ce2b6/2ebd7d2a8f/%C -o ControlPersist=10s -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 ubuntu@34.229.85.7 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && pip install ray'\n Exception in thread Thread-2:\n Traceback (most recent call last):\n   File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n     self.run()\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 351, in run\n     raise e\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 341, in run\n     self.do_update()\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 430, in do_update\n     self.cmd_runner.run(cmd)\n   File \"/home/haines/Projects/VF83/Ray_Cloud/lib/python3.6/site-packages/ray/autoscaler/updater.py\", line 263, in run\n     self.process_runner.check_call(final_cmd)\n   File \"/usr/lib/python3.6/subprocess.py\", line 311, in check_call\n     raise CalledProcessError(retcode, cmd)\n subprocess.CalledProcessError: Command '['ssh', '-i', '/home/haines/.ssh/ray-key2_us-east-1.pem', '-o', 'ConnectTimeout=120s', '-o', 'StrictHostKeyChecking=no', '-o', 'ControlMaster=auto', '-o', 'ControlPath=/tmp/ray_ssh_98734ce2b6/2ebd7d2a8f/%C', '-o', 'ControlPersist=10s', '-o', 'IdentitiesOnly=yes', '-o', 'ExitOnForwardFailure=yes', '-o', 'ServerAliveInterval=5', '-o', 'ServerAliveCountMax=3', 'ubuntu@34.229.85.7', 'bash', '--login', '-c', '-i', \"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && pip install ray'\"]' returned non-zero exit status 1.\n \n 2020-06-30 16:52:10,774\tERROR commands.py:285 -- get_or_create_head_node: Updating 34.229.85.7 failed\n 2020-06-30 16:52:10,797\tINFO log_timer.py:17 -- AWSNodeProvider: Set tag ray-node-status=update-failed on ['i-0dcaa2c55fbf456bd'] [LogTimer=545ms]\n \n </denchmark-code>\n \n The head node starts, but not the worker.  I can log into the former, but it seems that the Python3 installation isn't completing, and\n subsequent setup_commands in the config are not being executed (eg. boto3 is not being loaded).  The difficult thing is that this was working this morning.  Any ideas?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "richardrl", "commentT": "2020-07-01T15:23:43Z", "comment_text": "\n \t\tTry adding  pip install --upgrade pip to your setup commands before pip install ray. It just looks like the AMI you are using has an old version of pip and that is tripping up GRPC installation.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "richardrl", "commentT": "2020-07-01T21:43:58Z", "comment_text": "\n \t\tThanks again, that did the trick.  However, I am now back to the original problem of the redis-address deprecation.  If I use ray.init(address=\"auto\"), the Ray cluster cannot be found:-\n <denchmark-code>Traceback (most recent call last):\n   File \"Big_Data_Gen.py\", line 489, in <module>\n     ray.init(address=\"auto\")   #(redis_address=redadd+\":6379\")\n   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py\", line 643, in init\n     address, redis_address)\n   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/services.py\", line 273, in validate_redis_address\n     address = find_redis_address_or_die()\n   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/services.py\", line 165, in find_redis_address_or_die\n     \"Could not find any running Ray instance. \"\n ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting `address`.\n </denchmark-code>\n \n But if I use the redis address, then the warning halts execution as if it were an error:-\n <denchmark-code>Traceback (most recent call last):\n   File \"Big_Data_Gen.py\", line 489, in <module>\n     ray.init(redis_address=redadd+\":6379\")\n   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py\", line 627, in init\n     raise DeprecationWarning(\"The redis_address argument is deprecated. \"\n DeprecationWarning: The redis_address argument is deprecated. Please use address instead.\n </denchmark-code>\n \n I know that this was discussed in <denchmark-link:https://github.com/ray-project/ray/issues/7127>#7127</denchmark-link>\n , but there doesn't seem to be a way of configuring without redis.\n Yours,\n Steven Haines\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "richardrl", "commentT": "2020-07-02T00:55:24Z", "comment_text": "\n \t\tWhat if you did:\n ray.init(address=redadd+\":6379\")\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "richardrl", "commentT": "2020-07-02T19:10:45Z", "comment_text": "\n \t\tThanks Richard; that worked.  It seems that I was over-thinking the implications of the change.\n \t\t"}}}, "commit": {"commit_id": "0e82f0d7c381a46be831b6593533b49d30e498d2", "commit_author": "Ian Rodney", "commitT": "2020-06-12 16:38:38-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\autoscaler\\autoscaler.py", "file_new_name": "python\\ray\\autoscaler\\autoscaler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "364,365", "deleted_lines": "363", "method_info": {"method_name": "spawn_updater", "method_params": "self,node_id,init_commands,ray_start_commands", "method_startline": "350", "method_endline": "367"}}, "hunk_1": {"Ismethod": 1, "added_lines": "324,325", "deleted_lines": "324", "method_info": {"method_name": "recover_if_needed", "method_params": "self,node_id,now", "method_startline": "298", "method_endline": "327"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\autoscaler\\commands.py", "file_new_name": "python\\ray\\autoscaler\\commands.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "150,151", "deleted_lines": "150", "method_info": {"method_name": "kill_node", "method_params": "config_file,yes,hard,override_cluster_name", "method_startline": "120", "method_endline": "164"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\autoscaler\\docker.py", "file_new_name": "python\\ray\\autoscaler\\docker.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "106,112", "deleted_lines": "102,103,104,110", "method_info": {"method_name": "docker_start_cmds", "method_params": "user,image,mount,cname,user_options", "method_startline": "87", "method_endline": "114"}}, "hunk_1": {"Ismethod": 1, "added_lines": "83,84", "deleted_lines": null, "method_info": {"method_name": "check_docker_running_cmd", "method_params": "cname", "method_startline": "83", "method_endline": "84"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\autoscaler\\kubernetes\\node_provider.py", "file_new_name": "python\\ray\\autoscaler\\kubernetes\\node_provider.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "90,91", "deleted_lines": "90,91", "method_info": {"method_name": "get_command_runner", "method_params": "self,log_prefix,node_id,auth_config,cluster_name,process_runner,use_internal_ip", "method_startline": "90", "method_endline": "91"}}, "hunk_1": {"Ismethod": 1, "added_lines": "90,91,92,93,94,95,96,97", "deleted_lines": "90,91", "method_info": {"method_name": "get_command_runner", "method_params": "self,log_prefix,node_id,auth_config,cluster_name,process_runner,use_internal_ip,docker_config", "method_startline": "90", "method_endline": "97"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\autoscaler\\node_provider.py", "file_new_name": "python\\ray\\autoscaler\\node_provider.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "214,215", "deleted_lines": "214,215", "method_info": {"method_name": "get_command_runner", "method_params": "self,log_prefix,node_id,auth_config,cluster_name,process_runner,use_internal_ip", "method_startline": "214", "method_endline": "215"}}, "hunk_1": {"Ismethod": 1, "added_lines": "214,215,216,217,218,219,220,221", "deleted_lines": "214,215", "method_info": {"method_name": "get_command_runner", "method_params": "self,log_prefix,node_id,auth_config,cluster_name,process_runner,use_internal_ip,docker_config", "method_startline": "214", "method_endline": "221"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 9, "file_old_name": "python\\ray\\autoscaler\\updater.py", "file_new_name": "python\\ray\\autoscaler\\updater.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "335,336,337,338", "deleted_lines": null, "method_info": {"method_name": "run_rsync_down", "method_params": "self,source,target", "method_startline": "335", "method_endline": "338"}}, "hunk_1": {"Ismethod": 1, "added_lines": "373,374", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,node_id,provider_config,provider,auth_config,cluster_name,file_mounts,initialization_commands,setup_commands,ray_start_commands,runtime_hash,process_runner,use_internal_ip,docker_config", "method_startline": "361", "method_endline": "374"}}, "hunk_2": {"Ismethod": 1, "added_lines": "305,306,307,308,309,310", "deleted_lines": null, "method_info": {"method_name": "run", "method_params": "self,cmd,timeout,exit_on_fail,port_forward,with_output", "method_startline": "305", "method_endline": "310"}}, "hunk_3": {"Ismethod": 1, "added_lines": "340,341,342,343,344", "deleted_lines": null, "method_info": {"method_name": "remote_shell_command_str", "method_params": "self", "method_startline": "340", "method_endline": "344"}}, "hunk_4": {"Ismethod": 1, "added_lines": "346,347,348,349,350,351,352,353,354,355", "deleted_lines": null, "method_info": {"method_name": "docker_expand_user", "method_params": "self,string", "method_startline": "346", "method_endline": "355"}}, "hunk_5": {"Ismethod": 1, "added_lines": "329,330,331,332,333", "deleted_lines": null, "method_info": {"method_name": "run_rsync_up", "method_params": "self,source,target", "method_startline": "329", "method_endline": "333"}}, "hunk_6": {"Ismethod": 1, "added_lines": "319,320,321,322,323,324,325,326,327", "deleted_lines": "319", "method_info": {"method_name": "check_container_status", "method_params": "self", "method_startline": "319", "method_endline": "327"}}, "hunk_7": {"Ismethod": 1, "added_lines": "300,301,302,303,304,305,306,307,308,309,310,311,312", "deleted_lines": "312", "method_info": {"method_name": "__init__", "method_params": "self,node_id,provider_config,provider,auth_config,cluster_name,file_mounts,initialization_commands,setup_commands,ray_start_commands,runtime_hash,process_runner,use_internal_ip", "method_startline": "300", "method_endline": "312"}}, "hunk_8": {"Ismethod": 1, "added_lines": "299,300,301,302,303", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,docker_config,common_args", "method_startline": "299", "method_endline": "303"}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\scripts\\scripts.py", "file_new_name": "python\\ray\\scripts\\scripts.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "850,851,852", "deleted_lines": "850,851"}}}}}}