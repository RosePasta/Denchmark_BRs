{"BR": {"BR_id": "10585", "BR_author": "wuisawesome", "BRopenT": "2020-09-04T19:11:07Z", "BRcloseT": "2020-09-11T23:30:23Z", "BR_text": {"BRsummary": "[Core] Raylet breaks when many actor tasks are submitted", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n Tested with 16 core macbook pro\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n This is caused by a low ulimt but we should have a better error message.\n Note that actor pool ensures that there is at most one in flight task per actor.\n <denchmark-code>import ray\n from ray.util import ActorPool\n \n @ray.remote(num_cpus=0)\n class DummyActor:\n \n     def __init__(self):\n         pass\n \n     def do_stuff(self):\n         pass\n \n ray.init()\n \n things = [x for x in range(10000)]\n \n nworkers = int(ray.cluster_resources()['CPU']) * 4\n actors = [DummyActor.remote() for _ in range(int(nworkers))]\n \n pool = ActorPool(actors)\n \n res = pool.map(lambda a, v: a.do_stuff.remote(), things)\n \n for i, x in enumerate(res):\n     if i % 100 == 0:\n         print(x)\n </denchmark-code>\n \n Output:\n <denchmark-code>(pid=49596) F0904 12:07:30.034916 49596 377699776 raylet_client.cc:108]  Check failed: _s.ok() [RayletClient] Unable to register worker with raylet.: IOError: No such file or directory\n (pid=49596) *** Check failure stack trace: ***\n (pid=raylet) F0904 12:07:30.037915 49403 281886144 worker_pool.cc:364] Failed to start worker with return value system:24: Too many open files\n (pid=raylet) *** Check failure stack trace: ***\n (pid=raylet)     @        0x1083e0112  google::LogMessage::~LogMessage()\n (pid=raylet)     @        0x10837cdc5  ray::RayLog::~RayLog()\n (pid=raylet)     @        0x107f6f96e  ray::raylet::WorkerPool::StartProcess()\n (pid=raylet)     @        0x107f6d04f  ray::raylet::WorkerPool::StartWorkerProcess()\n (pid=raylet)     @        0x107f73707  ray::raylet::WorkerPool::PopWorker()\n (pid=raylet)     @        0x107ec6923  ray::raylet::NodeManager::DispatchTasks()\n (pid=raylet)     @        0x107ed8b09  ray::raylet::NodeManager::HandleWorkerAvailable()\n (pid=raylet)     @        0x107ed15cf  ray::raylet::NodeManager::HandleWorkerAvailable()\n (pid=raylet)     @        0x107ecfa86  ray::raylet::NodeManager::ProcessClientMessage()\n (pid=raylet)     @        0x107f3817a  std::__1::__function::__func<>::operator()()\n (pid=raylet)     @        0x1083558ee  ray::ClientConnection::ProcessMessage()\n (pid=raylet)     @        0x10835cdb0  boost::asio::detail::reactive_socket_recv_op<>::do_complete()\n (pid=raylet)     @        0x1087e830e  boost::asio::detail::scheduler::do_run_one()\n (pid=raylet)     @        0x1087dbca1  boost::asio::detail::scheduler::run()\n (pid=raylet)     @        0x1087dbb2c  boost::asio::io_context::run()\n (pid=raylet)     @        0x107ea7d8a  main\n (pid=raylet)     @     0x7fff6a420cc9  start\n (pid=49566) F0904 12:07:30.043242 49566 287137216 raylet_client.cc:108]  Check failed: _s.ok() [RayletClient] Unable to register worker with raylet.: IOError: No such file or directory\n (pid=49566) *** Check failure stack trace: ***\n (pid=49566)     @        0x10c6f44e2  google::LogMessage::~LogMessage()\n (pid=49566)     @        0x10c691745  ray::RayLog::~RayLog()\n (pid=49566)     @        0x10c2a1b99  ray::raylet::RayletClient::RayletClient()\n (pid=49566)     @        0x10c1d1e6a  ray::CoreWorker::CoreWorker()\n (pid=49566)     @        0x10c1cfbdf  ray::CoreWorkerProcess::CreateWorker()\n (pid=49566)     @        0x10c1ce913  ray::CoreWorkerProcess::CoreWorkerProcess()\n (pid=49566)     @        0x10c1cdab7  ray::CoreWorkerProcess::Initialize()\n (pid=49566)     @        0x10c13c275  __pyx_tp_new_3ray_7_raylet_CoreWorker()\n (pid=49566)     @        0x10b85ca8f  type_call\n (pid=49566)     @        0x10b7d14f3  _PyObject_FastCallKeywords\n (pid=49566)     @        0x10b90ee75  call_function\n (pid=49566)     @        0x10b90bb92  _PyEval_EvalFrameDefault\n (pid=49566)     @        0x10b90046e  _PyEval_EvalCodeWithName\n (pid=49566)     @        0x10b7d1a03  _PyFunction_FastCallKeywords\n (pid=49566)     @        0x10b90ed67  call_function\n (pid=49566)     @        0x10b90cb8d  _PyEval_EvalFrameDefault\n (pid=49566)     @        0x10b90046e  _PyEval_EvalCodeWithName\n (pid=49566)     @        0x10b963ce0  PyRun_FileExFlags\n (pid=49566)     @        0x10b963157  PyRun_SimpleFileExFlags\n (pid=49566)     @        0x10b990dc3  pymain_main\n (pid=49566)     @        0x10b7a3f2d  main\n (pid=49566)     @     0x7fff6a420cc9  start\n (pid=49566)     @                0xb  (unknown)\n (pid=49563) F0904 12:07:30.037027 49563 245689792 raylet_client.cc:108]  Check failed: _s.ok() [RayletClient] Unable to register worker with raylet.: IOError: No such file or directory\n (pid=49563) *** Check failure stack trace: ***\n (pid=49563)     @        0x10f06e4e2  google::LogMessage::~LogMessage()\n (pid=49563)     @        0x10f00b745  ray::RayLog::~RayLog()\n (pid=49563)     @        0x10ec1bb99  ray::raylet::RayletClient::RayletClient()\n (pid=49563)     @        0x10eb4be6a  ray::CoreWorker::CoreWorker()\n (pid=49563)     @        0x10eb49bdf  ray::CoreWorkerProcess::CreateWorker()\n (pid=49563)     @        0x10eb48913  ray::CoreWorkerProcess::CoreWorkerProcess()\n (pid=49563)     @        0x10eb47ab7  ray::CoreWorkerProcess::Initialize()\n (pid=49563)     @        0x10eab6275  __pyx_tp_new_3ray_7_raylet_CoreWorker()\n (pid=49563)     @        0x10ded3a8f  type_call\n (pid=49563)     @        0x10de484f3  _PyObject_FastCallKeywords\n (pid=49563)     @        0x10df85e75  call_function\n (pid=49563)     @        0x10df82b92  _PyEval_EvalFrameDefault\n (pid=49563)     @        0x10df7746e  _PyEval_EvalCodeWithName\n (pid=49563)     @        0x10de48a03  _PyFunction_FastCallKeywords\n (pid=49563)     @        0x10df85d67  call_function\n (pid=49563)     @        0x10df83b8d  _PyEval_EvalFrameDefault\n (pid=49563)     @        0x10df7746e  _PyEval_EvalCodeWithName\n (pid=49563)     @        0x10dfdace0  PyRun_FileExFlags\n (pid=49563)     @        0x10dfda157  PyRun_SimpleFileExFlags\n (pid=49563)     @        0x10e007dc3  pymain_main\n (pid=49563)     @        0x10de1af2d  main\n (pid=49563)     @     0x7fff6a420cc9  start\n (pid=49512) E0904 12:07:30.084020 49512 218574848 core_worker.cc:694] Raylet failed. Shutting down.\n (pid=49519) E0904 12:07:30.083940 49519 101613568 core_worker.cc:694] Raylet failed. Shutting down.\n (pid=49521) E0904 12:07:30.083511 49521 5890048 core_worker.cc:694] Raylet failed. Shutting down.\n (pid=49562) F0904 12:07:30.097999 49562 180841920 core_worker.cc:330]  Check failed: _s.ok() Bad status: IOError: Broken pipe\n (pid=49562) *** Check failure stack trace: ***\n (pid=49562)     @        0x101a884e2  google::LogMessage::~LogMessage()\n (pid=49562)     @        0x101a25745  ray::RayLog::~RayLog()\n (pid=49562)     @        0x1015661df  ray::CoreWorker::CoreWorker()\n (pid=49562)     @        0x101563bdf  ray::CoreWorkerProcess::CreateWorker()\n (pid=49562)     @        0x101562913  ray::CoreWorkerProcess::CoreWorkerProcess()\n (pid=49562)     @        0x101561ab7  ray::CoreWorkerProcess::Initialize()\n (pid=49562)     @        0x1014d0275  __pyx_tp_new_3ray_7_raylet_CoreWorker()\n (pid=49562)     @        0x100bf0a8f  type_call\n (pid=49562)     @        0x100b654f3  _PyObject_FastCallKeywords\n (pid=49562)     @        0x100ca2e75  call_function\n (pid=49562)     @        0x100c9fb92  _PyEval_EvalFrameDefault\n (pid=49562)     @        0x100c9446e  _PyEval_EvalCodeWithName\n (pid=49562)     @        0x100b65a03  _PyFunction_FastCallKeywords\n (pid=49562)     @        0x100ca2d67  call_function\n (pid=49562)     @        0x100ca0b8d  _PyEval_EvalFrameDefault\n (pid=49562)     @        0x100c9446e  _PyEval_EvalCodeWithName\n (pid=49562)     @        0x100cf7ce0  PyRun_FileExFlags\n (pid=49562)     @        0x100cf7157  PyRun_SimpleFileExFlags\n (pid=49562)     @        0x100d24dc3  pymain_main\n (pid=49562)     @        0x100b37f2d  main\n (pid=49562)     @     0x7fff6a420cc9  start\n (pid=49562)     @                0xb  (unknown)\n </denchmark-code>\n \n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:13:54Z", "comment_text": "\n \t\tThis seems to work fine for me on master. Can you provide the stacktrace from the raylet process (raylet.out and .err)?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:14:37Z", "comment_text": "\n \t\tAh sorry, didn't see this part: (pid=raylet) F0904 12:07:30.037915 49403 281886144 worker_pool.cc:364] Failed to start worker with return value system:24: Too many open files\n Have you tried with ulimit? Maybe you have some other processes running with too many open files?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:15:55Z", "comment_text": "\n \t\tfunny, i'm also on latest master :/\n raylet.out\n <denchmark-code>I0904 12:07:26.417099 49403 281886144 io_service_pool.cc:36] IOServicePool is running with 1 io_service.\n I0904 12:07:26.417464 49403 281886144 store_runner.cc:30] Allowing the Plasma store to use up to 2.69936GB of memory.\n I0904 12:07:26.417488 49403 281886144 store_runner.cc:44] Starting object store with directory /tmp and huge page support disabled\n I0904 12:07:26.418530 49403 281886144 grpc_server.cc:74] ObjectManager server started, listening on port 49698.\n I0904 12:07:26.453881 49403 281886144 node_manager.cc:166] Initializing NodeManager with ID 63baeb290c4d199e64fab2fe91861f27f0248ebe\n I0904 12:07:26.460155 49403 281886144 grpc_server.cc:74] NodeManager server started, listening on port 62164.\n I0904 12:07:26.463145 49403 281886144 agent_manager.cc:40] Not starting agent, the agent command is empty.\n I0904 12:07:26.474151 49403 281886144 service_based_accessor.cc:791] Received notification for node id = 63baeb290c4d199e64fab2fe91861f27f0248ebe, IsAlive = 1\n I0904 12:07:26.474448 49403 281886144 service_based_accessor.cc:791] Received notification for node id = 63baeb290c4d199e64fab2fe91861f27f0248ebe, IsAlive = 1\n F0904 12:07:30.037915 49403 281886144 worker_pool.cc:364] Failed to start worker with return value system:24: Too many open files\n F0904 12:07:30.037915 49403 281886144 worker_pool.cc:364] Failed to start worker with return value system:24: Too many open files\n F0904 12:07:30.037915 49403 281886144 worker_pool.cc:364] Failed to start worker with return value system:24: Too many open files\n F0904 12:07:30.037915 49403 281886144 worker_pool.cc:364] Failed to start worker with return value system:24: Too many open files\n </denchmark-code>\n \n raylet.err\n <denchmark-code>F0904 12:07:30.037915 49403 281886144 worker_pool.cc:364] Failed to start worker with return value system:24: Too many open files\n *** Check failure stack trace: ***\n     @        0x1083e0112  google::LogMessage::~LogMessage()\n     @        0x10837cdc5  ray::RayLog::~RayLog()\n     @        0x107f6f96e  ray::raylet::WorkerPool::StartProcess()\n     @        0x107f6d04f  ray::raylet::WorkerPool::StartWorkerProcess()\n     @        0x107f73707  ray::raylet::WorkerPool::PopWorker()\n     @        0x107ec6923  ray::raylet::NodeManager::DispatchTasks()\n     @        0x107ed8b09  ray::raylet::NodeManager::HandleWorkerAvailable()\n     @        0x107ed15cf  ray::raylet::NodeManager::HandleWorkerAvailable()\n     @        0x107ecfa86  ray::raylet::NodeManager::ProcessClientMessage()\n     @        0x107f3817a  std::__1::__function::__func<>::operator()()\n     @        0x1083558ee  ray::ClientConnection::ProcessMessage()\n     @        0x10835cdb0  boost::asio::detail::reactive_socket_recv_op<>::do_complete()\n     @        0x1087e830e  boost::asio::detail::scheduler::do_run_one()\n     @        0x1087dbca1  boost::asio::detail::scheduler::run()\n     @        0x1087dbb2c  boost::asio::io_context::run()\n     @        0x107ea7d8a  main\n     @     0x7fff6a420cc9  start\n </denchmark-code>\n \n (i believe these were properly redirected to the driver)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:20:19Z", "comment_text": "\n \t\t<denchmark-code>$ ulimit -n \n 256\n </denchmark-code>\n \n i should be starting 64 actor processes. let's see if someone else can reproduce this on a mac maybe?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:22:37Z", "comment_text": "\n \t\tMaybe check how many files you have open before starting Ray? Or increase ulimit -n.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:28:03Z", "comment_text": "\n \t\tI just tested it out on my laptop and the raylet crashed:\n <denchmark-code>(pid=raylet) F0904 14:25:50.808766  6518 319638976 worker_pool.cc:364] Failed to start worker with return value\n  system:24: Too many open files                                                                                \n (pid=raylet) *** Check failure stack trace: ***                                                                \n (pid=raylet)     @        0x109cae112  google::LogMessage::~LogMessage()                                       \n (pid=raylet)     @        0x109c4adc5  ray::RayLog::~RayLog()                                                  \n (pid=raylet)     @        0x10983d96e  ray::raylet::WorkerPool::StartProcess()                                 \n (pid=raylet)     @        0x10983b04f  ray::raylet::WorkerPool::StartWorkerProcess()\n (pid=raylet)     @        0x109841707  ray::raylet::WorkerPool::PopWorker()\n (pid=raylet)     @        0x109794923  ray::raylet::NodeManager::DispatchTasks()\n (pid=raylet)     @        0x1097a6b09  ray::raylet::NodeManager::HandleWorkerAvailable()\n (pid=raylet)     @        0x10979f5cf  ray::raylet::NodeManager::HandleWorkerAvailable()\n (pid=raylet)     @        0x10979f49e  ray::raylet::NodeManager::ProcessAnnounceWorkerPortMessage()\n (pid=raylet)     @        0x10979db30  ray::raylet::NodeManager::ProcessClientMessage()\n (pid=raylet)     @        0x10980617a  std::__1::__function::__func<>::operator()()\n (pid=raylet)     @        0x109c238ee  ray::ClientConnection::ProcessMessage()\n (pid=raylet)     @        0x109c2adb0  boost::asio::detail::reactive_socket_recv_op<>::do_complete()\n (pid=raylet)     @        0x10a0b630e  boost::asio::detail::scheduler::do_run_one()\n (pid=raylet)     @        0x10a0a9ca1  boost::asio::detail::scheduler::run()\n (pid=raylet)     @        0x10a0a9b2c  boost::asio::io_context::run()\n (pid=raylet)     @        0x109775d8a  main\n (pid=raylet)     @     0x7fff6ce4ccc9  start\n </denchmark-code>\n \n I also ran into a similar issue recently when trying to run some Serve tests where the GCS service crashed due to too many open files. I was not able to resolve it but it only happened locally - not on CI or linux boxes.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:28:52Z", "comment_text": "\n \t\tsetting ulimit works for me but definitely scary stack trace in the initial execution.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:29:58Z", "comment_text": "\n \t\tI guess this is a ulimit issue and we probably just open 4 files per process (i only know of 3). I'll downgrade this to P1, but we should probably just catch this error and make a less scary error message\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "wuisawesome", "commentT": "2020-09-04T19:30:55Z", "comment_text": "\n \t\tOn ubuntu ulimit defaults to 1024, on os x it's 256. that probably explains it...\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "wuisawesome", "commentT": "2020-09-04T20:13:06Z", "comment_text": "\n \t\tI guess we should warn users that they should increase ulimit when they start with big number of workers?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "wuisawesome", "commentT": "2020-09-04T21:48:09Z", "comment_text": "\n \t\tyeah, actually why don't we just setrlimit ourselves when starting ray?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "wuisawesome", "commentT": "2020-09-04T21:51:07Z", "comment_text": "\n \t\talso, imo 4*num_cpu's isn't a large number of workers, i think it's actually fairly reasonable (think about large numbers of tasks which perform io)\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "wuisawesome", "commentT": "2020-09-04T23:10:05Z", "comment_text": "\n \t\tHmm it seems a bit scary to me to be overriding that limit as part of Ray's source code...It's also pretty awkward since then we'd have to make the value configurable. We do it in the example cluster launcher scripts, but that's less sketchy since it's very clear and configurable, plus it's not going to be running on the user's laptop.\n I would be in favor of just making the error message less scary (we probably don't need a stacktrace) and adding a tip to let the user know that they should manually increase the limit.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "wuisawesome", "commentT": "2020-09-11T23:29:39Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wuisawesome>@wuisawesome</denchmark-link>\n  Can I close the issue?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "wuisawesome", "commentT": "2020-09-11T23:30:23Z", "comment_text": "\n \t\twhoops, yeah sorry\n \t\t"}}}, "commit": {"commit_id": "d9c68fca5c2e9beb87b4eb460f44c8d6a60c2583", "commit_author": "Alex Wu", "commitT": "2020-09-08 20:58:05-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "python\\ray\\node.py", "file_new_name": "python\\ray\\node.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "693,694,695,696,697", "deleted_lines": null, "method_info": {"method_name": "start_raylet", "method_params": "self,plasma_directory,object_store_memory,use_valgrind,use_profiler", "method_startline": "693", "method_endline": "697"}}, "hunk_1": {"Ismethod": 1, "added_lines": "693,694,695,696,697,718,719,720,721,722,723,724,725", "deleted_lines": "691,712,713,714,715,716,717,726", "method_info": {"method_name": "start_raylet", "method_params": "self,use_valgrind,use_profiler", "method_startline": "691", "method_endline": "736"}}, "hunk_2": {"Ismethod": 1, "added_lines": "653,659,660", "deleted_lines": "652,661", "method_info": {"method_name": "start_plasma_store", "method_params": "self", "method_startline": "652", "method_endline": "669"}}, "hunk_3": {"Ismethod": 1, "added_lines": "653,659,660", "deleted_lines": "661", "method_info": {"method_name": "start_plasma_store", "method_params": "self,plasma_directory,object_store_memory", "method_startline": "653", "method_endline": "671"}}, "hunk_4": {"Ismethod": 1, "added_lines": "820,821,822,823,824,825,826,827,828,829,830", "deleted_lines": null, "method_info": {"method_name": "start_ray_processes", "method_params": "self", "method_startline": "815", "method_endline": "835"}}, "hunk_5": {"Ismethod": 1, "added_lines": "280,281,282,283,284,285,286", "deleted_lines": "279,280,281,282,283,284", "method_info": {"method_name": "get_resource_spec", "method_params": "self", "method_startline": "256", "method_endline": "298"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "python\\ray\\services.py", "file_new_name": "python\\ray\\services.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1682,1683", "deleted_lines": "1686", "method_info": {"method_name": "start_plasma_store", "method_params": "resource_spec,plasma_store_socket_name,stdout_file,stderr_file,plasma_directory,keep_idle,huge_pages,fate_share,use_valgrind", "method_startline": "1682", "method_endline": "1690"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1682,1683", "deleted_lines": "1686", "method_info": {"method_name": "start_plasma_store", "method_params": "resource_spec,plasma_directory,object_store_memory,plasma_store_socket_name,stdout_file,stderr_file,keep_idle,huge_pages,fate_share,use_valgrind", "method_startline": "1681", "method_endline": "1690"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1259,1260", "deleted_lines": "1273", "method_info": {"method_name": "start_raylet", "method_params": "redis_address,node_ip_address,node_manager_port,raylet_name,plasma_store_name,worker_path,temp_dir,session_dir,resource_spec,min_worker_port,max_worker_port,object_manager_port,redis_password,metrics_agent_port,metrics_export_port,use_valgrind,use_profiler,stdout_file,stderr_file,config,include_java,java_worker_options,load_code_from_local,plasma_directory,huge_pages,fate_share,socket_to_use,head_node,start_initial_python_workers_for_first_job,object_spilling_config,code_search_path", "method_startline": "1250", "method_endline": "1280"}}, "hunk_3": {"Ismethod": 1, "added_lines": "1259,1260", "deleted_lines": "1273", "method_info": {"method_name": "start_raylet", "method_params": "redis_address,node_ip_address,node_manager_port,raylet_name,plasma_store_name,worker_path,temp_dir,session_dir,resource_spec,plasma_directory,object_store_memory,min_worker_port,max_worker_port,object_manager_port,redis_password,metrics_agent_port,metrics_export_port,use_valgrind,use_profiler,stdout_file,stderr_file,config,include_java,java_worker_options,load_code_from_local,huge_pages,fate_share,socket_to_use,head_node,start_initial_python_workers_for_first_job,object_spilling_config,code_search_path", "method_startline": "1250", "method_endline": "1281"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\core_worker\\task_manager.cc", "file_new_name": "src\\ray\\core_worker\\task_manager.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "304,305,318,319", "deleted_lines": "304,305,318,319", "method_info": {"method_name": "ray::TaskManager::PendingTaskFailed", "method_params": "task_id,error_type,status", "method_startline": "268", "method_endline": "339"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\raylet\\node_manager.cc", "file_new_name": "src\\ray\\raylet\\node_manager.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "2245,2249", "deleted_lines": "2245,2249", "method_info": {"method_name": "ray::raylet::NodeManager::MarkObjectsAsFailed", "method_params": "error_type,objects_to_fail,job_id", "method_startline": "2226", "method_endline": "2255"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\raylet\\worker_pool.cc", "file_new_name": "src\\ray\\raylet\\worker_pool.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "390,391,392,393,394,395,396,397,398", "deleted_lines": "390,391,392", "method_info": {"method_name": "ray::raylet::WorkerPool::StartProcess", "method_params": "worker_command_args,env", "method_startline": "370", "method_endline": "401"}}}}}}}