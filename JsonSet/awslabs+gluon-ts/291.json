{"BR": {"BR_id": "291", "BR_author": "AaronSpieler", "BRopenT": "2019-09-06T12:57:49Z", "BRcloseT": "2020-01-21T10:02:23Z", "BR_text": {"BRsummary": "Item metrics dataframe incorrectly formatted", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n The data-frame containing the per time step metrics has a column item_id and filled with NaNs.\n <denchmark-link:https://user-images.githubusercontent.com/25365592/64429596-a2796200-d0b6-11e9-89fd-a77bd31ffcc9.png></denchmark-link>\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n For example in the extended tutorial:\n <denchmark-code>evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n item_metrics.head()\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n \n Operating system: MacOs\n Python version: 3.6.9\n GluonTS version: 0.3.3\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "AaronSpieler", "commentT": "2019-09-07T11:30:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AaronSpieler>@AaronSpieler</denchmark-link>\n  I believe that depends on the dataset you use: if the entries in the dataset have an \u201citem_id\u201d field, then that will be used to tag the forecasts and appropriately fill in the associated field in the metrics dataframe.\n What dataset did that occur with? If it\u2019s one of the provided datasets, then the field may be missing or have a different name, which should be fixed.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "AaronSpieler", "commentT": "2019-09-09T09:31:38Z", "comment_text": "\n \t\tIts the m4_hourly dataset (as used in the quick tutorial).\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "AaronSpieler", "commentT": "2019-10-10T12:22:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AaronSpieler>@AaronSpieler</denchmark-link>\n  can you follow up on this?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "AaronSpieler", "commentT": "2019-10-22T08:48:20Z", "comment_text": "\n \t\tSo all the provided datasets I tested contain a categorical feature called \"feat_static_cat\", which is equal to the corresponding index of the series.\n <denchmark-link:https://user-images.githubusercontent.com/25365592/67270295-ab809000-f4b8-11e9-85aa-cc05ba93368c.png></denchmark-link>\n \n Is this supposed to be renamed to \"item_id\" <denchmark-link:https://github.com/lostella>@lostella</denchmark-link>\n  ?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "AaronSpieler", "commentT": "2019-10-22T11:59:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jaheba>@jaheba</denchmark-link>\n  This is probably related to the fact that, unlike in section 1.3 of the Extended Forecasting Tutorial (<denchmark-link:https://gluon-ts.mxnet.io/examples/extended_forecasting_tutorial/extended_tutorial.html#1.3-Use-your-time-series-and-features>here</denchmark-link>\n ), in version 0.3.3  does not have the  attribute anymore.\n <denchmark-link:https://user-images.githubusercontent.com/32721837/67282999-2a82c200-f4d3-11e9-81df-7087232a0751.png></denchmark-link>\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n <denchmark-code>from gluonts.transform import FieldName\n [f\"FieldName.{k} = '{v}'\" for k, v in FieldName.__dict__.items() if not k.startswith('_')]\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n \n Operating system: CentOS\n Python version: 3.6.1\n GluonTS version: 0.3.3\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "AaronSpieler", "commentT": "2019-10-22T12:30:44Z", "comment_text": "\n \t\tMhhh, I see. Thanks <denchmark-link:https://github.com/matsmaiwald>@matsmaiwald</denchmark-link>\n \n So FieldName.ITEM_ID was removed.\n However, that does not explain why the values that were stored in \"item_id\" were moved to \"feat_static_cat\". As far as I can tell in the case of the provided datasets they really only represent the index.\n Also, all provided datasets seem to be missing any covariates? Was this always the case? <denchmark-link:https://github.com/jaheba>@jaheba</denchmark-link>\n  <denchmark-link:https://github.com/lostella>@lostella</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "AaronSpieler", "commentT": "2019-10-23T21:31:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AaronSpieler>@AaronSpieler</denchmark-link>\n  the \u201citem_id\u201d serves a different purpose than feature fields: it is used to identify which time series from a dataset a forecast refers to. So for example, if you create multiple inference scenarios from a time series (like cutting it at two different points in time) you are still able to relate both predictions to the same time series, and for example aggregate evaluation metrics accordingly. Categorical features instead are usually fed into the networks, and setting them to an incremental, 0-based counter (essentially an ID) is one way to go in case no other relevant groupings are there.\n Note however, that a model trained by using such feature will be able to make predictions on time series with the same categorical features values, so the ID case puts some restrictions (no out of sample predictions allowed).\n Also yes, I believe no covariates was always the case for the built in datasets.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "AaronSpieler", "commentT": "2019-10-23T21:35:20Z", "comment_text": "\n \t\tNote that FieldName.ITEM_ID was not removed, but rather introduced: the documentation website happens to be ahead of 0.3.3, and reflects master instead (and we should get that fixed).\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "AaronSpieler", "commentT": "2019-11-11T17:19:53Z", "comment_text": "\n \t\tOk, then it seems ITEM_ID is just not properly set for the datasets. I will look into it again and see to it that it gets fixed.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "AaronSpieler", "commentT": "2019-12-07T14:16:11Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/AaronSpieler>@AaronSpieler</denchmark-link>\n  , do you have any updates on this question yet? In particular on how to include the  to associate a time series with its corresponding error measures (instead of getting NaNs for all ids)?\n Update:\n Simply including an item_id field in the dataset seems to be enough (did not expect it to be that easy):\n <denchmark-link:https://user-images.githubusercontent.com/47742163/70376722-3c49e880-190c-11ea-9238-0638b208e31a.png></denchmark-link>\n \n I used the original series id, i.e. W1 for first weekly time series, W2 for second weekly series, and so forth...\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "AaronSpieler", "commentT": "2020-01-01T21:39:18Z", "comment_text": "\n \t\tFixed by pull request: <denchmark-link:https://github.com/awslabs/gluon-ts/pull/546>#546</denchmark-link>\n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "AaronSpieler", "commentT": "2020-01-19T15:53:13Z", "comment_text": "\n \t\tI think this should be fixed by adjusting the scripts that generate the affected datasets, so that they write the \u201citem_id\u201d field\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "AaronSpieler", "commentT": "2020-01-21T19:11:56Z", "comment_text": "\n \t\tThe problem should be gone now if you use the master branch: loading the datasets once with \u201cregenerate=True\u201d should do the trick. This change will be included in the next release.\n \t\t"}}}, "commit": {"commit_id": "1171f9e9e259fdc2cfe11030f0aa15f88c735f97", "commit_author": "Lorenzo Stella", "commitT": "2020-01-21 11:02:22+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\gluonts\\dataset\\repository\\_gp_copula_2019.py", "file_new_name": "src\\gluonts\\dataset\\repository\\_gp_copula_2019.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "150", "deleted_lines": null, "method_info": {"method_name": "save_dataset", "method_params": "Path,GPCopulaDataset", "method_startline": "137", "method_endline": "154"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\gluonts\\dataset\\repository\\_lstnet.py", "file_new_name": "src\\gluonts\\dataset\\repository\\_lstnet.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "169,196", "deleted_lines": null, "method_info": {"method_name": "generate_lstnet_dataset", "method_params": "Path,str", "method_startline": "120", "method_endline": "202"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\gluonts\\dataset\\repository\\_m4.py", "file_new_name": "src\\gluonts\\dataset\\repository\\_m4.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "75,76,77,78,79,80,88,89,90,91,92,93", "deleted_lines": "75,83"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\gluonts\\dataset\\repository\\_util.py", "file_new_name": "src\\gluonts\\dataset\\repository\\_util.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "23", "deleted_lines": "23", "method_info": {"method_name": "to_dict", "method_params": "ndarray,str,None", "method_startline": "22", "method_endline": "23"}}, "hunk_1": {"Ismethod": 1, "added_lines": "23,24,25,26", "deleted_lines": "23", "method_info": {"method_name": "to_dict", "method_params": "ndarray,str,None,None", "method_startline": "22", "method_endline": "26"}}}}}}}