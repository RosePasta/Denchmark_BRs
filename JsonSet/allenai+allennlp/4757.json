{"BR": {"BR_id": "4757", "BR_author": "mklimasz", "BRopenT": "2020-10-28T07:46:07Z", "BRcloseT": "2020-10-28T19:38:48Z", "BR_text": {"BRsummary": "PretrainedTransformerTokenizer fails when disabling \"fast\" tokenizer", "BRdescription": "\n <denchmark-h:h2>Checklist</denchmark-h>\n \n \n  I have verified that the issue exists against the master branch of AllenNLP.\n  I have read the relevant section in the contribution guide on reporting bugs.\n  I have checked the issues list for similar or identical bug reports.\n  I have checked the pull requests list for existing proposed fixes.\n  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.\n  I have included in the \"Description\" section below a traceback from any exceptions related to this bug.\n  I have included in the \"Related issues or possible duplicates\" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).\n  I have included in the \"Environment\" section below the name of the operating system and Python version that I was using when I discovered this bug.\n  I have included in the \"Environment\" section below the output of pip freeze.\n  I have included in the \"Steps to reproduce\" section below a minimally reproducible example.\n \n <denchmark-h:h2>Description</denchmark-h>\n \n Tested on 1.2.0rc1 and master\n Intra work tokenizer doesn't work when we deliberately set use fast tokenizer to false (not sure if it's new transformers change).\n I think that setting return_token_type_ids to None instead of False is solution here.\n \n Python traceback:\n \n Traceback (most recent call last):\n   File \"bug_example.py\", line 4, in <module>\n     tokenizer_kwargs={\"use_fast\": False}).intra_word_tokenize([\"My\", \"text\", \"will\"])\n   File \"X/venv/lib/python3.6/site-packages/allennlp-1.2.0rc1-py3.6.egg/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py\", line 387, in intra_word_tokenize\n     tokens, offsets = self._intra_word_tokenize(string_tokens)\n   File \"X/venv/lib/python3.6/site-packages/allennlp-1.2.0rc1-py3.6.egg/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py\", line 354, in _intra_word_tokenize\n     return_token_type_ids=False,\n   File \"X/venv/lib/python3.6/site-packages/transformers-3.4.0-py3.6.egg/transformers/tokenization_utils_base.py\", line 2229, in encode_plus\n     **kwargs,\n   File \"X/venv/lib/python3.6/site-packages/transformers-3.4.0-py3.6.egg/transformers/tokenization_utils.py\", line 490, in _encode_plus\n     verbose=verbose,\n   File \"X/venv/lib/python3.6/site-packages/transformers-3.4.0-py3.6.egg/transformers/tokenization_utils_base.py\", line 2617, in prepare_for_model\n     \"Asking to return token_type_ids while setting add_special_tokens to False \"\n ValueError: Asking to return token_type_ids while setting add_special_tokens to False results in an undefined behavior. Please set add_special_tokens to True or set return_token_type_ids to None.\n \n \n \n \n <denchmark-h:h2>Related issues or possible duplicates</denchmark-h>\n \n Not to my knowledge\n <denchmark-h:h2>Environment</denchmark-h>\n \n OS:\n Ubuntu 18.04 LTS\n Python version:\n 3.6.9 and 3.8.0\n \n Output of pip freeze:\n \n absl-py==0.9.0\n allennlp==1.2.0rc1\n attrs==20.2.0\n blis==0.4.1\n boto3==1.16.5\n botocore==1.19.5\n cached-property==1.5.2\n cachetools==4.1.1\n catalogue==1.0.0\n certifi==2020.6.20\n chardet==3.0.4\n click==7.1.2\n conllu==2.3.2\n cymem==2.0.3\n dataclasses==0.5\n dataclasses-json==0.5.2\n en-core-web-sm==2.3.1\n filelock==3.0.12\n future==0.18.2\n google-auth==1.22.1\n google-auth-oauthlib==0.4.1\n grpcio==1.33.1\n h5py==3.0.0rc1\n idna==2.10\n importlib-metadata==2.0.0\n iniconfig==1.1.1\n jmespath==0.10.0\n joblib==0.14.1\n jsonnet==0.15.0\n jsonpickle==1.4.1\n Markdown==3.3.3\n marshmallow==3.8.0\n marshmallow-enum==1.5.1\n murmurhash==1.0.2\n mypy-extensions==0.4.3\n nltk==3.5\n numpy==1.19.2\n oauthlib==3.1.0\n overrides==3.1.0\n packaging==20.4\n pkg-resources==0.0.0\n plac==1.1.3\n pluggy==0.13.1\n preshed==3.0.2\n protobuf==4.0.0rc2\n py==1.9.0\n pyasn1==0.4.8\n pyasn1-modules==0.2.8\n pyparsing==3.0.0a2\n pytest==6.1.1\n python-dateutil==2.8.1\n regex==2020.10.23\n requests==2.23.0\n requests-oauthlib==1.3.0\n rsa==4.6\n s3transfer==0.3.3\n sacremoses==0.0.43\n scikit-learn==0.23.2\n scipy==1.5.3\n sentencepiece==0.1.94\n six==1.15.0\n spacy==2.3.2\n srsly==1.0.2\n stringcase==1.2.0\n tensorboard==2.1.0\n tensorboardX==2.1\n thinc==7.4.1\n threadpoolctl==2.1.0\n tokenizers==0.9.2\n toml==0.10.1\n torch==1.6.0\n tqdm==4.43.0\n transformers==3.4.0\n typing-extensions==3.7.4.3\n typing-inspect==0.6.0\n urllib3==1.25.11\n wasabi==0.8.0\n Werkzeug==1.0.1\n zipp==3.4.0\n \n \n \n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n \n Example source:\n \n from allennlp.data.tokenizers import PretrainedTransformerTokenizer\n \n PretrainedTransformerTokenizer(\"bert-base-cased\",\n                                tokenizer_kwargs={\"use_fast\": False}).intra_word_tokenize([\"My\", \"text\", \"will\"])\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mklimasz", "commentT": "2020-10-28T19:38:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/epwalsh>@epwalsh</denchmark-link>\n  thanks for the merge. I'm closing this issue as solved.\n \t\t"}}}, "commit": {"commit_id": "baca7545252196245f8288b35de8fa78e381e086", "commit_author": "Mateusz Klimaszewski", "commitT": "2020-10-28 09:26:14-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "109", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\pretrained_transformer_tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\pretrained_transformer_tokenizer.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "354"}}}}}}