{"BR": {"BR_id": "318", "BR_author": "jamestang0219", "BRopenT": "2016-11-02T10:01:25Z", "BRcloseT": "2016-11-02T17:47:13Z", "BR_text": {"BRsummary": "How to test some samples while training in newest version?", "BRdescription": "\n I updated my paddle version to 0.8.0b3, and then trained a RNN model.\n I found that there is no Tester while training.\n But I used previous version to train the model, Tester can test the test data every 100 batches.\n Here is my train config:\n <denchmark-code>paddle train \\\n --config=$mod \\\n --save_dir=./model_desc_full \\\n --trainer_count=4 \\\n --log_period=100 \\\n --num_passes=10 \\\n --use_gpu=true \\\n --show_parameter_stats_period=1000 \\\n --test_all_data_in_one_period=1 \\\n --config_args=is_predict=0 \\\n </denchmark-code>\n \n This config worked in previous version, but the Tester disappear in newest version.\n I wanna know hot to set the config to TEST while TRAINING\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jamestang0219", "commentT": "2016-11-02T10:17:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jamestang0219>@jamestang0219</denchmark-link>\n  set --test_period=1000 will do test job after 1000 mini-batch.\n We changed the 's default value to zero, means test after one pass.\n It is very kind if you have time to submit a PR change demo's run.sh, if you don't have time, please let us know and leave them to us.\n <denchmark-link:https://github.com/backyes>@backyes</denchmark-link>\n  Please change the demo's shell script if <denchmark-link:https://github.com/jamestang0219>@jamestang0219</denchmark-link>\n  don't want submit a PR, because the default value of  has been changed.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jamestang0219", "commentT": "2016-11-02T10:29:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/reyoung>@reyoung</denchmark-link>\n \n Here is the log of previous version:\n `I1101 07:53:16.276069 119355 TrainerInternal.cpp:162]  Batch=8800 samples=281600 AvgCost=0.0174819 CurrentCost=0.0205525 Eval: classification_error_evaluator=0.00603693  CurrentEval: classification_error_evaluator=0.008125\n ...................................................................................................\n I1101 07:53:42.518815 119355 TrainerInternal.cpp:162]  Batch=8900 samples=284800 AvgCost=0.0175253 CurrentCost=0.0213492 Eval: classification_error_evaluator=0.00604986  CurrentEval: classification_error_evaluator=0.0071875\n ...................................................................................................I1101 07:54:08.425493 119355 TrainerInternal.cpp:204] .w0  avg_abs_val=0.106658    max_val=3.63432     avg_abs_grad=1.56957e-07 max_grad=0.00512181\n I1101 07:54:08.426228 119355 TrainerInternal.cpp:204] .w0   avg_abs_val=0.16739     max_val=0.997636    avg_abs_grad=0.00018073  max_grad=0.00459319\n I1101 07:54:08.426569 119355 TrainerInternal.cpp:204] .wbias avg_abs_val=0.195303    max_val=0.520971    avg_abs_grad=0.000887678 max_grad=0.0156855\n I1101 07:54:08.427664 119355 TrainerInternal.cpp:204] .w0  avg_abs_val=0.125715    max_val=1.15954     avg_abs_grad=0.000132394 max_grad=0.0128729\n I1101 07:54:08.428009 119355 TrainerInternal.cpp:204] .wbias avg_abs_val=0.160678    max_val=0.589867    avg_abs_grad=0.000734516 max_grad=0.0196687\n I1101 07:54:08.428349 119355 TrainerInternal.cpp:204] .w0   avg_abs_val=0.461006    max_val=1.20577     avg_abs_grad=0.00776989  max_grad=0.0120312\n I1101 07:54:08.428617 119355 TrainerInternal.cpp:204] .wbias avg_abs_val=0.310792    max_val=0.310793    avg_abs_grad=0.0090016   max_grad=0.00900169\n I1101 07:54:08.428644 119355 TrainerInternal.cpp:162]  Batch=9000 samples=288000 AvgCost=0.0175542 CurrentCost=0.0201257 Eval: classification_error_evaluator=0.00605903  CurrentEval: classification_error_evaluator=0.006875\n I1101 07:54:10.472939 119355 Tester.cpp:111]  Test samples=1000 cost=0.466531 Eval: classification_error_evaluator=0.159\n ...................................................................................................\n I1101 07:54:35.642098 119355 TrainerInternal.cpp:162]  Batch=9100 samples=291200 AvgCost=0.0175926 CurrentCost=0.0210439 Eval: classification_error_evaluator=0.00606799  CurrentEval: classification_error_evaluator=0.006875\n ...................................................................................................\n I1101 07:55:01.101780 119355 TrainerInternal.cpp:162]  Batch=9200 samples=294400 AvgCost=0.01767 CurrentCost=0.0247131 Eval: classification_error_evaluator=0.00611073  CurrentEval: classification_error_evaluator=0.01`\n It worked well for testing every 1000 batches\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jamestang0219", "commentT": "2016-11-02T10:31:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/reyoung>@reyoung</denchmark-link>\n \n And here is the newest version logs:\n `I1102 18:22:04.354857 34647 TrainerInternal.cpp:165]  Batch=3800 samples=486400 AvgCost=0.107311 CurrentCost=0.0766458 Eval: classification_error_evaluator=0.041801  CurrentEval: classification_error_evaluator=0.0296875\n ...................................................................................................\n I1102 18:23:05.413488 34647 TrainerInternal.cpp:165]  Batch=3900 samples=499200 AvgCost=0.10815 CurrentCost=0.140034 Eval: classification_error_evaluator=0.0420613  CurrentEval: classification_error_evaluator=0.0519531\n ...................................................................................................I1102 18:24:05.555019 34647 TrainerInternal.cpp:207] .w0  avg_abs_val=0.0516422   max_val=0.87552     avg_abs_grad=1.31203e-05 max_grad=0.202811\n I1102 18:24:05.556380 34647 TrainerInternal.cpp:207] .w0   avg_abs_val=0.133105    max_val=1.14789     avg_abs_grad=0.00251638  max_grad=0.252082\n I1102 18:24:05.556483 34647 TrainerInternal.cpp:207] .wbias avg_abs_val=0.13841     max_val=0.452819    avg_abs_grad=0.0268143   max_grad=2.08488\n I1102 18:24:05.557827 34647 TrainerInternal.cpp:207] .w0  avg_abs_val=0.0976409   max_val=0.819044    avg_abs_grad=0.00756949  max_grad=2.52075\n I1102 18:24:05.557920 34647 TrainerInternal.cpp:207] .wbias avg_abs_val=0.117548    max_val=0.493404    avg_abs_grad=0.0481846   max_grad=5.2654\n I1102 18:24:05.558040 34647 TrainerInternal.cpp:207] .w0   avg_abs_val=0.109183    max_val=0.359466    avg_abs_grad=1.06527     max_grad=5.85286\n I1102 18:24:05.558117 34647 TrainerInternal.cpp:207] .wbias avg_abs_val=0.0851114   max_val=0.0851116   avg_abs_grad=3.39263     max_grad=3.39263\n I1102 18:24:05.558130 34647 TrainerInternal.cpp:165]  Batch=4000 samples=512000 AvgCost=0.110242 CurrentCost=0.191823 Eval: classification_error_evaluator=0.0426113  CurrentEval: classification_error_evaluator=0.0640625\n ...................................................................................................\n I1102 18:25:05.374027 34647 TrainerInternal.cpp:165]  Batch=4100 samples=524800 AvgCost=0.115381 CurrentCost=0.320934 Eval: classification_error_evaluator=0.0453944  CurrentEval: classification_error_evaluator=0.156719`\n and I also add the --test_period=1000 config, but it doesn't test the test data while training.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jamestang0219", "commentT": "2016-11-02T10:49:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/reyoung>@reyoung</denchmark-link>\n \n One more question.\n While training, the AvgCost sometimes became nan, and it caused training failed.\n So what's problem?\n `I1102 18:24:05.558130 34647 TrainerInternal.cpp:165]  Batch=4000 samples=512000 AvgCost=0.110242 CurrentCost=0.191823 Eval: classification_error_evaluator=0.0426113  CurrentEval: classification_error_evaluator=0.0640625\n ...................................................................................................\n I1102 18:25:05.374027 34647 TrainerInternal.cpp:165]  Batch=4100 samples=524800 AvgCost=0.115381 CurrentCost=0.320934 Eval: classification_error_evaluator=0.0453944  CurrentEval: classification_error_evaluator=0.156719\n ...................................................................................................\n I1102 18:26:06.421185 34647 TrainerInternal.cpp:165]  Batch=4200 samples=537600 AvgCost=nan CurrentCost=nan Eval: classification_error_evaluator=0.0540532  CurrentEval: classification_error_evaluator=0.469609\n ...................................................................................................\n I1102 18:27:06.688400 34647 TrainerInternal.cpp:165]  Batch=4300 samples=550400 AvgCost=nan CurrentCost=nan Eval: classification_error_evaluator=0.0715807  CurrentEval: classification_error_evaluator=0.952656\n *** Aborted at 1478082836 (unix time) try \"date -d @1478082836\" if you are using GNU date ***\n PC: @           0x700e89 paddle::GpuVectorT<>::getAbsMax()\n *** SIGFPE (@0x700e89) received by PID 34647 (TID 0x7f7d72fb9700) from PID 7343753; stack trace: ***\n <denchmark-code>@     0x7f7da965a330 (unknown)\n \n @           0x700e89 paddle::GpuVectorT<>::getAbsMax()\n \n @           0x6a5a69 _ZNSt17_Function_handlerIFvPN6paddle9ParameterEEZNS0_15TrainerInternal13trainOneBatchElRKNS0_9DataBatchEPSt6vectorINS0_8ArgumentESaIS9_EEEUlS2_E_E9_M_invokeERKSt9_Any_dataS2_\n \n @           0x6579bc paddle::TrainerThread::doCallback()\n \n @           0x657e91 paddle::TrainerThread::gradCollectThread()\n \n @     0x7f7da8836a60 (unknown)\n \n @     0x7f7da9652184 start_thread\n \n @     0x7f7da7f9e37d (unknown)\n \n @                0x0 (unknown)\n </denchmark-code>\n \n /usr/local/paddle/bin//paddle: line 81: 34647 Floating point exception(core dumped) ${DEBUGGER} $MYDIR/../opt/paddle/bin/paddle_trainer ${@:2}`\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jamestang0219", "commentT": "2016-11-02T13:47:56Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/reyoung>@reyoung</denchmark-link>\n  had found some clue for this BUG, he will fix it later\n \t\t"}}}, "commit": {"commit_id": "968464cc605ed67c658af6c65d617b7036bfecfb", "commit_author": "Yu Yang", "commitT": "2016-11-02 10:47:13-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "paddle\\trainer\\Tester.cpp", "file_new_name": "paddle\\trainer\\Tester.cpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "119", "deleted_lines": null, "method_info": {"method_name": "paddle::Tester::testOnePeriod", "method_params": "", "method_startline": "93", "method_endline": "120"}}}}}}}