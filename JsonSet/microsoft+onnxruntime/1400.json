{"BR": {"BR_id": "1400", "BR_author": "xgirones", "BRopenT": "2019-07-13T19:15:31Z", "BRcloseT": "2019-07-19T19:10:01Z", "BR_text": {"BRsummary": "Wrong result when evaluating a batch of padded sequences with varying lengths on GPU", "BRdescription": "\n Describe the bug\n When supplying the attached model with a batch of padded sequences with varying lengths the results on GPU are incorrect:\n <denchmark-code>Discrepancy between CPU and GPU evaluations.\n Number of sequences in batch: 16\n Sequence lengths: [12 32  4 52  8 64 20 36 48 40 24 60 44 16 28 56]\n Maximum absolute pairwise difference: 0.9621068835258484\n Average absolute pairwise difference: 0.019491384112097738\n </denchmark-code>\n \n On the other hand, the results on CPU are fine:\n <denchmark-code>Discrepancy between CPU and PyTorch evaluations.\n Number of sequences in batch: 16\n Sequence lengths: [12 32  4 52  8 64 20 36 48 40 24 60 44 16 28 56]\n Maximum absolute pairwise difference: 1.2218952178955078e-06\n Average absolute pairwise difference: 1.330953997859751e-08\n </denchmark-code>\n \n Urgency\n Stuck on this\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\n ONNX Runtime installed from (source or binary): Source\n ONNX Runtime version: Code pulled from the master branch in 13/7/2019\n Python version: 3.6\n Visual Studio version (if applicable): 2017\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version: 10.1/7.6\n GPU model and memory: NVIDA Quadro K1200 with 4GB\n \n \n Unzip the attached <denchmark-link:https://github.com/microsoft/onnxruntime/files/3389130/discrepancy_CPU_GPU.zip>discrepancy_CPU_GPU.zip</denchmark-link>\n  file and run the model  in the  folder on both CPU and GPU using  and  as inputs (all  and  in the folder  are the same).\n Expected behavior\n Results on GPU and CPU should be the same.\n Screenshots\n If applicable, add screenshots to help explain your problem.\n \n <denchmark-link:https://github.com/microsoft/onnxruntime/files/3389130/discrepancy_CPU_GPU.zip>discrepancy_CPU_GPU.zip</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xgirones", "commentT": "2019-07-16T19:09:35Z", "comment_text": "\n \t\tOur cuda implementation for LSTM uses cudnn libray, it has some limitation: the sequences in the mini-batch need to be sorted in descending order according to length. refers to\n <denchmark-link:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNForwardInferenceEx>https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNForwardInferenceEx</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xgirones", "commentT": "2019-07-16T20:49:35Z", "comment_text": "\n \t\tHi Hector,\n Thanks for your answer. I tried with sorted sequences as you suggested but the problem did not go away:\n <denchmark-code>Discrepancy between CPU and GPU evaluations.\n Number of sequences in batch: 16\n Sequence lengths: [64 60 56 52 48 44 40 36 32 28 24 20 16 12  8  4]\n Total non zero elements: 21216\n Maximum absolute pairwise difference: 0.9727079272270203\n Average absolute pairwise difference: 0.018673043804830376\n </denchmark-code>\n \n Here you can find the model and the input/output tensors in case you need to take a look at them:\n <denchmark-link:https://github.com/microsoft/onnxruntime/files/3399061/discrepancy_CPU_GPU_sorted.zip>discrepancy_CPU_GPU_sorted.zip</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "xgirones", "commentT": "2019-07-17T21:35:34Z", "comment_text": "\n \t\tRoot cause addressed, will send a PR.\n <denchmark-link:https://github.com/xgirones>@xgirones</denchmark-link>\n  By they way, Is it possible for us to put your model in our build pipeline to make sure no regression in future?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "xgirones", "commentT": "2019-07-17T22:47:06Z", "comment_text": "\n \t\tThis is great news, thanks!\n \n @xgirones By they way, Is it possible for us to put your model in our build pipeline to make sure no regression in future?\n \n Yes, of course. You can use it freely if you find it convenient. It is just a toy model I created to learn more about the best loss functions for sequences.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "xgirones", "commentT": "2019-07-18T16:28:30Z", "comment_text": "\n \t\tThe fix is merged. Also sorted sequence_lengths is not a limitation any more. Your first model also works. I'll add them to our test pipeline.\n Great thanks for your findings!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "xgirones", "commentT": "2019-07-19T18:10:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/xgirones>@xgirones</denchmark-link>\n  Do you mind to close the issue or you still need some verification?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "xgirones", "commentT": "2019-07-19T19:10:01Z", "comment_text": "\n \t\tClosing it... Thanks for your help!\n \t\t"}}}, "commit": {"commit_id": "1ff957f96e62e6eb4b4439469cd10648be748efc", "commit_author": "Hector Li", "commitT": "2019-07-18 09:17:44-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cuda\\cudnn_common.cc", "file_new_name": "onnxruntime\\core\\providers\\cuda\\cudnn_common.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "86,87", "deleted_lines": "86", "method_info": {"method_name": "onnxruntime::cuda::CudnnDataTensor::Set", "method_params": "dataType,max_seq_length,batch_size,data_size,seq_lengths", "method_startline": "79", "method_endline": "96"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cuda\\rnn\\cudnn_rnn_base.cc", "file_new_name": "onnxruntime\\core\\providers\\cuda\\rnn\\cudnn_rnn_base.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "236,239,240,241,292,293,294,295", "deleted_lines": "223,237,238", "method_info": {"method_name": "onnxruntime::cuda::CudnnRnnBase<T>::ComputeInternal", "method_params": "ctx", "method_startline": "153", "method_endline": "343"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\test\\providers\\cpu\\rnn\\deep_cpu_lstm_op_test.cc", "file_new_name": "onnxruntime\\test\\providers\\cpu\\rnn\\deep_cpu_lstm_op_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "LSTMTest,ONNXRuntime_TestLSTMSequenceLengthShorterThanInputSequenceLengthNoP", "method_startline": "1094", "method_endline": "1123"}}}}}}}