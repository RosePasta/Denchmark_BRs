{"BR": {"BR_id": "1201", "BR_author": "Dunrar", "BRopenT": "2020-03-20T22:35:06Z", "BRcloseT": "2020-03-31T06:24:27Z", "BR_text": {"BRsummary": "Early stopping not working on 0.7.1", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Early stopping does not work anymore. When I downgrade from 0.7.1 or the current dev version to 0.6.0 early stopping works again, with the same code.\n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>def main(hparams):\n     if hparams.early_stopping == 'yes':\n         early_stopping = EarlyStopping(\n             monitor='batch/mean_absolute_loss',\n             min_delta=hparams.min_delta,\n             patience=hparams.patience,\n             mode='min'\n         )\n     else:\n         early_stopping = False\n \n     model = MemoryTest(hparams)\n     trainer = pl.Trainer(\n         val_percent_check=0,\n         early_stop_callback=early_stopping,\n         default_save_path=src.settings.LOG_DIR,\n         max_epochs=hparams.epochs\n     )\n \n     trainer.fit(model)\n </denchmark-code>\n \n <denchmark-code>class MemoryTest(pl.LightningModule):\n     # Main Testing Unit for Experiments on Recurrent Cells\n     def __init__(self, hp):\n         super(MemoryTest, self).__init__()\n         self.predict_col = hp.predict_col\n         self.n_datasamples = hp.n_datasamples\n         self.dataset = hp.dataset\n         if self.dataset is 'rand':\n             self.seq_len = None\n         else:\n             self.seq_len = hp.seq_len\n         self.hparams = hp\n         self.learning_rate = hp.learning_rate\n         self.training_losses = []\n         self.final_loss = None\n \n         self.model = RecurrentModel(1, hp.n_cells, hp.n_layers, celltype=hp.celltype)\n \n     def forward(self, input, input_len):\n         return self.model(input, input_len)\n \n     def training_step(self, batch, batch_idx):\n         x, y, input_len = batch\n         features_y = self.forward(x, input_len)\n \n         loss = F.mse_loss(features_y, y)\n         mean_absolute_loss = F.l1_loss(features_y, y)\n \n         self.training_losses.append(mean_absolute_loss.item())\n \n         neptune_logs = {'batch/train_loss': loss, 'batch/mean_absolute_loss': mean_absolute_loss}\n         return {'loss': loss, 'batch/mean_absolute_loss': mean_absolute_loss, 'log': neptune_logs}\n \n     def on_epoch_end(self):\n         train_loss_mean = np.mean(self.training_losses)\n         self.final_loss = train_loss_mean\n         self.training_losses = []  # reset for next epoch\n \n     def configure_optimizers(self):\n         return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n \n     @pl.data_loader\n     def train_dataloader(self):\n         train_dataset = dg.RandomDataset(self.predict_col, self.n_datasamples)\n         if self.dataset == 'rand_fix':\n             train_dataset = dg.RandomDatasetFix(self.predict_col, self.n_datasamples, self.seq_len)\n         if self.dataset == 'correlated':\n             train_dataset = dg.CorrelatedDataset(self.predict_col, self.n_datasamples)\n         train_loader = DataLoader(dataset=train_dataset, batch_size=1)\n         return train_loader\n \n     @staticmethod\n     def add_model_specific_args(parent_parser):\n         # MODEL specific\n         model_parser = ArgumentParser(parents=[parent_parser])\n         model_parser.add_argument('--learning_rate', default=1e-2, type=float)\n         model_parser.add_argument('--n_layers', default=1, type=int)\n         model_parser.add_argument('--n_cells', default=5, type=int)\n         model_parser.add_argument('--celltype', default='LSTM', type=str)\n \n         # training specific (for this model)\n         model_parser.add_argument('--epochs', default=500, type=int)\n         model_parser.add_argument('--patience', default=5, type=int)\n         model_parser.add_argument('--min_delta', default=0.1, type=float)\n         model_parser.add_argument('--early_stopping', default='yes', type=str)\n \n         # data specific\n         model_parser.add_argument('--n_datasamples', default=1000, type=int)\n         model_parser.add_argument('--seq_len', default=10, type=int)\n         model_parser.add_argument('--dataset', default='rand', type=str)\n         model_parser.add_argument('--predict_col', default=1, type=int)\n \n         return model_parser\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Early-stopping to take effect again.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Dunrar", "commentT": "2020-03-20T22:44:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Dunrar>@Dunrar</denchmark-link>\n  would you check it on actual master?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Dunrar", "commentT": "2020-03-21T13:22:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  do you mean the bleeding edge version via ?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Dunrar", "commentT": "2020-03-21T14:44:39Z", "comment_text": "\n \t\tOkay, I tried that but early stopping still does not work\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Dunrar", "commentT": "2020-03-21T15:16:18Z", "comment_text": "\n \t\tThe code sample you provide does not define a validation step/end/dataloader.\n I would expect that early stopping does not work without it. How could it?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Dunrar", "commentT": "2020-03-21T15:19:00Z", "comment_text": "\n \t\tif no val step is present, it uses the training step for early stopping\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Dunrar", "commentT": "2020-03-21T15:20:04Z", "comment_text": "\n \t\toh, my bad! Then I will have a closer look at this issue.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "Dunrar", "commentT": "2020-03-24T12:55:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  little update. In training_loop.py the line  is to blame. Just deleting the self.disable_validation and is_val_epoch checks solves the problem in my case, but there is probably more to take into consideration.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "Dunrar", "commentT": "2020-03-24T13:05:25Z", "comment_text": "\n \t\tI also came to that point when I looked at it 2 days ago, will have more time to look at it soon. If I remember correctly, the tests didnt pass and I was tracking down at which point the change was introduced to figure out the reason it is there.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "Dunrar", "commentT": "2020-03-31T06:50:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Dunrar>@Dunrar</denchmark-link>\n  Thanks for the help. Your suggestion worked and I was able to make a test so that it doesn't break in the future :)\n cheers!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "Dunrar", "commentT": "2020-03-31T06:54:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  Thank you!\n \t\t"}}}, "commit": {"commit_id": "1aba411da96ed95419d13ec1f86a0d38a232f73e", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-03-31 06:24:26+00:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "46", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\early_stopping.rst", "file_new_name": "docs\\source\\early_stopping.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7,8,33,34,35,36,37,38,39,40,43,44", "deleted_lines": "7,8,35"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "370,371", "deleted_lines": "370,371", "method_info": {"method_name": "train", "method_params": "self", "method_startline": "284", "method_endline": "383"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\trainer\\test_callbacks.py", "file_new_name": "tests\\trainer\\test_callbacks.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "162,163,164,165,166", "deleted_lines": null, "method_info": {"method_name": "test_early_stopping_without_val_step.training_step", "method_params": "self,args,kwargs", "method_startline": "162", "method_endline": "166"}}, "hunk_1": {"Ismethod": 1, "added_lines": "156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183", "deleted_lines": null, "method_info": {"method_name": "test_early_stopping_without_val_step", "method_params": "tmpdir", "method_startline": "156", "method_endline": "183"}}}}}}}