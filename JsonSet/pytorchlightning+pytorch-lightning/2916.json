{"BR": {"BR_id": "2916", "BR_author": "angshine", "BRopenT": "2020-08-11T15:41:05Z", "BRcloseT": "2020-08-12T10:31:18Z", "BR_text": {"BRsummary": "ModelCheckpoint with custom filepath don't support training on multiple nodes", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When training on multiple nodes using  with custom , it will raise  caused by the following line of code: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/97e6f35b34437c89d422bd440dca4a8d2c4d5a9f/pytorch_lightning/callbacks/model_checkpoint.py#L127>model_checkpoint.py#L127</denchmark-link>\n .\n Maybe a try-except block is needed?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "angshine", "commentT": "2020-08-11T16:50:09Z", "comment_text": "\n \t\tNo I think we just need to pass in exist_ok=True into makedirs :)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "angshine", "commentT": "2020-08-11T16:52:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/f4hy>@f4hy</denchmark-link>\n  your PR added this line. Do you know a good way to fix it?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "angshine", "commentT": "2020-08-11T18:02:58Z", "comment_text": "\n \t\tAh sorry. I think I know what's up. I'll get a patch out this evening. Sorry!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "angshine", "commentT": "2020-08-12T05:07:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/angshine>@angshine</denchmark-link>\n  I found a few issues with the model checkpoint path stuff. Not 100% sure I found the particular bug you were seeing but I think this should fix it. Can you give my branch in the above PR a test? Sorry to have introduced this bug for you.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "angshine", "commentT": "2020-09-03T06:19:39Z", "comment_text": "\n \t\tSorry for the late reply, but it seems that this bug has not been fully fixed. <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/ee72271d205beb2c012e34425cb16e189cc56c7d/pytorch_lightning/utilities/cloud_io.py#L88>This line</denchmark-link>\n  still raises an exception:   when training with DDP. I still need to manually add a  to ignore the exception.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "angshine", "commentT": "2020-09-04T04:17:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/angshine>@angshine</denchmark-link>\n  That line has been completely replaced now on master. Can you give it another try. I hope <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3320>#3320</denchmark-link>\n   has finally resolved this.\n \t\t"}}}, "commit": {"commit_id": "56396abe9839fa075bcc087c32f098145b0bdc9f", "commit_author": "Brendan Fahy", "commitT": "2020-08-12 06:31:17-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_new_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "177,178,179,180,181,182", "deleted_lines": "177", "method_info": {"method_name": "_del_model", "method_params": "self,filepath", "method_startline": "169", "method_endline": "182"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "440,442,444,445,446", "deleted_lines": "439,441", "method_info": {"method_name": "transfer_distrib_spawn_state_on_fit_end", "method_params": "self,model,mp_queue,results", "method_startline": "418", "method_endline": "447"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "884", "deleted_lines": "883", "method_info": {"method_name": "default_root_dir", "method_params": "self", "method_startline": "879", "method_endline": "887"}}, "hunk_1": {"Ismethod": 1, "added_lines": "895", "deleted_lines": "894", "method_info": {"method_name": "weights_save_path", "method_params": "self", "method_startline": "890", "method_endline": "898"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_io.py", "file_new_name": "pytorch_lightning\\trainer\\training_io.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "273,278,280,281,282", "deleted_lines": "272,277,279,280", "method_info": {"method_name": "_atomic_save", "method_params": "self,checkpoint,str", "method_startline": "260", "method_endline": "282"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\utilities\\cloud_io.py", "file_new_name": "pytorch_lightning\\utilities\\cloud_io.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "72,73", "deleted_lines": null, "method_info": {"method_name": "makedirs", "method_params": "pathlike", "method_startline": "70", "method_endline": "75"}}, "hunk_1": {"Ismethod": 1, "added_lines": "31,32,33,34,35,36", "deleted_lines": null, "method_info": {"method_name": "is_remote_path", "method_params": "pathlike", "method_startline": "31", "method_endline": "36"}}}}}}}