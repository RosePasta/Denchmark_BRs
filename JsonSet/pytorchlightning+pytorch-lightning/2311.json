{"BR": {"BR_id": "2311", "BR_author": "drozzy", "BRopenT": "2020-06-21T23:44:11Z", "BRcloseT": "2020-09-19T23:00:59Z", "BR_text": {"BRsummary": "overfit_batches doesn't work", "BRdescription": "\n When I try to use overfit_batches:\n <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/debugging.html#make-model-overfit-on-subset-of-data>https://pytorch-lightning.readthedocs.io/en/latest/debugging.html#make-model-overfit-on-subset-of-data</denchmark-link>\n \n <denchmark-code> trainer = Trainer(gpus=num_gpus, max_epochs=config.epochs, overfit_batches=0.01, logger=logger)\n </denchmark-code>\n \n my code fails with:\n <denchmark-code>   trainer.fit(module)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in fit\n     self.single_gpu_train(model)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 176, in single_gpu_train\n     self.run_pretrain_routine(model)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1065, in run_pretrain_routine\n     self.reset_val_dataloader(ref_model)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 331, in reset_val_dataloader\n     self._reset_eval_dataloader(model, 'val')\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 314, in _reset_eval_dataloader\n     f'you requested to check {limit_eval_batches} of the {mode} dataloader but'\n pytorch_lightning.utilities.exceptions.MisconfigurationException: you requested to check 0.01 of the val dataloader but 0.01*0 = 0. Please increase the limit_val_batches. Try at least limit_val_batches=0.09090909090909091\n </denchmark-code>\n \n P.S.: I also tried setting limit_val_batches=0.09090909090909091. Same error.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "drozzy", "commentT": "2020-06-21T23:47:34Z", "comment_text": "\n \t\tDid you check if the length of your dataloader (how many iteration it has) is different than 0?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "drozzy", "commentT": "2020-06-21T23:49:05Z", "comment_text": "\n \t\tNot sure what you mean, but I am able to train the regular way... without the overfit_batches setting.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "drozzy", "commentT": "2020-06-21T23:50:29Z", "comment_text": "\n \t\tDoes the validation step of your model goes without any problem?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "drozzy", "commentT": "2020-06-21T23:56:00Z", "comment_text": "\n \t\tOh yeah.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "drozzy", "commentT": "2020-06-22T20:04:24Z", "comment_text": "\n \t\t\n Not sure what you mean, but I am able to train the regular way... without the overfit_batches setting.\n \n can you check the value of len(valid_dataloader)??\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "drozzy", "commentT": "2020-06-22T21:52:27Z", "comment_text": "\n \t\tI've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "drozzy", "commentT": "2020-06-23T00:26:27Z", "comment_text": "\n \t\t\n When I try to use overfit_batches:\n https://pytorch-lightning.readthedocs.io/en/latest/debugging.html#make-model-overfit-on-subset-of-data\n  trainer = Trainer(gpus=num_gpus, max_epochs=config.epochs, overfit_batches=0.01, logger=logger)\n \n my code fails with:\n    trainer.fit(module)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in fit\n     self.single_gpu_train(model)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 176, in single_gpu_train\n     self.run_pretrain_routine(model)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1065, in run_pretrain_routine\n     self.reset_val_dataloader(ref_model)\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 331, in reset_val_dataloader\n     self._reset_eval_dataloader(model, 'val')\n   File \"/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 314, in _reset_eval_dataloader\n     f'you requested to check {limit_eval_batches} of the {mode} dataloader but'\n pytorch_lightning.utilities.exceptions.MisconfigurationException: you requested to check 0.01 of the val dataloader but 0.01*0 = 0. Please increase the limit_val_batches. Try at least limit_val_batches=0.09090909090909091\n \n P.S.: I also tried setting limit_val_batches=0.09090909090909091. Same error.\n \n try using a slightly bigger overfit_batches (e.g. overfit_batches=0.1) or number of batches (e.g. overfit_batches=10)\n \n I've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.\n \n Probably the training get's stuck in local optima.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "drozzy", "commentT": "2020-06-23T07:38:17Z", "comment_text": "\n \t\t\n I've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.\n \n overfit_batches just reduces your num_batches so that it can overfit your model on a small batch to check whether the model can adapt your dataset or not. It will still run for n epochs even if you set overfit_batches to any value.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "drozzy", "commentT": "2020-06-24T03:56:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Kshitij09>@Kshitij09</denchmark-link>\n  you also need to adjust your learning rate... otherwise it might get stuck in a local min (likely lower your lr)\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "drozzy", "commentT": "2020-06-24T13:34:20Z", "comment_text": "\n \t\t\n \n I've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.\n \n overfit_batches just reduces your num_batches so that it can overfit your model on a small batch to check whether the model can adapt your dataset or not. It will still run for n epochs even if you set overfit_batches to any value.\n \n <denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>\n  so do I need to couple it with  ?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "drozzy", "commentT": "2020-06-24T13:36:54Z", "comment_text": "\n \t\t\n @Kshitij09 you also need to adjust your learning rate... otherwise it might get stuck in a local min (likely lower your lr)\n \n <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  yes I've also incorporated  with this which dropped lr upto  but didn't stop training\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "drozzy", "commentT": "2020-08-25T15:31:29Z", "comment_text": "\n \t\tThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "drozzy", "commentT": "2020-09-02T09:31:51Z", "comment_text": "\n \t\timport os\n import time\n \n import torch\n import torch.nn.functional as F\n from torchvision.datasets import MNIST\n from torch.utils.data import DataLoader, random_split\n from torchvision import transforms\n import pytorch_lightning as pl\n \n \n class LitClassifier(pl.LightningModule):\n \n     def __init__(self):\n         super().__init__()\n         self.l1 = torch.nn.Linear(28 * 28, 10)\n         self.i = 0\n \n     def forward(self, x):\n         return torch.relu(self.l1(x.view(x.size(0), -1)))\n \n     def training_step(self, batch, batch_idx):\n         x, (y, idxs) = batch\n         print(f\"training step {self.i}, batch_idx {batch_idx}, items: {idxs.cpu().numpy()}\")\n         self.i += 1\n         y_hat = self(x)\n         loss = F.cross_entropy(y_hat, y)\n         result = pl.TrainResult(loss)\n         result.log('train_loss', loss, on_epoch=True)\n         return result\n \n     def validation_step(self, batch, batch_idx):\n         x, (y, idxs) = batch\n         y_hat = self(x)\n         loss = F.cross_entropy(y_hat, y)\n         result = pl.EvalResult(checkpoint_on=loss)\n         result.log('val_loss', loss)\n         return result\n \n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters(), lr=0.02)\n \n \n class MNISTDataset(MNIST):\n     def __getitem__(self, item):\n         x, y = super().__getitem__(item)\n         return (x, (y, item))\n \n \n # train!\n dataset = MNISTDataset(os.getcwd(), download=True, transform=transforms.ToTensor())\n train, val = random_split(dataset, [55000, 5000])\n \n model = LitClassifier()\n trainer = pl.Trainer(overfit_batches=1, gpus=1, progress_bar_refresh_rate=0)\n trainer.fit(model, DataLoader(train, shuffle=False, batch_size=4, num_workers=0),\n             DataLoader(val, batch_size=4, num_workers=0))\n Produces\n <denchmark-code>GPU available: True, used: True\n TPU available: False, using: 0 TPU cores\n CUDA_VISIBLE_DEVICES: [0]\n /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not l\n og computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n warnings.warn(*args, **kwargs)\n \n | Name | Type   | Params\n --------------------------------\n 0 | l1   | Linear | 7 K\n /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa\n der, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try\n 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa\n der, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try\n 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n training step 0, batch_idx 0, items: [13556 55560 49266  5079]\n training step 1, batch_idx 0, items: [13556 55560 49266  5079]\n training step 2, batch_idx 0, items: [13556 55560 49266  5079]\n training step 3, batch_idx 0, items: [13556 55560 49266  5079]\n training step 4, batch_idx 0, items: [13556 55560 49266  5079]\n training step 5, batch_idx 0, items: [13556 55560 49266  5079]\n training step 6, batch_idx 0, items: [13556 55560 49266  5079]\n training step 7, batch_idx 0, items: [13556 55560 49266  5079]\n training step 8, batch_idx 0, items: [13556 55560 49266  5079]\n training step 9, batch_idx 0, items: [13556 55560 49266  5079]\n training step 10, batch_idx 0, items: [13556 55560 49266  5079]\n training step 11, batch_idx 0, items: [13556 55560 49266  5079]\n training step 12, batch_idx 0, items: [13556 55560 49266  5079]\n training step 13, batch_idx 0, items: [13556 55560 49266  5079]\n training step 14, batch_idx 0, items: [13556 55560 49266  5079]\n training step 15, batch_idx 0, items: [13556 55560 49266  5079]\n training step 16, batch_idx 0, items: [13556 55560 49266  5079]\n training step 17, batch_idx 0, items: [13556 55560 49266  5079]\n training step 18, batch_idx 0, items: [13556 55560 49266  5079]\n training step 19, batch_idx 0, items: [13556 55560 49266  5079]\n training step 20, batch_idx 0, items: [13556 55560 49266  5079]\n training step 21, batch_idx 0, items: [13556 55560 49266  5079]\n training step 22, batch_idx 0, items: [13556 55560 49266  5079]\n training step 23, batch_idx 0, items: [13556 55560 49266  5079]\n training step 24, batch_idx 0, items: [13556 55560 49266  5079]\n </denchmark-code>\n \n Toggling shuffle=True results in\n <denchmark-code>GPU available: True, used: True\n TPU available: False, using: 0 TPU cores\n CUDA_VISIBLE_DEVICES: [0]\n /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not l\n og computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n warnings.warn(*args, **kwargs)\n \n | Name | Type   | Params\n --------------------------------\n 0 | l1   | Linear | 7 K\n /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: You request\n ed to overfit but enabled training dataloader shuffling. We are turning it off for you.\n warnings.warn(*args, **kwargs)\n /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa\n der, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try\n 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa\n der, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try\n 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n training step 0, batch_idx 0, items: [19838 40946 18620 21942]\n training step 1, batch_idx 0, items: [59940 37871 44153  6899]\n training step 2, batch_idx 0, items: [53096 12012 22479 27454]\n training step 3, batch_idx 0, items: [12640 55771 22517 27844]\n training step 4, batch_idx 0, items: [35820 56735 43191 58511]\n training step 5, batch_idx 0, items: [35818 38129  4901 46901]\n training step 6, batch_idx 0, items: [21038 14631 15166 15581]\n training step 7, batch_idx 0, items: [ 7095 15539  8672 39255]\n training step 8, batch_idx 0, items: [ 6397 24324 27822 53308]\n training step 9, batch_idx 0, items: [ 7261 45991 58502 38393]\n training step 10, batch_idx 0, items: [50646 43129  4348 32436]\n training step 11, batch_idx 0, items: [11271 13858 11991 43261]\n training step 12, batch_idx 0, items: [29346 42714 52281 36790]\n training step 13, batch_idx 0, items: [21324 32598 43017  8024]\n training step 14, batch_idx 0, items: [30809 50140  5554 36657]\n training step 15, batch_idx 0, items: [ 1462  4226 44369 40183]\n training step 16, batch_idx 0, items: [53579 10375 22340  4105]\n training step 17, batch_idx 0, items: [47785 10585 12661 35176]\n training step 18, batch_idx 0, items: [16489 26748 25997  8344]\n training step 19, batch_idx 0, items: [38492 45758 56593 37933]\n training step 20, batch_idx 0, items: [  527  4662 29285 26215]\n training step 21, batch_idx 0, items: [ 1838 42586  9805 13441]\n training step 22, batch_idx 0, items: [12649 11892  4140 56752]\n training step 23, batch_idx 0, items: [48902 57464 57910 54211]\n training step 24, batch_idx 0, items: [38774 16780 10018 49934]\n </denchmark-code>\n \n Note the warning\n <denchmark-code>/home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: You request\n ed to overfit but enabled training dataloader shuffling. We are turning it off for you.\n </denchmark-code>\n \n Is incorrect and very misleading to the user.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "drozzy", "commentT": "2020-09-02T12:12:34Z", "comment_text": "\n \t\tSimilar issue here.\n With DDP backend the trainer keeps drawing random samples even if shuffle is manually set to False (works fine with DP backend).\n Other configs/params:\n gpus=2\n batch_size=1\n overfit_batches=4\n accumulate_grad_batches=1\n num_workers=2 (larger values resulted in the trainer draw overfit_batchesnum_workersbatch_size random samples every epoch which is hard to follow)\n Minimal code for reproduction (I also tried with different number of batches, batch size, num workers, etc.)\n <denchmark-code>import torch\n from torch.nn import Conv2d\n from torch.optim import SGD\n from torch.utils.data import DataLoader, Dataset\n from pytorch_lightning.metrics.regression import MSE\n import pytorch_lightning as pl\n from pytorch_lightning import Trainer\n \n \n class MyDataset(Dataset):\n     def __init__(self, size=100):\n         super(MyDataset, self).__init__()\n         self.data = torch.stack([idx * torch.ones(3,100,100) for idx in range(size)])\n         self.idx_list = []\n \n     def __getitem__(self, idx):\n         return self.data[idx]\n \n     def __len__(self):\n         return self.data.shape[0]\n \n class MyModel(pl.LightningModule):\n     def __init__(self):\n         super(MyModel, self).__init__()\n         self.conv_1 = Conv2d(in_channels=3, out_channels=3, kernel_size=1, stride=1)\n         self.loss = MSE()\n         self.idx_list = []\n     def forward(self, batch):\n         return self.conv_1(batch)\n     \n     def training_step(self, batch, batch_idx):\n         idx = batch[0,0,0,0].detach()\n         pred = self.forward(batch)\n         loss = self.loss(pred, batch)\n         return {'loss': loss, 'idx': idx}\n \n     def training_epoch_end(self, outputs):\n         idx_list = torch.tensor([x['idx'] for x in outputs])\n         print('Epoch: {}, device: {} samples: {}'.format(self.current_epoch, self.device, idx_list))\n         return torch.stack([x['loss'] for x in outputs]).mean()\n \n     def setup(self, stage):\n         self.dataset = MyDataset()\n \n     def train_dataloader(self):\n         loader = DataLoader(self.dataset, batch_size=1, num_workers=20, pin_memory=True, shuffle=False)\n         return loader\n \n     def configure_optimizers(self):\n         return SGD(self.parameters(), lr=0.001)\n \n \n def main():\n \n     pl_model = MyModel()\n     # trainer = Trainer(distributed_backend='ddp', num_nodes=1, gpus=2, overfit_batches=4)\n     trainer = Trainer(distributed_backend='ddp', gpus=2, overfit_batches=5, max_epochs=4, check_val_every_n_epoch=100)\n     trainer.fit(pl_model)\n \n \n if __name__ == '__main__':\n     main()\n </denchmark-code>\n \n Output (ddp backend):\n \n Epoch: 0, device: cuda:0 samples: tensor([44., 93., 71., 37., 53.])\n Epoch: 0, device: cuda:1 samples: tensor([19., 90., 69., 95., 91.])\n Epoch: 1, device: cuda:0 samples: tensor([45., 90., 35., 17., 79.])\n Epoch: 1, device: cuda:1 samples: tensor([15., 32., 63., 72., 96.])\n Epoch: 2, device: cuda:0 samples: tensor([48.,  1., 90., 10.,  7.])\n Epoch: 2, device: cuda:1 samples: tensor([97., 81., 49.,  8., 20.])\n Epoch: 3, device: cuda:0 samples: tensor([86., 89.,  3., 22., 25.])\n Epoch: 3, device: cuda:1 samples: tensor([42., 92., 20., 48., 93.])\n \n Output (dp backend):\n \n Epoch: 0, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])\n Epoch: 1, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])\n Epoch: 2, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])\n Epoch: 3, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])\n \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "drozzy", "commentT": "2020-09-17T22:04:54Z", "comment_text": "\n \t\tI just looked at it. In summary:\n \n \n OP had this problem:\n MisconfigurationException: you requested to check 0.01 of the val dataloader but 0.01*0 = 0. Please increase the limit_val_batches. Try at least limit_val_batches=0.09090909090909091\n This message is correct, it is telling you that the percentage you have chosen corresponds to less than one batch.\n Solution:\n You need to increase the value. But what you probably want is overfit_batches=1, and this works with exactly one batch without error.\n \n \n The example code by @willprice works on master #3501\n \n \n The example by @itsikad shows an issue with DDP. As far as I can tell, this is the only remaining problem in this thread here. I can take a look\n \n \n \t\t"}}}, "commit": {"commit_id": "e6c7548b306055e41552e23d57f0057e7f441256", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-09-19 19:00:58-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "69,70,71", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\data_loading.py", "file_new_name": "pytorch_lightning\\trainer\\data_loading.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "158", "deleted_lines": "158", "method_info": {"method_name": "_get_distributed_sampler", "method_params": "self,dataloader,train", "method_startline": "143", "method_endline": "160"}}}}}}}