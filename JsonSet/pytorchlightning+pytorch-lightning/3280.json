{"BR": {"BR_id": "3280", "BR_author": "hyukyu", "BRopenT": "2020-08-31T06:32:08Z", "BRcloseT": "2020-09-11T14:55:59Z", "BR_text": {"BRsummary": "Error in transfer_batch_to_device when None type is in the batch", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n There should be no torchtext pre-installed\n Run the sample code\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>from torch.utils.data import DataLoader\n import pytorch_lightning as pl\n \n \n def collate_fn(batch):\n     return batch\n \n \n class MyDataModule(pl.LightningDataModule):\n     def __init__(self):\n         super().__init__()\n \n     def prepare_data(self):\n         pass\n \n     def setup(self, stage):\n         self.train = [{\"input\": torch.randn(1,2), \"output\": None}]\n \n     def train_dataloader(self):\n         return DataLoader(self.train, batch_size=1, collate_fn=collate_fn)\n \n \n class MyModel(pl.LightningModule):\n     def __init__(self):\n         super().__init__()\n         self.linear = torch.nn.Linear(2, 1)\n \n     def forward(self, x):\n         return self.linear(x)\n \n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters())\n \n     def training_step(self, batch, batch_idx):\n         x = batch[0][\"input\"]\n         y = batch[0][\"output\"]\n         loss = self(x)\n         result = pl.TrainResult(loss)\n         result.log('train_loss', loss, on_epoch=True)\n         return result\n \n \n def main():\n     # Dataset\n     data_module = MyDataModule()\n \n     # Model\n     model = MyModel()\n \n     # Train\n     trainer = pl.Trainer(max_steps=1)\n     trainer.fit(model, datamodule=data_module)\n \n \n if __name__ == \"__main__\":\n     main()\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The above code runs fine if the package torchtext is installed. However, the code raises following error if torchtext is not available and I believe this inconsistency is a bug.\n <denchmark-code> File \"python3.6/site-packages/pytorch_lightning/utilities/apply_func.py\", line 122, in batch_to\n     return data.to(device, **kwargs)\n AttributeError: 'NoneType' object has no attribute 'to'\n python-BaseException\n </denchmark-code>\n \n I think this line of the <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/f46318ebfeb785a659c49091a6871584ccde3ee1/pytorch_lightning/utilities/apply_func.py#L24>code</denchmark-link>\n  is the cause of the problem\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n \n GPU:\n \n TITAN Xp\n TITAN Xp\n TITAN Xp\n TITAN Xp\n TITAN Xp\n TITAN Xp\n TITAN Xp\n TITAN Xp\n \n \n available:         True\n version:           10.2\n \n \n Packages:\n \n numpy:             1.18.4\n pyTorch_debug:     False\n pyTorch_version:   1.6.0\n pytorch-lightning: 0.9.0\n tensorboard:       2.2.1\n tqdm:              4.46.0\n \n \n System:\n \n OS:                Linux\n architecture:\n \n 64bit\n ELF\n \n \n processor:         x86_64\n python:            3.6.8\n version:           #81~16.04.1-Ubuntu SMP Tue Nov 26 16:34:21 UTC 2019\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hyukyu", "commentT": "2020-08-31T06:32:47Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hyukyu", "commentT": "2020-09-01T13:36:08Z", "comment_text": "\n \t\tThanks! Mind sending us a PR with the fix?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hyukyu", "commentT": "2020-09-11T00:08:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  mind taking a look?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hyukyu", "commentT": "2020-09-11T07:08:23Z", "comment_text": "\n \t\tI submitted a fix.\n <denchmark-link:https://github.com/hyukyu>@hyukyu</denchmark-link>\n  Be aware, torchtext will remove/deprecate this Batch object soon. So in the future you should switch to their new dataloading pipeline that integrates better with PyTorch and Lightning.\n \t\t"}}}, "commit": {"commit_id": "bd5f53c51994e14c79404c9dcededae53b21b664", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-09-11 10:55:58-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\utilities\\apply_func.py", "file_new_name": "pytorch_lightning\\utilities\\apply_func.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "124,125", "deleted_lines": "124", "method_info": {"method_name": "move_data_to_device", "method_params": "Any,device", "method_startline": "92", "method_endline": "125"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\models\\test_gpu.py", "file_new_name": "tests\\models\\test_gpu.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "344,345,346,347,348,349", "deleted_lines": null, "method_info": {"method_name": "test_single_gpu_batch_parse", "method_params": "", "method_startline": "340", "method_endline": "431"}}}}}}}