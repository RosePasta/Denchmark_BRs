{"BR": {"BR_id": "1522", "BR_author": "Jonas-Jaeger", "BRopenT": "2020-04-17T23:15:19Z", "BRcloseT": "2020-04-19T03:07:16Z", "BR_text": {"BRsummary": "Performance drop when activating gradient clipping", "BRdescription": "\n Hello all,\n I experienced a substantial drop in computation time when activating gradient clipping (by passing a non-zero value to the keyword argument gradient_clip_val when initializing the Trainer).\n I noticed that in the current implementation of the clipping_gradient method in pytorch-lightning/trainer/training_tricks.py redundant computations are made by first computing the 2-norm and second squaring this result, which could be shortened by computing the sum of squares directly. This saves one square root and squaring operation per parameter set.\n Best,\n Jonas\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>cuda:\n \tGPU:\n \tavailable:           False\n \tversion:             None\n packages:\n \tnumpy:               1.18.1\n \tpyTorch_debug:       False\n \tpyTorch_version:     1.4.0\n \tpytorch-lightning:   0.7.4-dev\n \ttensorboard:         2.2.1\n \ttqdm:                4.45.0\n system:\n \tOS:                  Darwin\n \tarchitecture:\n \t\t64bit\n \t\t\n \tprocessor:           i386\n \tpython:              3.8.2\n \tversion:             Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n I trained a relatively small (two-layered) MLP on MNIST; perhaps this performance drop does not become that apparent when training on larger network architectures.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Jonas-Jaeger", "commentT": "2020-04-17T23:15:56Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Jonas-Jaeger", "commentT": "2020-04-17T23:20:13Z", "comment_text": "\n \t\tUnfortunately, it seems that I do not have push access to this repository to push the proposed fix and create a pull request for this issue.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Jonas-Jaeger", "commentT": "2020-04-17T23:42:13Z", "comment_text": "\n \t\tfork the repo\n make the fix\n submit a PR\n :)\n \t\t"}}}, "commit": {"commit_id": "e02146943d3373020b7fa6e8acc31dc18b4201e4", "commit_author": "Jonas-Jaeger", "commitT": "2020-04-18 23:07:15-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_tricks.py", "file_new_name": "pytorch_lightning\\trainer\\training_tricks.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "43", "deleted_lines": "43", "method_info": {"method_name": "clip_gradients", "method_params": "self", "method_startline": "26", "method_endline": "49"}}}}}}}