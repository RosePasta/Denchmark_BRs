{"BR": {"BR_id": "1485", "BR_author": "olineumann", "BRopenT": "2020-04-14T09:02:22Z", "BRcloseT": "2020-05-02T12:50:48Z", "BR_text": {"BRsummary": "wandb logger 'global_step' affects other logger", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n The wandb logger adds a 'global_step' to the metric dict which appears in all other loggers (e.g. Tensorboard). Only the wandb logger is adding 'global_step' to metric and I think it is not necessary. Another side effect of that is, that 'global_step' is also added to empty dicts which then are logged and resulting to strange graphs like this:\n <denchmark-link:https://user-images.githubusercontent.com/12863612/79206766-1dbaa780-7e40-11ea-9c1c-c05e92c94641.png></denchmark-link>\n \n or this\n <denchmark-link:https://user-images.githubusercontent.com/12863612/79206838-34f99500-7e40-11ea-80bd-87462acf81fc.png></denchmark-link>\n \n I also wrote a simple logger class to print out metrics. I got this output:\n <denchmark-code>Step  0\n {'global_step': 0}\n Step  10\n {'global_step': 10}\n [...]\n Step  190\n {'global_step': 190}\n Step  200\n {'global_step': 200}\n Step  0\n {'val/mse': 0.01713273860514164, 'train/mse': 0.04259789362549782, 'global_step': 0}\n Step  207\n {'global_step': 207}\n Step  217\n {'global_step': 217}\n [...]\n Step  397\n {'global_step': 397}\n Step  407\n {'global_step': 407}\n Step  1\n {'val/mse': 0.013123581185936928, 'train/mse': 0.01449404377490282, 'global_step': 1}\n Step  414\n {'global_step': 414}\n Step  424\n {'global_step': 424}\n ...\n Step  604\n {'global_step': 604}\n Step  614\n {'global_step': 614}\n Step  2\n {'val/mse': 0.012394818477332592, 'train/mse': 0.012575697153806686, 'global_step': 2}\n \n [...]\n \n Step  5\n {'val/mse': 0.012411396019160748, 'train/mse': 0.011899641714990139, 'global_step': 5}\n Step  1242\n {'global_step': 1242}\n Step  1252\n {'global_step': 1252}\n [...]\n Step  1432\n {'global_step': 1432}\n Step  1442\n {'global_step': 1442}\n Step  6\n {'val/mse': 0.01244258601218462, 'train/mse': 0.011944737285375595, 'global_step': 6}\n Step  1449\n {'global_step': 1449}\n Step  1459\n {'global_step': 1459}\n [...]\n Step  1639\n {'global_step': 1639}\n Step  1649\n {'global_step': 1649}\n Step  7\n {'val/mse': 0.01261985208839178, 'train/mse': 0.011924241669476032, 'global_step': 7}\n Step  1656\n {'global_step': 1656}\n Step  1666\n {'global_step': 1666}\n [...]\n Step  1846\n {'global_step': 1846}\n Step  1856\n {'global_step': 1856}\n Step  8\n {'val/mse': 0.012863481417298317, 'train/mse': 0.011850016191601753, 'global_step': 8}\n Step  1863\n {'global_step': 1863}\n Step  1873\n [...]\n Step  2053\n {'global_step': 2053}\n Step  2063\n {'global_step': 2063}\n </denchmark-code>\n \n Also notice: I set max_epochs to 10 so expected to be 10 measurements. The last one is missing. But this could be handled in an other issue.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Use training_epoch_end and validation_epoch_end to log metric like {'log': {'loss': loss}} (see code bellow)\n Run training with wandb logger and one more logger of your choice.\n See global_step graphs.\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n Important LightningModule Methods:\n def training_step(self, batch, batch_idx):\n         # calculate actual model prediction given batch\n         # and calculate loss\n         x, y = batch\n         y_hat = self(x)\n         \n         # print out current loss on training every n-th iteration\n         loss = F.mse_loss(y_hat, y)\n         return {\n             \"loss\": loss\n         }\n     \n def training_epoch_end(self, outputs):\n     loss_mean = torch.stack([x[\"loss\"] for x in outputs]).mean().item()\n     return {\n         \"log\": {\n             \"train/mse\": loss_mean,\n             \"step\": self.current_epoch\n         }\n     }\n \n def validation_step(self, batch, batch_idx):\n     x, y = batch\n     y_hat = self(x)        \n     return {'val_loss': F.mse_loss(y_hat, y)}\n \n def validation_epoch_end(self, outputs):\n     val_loss_mean = torch.stack([x[\"val_loss\"] for x in outputs]).mean().item()\n     return {\n         \"val_loss\": val_loss_mean,\n         \"log\": {\n             \"val/mse\": val_loss_mean,\n             \"step\": self.current_epoch\n         }\n     }\n Training:\n clbk_terminal = TerminalCallback()\n checkpoint = ModelCheckpoint(filepath=\"ckpts/\" + name + \"{_val_loss:.5f}_{epoch:03d}\", prefix=\"BasicNN_\", monitor=\"val_loss\", verbose=False, save_top_k=3, save_weights_only=True)\n earlystopping = EarlyStopping(monitor=\"val_loss\", patience=25, verbose=True)\n loggers = [\n     WandbLogger(project=\"nwp-energy-load\", name=name, log_model=True),\n     TensorBoardLogger(save_dir=\"tb_logs\", name=name, version=0),\n     MyLogger() # only prints metric; can also be ignored\n ]\n \n trainer = Trainer(gpus=-1, max_epochs=10, progress_bar_refresh_rate=0, logger=loggers, log_save_interval=1, row_log_interval=10,\n                   callbacks=[], early_stop_callback=earlystopping, checkpoint_callback=checkpoint)\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Is 'global_step' needed in wandb logger? If so, it should not affect other loggers. Also if there is nothing to log (e.g. in training_step) the logger should log nothing.\n <denchmark-h:h3>Environment</denchmark-h>\n \n Linux Arch\n Python 3.8.2\n Pytorch 1.4.0\n Pytorch_Lightning 0.7.3\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "olineumann", "commentT": "2020-04-14T09:03:13Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "olineumann", "commentT": "2020-04-14T12:28:11Z", "comment_text": "\n \t\tI looked over the code and found the issue. Maybe the wandb python API didn't accepted step as parameter in the log function in a version before. So the step was added to the metric dict (and not copied so affected other loggers).\n Also I think that empty metric dicts could be skipped in the base logger. You can see my fix in the following commit:\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/compare/master...olineumann:issue/wandb_global_step>master...olineumann:issue/wandb_global_step</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "olineumann", "commentT": "2020-04-14T17:25:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/olineumann>@olineumann</denchmark-link>\n  nice, mind send a PR, seems that is could be one-click only :]\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "olineumann", "commentT": "2020-05-27T21:43:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  Now  is giving warnings  and not logging when I try to use the WandbLogger with k-fold cross-validation because there I am using the same instance of wandb_logger but using  multiple times for different train_dl and valid_dl. Since the step gets repeated in each case, it's not logging anything after the 1st fold is complete even though the log keys are completely different. For now, I have to create a different logger separately for each fold, but is there any other way around to make it work with the single instance.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "olineumann", "commentT": "2020-05-27T22:54:29Z", "comment_text": "\n \t\tI also noticed that 'epoch' appearing now after upgrading to the current version in the metric dict without me logging any 'epoch'. It comes from trainer.logging:\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/logging.py#L69>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/logging.py#L69</denchmark-link>\n \n Because of this the step=global_step and when I log something in my pytorch_lightning module with step=epoch it will crash because of the same reason. The only way to solve it is to pass a log dict in train_step and validation_step only containing 'step': epoch.\n What would be the best solution? I think:\n \n First case: User passes no step in log dict, so nothing is added to the dict and wandb.log(..., step=None).\n Second case: If the user passes step, 'step' should be added to the dict (without affecting other loggers!) and also, in this case, wandb.log(..., step=None).\n \n Sounds that useful/logical? I think this should work but I'm also tired. I can create a PR if wanted in the next days (Fr/Sa).\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "olineumann", "commentT": "2020-05-28T09:17:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/olineumann>@olineumann</denchmark-link>\n  I would prefer the second as we do not want to affect the other loggers\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "olineumann", "commentT": "2020-06-05T20:37:54Z", "comment_text": "\n \t\tI just wanted to fix it and pulled the current master from GitHub. It seems to be fixed already.\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/wandb.py#L131>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/wandb.py#L131</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "152a2eb30ce82deefdb738b81fda66a9c218ed76", "commit_author": "Oliver Neumann", "commitT": "2020-05-02 08:50:47-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\base.py", "file_new_name": "pytorch_lightning\\loggers\\base.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "128", "deleted_lines": "128", "method_info": {"method_name": "agg_and_log_metrics", "method_params": "self,str,None", "method_startline": "116", "method_endline": "129"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\wandb.py", "file_new_name": "pytorch_lightning\\loggers\\wandb.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "122", "deleted_lines": "122", "method_info": {"method_name": "log_metrics", "method_params": "self,str,None", "method_startline": "121", "method_endline": "122"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\loggers\\test_wandb.py", "file_new_name": "tests\\loggers\\test_wandb.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "17,21", "deleted_lines": "17,21", "method_info": {"method_name": "test_wandb_logger", "method_params": "wandb", "method_startline": "11", "method_endline": "30"}}}}}}}