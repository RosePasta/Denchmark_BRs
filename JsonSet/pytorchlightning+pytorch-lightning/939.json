{"BR": {"BR_id": "939", "BR_author": "helldragger", "BRopenT": "2020-02-25T11:51:32Z", "BRcloseT": "2020-02-27T20:54:07Z", "BR_text": {"BRsummary": "logger is NoneType hence doesn't have any experiment or other functionality in a lightning module", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When trying to use the logging abilities of lightning, I hit a wall, the default and tensorboard loggers both seem to stay uninitialized when calling trainer.fit(model), resulting in crashes everytime I try to log something.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Create a lightning module as such\n <denchmark-code>class SimpleRegressor(pl.LightningModule):\n     ...\n </denchmark-code>\n \n Use the logger anywhere to get this kind of stacktrace:\n <denchmark-code>d:\\Documents\\projects\\MetaWatch\\MetaWatch\\notebooks\\audio-video-interest\\simple_regressor.py in configure_optimizers(self)\n     105         #see https://pytorch-lightning.readthedocs.io/en/latest/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers\n     106         # REQUIRED\n --> 107         self.logger.experiment.add_hparams({'hidden_layer_size':self.hidden_layer_size,\n     108                                             'linear_layer_size':self.linear_layer_size,\n     109                                             'lstm_layers':self.lstm_layers})\n \n AttributeError: 'NoneType' object has no attribute 'experiment'\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>import pytorch_lightning as pl\n \n class SimpleRegressor(pl.LightningModule):\n     def __init__(self, cuda=False):\n         super(SimpleRegressor, self).__init__()\n         self.logger.experiment.add_hparams({'hidden_layer_size':1})\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n To log as described in the documentation.\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>PyTorch version: 1.4.0\n Is debug build: No\n CUDA used to build PyTorch: 10.1\n \n OS: Microsoft Windows 10 Pro    \n GCC version: Could not collect  \n CMake version: Could not collect\n \n Python version: 3.7\n Is CUDA available: Yes\n CUDA runtime version: Could not collect\n GPU models and configuration: GPU 0: GeForce GTX 970\n Nvidia driver version: 441.12\n cuDNN version: Could not collect\n \n Versions of relevant libraries:\n [pip3] numpy==1.18.1\n [pip3] pytorch-lightning==0.6.0\n [pip3] tinynumpy==1.2.1\n [pip3] torch==1.4.0\n [pip3] torchvision==0.4.1\n [conda] Could not collect\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "helldragger", "commentT": "2020-02-25T11:52:17Z", "comment_text": "\n \t\tHey, thanks for your contribution! Great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "helldragger", "commentT": "2020-02-25T19:04:09Z", "comment_text": "\n \t\tThanks for the issue! The intended way to acheive this is through a hook. When __init__ is called on the LightningModule, the loggers won't have been created yet. I don't think there's any way to change that so we should update the docs to use a hook instead of __init__.\n <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>\n  any other thoughts on this?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "helldragger", "commentT": "2020-02-26T21:00:09Z", "comment_text": "\n \t\tit also doesn't work in other functions, I tried in the training step, in the configure_optimizers too\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "helldragger", "commentT": "2020-02-27T09:09:05Z", "comment_text": "\n \t\tOk, most of these work on master (i.e. if you install from github) for me - except configure_optimizers.\n I've opened a PR (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/964>#964</denchmark-link>\n ) which fixes that and cleans up the docs a bit.\n \t\t"}}}, "commit": {"commit_id": "f5e0df390c6e1eaf11ad488e297aa2d383daa177", "commit_author": "Ethan Harris", "commitT": "2020-02-27 15:54:06-05:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\experiment_logging.rst", "file_new_name": "docs\\source\\experiment_logging.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "25,31,55,61,79,85,105,111,130,136,154,160,162,163,165,166,167", "deleted_lines": "25,31,55,61,79,85,105,111,130,136,154,160"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\__init__.py", "file_new_name": "pytorch_lightning\\loggers\\__init__.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "3,4,18,19,20,21,22,23,24,30,61,67,78,79,88", "deleted_lines": "3,17,18,24,55,61,80"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\base.py", "file_new_name": "pytorch_lightning\\loggers\\base.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "103,104", "deleted_lines": null, "method_info": {"method_name": "__getitem__", "method_params": "self,int", "method_startline": "103", "method_endline": "104"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1071", "deleted_lines": "1068,1070,1071", "method_info": {"method_name": "run_pretrain_routine", "method_params": "self,LightningModule", "method_startline": "1055", "method_endline": "1159"}}}}}}}