{"BR": {"BR_id": "1442", "BR_author": "alexeykarnachev", "BRopenT": "2020-04-10T12:52:39Z", "BRcloseT": "2020-04-10T15:43:07Z", "BR_text": {"BRsummary": "Failed to configure_optimizers from dictionary without lr_scheduler field presented", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Optimizer is failed to be configured from the dictionary without lr_sheduler field.\n Consider an example of the Module configure_optimizers method:\n         def configure_optimizers(self):\n             config = {\n                 'optimizer': torch.optim.SGD(params=self.parameters(), lr=1e-03)\n             }\n             return config\n Then, we run a simple trainer:\n     trainer_options = dict(default_save_path=tmpdir, max_epochs=1)\n     trainer = Trainer(**trainer_options)\n     _ = trainer.fit(model)\n And we fail with an error:\n <denchmark-code>UnboundLocalError: local variable 'lr_schedulers' referenced before assignment\n </denchmark-code>\n \n I believe, that the reason is that lr_schedulers local variable is not determined here:\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/optimizers.py\n \n \n         Lines 36 to 42\n       in\n       8dd9b80\n \n \n \n \n \n \n  # single dictionary \n \n \n \n  elif isinstance(optim_conf, dict): \n \n \n \n  optimizer = optim_conf[\"optimizer\"] \n \n \n \n  lr_scheduler = optim_conf.get(\"lr_scheduler\", []) \n \n \n \n  if lr_scheduler: \n \n \n \n  lr_schedulers = self.configure_schedulers([lr_scheduler]) \n \n \n \n  return [optimizer], lr_schedulers, [] \n \n \n \n \n \n I think, it could be fixed like this:\n         # single dictionary\n         elif isinstance(optim_conf, dict):\n             optimizer = optim_conf[\"optimizer\"]\n             lr_scheduler = optim_conf.get(\"lr_scheduler\", [])\n             if lr_scheduler:\n                 lr_schedulers = self.configure_schedulers([lr_scheduler])\n             else:\n                 lr_schedulers = []\n             return [optimizer], lr_schedulers, []\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Create a simple Module with configure_optimizers which looks like above.\n Run the fit Trainer method with the model.\n See error\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-link:https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc>https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc</denchmark-link>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n I suppose, that such the configuration: {\"optimizer\": ...}, without \"lr_scheduler\" must be a valid one, and this error must not be occurred.\n <denchmark-h:h3>Environment</denchmark-h>\n \n OS: Linux\n architecture: 64bit\n processor: x86_64\n python: 3.7.6\n version: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/97>#97</denchmark-link>\n ~16.04.1-Ubuntu SMP Wed Apr 1 03:03:31 UTC 2020\n pytorch-lightning: 0.7.3rc1\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "alexeykarnachev", "commentT": "2020-04-10T13:21:59Z", "comment_text": "\n \t\tyeah, agreed that dict should work without the scheduler. mind submitting a PR?\n \t\t"}}}, "commit": {"commit_id": "4c34d16a349bc96a717be5674606c2577fab8946", "commit_author": "Alexey Karnachev", "commitT": "2020-04-10 11:43:06-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\optimizers.py", "file_new_name": "pytorch_lightning\\trainer\\optimizers.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "42,43", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\supporters.py", "file_new_name": "pytorch_lightning\\trainer\\supporters.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23", "deleted_lines": null}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "557,558", "deleted_lines": "557", "method_info": {"method_name": "add_argparse_args.allowed_type", "method_params": "x", "method_startline": "557", "method_endline": "558"}}, "hunk_1": {"Ismethod": 1, "added_lines": "557,558", "deleted_lines": "557", "method_info": {"method_name": "add_argparse_args", "method_params": "cls,ArgumentParser", "method_startline": "536", "method_endline": "568"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\trainer\\test_optimizers.py", "file_new_name": "tests\\trainer\\test_optimizers.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300", "deleted_lines": null, "method_info": {"method_name": "test_configure_optimizer_from_dict", "method_params": "tmpdir", "method_startline": "280", "method_endline": "300"}}, "hunk_1": {"Ismethod": 1, "added_lines": "286,287,288,289,290", "deleted_lines": null, "method_info": {"method_name": "test_configure_optimizer_from_dict.configure_optimizers", "method_params": "self", "method_startline": "286", "method_endline": "290"}}}}}}}