{"BR": {"BR_id": "2372", "BR_author": "xiadingZ", "BRopenT": "2020-06-26T13:35:38Z", "BRcloseT": "2020-06-30T14:03:50Z", "BR_text": {"BRsummary": "training_epoch_end's outputs doesn't have 'loss' key", "BRdescription": "\n pytorch-lightning: build from master\n <denchmark-code>Traceback (most recent call last):\n   File \"main.py\", line 140, in <module>\n     main(hparams)\n   File \"main.py\", line 72, in main\n     trainer.fit(model)\n   File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 881, in fit\n     self.ddp_train(task, model)\n   File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 539, in ddp_train\n     self.run_pretrain_routine(model)\n   File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1091, in run_pretrain_routine\n     self.train()\n   File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 376, in train\n     self.run_training_epoch()\n   File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 510, in run_training_epoch\n     self.run_training_epoch_end(epoch_output)\n   File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 535, in run_training_epoch_end\n     epoch_output = model.training_epoch_end(epoch_output)\n   File \"/mnt/lustre/maxiao1/PVM/models/baseline.py\", line 335, in training_epoch_end\n     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n   File \"/mnt/lustre/maxiao1/PVM/models/baseline.py\", line 335, in <listcomp>\n     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n KeyError: 'loss'\n </denchmark-code>\n \n This is my code:\n <denchmark-code>    def training_step(self, batch, batch_idx):\n         ...\n         return {'loss': loss, \"train_acc\": acc}\n \n     def training_epoch_end(self, outputs):\n         avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n         avg_acc = torch.stack([x['train_acc'] for x in outputs]).mean()\n         logs = {'loss': avg_loss, 'train_acc': avg_acc}\n         progress_bar = {'train_loss': avg_loss, 'train_acc': avg_acc}\n         results = {\n             'log': logs,\n             'progress_bar': progress_bar\n         }\n         return results\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xiadingZ", "commentT": "2020-06-26T21:54:01Z", "comment_text": "\n \t\tTry: avg_loss = torch.stack([x['batch_loss'] for x in outputs]).mean()\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xiadingZ", "commentT": "2020-06-27T01:35:18Z", "comment_text": "\n \t\tThanks\uff0c it works\n but 'train_acc' key doesn't exist, neither do batch_train_acc. How to access other keys returned in training_step?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "xiadingZ", "commentT": "2020-06-27T11:35:13Z", "comment_text": "\n \t\tAs of now in lightning you can access them using x['callback_metrics']['loss'] and x['callback_metrics']['train_acc'], but I think it should be handled in a similar way we do this with validation_epoch_end and test_epoch_end.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "xiadingZ", "commentT": "2020-06-29T16:47:24Z", "comment_text": "\n \t\tHi! One hint: for me it works with \"loss\" under windows but not under ubuntu.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "xiadingZ", "commentT": "2020-06-29T17:20:30Z", "comment_text": "\n \t\tWeird!! Why is this think platform dependent?? \ud83e\udd14\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "xiadingZ", "commentT": "2020-06-30T07:45:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Pet222>@Pet222</denchmark-link>\n  , are u sure that versions on ubuntu and windows are same?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "xiadingZ", "commentT": "2020-06-30T10:29:19Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  is this intended behaviour? I was surprised to see this breaking change being introduced with no warning.\n If it is intended, why not have consistent behaviour over  and .\n If it is not intended, as it seems due to the \"bug fix\" tag, are you working on it or should I make a PR for this?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "xiadingZ", "commentT": "2020-06-30T11:43:59Z", "comment_text": "\n \t\twhat is the behavior? that the \"loss\" key is not in training_epoch_end? If so, that's a bug because it should be there\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "xiadingZ", "commentT": "2020-06-30T11:49:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  , on the latest version, the  key was changed to the . I think it was changed <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/0f073819d3e0df8db7602eab489b1bad0fc0949c#diff-c45bd21c331565cbe62aaa12fa43aa0aR717>here</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "xiadingZ", "commentT": "2020-06-30T11:50:10Z", "comment_text": "\n \t\tYes, the fact that you need to access it through 'callback metrics'.\n Got it!\n On Tue, 30 Jun 2020 at 12:44, William Falcon ***@***.***> wrote:\n  what is the behavior? that the \"loss\" key is not in training_epoch_end? If\n  so, that's a bug because it should be there\n \n  \u2014\n  You are receiving this because you commented.\n  Reply to this email directly, view it on GitHub\n  <<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2372#issuecomment-651740702>#2372 (comment)</denchmark-link>\n >,\n  or unsubscribe\n  <<denchmark-link:https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ>https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ</denchmark-link>\n >\n  .\n \n -- \n Best Regards,\n Miguel Vera\n \n +351 915 198 452\n miguel.coimbra.vera@protonmail.com\n Github/Captainvera <<denchmark-link:http://www.github.com/captainvera>http://www.github.com/captainvera</denchmark-link>\n >\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "xiadingZ", "commentT": "2020-06-30T12:19:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/captainvera>@captainvera</denchmark-link>\n  would love a PR :)\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "xiadingZ", "commentT": "2020-06-30T13:24:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/captainvera>@captainvera</denchmark-link>\n  <denchmark-link:https://github.com/xiadingZ>@xiadingZ</denchmark-link>\n  sorry about that! it was a bad bug.\n Made a PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2428>#2428</denchmark-link>\n   and added tests to make sure this doesn't happen again!\n try master now!\n we\u2019ll push a new minor again since this is a key bug (and we have a few other key bugs)\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "xiadingZ", "commentT": "2020-06-30T14:11:11Z", "comment_text": "\n \t\tWell, that was fast, thanks!\n \t\t"}}}, "commit": {"commit_id": "a42a0e16ddd75dd7199ecefe4d10c2941c17ba76", "commit_author": "William Falcon", "commitT": "2020-06-30 10:03:49-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "779,792", "deleted_lines": "791", "method_info": {"method_name": "optimizer_closure", "method_params": "self,split_batch,batch_idx,opt_idx,optimizer,hiddens", "method_startline": "759", "method_endline": "828"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\base\\deterministic_model.py", "file_new_name": "tests\\base\\deterministic_model.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "92,93,96", "deleted_lines": "92,93,96", "method_info": {"method_name": "training_step_end_dict", "method_params": "self,output", "method_startline": "76", "method_endline": "96"}}, "hunk_1": {"Ismethod": 1, "added_lines": "107,108,109,110,114", "deleted_lines": "107,108,112", "method_info": {"method_name": "training_epoch_end_dict", "method_params": "self,outputs", "method_startline": "98", "method_endline": "118"}}, "hunk_2": {"Ismethod": 1, "added_lines": "61", "deleted_lines": "61", "method_info": {"method_name": "training_step_dict_return", "method_params": "self,batch,batch_idx", "method_startline": "54", "method_endline": "61"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\trainer\\test_trainer_steps.py", "file_new_name": "tests\\trainer\\test_trainer_steps.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "34,35,36,37,38,39", "deleted_lines": "34", "method_info": {"method_name": "test_training_step_dict", "method_params": "tmpdir", "method_startline": "5", "method_endline": "41"}}, "hunk_1": {"Ismethod": 1, "added_lines": "149,150", "deleted_lines": "141", "method_info": {"method_name": "test_train_step_epoch_end", "method_params": "tmpdir", "method_startline": "118", "method_endline": "152"}}, "hunk_2": {"Ismethod": 1, "added_lines": "67,68,70,71,72,73,74", "deleted_lines": "62,63,65,66,67", "method_info": {"method_name": "training_step_with_step_end", "method_params": "tmpdir", "method_startline": "44", "method_endline": "74"}}, "hunk_3": {"Ismethod": 1, "added_lines": "109,110,112,113,114,115", "deleted_lines": "102,103,105,106,107", "method_info": {"method_name": "test_full_training_loop_dict", "method_params": "tmpdir", "method_startline": "77", "method_endline": "115"}}}}}}}