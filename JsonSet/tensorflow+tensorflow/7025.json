{"BR": {"BR_id": "7025", "BR_author": "yaroslavvb", "BRopenT": "2017-01-23T23:39:53Z", "BRcloseT": "2017-05-01T16:22:27Z", "BR_text": {"BRsummary": "Getting \"Dst tensor is not initialized.\" when really the problem is out of GPU memory", "BRdescription": "\n This is the stack trace we sometimes get when trying to use TensorFlow on a GPU that's occupied by another process. It would help debugging if the error said something about memory.\n <denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>\n \n tf.version: '0.12.1-1934-g27fca7d-dirty'\n (nightly from last week)\n <denchmark-code>W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n name: TITAN X (Pascal)\n major: 6 minor: 1 memoryClockRate (GHz) 1.531\n pciBusID 0000:04:00.0\n Total memory: 11.90GiB\n Free memory: 381.44MiB\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)\n Traceback (most recent call last):\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\n     return fn(*args)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\n     status, run_metadata)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/contextlib.py\", line 66, in __exit__\n     next(self.gen)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n     pywrap_tensorflow.TF_GetCode(status))\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"memory_test.py\", line 87, in <module>\n     profile_densenet(False)\n   File \"memory_test.py\", line 65, in profile_densenet\n     sess.run(net.initializer, {net.x_init: trainx[:init_batch_size]})\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\n     run_metadata_ptr)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\n     feed_dict_string, options, run_metadata)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n     target_list, options, run_metadata)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n     raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n Caused by op 'zeros_1266', defined at:\n   File \"memory_test.py\", line 87, in <module>\n     profile_densenet(False)\n   File \"memory_test.py\", line 59, in profile_densenet\n     net = densenet_lib.densenet(init_batch_size, batch_size, layers_per_block, filters_per_layer, save_memory=save_memory)\n   File \"/home/yaroslav/openai.git/densenet/densenet.py\", line 183, in densenet\n     optimizer = nn.adamax_updates(all_params, loss, lr=tf_lr)\n   File \"/home/yaroslav/openai.git/densenet/nn.py\", line 41, in adamax_updates\n     mg = tf.Variable(tf.zeros(int_shape(p)), p.name + '_adamax_mg')\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1376, in zeros\n     output = constant(zero, shape=shape, dtype=dtype, name=name)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n     attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n     original_op=self._default_original_op, op_def=op_def)\n   File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n     self._traceback = _extract_stack()\n \n InternalError (see above for traceback): Dst tensor is not initialized.\n \t [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "yaroslavvb", "commentT": "2017-01-24T00:14:48Z", "comment_text": "\n \t\tClassifying as \"docs\" but this is really an error message / error propagation issue.\n <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>\n  do you by any chance have a PR in mind that you could submit?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "yaroslavvb", "commentT": "2017-01-24T00:24:57Z", "comment_text": "\n \t\tI can change the message to read \"Dst tensor is not initialized (possibly caused by Out of Memory)\" unless <denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>\n  has a better idea. PS, this is not a hypothetical problem, we've had people thinking it's a regression in tensorflow/training scripts because of this error\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "yaroslavvb", "commentT": "2017-01-24T01:10:56Z", "comment_text": "\n \t\tMy feeling is that this should fail at the place of memory allocation, not\n at the point of memory copy. There needs to be some digging to find out why\n it didn't fail earlier.\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Mon, Jan 23, 2017 at 4:25 PM, Yaroslav Bulatov ***@***.***> wrote:\n  I can change the message to read \"Dst tensor is not initialized (possibly\n  caused by Out of Memory)\" unless @zheng-xq <https://github.com/zheng-xq>\n  has a better idea\n \n  \u2014\n  You are receiving this because you were mentioned.\n  Reply to this email directly, view it on GitHub\n  <#7025 (comment)>,\n  or mute the thread\n  <https://github.com/notifications/unsubscribe-auth/APAgTtwKSbjLp9qQ0r0te9E38jX4uMb2ks5rVUUVgaJpZM4Lrr-s>\n  .\n \n \n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "yaroslavvb", "commentT": "2017-01-26T18:32:35Z", "comment_text": "\n \t\tHi  <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>\n  <denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>\n \n I'm getting this Dst Tensor Not Initialized error.\n (See my comment (the last one)  in this issue elsewhere: <denchmark-link:https://github.com/aymericdamien/TensorFlow-Examples/issues/38>aymericdamien/TensorFlow-Examples#38</denchmark-link>\n )\n I'm reproducing the stack trace here in case it helps diagnose the issue:\n <denchmark-code>\u25b6 python imagenet_inference.py \n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\n I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n name: GeForce GT 750M\n major: 3 minor: 0 memoryClockRate (GHz) 0.9255\n pciBusID 0000:01:00.0\n Total memory: 2.00GiB\n Free memory: 305.92MiB\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State: \n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024\n I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184\n I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: \n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \n Limit:                   111063040\n InUse:                     9337856\n MaxInUse:                  9337856\n NumAllocs:                      11\n MaxAllocSize:              3538944\n \n W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________\n W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.\n W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\n E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n Traceback (most recent call last):\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n     return fn(*args)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n     status, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py\", line 66, in __exit__\n     next(self.gen)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n     pywrap_tensorflow.TF_GetCode(status))\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"imagenet_inference.py\", line 19, in <module>\n     sess.run(init)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\n     run_metadata_ptr)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n     feed_dict_string, options, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n     target_list, options, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n     raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n Caused by op 'Variable_10/initial_value', defined at:\n   File \"imagenet_inference.py\", line 16, in <module>\n     probs = AlexNet(x, feature_extract=False)\n   File \"/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py\", line 139, in AlexNet\n     fc6W = tf.Variable(net_data[\"fc6\"][0])\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n     expected_shape=expected_shape)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n     initial_value, name=\"initial_value\", dtype=dtype)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\n     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\n     return constant(v, dtype=dtype, name=name)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n     attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n     original_op=self._default_original_op, op_def=op_def)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n     self._traceback = _extract_stack()\n \n InternalError (see above for traceback): Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n </denchmark-code>\n \n Here's deviceQuery successfully reporting seeing the GPU:\n <denchmark-code> py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_HOME \n /usr/local/cuda\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_VISIBLE_DEVICES\n \n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./deviceQuery \n ./deviceQuery Starting...\n \n  CUDA Device Query (Runtime API) version (CUDART static linking)\n \n Detected 1 CUDA Capable device(s)\n \n Device 0: \"GeForce GT 750M\"\n   CUDA Driver Version / Runtime Version          8.0 / 8.0\n   CUDA Capability Major/Minor version number:    3.0\n   Total amount of global memory:                 2048 MBytes (2147024896 bytes)\n   ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n   GPU Max Clock rate:                            926 MHz (0.93 GHz)\n   Memory Clock rate:                             2508 Mhz\n   Memory Bus Width:                              128-bit\n   L2 Cache Size:                                 262144 bytes\n   Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n   Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n   Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n   Total amount of constant memory:               65536 bytes\n   Total amount of shared memory per block:       49152 bytes\n   Total number of registers available per block: 65536\n   Warp size:                                     32\n   Maximum number of threads per multiprocessor:  2048\n   Maximum number of threads per block:           1024\n   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n   Maximum memory pitch:                          2147483647 bytes\n   Texture alignment:                             512 bytes\n   Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n   Run time limit on kernels:                     Yes\n   Integrated GPU sharing Host Memory:            No\n   Support host page-locked memory mapping:       Yes\n   Alignment requirement for Surfaces:            Yes\n   Device has ECC support:                        Disabled\n   Device supports Unified Addressing (UVA):      Yes\n   Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n   Compute Mode:\n      < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n \n deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\n Result = PASS\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 \n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 \n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 \n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./bandwidthTest \n [CUDA Bandwidth Test] - Starting...\n Running on...\n \n  Device 0: GeForce GT 750M\n  Quick Mode\n \n  Host to Device Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t3633.5\n \n  Device to Host Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t6343.5\n \n  Device to Device Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t42554.1\n \n Result = PASS\n \n NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n </denchmark-code>\n \n My System:\n <denchmark-code>MacBook Pro (Retina, 15-inch, Late 2013)\n 2.3 GHz Intel Core i7\n 16 GB 1600 MHz DDR3\n NVIDIA GeForce GT 750M 2048 MB\n \n ---\n from System Report > Graphics\n NVIDIA GeForce GT 750M:\n \n   Chipset Model:\tNVIDIA GeForce GT 750M\n   Type:\tGPU\n   Bus:\tPCIe\n   PCIe Lane Width:\tx8\n   VRAM (Total):\t2048 MB\n   Vendor:\tNVIDIA (0x10de)\n   Device ID:\t0x0fe9\n   Revision ID:\t0x00a2\n   ROM Revision:\t3776\n   gMux Version:\t4.0.8 [3.2.8]\n   Displays:\n Color LCD:\n   Display Type:\tRetina LCD\n   Resolution:\t2880 x 1800 Retina\n   Retina:\tYes\n   Pixel Depth:\t32-Bit Color (ARGB8888)\n   Main Display:\tYes\n   Mirror:\tOff\n   Online:\tYes\n   Built-In:\tYes\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "yaroslavvb", "commentT": "2017-01-26T18:47:55Z", "comment_text": "\n \t\tSounds like you are running out of GPU memory\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Jan 26, 2017 10:33 AM, \"Atul Acharya\" ***@***.***> wrote:\n  Hi @yaroslavvb <https://github.com/yaroslavvb> @zheng-xq\n  <https://github.com/zheng-xq>\n \n  I'm getting this Dst Tensor Not Initialized error.\n \n  (See my comment (the last one) in this issue elsewhere:\n  aymericdamien/TensorFlow-Examples#38\n  <aymericdamien/TensorFlow-Examples#38>)\n \n  I'm reproducing the stack trace here in case it helps diagnose the issue:\n \n  \u25b6 python imagenet_inference.py\n  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\n  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\n  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\n  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\n  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\n  I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\n  I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\n  name: GeForce GT 750M\n  major: 3 minor: 0 memoryClockRate (GHz) 0.9255\n  pciBusID 0000:01:00.0\n  Total memory: 2.00GiB\n  Free memory: 305.92MiB\n  I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\n  I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\n  I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n  I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State:\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472\n  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024\n  I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184\n  I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size:\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB\n  I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\n  Limit:                   111063040\n  InUse:                     9337856\n  MaxInUse:                  9337856\n  NumAllocs:                      11\n  MaxAllocSize:              3538944\n \n  W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________\n  W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.\n  W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\n  E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\n  \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n  Traceback (most recent call last):\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n      return fn(*args)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n      status, run_metadata)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py\", line 66, in __exit__\n      next(self.gen)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n      pywrap_tensorflow.TF_GetCode(status))\n  tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n  \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n  During handling of the above exception, another exception occurred:\n \n  Traceback (most recent call last):\n    File \"imagenet_inference.py\", line 19, in <module>\n      sess.run(init)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\n      run_metadata_ptr)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n      feed_dict_string, options, run_metadata)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n      target_list, options, run_metadata)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n      raise type(e)(node_def, op, message)\n  tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n  \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n  Caused by op 'Variable_10/initial_value', defined at:\n    File \"imagenet_inference.py\", line 16, in <module>\n      probs = AlexNet(x, feature_extract=False)\n    File \"/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py\", line 139, in AlexNet\n      fc6W = tf.Variable(net_data[\"fc6\"][0])\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n      expected_shape=expected_shape)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n      initial_value, name=\"initial_value\", dtype=dtype)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\n      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\n      return constant(v, dtype=dtype, name=name)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n      attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n      original_op=self._default_original_op, op_def=op_def)\n    File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n      self._traceback = _extract_stack()\n \n  InternalError (see above for traceback): Dst tensor is not initialized.\n  \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n \n \n  Here's deviceQuery successfully reporting seeing the GPU:\n \n   py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_HOME\n  /usr/local/cuda\n   py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_VISIBLE_DEVICES\n \n   py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./deviceQuery\n  ./deviceQuery Starting...\n \n   CUDA Device Query (Runtime API) version (CUDART static linking)\n \n  Detected 1 CUDA Capable device(s)\n \n  Device 0: \"GeForce GT 750M\"\n    CUDA Driver Version / Runtime Version          8.0 / 8.0\n    CUDA Capability Major/Minor version number:    3.0\n    Total amount of global memory:                 2048 MBytes (2147024896 bytes)\n    ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n    GPU Max Clock rate:                            926 MHz (0.93 GHz)\n    Memory Clock rate:                             2508 Mhz\n    Memory Bus Width:                              128-bit\n    L2 Cache Size:                                 262144 bytes\n    Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n    Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n    Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n    Total amount of constant memory:               65536 bytes\n    Total amount of shared memory per block:       49152 bytes\n    Total number of registers available per block: 65536\n    Warp size:                                     32\n    Maximum number of threads per multiprocessor:  2048\n    Maximum number of threads per block:           1024\n    Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n    Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n    Maximum memory pitch:                          2147483647 bytes\n    Texture alignment:                             512 bytes\n    Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n    Run time limit on kernels:                     Yes\n    Integrated GPU sharing Host Memory:            No\n    Support host page-locked memory mapping:       Yes\n    Alignment requirement for Surfaces:            Yes\n    Device has ECC support:                        Disabled\n    Device supports Unified Addressing (UVA):      Yes\n    Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n    Compute Mode:\n       < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n \n  deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\n  Result = PASS\n   py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n   py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n   py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n   py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./bandwidthTest\n  [CUDA Bandwidth Test] - Starting...\n  Running on...\n \n   Device 0: GeForce GT 750M\n   Quick Mode\n \n   Host to Device Bandwidth, 1 Device(s)\n   PINNED Memory Transfers\n     Transfer Size (Bytes)\tBandwidth(MB/s)\n     33554432\t\t\t3633.5\n \n   Device to Host Bandwidth, 1 Device(s)\n   PINNED Memory Transfers\n     Transfer Size (Bytes)\tBandwidth(MB/s)\n     33554432\t\t\t6343.5\n \n   Device to Device Bandwidth, 1 Device(s)\n   PINNED Memory Transfers\n     Transfer Size (Bytes)\tBandwidth(MB/s)\n     33554432\t\t\t42554.1\n \n  Result = PASS\n \n  NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n \n  My System:\n \n  MacBook Pro (Retina, 15-inch, Late 2013)\n  2.3 GHz Intel Core i7\n  16 GB 1600 MHz DDR3\n  NVIDIA GeForce GT 750M 2048 MB\n \n  ---\n  from System Report > Graphics\n  NVIDIA GeForce GT 750M:\n \n    Chipset Model:\tNVIDIA GeForce GT 750M\n    Type:\tGPU\n    Bus:\tPCIe\n    PCIe Lane Width:\tx8\n    VRAM (Total):\t2048 MB\n    Vendor:\tNVIDIA (0x10de)\n    Device ID:\t0x0fe9\n    Revision ID:\t0x00a2\n    ROM Revision:\t3776\n    gMux Version:\t4.0.8 [3.2.8]\n    Displays:\n  Color LCD:\n    Display Type:\tRetina LCD\n    Resolution:\t2880 x 1800 Retina\n    Retina:\tYes\n    Pixel Depth:\t32-Bit Color (ARGB8888)\n    Main Display:\tYes\n    Mirror:\tOff\n    Online:\tYes\n    Built-In:\tYes\n \n  \u2014\n  You are receiving this because you were mentioned.\n  Reply to this email directly, view it on GitHub\n  <#7025 (comment)>,\n  or mute the thread\n  <https://github.com/notifications/unsubscribe-auth/AABaHJK2XuJg9IHUT3Rb63Nbtahdgr8sks5rWObHgaJpZM4Lrr-s>\n  .\n \n \n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "yaroslavvb", "commentT": "2017-04-06T22:27:56Z", "comment_text": "\n \t\tI get the same error and pretty sure it is for the same reason. I am on tf 1.0.0 and using keras 2.0.2.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "yaroslavvb", "commentT": "2017-04-28T22:30:17Z", "comment_text": "\n \t\tFor now, I changed the message to \"Dst tensor is not initialized (are you out of memory?).\" (not yet on Github).\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "yaroslavvb", "commentT": "2017-04-28T22:41:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/vrv>@vrv</denchmark-link>\n  may have localized the problem to \n , which doesn't check for errors and thus produces an uninitialized tensor.\n <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>\n  If you can reproduce the situation, would you mind checking if  at that point detects the problem?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "yaroslavvb", "commentT": "2017-04-28T23:07:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/vrv>@vrv</denchmark-link>\n  Has a CL that will hopefully fix this.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "yaroslavvb", "commentT": "2017-04-28T23:45:30Z", "comment_text": "\n \t\tYeah, verified I have a CL that fixes this, submitting it internally now.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "yaroslavvb", "commentT": "2018-03-15T15:42:14Z", "comment_text": "\n \t\tI just updated TF to 1.6 and still experience the same issue.\n I am running the smallest possible model, Cart-Pole, on GTX1080 8MB. Is it a TensorFlow bug that can be fixed somehow or we are simply trying to fit too big models (overenthusiastic the batch size probably the main reason for that)?\n sebtac\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "yaroslavvb", "commentT": "2018-05-21T18:11:49Z", "comment_text": "\n \t\tStill experiencing this issue with recent version <denchmark-link:https://github.com/tensorflow/tensorflow/commit/068fd9c936dbf8c9ace9edae9e7bb9e64256d381>068fd9c</denchmark-link>\n .\n (I can confirm this is due to an OOM issue.)\n <denchmark-code>tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: _arg_q_actions_0_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_339__arg_q_actions_0_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n \t [[Node: loss/assert_broadcastable/AssertGuard/Assert/Switch/_33 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_91_loss/assert_broadcastable/AssertGuard/Assert/Switch\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n </denchmark-code>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "yaroslavvb", "commentT": "2018-08-01T14:39:13Z", "comment_text": "\n \t\tGetting this in TF 1.9.0 when I increase the size of my test set to a large number of samples (possibly a memory issue).\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "yaroslavvb", "commentT": "2018-09-04T17:51:28Z", "comment_text": "\n \t\tI am also getting the same error in TF 1.8.0  also in the latest version. My machine is 3x NVidia Tesla P40 22GB\n raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n [[Node: conv4_22_1x1_increase/Conv2D-0-1-TransposeNCHWToNHWC-LayoutOptimizer/_2025 = _Recv<denchmark-link:>client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3008_conv4_22_1x1_increase/Conv2D-0-1-TransposeNCHWToNHWC-LayoutOptimizer\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"</denchmark-link>\n ]]\n [[Node: adversarial/Mean/_3087 = _Recv<denchmark-link:>client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_5391_adversarial/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</denchmark-link>\n ]]\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "yaroslavvb", "commentT": "2018-12-14T08:44:15Z", "comment_text": "\n \t\tIf not solved yet... free memory... previously generated not-used embedding, models etc...\n del all_embs, (model_names...), (model_input_names)\n import gc; gc.collect()\n time.sleep(10)\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "yaroslavvb", "commentT": "2018-12-22T17:55:19Z", "comment_text": "\n \t\tHas anyone solved this?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "yaroslavvb", "commentT": "2018-12-26T15:05:48Z", "comment_text": "\n \t\tMake sure multiple processes are not accessing GPU and see if that solves the problem. It seemed to be the solution for me. Especially if you're running your code in Jupyter notebook, kill any other running notebook kernel.\n \t\t"}}}, "commit": {"commit_id": "0a9b39caefd437fec742ae48b25061abd6e2699b", "commit_author": "Vijay Vasudevan", "commitT": "2017-04-28 18:04:24-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc", "file_new_name": "tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "468,469,470,471,472,473,474,475", "deleted_lines": null, "method_info": {"method_name": "tensorflow::BaseGPUDevice::MakeTensorFromProto", "method_params": "tensor_proto,alloc_attrs,tensor", "method_startline": "447", "method_endline": "487"}}}}}}}