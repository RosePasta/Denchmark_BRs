{"BR": {"BR_id": "41674", "BR_author": "dansitu", "BRopenT": "2020-07-23T20:04:14Z", "BRcloseT": "2020-07-25T00:19:47Z", "BR_text": {"BRsummary": "TensorFlow Lite for Microcontrollers sigaborts with a MobileNetV2 alpha=0.1 model", "BRdescription": "\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.5, Linux\n GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.62) c++1\n \n Describe the current behavior\n I am using TensorFlow Lite for Microcontrollers at commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/4f69f62c61ecf3cd23286324af62d00643186ec2>4f69f62</denchmark-link>\n .\n I've trained two MobileNetV2 models in Keras with 48x48 input size and a single input channel, then converted to int8 quantized.\n I am attempting to run both models using TensorFlow Lite for Microcontrollers on x86, built with clang on MacOS and with gcc on Ubuntu.\n The first model has a MobileNetV2 filter scaling factor (a) of 0.35. This model runs perfectly.\n The second model has a scaling factor of 0.1. This model sigaborts during the Invoke() call.\n Strangely, both models run perfectly when executed from the OpenMV H7+ (Arm Cortex-M7), and the smaller model runs perfectly on the H7. It might be worth noting that on the OpenMV devices the model is stored in dynamic memory. That said, I've tried declaring the model without static on x86 and it has no impact.\n I've attached zips containing both model files, plus an example program that exhibits the sigabort.\n To build and run the example with an empty input:\n <denchmark-code>make -f Makefile.tflite\n ./build/edge-impulse-standalone \"\"\n </denchmark-code>\n \n Within the example code, the call to Invoke() is on line 260 of edge-impulse-sdk/classifier/ei_run_classifier.h.\n To switch to the 0.35 model, which doesn't sigabort, replace tflite-model, model-parameters, and edge-impulse-sdk directories with the versions contained within 0.35 grayscale.zip.\n Describe the expected behavior\n The a=0.1 model should run successfully on x86, the same as the 0.35 does.\n Standalone code to reproduce the issue\n Example code here:\n <denchmark-link:https://github.com/tensorflow/tensorflow/files/4968485/example-standalone-inferencing.zip>example-standalone-inferencing.zip</denchmark-link>\n \n The  model files are located here:\n <denchmark-link:https://github.com/tensorflow/tensorflow/files/4968496/models.zip>models.zip</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dansitu", "commentT": "2020-07-24T09:40:04Z", "comment_text": "\n \t\tThe actual crash is here: tensorflow/lite/kernels/internal/reference/integer_ops/add.h in the Add function.\n <denchmark-code>TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);\n </denchmark-code>\n \n Here params.input1_offset is 128, which is invalid (max is 127). I assume this is a parser or quantization bug.\n Changing this parameter to 127 solves the issue, but I'm not sure where it comes from.\n <denchmark-code>  ArithmeticParams *p = (ArithmeticParams*)&params;\n \n   if (p->input1_offset > 127) {\n       p->input1_offset = 127;\n   }\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dansitu", "commentT": "2020-07-24T17:55:26Z", "comment_text": "\n \t\tThank you Jan! Since this seems to perhaps be a converter issue, I'm attaching the SavedModel files for both models.\n <denchmark-link:https://github.com/tensorflow/tensorflow/files/4973569/0.1-grayscale-savedmodel.zip>0.1-grayscale-savedmodel.zip</denchmark-link>\n \n <denchmark-link:https://github.com/tensorflow/tensorflow/files/4973570/0.35-grayscale-savedmodel.zip>0.35-grayscale-savedmodel.zip</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dansitu", "commentT": "2020-07-25T00:19:49Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41674>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41674>No</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dansitu", "commentT": "2020-08-09T21:28:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dansitu>@dansitu</denchmark-link>\n \n <denchmark-link:https://github.com/janjongboom>@janjongboom</denchmark-link>\n \n Hi, i have run into problems running similar NN to yours.\n `base_model = tf.keras.applications.MobileNetV2(input_shape=(48, 48, 1), alpha=0.35, weights=None, include_top=False)\n x = base_model.output\n x = tf.keras.layers.Flatten()(x)\n x = Dense(2)(x) #final layer with softmax activation for N classes\n preds = tf.keras.layers.Softmax()(x)\n model=Model(inputs=base_model.input,outputs=preds) #specify the inputs and outputs\n converter = tf.lite.TFLiteConverter.from_keras_model(model)\n def representative_dataset():\n for i in range(500):\n yield([np.random.rand(1,48,48,1).astype(np.float32)])\n converter.optimizations = [tf.lite.Optimize.DEFAULT]\n converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n converter.representative_dataset = representative_dataset\n tflite_model = converter.convert()\n open(\"mobilenet_test.tflite\", \"wb\").write(tflite_model)\n `\n the model crush on the microcontroller in assert (tflite::PreprocessSoftmaxScaling)\n thanks\n \t\t"}}}, "commit": {"commit_id": "dd918be82cb9702cc9ca022179629fbd8c6d3ed9", "commit_author": "Nat Jeffries", "commitT": "2020-07-24 17:14:38-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\lite\\kernels\\internal\\reference\\integer_ops\\add.h", "file_new_name": "tensorflow\\lite\\kernels\\internal\\reference\\integer_ops\\add.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "71,72", "deleted_lines": "69,70,71,72,73", "method_info": {"method_name": "tflite::reference_integer_ops::Add", "method_params": "params,input1_shape,input1_data,input2_shape,input2_data,output_shape,output_data", "method_startline": "67", "method_endline": "77"}}, "hunk_1": {"Ismethod": 1, "added_lines": "42", "deleted_lines": "64,65", "method_info": {"method_name": "tflite::reference_integer_ops::AddElementwise", "method_params": "size,params,input1_data,input2_data,output_data", "method_startline": "39", "method_endline": "65"}}, "hunk_2": {"Ismethod": 1, "added_lines": "26,27,28,29,30,31,32,33,34,35", "deleted_lines": "31,32,33,34,35", "method_info": {"method_name": "tflite::reference_integer_ops::CheckArithmeticParams", "method_params": "params", "method_startline": "26", "method_endline": "35"}}}}}}}