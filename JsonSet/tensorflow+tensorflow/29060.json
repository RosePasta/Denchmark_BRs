{"BR": {"BR_id": "29060", "BR_author": "apls777", "BRopenT": "2019-05-27T14:33:15Z", "BRcloseT": "2019-06-11T22:28:20Z", "BR_text": {"BRsummary": "\"Cache iterator is in an invalid state\" error", "BRdescription": "\n System information\n \n OS Platform and Distribution: macOS High Sierra 10.13.6\n TensorFlow for CPU installed from PyPI\n TensorFlow version: v1.13.0-rc2-5-g6612da8951, 1.13.1\n Python version: 3.6.6\n \n Describe the current behavior\n Minimal not working example:\n import tensorflow as tf\n from tensorflow.python.framework.errors_impl import OutOfRangeError\n \n \n dataset = tf.data.Dataset.range(10)\n dataset = dataset.cache('cache1')\n dataset = dataset.map(lambda a: a)\n dataset = dataset.batch(4)\n \n batch = dataset.make_one_shot_iterator().get_next()\n \n with tf.Session() as sess:\n     while True:\n         try:\n             res = sess.run(batch)\n             print(res)\n         except OutOfRangeError:\n             print('out-of-range')\n             break\n The code above properly iterates through the dataset only the first run when a cache doesn't exist. But when it loads the dataset from the cache file it crashes with an error:\n <denchmark-code>tensorflow.python.framework.errors_impl.InternalError: Cache iterator is in an invalid state. (Perhaps GetNext called after end_of_sequence?)\n \t [[{{node IteratorGetNext}}]\n </denchmark-code>\n \n A workaround\n It happens because the map operation follows right after the cache. It starts working as expected if some other dataset operation is added between cache and map steps. For example:\n ...\n \n dataset = tf.data.Dataset.range(10)\n dataset = dataset.cache('cache1')\n dataset = dataset.filter(lambda x: True)  # a fake filter is added\n dataset = dataset.map(lambda a: a)\n dataset = dataset.batch(4)\n \n ...\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "apls777", "commentT": "2019-05-28T09:27:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>\n  I have executed the first part of the code in TF 1.12.0, did not receive any error. Please try and let us know how it progresses. Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "apls777", "commentT": "2019-05-28T10:04:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/muddham>@muddham</denchmark-link>\n  It works in TF 1.12.0. Just changed the filename to  instead of , otherwise, it throws an error for the first run:\n <denchmark-code>tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory\n \t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "apls777", "commentT": "2019-05-28T10:38:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>\n  As it is working now, are you happy for this issue to be closed?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "apls777", "commentT": "2019-05-28T10:51:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/muddham>@muddham</denchmark-link>\n  No, clearly it\u2019s a bug that should be fixed in the latest stable version.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "apls777", "commentT": "2019-05-30T11:16:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>\n  I ran the code in TF 1.13 GPU the output I got was below.\n [0 1 2 3]\n [4 5 6 7]\n [8 9]\n out-of-range\n In TF 1.13 CPU I got the below error.\n AttributeError: 'BatchDataset' object has no attribute 'make_one_shot_iterator'\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "apls777", "commentT": "2019-05-30T12:20:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/muddham>@muddham</denchmark-link>\n  Did you run the code twice to load the dataset from a cache? Are you sure you're running it on TF 1.13 ? What version of Python do you use?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "apls777", "commentT": "2019-06-06T11:59:44Z", "comment_text": "\n \t\t\n @muddham Did you run the code twice to load the dataset from a cache? Are you sure you're running it on TF 1.13 .1? What version of Python do you use?\n \n I am able to reproduce the issue with TF 1.13.1 and Python 3.6.7.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "apls777", "commentT": "2019-06-11T04:51:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>\n  thank you for reporting the issue and providing a minimal reproducible example.\n I can confirm this is an issue. The problem is that the existing cache transformation produces an error if the dataset transformation that consumes its input asks for an input after the cache transformations has reached the end of its input. This happens in your repro because  will get fused into  which will ask for  worth of elements at once. Because the input cardinality (10) is not divisible evenly by the batch size (4), the invalid cache op kernel behavior is triggered. Inserting a transformation between  and  will prevent the fusion from happening (and so does disabling the fusion through tf.data <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/OptimizationOptions#map_and_batch_fusion>options</denchmark-link>\n .\n I have a fix for this and expect it to merged to master by the end of this week.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "apls777", "commentT": "2019-06-11T22:28:21Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29060>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29060>No</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "apls777", "commentT": "2019-06-25T16:32:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>\n \n I'm running into this issue when caching before interleave\n tf 2.0.0-beta1 and python 3.6.8\n     filenames_dataset = filenames_dataset.cache(\"./some_path\")\n \n     return filenames_dataset.interleave(\n         lambda f: map_file_to_xy_dataset(f, predict_task_fn, params),\n         cycle_length=params[CYCLE_LENGTH_KEY],\n         block_length=params[BLOCK_LENGTH_KEY],\n         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n <denchmark-code>Iterator \"Iterator::Model::Prefetch::Batch::Shuffle::ParallelInterleaveV2\" returned OutOfRange without setting `*end_of_sequence`. This indicates that an error may have occurred. Original message: Attempting to call get_next after iteration should have finished. [Op:IteratorGetNextSync]\n </denchmark-code>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "apls777", "commentT": "2019-06-25T17:18:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/devstein>@devstein</denchmark-link>\n  thank you for reporting the problem. This is a different issue, so please create a new issue for it and reference it here. Thank you.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "apls777", "commentT": "2019-06-25T17:19:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n  Will do!\n \t\t"}}}, "commit": {"commit_id": "003e400902b85626d32727589142d12269306703", "commit_author": "Jiri Simsa", "commitT": "2019-06-11 15:25:55-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\core\\kernels\\data\\cache_dataset_ops.cc", "file_new_name": "tensorflow\\core\\kernels\\data\\cache_dataset_ops.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "366,367,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388", "deleted_lines": "367,368,369,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389", "method_info": {"method_name": "tensorflow::data::CacheDatasetOp::FileDataset::FileIterator::FileWriterIterator::EXCLUSIVE_LOCKS_REQUIRED", "method_params": "mu_", "method_startline": "332", "method_endline": "389"}}, "hunk_1": {"Ismethod": 1, "added_lines": "167,168,169", "deleted_lines": "167,168,169,170", "method_info": {"method_name": "tensorflow::data::CacheDatasetOp::FileDataset::FileIterator::RestoreInternal", "method_params": "ctx,reader", "method_startline": "150", "method_endline": "175"}}, "hunk_2": {"Ismethod": 1, "added_lines": "450,451,461", "deleted_lines": "454,455,456,457,467,468", "method_info": {"method_name": "tensorflow::data::CacheDatasetOp::FileDataset::FileIterator::FileReaderIterator::GetNextInternal", "method_params": "ctx,out_tensors,end_of_sequence", "method_startline": "443", "method_endline": "477"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\data\\kernel_tests\\cache_test.py", "file_new_name": "tensorflow\\python\\data\\kernel_tests\\cache_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "167,168,169,170,171", "deleted_lines": null, "method_info": {"method_name": "testReadingPastEndOfSequence", "method_params": "self", "method_startline": "167", "method_endline": "171"}}}}}}}