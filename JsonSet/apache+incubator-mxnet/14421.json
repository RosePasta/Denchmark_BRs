{"BR": {"BR_id": "14421", "BR_author": "jmerkow", "BRopenT": "2019-03-13T18:58:52Z", "BRcloseT": "2019-06-27T01:47:28Z", "BR_text": {"BRsummary": "Updating mxnet from 1.0.0, networks give different outputs", "BRdescription": "\n I am working in a production environment, where have some networks implemented in mxnet 1.0.0. I am working updating our systems and trying to push to the latest mxnet (1.4.x as of now) but when we upgrade our networks produce different outputs.\n We are using symbols saved to json files and arg/aux_params stored in .params files. these were all produced by mxnet 1.0.0 or earlier.\n When using the latest mxnet (or 1.4.x) we are getting different outputs for the same inputs, with our saved models. I have been trying to use git bisect or slowly upgrading versions to figure out where this breaking change occurred but there are issues with your git history and/or some strange (compiler??) incompatibilities which prevent getting a clean checkout/build for nearly all of the intermediate versions\u2026\n These are VERY different outputs, im rounding to about the 5 decimal point, or higher, so these aren\u2019t numerical differences.\n And we are using modules, not gluons (part of the point is upgrade and move towards gluons)\n Does anyone have any idea what could be causing this? Or what to look into to solve it?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jmerkow", "commentT": "2019-03-13T18:58:56Z", "comment_text": "\n \t\tHey, this is the MXNet Label Bot.\n Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.\n Here are my recommended labels: Bug\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jmerkow", "commentT": "2019-03-14T00:30:41Z", "comment_text": "\n \t\tDo you have a sample model which can be shared to reproduce this? There is a backwards compatibility checker for models as part of the nightly pipeline. It trains models on earlier releases and checks for consistency on the latest builds. It's possible there is an edge case which is being missed.\n <denchmark-link:https://github.com/mxnet-label-bot>@mxnet-label-bot</denchmark-link>\n  add [Pending Requestor Info, Bug]\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jmerkow", "commentT": "2019-03-17T19:37:03Z", "comment_text": "\n \t\tCould I run the tool and pass the output instead?\n I need to run down some checks first as well...  I want to:\n \n make sure the weights are the same after being loaded\n that there were no changes to any of the functions used for batch transform on the MXNet side (i.e. resize_short, center_crop, etc).\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jmerkow", "commentT": "2019-03-19T05:51:28Z", "comment_text": "\n \t\tHow do you use your model to inference? If you are using mx.module, please make sure the parameter for_training is set to be 0.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jmerkow", "commentT": "2019-04-11T19:44:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jmerkow>@jmerkow</denchmark-link>\n  The tool does save and load the same weights and checks the forward pass on the model.\n It does not check in the intermediate layers in the model graph.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jmerkow", "commentT": "2019-05-20T16:22:39Z", "comment_text": "\n \t\tOk I am working on this again.\n I've checked that the parameter files are the same. i.e. something like this:\n <denchmark-code>## in a docker with mx1.4.0 loaded\n np_arg_params = {key: value.asnumpy() for key, value in arg_params.items()}\n np.save('mx140.npy', np_arg_params)\n ...\n \n ## in another docker with mx100 loaded\n import numpy as np\n np_arg_params2 = np.load('../140/mx140.npy')[()]\n assert set(np_arg_params2.keys()).difference(np_arg_params.keys()) == set()\n assert set(np_arg_params.keys()).difference(np_arg_params2.keys()) == set()\n assert all((np_arg_params[k]-np_arg_params2[k]).sum() ==0 for k in np_arg_params.keys())\n </denchmark-code>\n \n I double checked that the actual input to the image is the same (after transforms etc).\n We use the module doing something like this:\n <denchmark-code>        mod.forward(batch) # batch is basically a named tuple with an NDArray at batch.data\n         output = mod.get_outputs()[0].asnumpy() \n </denchmark-code>\n \n EDIT: Ok I went through and save the outputs of each layer, and I tracked down the first output that has a difference and its a run-of-the-mill 2D convolution layer:\n <denchmark-code>[{u'attr': {u'kernel': u'(3, 3)',\n    u'no_bias': u'True',\n    u'num_filter': u'96',\n    u'pad': u'(0, 0)',\n    u'stride': u'(1, 1)'},\n   u'inputs': [[42, 0, 0], [43, 0, 0]],\n   u'name': u'in_stem_conv6_3*3_conv2d',\n   u'op': u'Convolution'}]\n </denchmark-code>\n \n <denchmark-link:https://github.com/Piyush3dB>@Piyush3dB</denchmark-link>\n  <denchmark-link:https://github.com/andrewfayres>@andrewfayres</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jmerkow", "commentT": "2019-05-20T17:54:16Z", "comment_text": "\n \t\tDoing some more analysis, it looks like there are some differences at the convolution layer described above, but the changes are relatively minor. However, there is a global pooling layer at the end of the network which seems to have a VERY VERY large difference.  I'm using the following to calculate error:\n <denchmark-code>for n in layers:\n     x, y = output[n], mx140_outputs[n]\n     print(n, np.mean([np.abs(((xi-yi)/(xi+1e-10)).sum()) for xi, yi in zip(x,y)]))\n </denchmark-code>\n \n the error values are all less than 0.1, except after the global pooling layer i get global_avgpool_output 8.13365e+11\n Was there some change to global pooling that would cause this?\n <denchmark-code>{u'attr': {u'global_pool': u'True',\n    u'kernel': u'(8, 8)',\n    u'pad': u'(1, 1)',\n    u'pool_type': u'avg'},\n   u'inputs': [[1229, 0, 0]],\n   u'name': u'global_avgpool',\n   u'op': u'Pooling'}\n </denchmark-code>\n \n Also looking at the network itself, the layer going into the global pool is 5x5, but the kernel is set to 8x8.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "jmerkow", "commentT": "2019-05-20T18:58:03Z", "comment_text": "\n \t\tOk I may have figured it out.\n It appears that now global pooling ignores padding (including Pooling_v1). If i set padding=0 on the global pooling layer i get the same values (incorrect) in mx 1.0.0 as mx 1.4.0.  Is there a way to prevent this?\n I'd like pooling to use the padding values when calculating the global pool.\n <denchmark-link:https://github.com/Piyush3dB>@Piyush3dB</denchmark-link>\n  <denchmark-link:https://github.com/andrewfayres>@andrewfayres</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "jmerkow", "commentT": "2019-05-20T19:37:45Z", "comment_text": "\n \t\tHere is a small example that you can use to reproduce the issue:\n Code:\n <denchmark-code>import mxnet as mx\n class Batch(object):\n     def __init__(self, data):\n         self.data = data\n     def get_batch_shape(self):\n         return tuple(self.data[0].shape)\n \n data = mx.sym.Variable('data')\n gpool = mx.sym.Pooling(data, name='gpooling', global_pool=True, pad=(1,1), pool_type='avg', kernel=(8,8))\n mod = mx.mod.Module(gpool, context=mx.gpu(0), label_names=[])\n data = Batch([mx.ndarray.ones((1, 3, 5, 5))])\n mod.bind(for_training=True, force_rebind=True, data_shapes=[('data', data.get_batch_shape())],)\n mod.init_params()\n mod.forward(data)\n print(mod.get_outputs()[0].asnumpy().squeeze().tolist())\n </denchmark-code>\n \n MX 1.0.0 output:\n \n [0.6399999856948853, 0.6399999856948853, 0.6399999856948853]\n \n MX 1.4.0 output:\n \n [1.0, 1.0, 1.0]\n \n This appears to be a bug with <denchmark-link:https://github.com/apache/incubator-mxnet/pull/9730>#9730</denchmark-link>\n ...\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "jmerkow", "commentT": "2019-06-14T20:47:06Z", "comment_text": "\n \t\tSo the correct behaviour is the new behaviour, are you requesting a flag to have the option to use the old incorrect behaviour?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "jmerkow", "commentT": "2019-06-14T22:01:54Z", "comment_text": "\n \t\tI think this is the correct behaviour for global average pooling. I don't see why you would want to use global average pooling with the buggy behaviour. After looking into this,  I would suggest to close this issue. Also per the documentation using global pooling with reset the kernel size to the full image. Also padding is not considered with using global pooling.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "jmerkow", "commentT": "2019-06-17T18:14:51Z", "comment_text": "\n \t\tI think this is the first paper that proposed global pooling. <denchmark-link:https://arxiv.org/abs/1312.4400>https://arxiv.org/abs/1312.4400</denchmark-link>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "jmerkow", "commentT": "2019-06-17T18:22:46Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/larroy>@larroy</denchmark-link>\n . It\u2019s not about correct behavior. Networks already trained with the buggy behavior will not work after updating. I was looking for a work around so we could update and without retraining. We have a number of networks trained with the buggy behavior in production. So this is effectively blocking us from upgrading. If you want to close the PR that\u2019s fine, unless there is a reason to it might be good to leave around so people can use it if they have the same issues\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "jmerkow", "commentT": "2019-06-17T22:34:22Z", "comment_text": "\n \t\tI understand your problem, I'm sorry that this bug caused you problems when updating to more recent versions, I haven't had notice of other users being affected by this. Would you be willing to make a contribution to support having the old behavior? This could be a path forward for your case. Unfortunately like every resource, our energy is limited and we have to pick our battles, this seems like a corner case and how unfortunate as is I don't think we can dedicate efforts in supporting a past buggy behaviour for such an old version. If I may ask, why can't you retrain with a more recent version?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "jmerkow", "commentT": "2019-06-18T20:09:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/larroy>@larroy</denchmark-link>\n  <denchmark-link:https://github.com/apache/incubator-mxnet/pull/15026>#15026</denchmark-link>\n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "jmerkow", "commentT": "2019-06-27T01:47:28Z", "comment_text": "\n \t\tClosing this issue as discussed in the PR <denchmark-link:https://github.com/apache/incubator-mxnet/pull/15026>#15026</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "ba30644612357930fd4543f01800d89be7963f8e", "commit_author": "Pedro Larroy", "commitT": "2019-06-26 17:13:53-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\operator\\nn\\pooling.cc", "file_new_name": "src\\operator\\nn\\pooling.cc", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "392,393", "deleted_lines": "392,393"}}}}}}