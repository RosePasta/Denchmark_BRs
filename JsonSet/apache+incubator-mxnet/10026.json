{"BR": {"BR_id": "10026", "BR_author": "marcoabreu", "BRopenT": "2018-03-07T18:23:34Z", "BRcloseT": "2018-08-17T21:51:41Z", "BR_text": {"BRsummary": "MXNET_MKLDNN_DEBUG=1 produces errors", "BRdescription": "\n <denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/PR-9995/32/pipeline/483>http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/PR-9995/32/pipeline/483</denchmark-link>\n \n Setting MXNET_MKLDNN_DEBUG=1 as environment variable will produce the following error in tests. This happens across all configurations and seeds. I do not think that this is a test failure.\n <denchmark-code>======================================================================\n \n ERROR: test_gluon_model_zoo.test_models\n \n ----------------------------------------------------------------------\n \n Traceback (most recent call last):\n \n   File \"/usr/local/lib/python2.7/dist-packages/nose/case.py\", line 197, in runTest\n \n     self.test(*self.arg)\n \n   File \"/work/mxnet/tests/python/unittest/common.py\", line 157, in test_new\n \n     orig_test(*args, **kwargs)\n \n   File \"/work/mxnet/tests/python/unittest/test_gluon_model_zoo.py\", line 50, in test_models\n \n     model(mx.nd.random.uniform(shape=data_shape)).wait_to_read()\n \n   File \"/work/mxnet/python/mxnet/ndarray/ndarray.py\", line 1650, in wait_to_read\n \n     check_call(_LIB.MXNDArrayWaitToRead(self.handle))\n \n   File \"/work/mxnet/python/mxnet/base.py\", line 149, in check_call\n \n     raise MXNetError(py_str(_LIB.MXGetLastError()))\n \n MXNetError: [17:10:12] src/operator/nn/mkldnn/mkldnn_base.cc:395: Check failed: similar \n \n \n \n Stack trace returned 10 entries:\n \n [bt] (0) /work/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f06ccf3745b]\n \n [bt] (1) /work/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f06ccf38478]\n \n [bt] (2) /work/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::OpCheck::Run(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>, nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)+0x3ca8) [0x7f06ccf54198]\n \n [bt] (3) /work/mxnet/python/mxnet/../../lib/libmxnet.so(+0x2a910d9) [0x7f06cf55a0d9]\n \n [bt] (4) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x7c) [0x7f06cf77608c]\n \n [bt] (5) /work/mxnet/python/mxnet/../../lib/libmxnet.so(+0x3148fdb) [0x7f06cfc11fdb]\n \n [bt] (6) /work/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0xcb5) [0x7f06cfc0b1a5]\n \n [bt] (7) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (std::shared_ptr<dmlc::ManualEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr<dmlc::ManualEvent>&&)+0xd9) [0x7f06cfc1d309]\n \n [bt] (8) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::thread::_Impl<std::_Bind_simple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)> (std::shared_ptr<dmlc::ManualEvent>)> >::_M_run()+0x4a) [0x7f06cfc1c43a]\n \n [bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80) [0x7f06d7ca4c80]\n \n \n \n \n \n -------------------- >> begin captured stdout << ---------------------\n \n ResNetV1(\n \n   (features): HybridSequential(\n \n     (0): Conv2D(None -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n \n     (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n     (2): Activation(relu)\n \n     (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n \n     (4): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (5): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n         (downsample): HybridSequential(\n \n           (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (6): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n         (downsample): HybridSequential(\n \n           (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (7): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n         (downsample): HybridSequential(\n \n           (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n \n   )\n \n   (output): Dense(512 -> 1000, linear)\n \n )\n \n ResNetV1(\n \n   (features): HybridSequential(\n \n     (0): Conv2D(None -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n \n     (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n     (2): Activation(relu)\n \n     (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n \n     (4): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (2): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (5): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n         (downsample): HybridSequential(\n \n           (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (2): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (3): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (6): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n         (downsample): HybridSequential(\n \n           (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (2): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (3): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (4): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (5): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (7): HybridSequential(\n \n       (0): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n         (downsample): HybridSequential(\n \n           (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (1): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n       (2): BasicBlockV1(\n \n         (body): HybridSequential(\n \n           (0): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n           (2): Activation(relu)\n \n           (3): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n \n           (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=None)\n \n         )\n \n       )\n \n     )\n \n     (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n \n   )\n \n   (output): Dense(512 -> 1000, linear)\n \n )\n \n \n \n --------------------- >> end captured stdout << ----------------------\n \n -------------------- >> begin captured logging << --------------------\n \n common: INFO: Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=1825457337 to reproduce.\n \n common: INFO: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1579343143 to reproduce.\n \n --------------------- >> end captured logging << ---------------------\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "marcoabreu", "commentT": "2018-03-08T11:03:59Z", "comment_text": "\n \t\t<denchmark-link:https://issues.apache.org/jira/browse/MXNET-60>https://issues.apache.org/jira/browse/MXNET-60</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "marcoabreu", "commentT": "2018-03-22T06:21:46Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/marcoabreu>@marcoabreu</denchmark-link>\n  <denchmark-link:https://github.com/cjolivier01>@cjolivier01</denchmark-link>\n   This is a very useful function to verify the correctness of MKL-DNN OP.\n After debugging, there are two types of failures in DEBUG mode.\n  <denchmark-link:https://github.com/cjolivier01>@cjolivier01</denchmark-link>\n \n The current both rtol and atol in SimilarArray are 1e-3.\n In the fail case, the two data are  and . Because these two numbers are small, the atol is larger than 1e-3. But I think this difference is acceptable.\n I suggest that we change the atol to 1e-2.\n \n \n \n incubator-mxnet/src/operator/nn/mkldnn/mkldnn_base.cc\n \n \n          Line 391\n       in\n       46e47cb\n \n \n \n \n \n \n  bool similar = SimilarArray<DType>(outputs[i], outputs_[i], 1e-3, 1e-3); \n \n \n \n \n \n \n [15:28:36] src/operator/nn/mkldnn/mkldnn_base.cc:346: data1[i]: 0.675079 data2[i]: 0.677384\n [15:28:36] src/operator/nn/mkldnn/mkldnn_base.cc:347: atol + rtol * std::abs(data2[i]):0.00167738\n Abs(abs(data1[i] - data2[i] )= 0.002305\n if (std::abs(data1[i] - data2[i]) > atol + rtol * std::abs(data2[i])) return false\n \n  <denchmark-link:https://github.com/piiswrong>@piiswrong</denchmark-link>\n \n \n \n I suggest changing this to  so that the MKL-DNN flatten function will be used.\n After I changed these two, all cases passed.\n <denchmark-code>[patric@mlt-skx080 master]$ git diff\n diff --git a/3rdparty/mkldnn b/3rdparty/mkldnn\n --- a/3rdparty/mkldnn\n +++ b/3rdparty/mkldnn\n @@ -1 +1 @@\n -Subproject commit f5218ff4fd2d16d13aada2e632afd18f2514fee3\n +Subproject commit f5218ff4fd2d16d13aada2e632afd18f2514fee3-dirty\n diff --git a/python/mxnet/gluon/nn/basic_layers.py b/python/mxnet/gluon/nn/basic_layers.py\n index 3801c84..b7bba8a 100644\n --- a/python/mxnet/gluon/nn/basic_layers.py\n +++ b/python/mxnet/gluon/nn/basic_layers.py\n @@ -405,7 +405,7 @@ class Flatten(HybridBlock):\n          super(Flatten, self).__init__(**kwargs)\n \n      def hybrid_forward(self, F, x):\n -        return x.reshape((0, -1))\n +        return F.Flatten(x)\n \n      def __repr__(self):\n          return self.__class__.__name__\n diff --git a/src/operator/nn/mkldnn/mkldnn_base.cc b/src/operator/nn/mkldnn/mkldnn_base.cc\n index 820cca1..c9582cd 100644\n --- a/src/operator/nn/mkldnn/mkldnn_base.cc\n +++ b/src/operator/nn/mkldnn/mkldnn_base.cc\n @@ -388,7 +388,7 @@ void OpCheck::Run(mxnet::FCompute fn, const nnvm::NodeAttrs &attrs,\n      if (req[i] == kNullOp)\n        continue;\n      MSHADOW_TYPE_SWITCH(outputs[i].dtype(), DType, {\n -      bool similar = SimilarArray<DType>(outputs[i], outputs_[i], 1e-3, 1e-3);\n +      bool similar = SimilarArray<DType>(outputs[i], outputs_[i], 1e-3, 1e-2);\n        if (!similar) {\n          LOG(ERROR) << attrs.op->name << \" fails\";\n        }\n </denchmark-code>\n \n  <denchmark-link:https://github.com/zheng-da>@zheng-da</denchmark-link>\n \n In theory, this implementation should work with MKL-DNN type as well. But it doesn't.\n I think there is a bug.  I am still debugging this now.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "marcoabreu", "commentT": "2018-03-27T08:41:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/marcoabreu>@marcoabreu</denchmark-link>\n  We got the root cause of 3) in above comments. It's not the MKL-DNN implementation issues. Just need to improve the test method under MXNET_MKLDNN_DEBUG\n <denchmark-link:https://github.com/cjolivier01>@cjolivier01</denchmark-link>\n  Thanks for the nice functionality to check the results of MKL-DNN.\n We found there is a special situation which needs to be improved as 3) in my above comments.\n The  is executed to change the view of NDAarry but the real data don't change yet.\n When come to debug mode, OpCheck tries to get the MKLDNN memory without checking if it is a view, so the case fails.\n \n \n \n incubator-mxnet/src/operator/nn/mkldnn/mkldnn_base.cc\n \n \n          Line 357\n       in\n       bd9b9c8\n \n \n \n \n \n \n  auto mem = inputs_[i].GetMKLDNNData(); \n \n \n \n \n \n So, a possible fix for OpCheck.Init,  <denchmark-link:https://github.com/zheng-da>@zheng-da</denchmark-link>\n  please help take a review.\n <denchmark-code>+    //auto mem = inputs_[i].GetMKLDNNData();\n +    NDArray data = inputs_[i];\n +    const TShape& ishape = inputs_[i].shape();\n +    if (data.IsMKLDNNData() && data.IsView())\n +        data = data.MKLDNNDataReshape(Shape2(ishape.ProdShape(0, ishape.ndim()-1),\n +                                     ishape[ishape.ndim()-1]));\n +    auto mem = data.GetMKLDNNData(); \n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "marcoabreu", "commentT": "2018-03-28T00:54:36Z", "comment_text": "\n \t\tWhen I wrote the test, I followed the python test. <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L470>https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L470</denchmark-link>\n \n When assert_almost_equal is called <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L1313>https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/test_utils.py#L1313</denchmark-link>\n , it uses 1e-3 for both rtol and atol. I didn't know why the test fails.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "marcoabreu", "commentT": "2018-03-28T00:58:16Z", "comment_text": "\n \t\tAs for modifying OpCheck.Init, you can do\n <denchmark-code>  if (data.IsMKLDNNData() && data.IsView())\n     data = in_data[fullc::kData].Reorder2Default();\n </denchmark-code>\n \n Please see the example here: <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/mkldnn/mkldnn_fully_connected.cc#L95>https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/mkldnn/mkldnn_fully_connected.cc#L95</denchmark-link>\n \n This is how we should deal with reshaped MKLDNN NDArray. Unfortunately, we can't do in-place layout conversion in the NDArray, which caused a race condition.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "marcoabreu", "commentT": "2018-03-28T01:04:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/zheng-da>@zheng-da</denchmark-link>\n  I have looked into Reorder2Default, but it will also convert to the original shape rather than the new shape of 'reshape'.\n And that's another point we want to improve later.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "marcoabreu", "commentT": "2018-07-13T20:45:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pengzhao-intel>@pengzhao-intel</denchmark-link>\n  I believe when you call copyfrom it will convert the input memory shape into the same shape as the target. so if you Reorder2Default but then call copyfrom the mkldnn memory will be the new shape\n <denchmark-code>NDArray data = inputs_[i];\n inputs.emplace_back(data.shape(), ctx, false, data.dtype());\n if (data.IsMKLDNNData() && data.IsView())\n     data = in_data[fullc::kData].Reorder2Default();\n auto mem = inputs_[i].GetMKLDNNData();\n inputs[i].CopyFrom(*mem);\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "marcoabreu", "commentT": "2018-08-07T19:21:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/apache/incubator-mxnet/pull/12069>#12069</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "marcoabreu", "commentT": "2018-08-17T21:46:51Z", "comment_text": "\n \t\tabove PR addresses issue. <denchmark-link:https://github.com/marcoabreu>@marcoabreu</denchmark-link>\n  can you close?\n \t\t"}}}, "commit": {"commit_id": "525ead9caaf49035b0310ef7c8b686b393463760", "commit_author": "Alexander Zai", "commitT": "2018-08-14 10:22:44-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "ci\\docker\\runtime_functions.sh", "file_new_name": "ci\\docker\\runtime_functions.sh", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "584,594,603,612,644,671,681", "deleted_lines": "584,585,586,596,597,598,607,608,609,618,619,620,652,653,654,681,682,683,693,694,695"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\mxnet\\gluon\\nn\\basic_layers.py", "file_new_name": "python\\mxnet\\gluon\\nn\\basic_layers.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "430", "deleted_lines": "430", "method_info": {"method_name": "hybrid_forward", "method_params": "self,F,x", "method_startline": "429", "method_endline": "430"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\operator\\nn\\lrn.cc", "file_new_name": "src\\operator\\nn\\lrn.cc", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "207,208", "deleted_lines": null}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\nn\\mkldnn\\mkldnn_base.cc", "file_new_name": "src\\operator\\nn\\mkldnn\\mkldnn_base.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "476,477,478,479,480,499,500,501,502,503,519", "deleted_lines": "476,477,478,512", "method_info": {"method_name": "mxnet::SimilarArray", "method_params": "arr1,arr2,rtol,atol", "method_startline": "431", "method_endline": "556"}}}}}}}