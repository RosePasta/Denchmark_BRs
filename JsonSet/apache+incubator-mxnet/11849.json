{"BR": {"BR_id": "11849", "BR_author": "wgchang", "BRopenT": "2018-07-21T16:50:16Z", "BRcloseT": "2018-09-18T22:47:15Z", "BR_text": {"BRsummary": "gluon.SymbolBlock cannot imports resnet trained with dtype=\"float16\"", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n Cannot load fine-tuned resnet101 (incubator-mxnet/example/image-classification/symbols/resnet.py) with dtype=\"float16\"  with \"gluon.SymbolBlock.imports\" method.\n <denchmark-h:h2>Error Message:</denchmark-h>\n \n AssertionError: Failed loading Parameter 'stage3_unit2_conv2_weight' from saved params: dtype incompatible expected <type 'numpy.float32'> vs saved <type 'numpy.float16'>\n <denchmark-h:h2>Minimum reproducible example</denchmark-h>\n \n net = gluon.SymbolBlock.imports('resnet-101-symbol.json',['data','softmax_label'],'resnet-101-0007.params')  # This line gives the error message.\n net = gluon.SymbolBlock.imports('resnet-101-symbol.json',['data','softmax_label']) \n print(net.collect_params())\n <denchmark-h:h2>My Questions</denchmark-h>\n \n In incubator-mxnet/example/image-classification/symbols/resnet.py,\n there is mx.sym.Cast for type conversion.\n I fine-tuned resnet101 with dtype=\"float16\", and I need to load this model as HybridBlock, However, the method gluon.SymbolBlock.imports makes every params' type in the network as float32. Therefore, the trained model cannot be updated.\n Here, resnet-101-0007.params are trained with argument dtype='float16'\n In resnet-101-symbol.json file, there is the Cast op.\n {\n \"op\": \"Cast\",\n \"name\": \"cast0\",\n \"attrs\": {\"dtype\": \"float16\"},\n \"inputs\": [[7, 0, 0]]\n },\n It seems that gluon.SymbolBlock.imports does not consider the type conversion operator.\n For now, I think I need to load all parameter manualy, and change types then save.\n Is there any other solution to solve this problem?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wgchang", "commentT": "2018-07-23T22:24:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sandeep-krishnamurthy>@sandeep-krishnamurthy</denchmark-link>\n  Please help to label this issue \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wgchang", "commentT": "2018-08-14T08:36:57Z", "comment_text": "\n \t\tGot the same problem here. Any updates here?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wgchang", "commentT": "2018-08-22T16:59:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rahul003>@rahul003</denchmark-link>\n  same problem here, can't load back saved float16 models\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wgchang", "commentT": "2018-08-29T22:16:44Z", "comment_text": "\n \t\tUsing below snippet:\n <denchmark-code>import mxnet as mx\n \n ctx = mx.cpu(0)\n data = mx.nd.zeros((1,3,224,224), ctx=ctx, dtype='float64')\n net_fp32 = mx.gluon.model_zoo.vision.resnet34_v2(pretrained=True, ctx=ctx)\n net_fp32.cast('float64')\n net_fp32.hybridize()\n pred = net_fp32.forward(data)\n net_fp32.export('resnet34_fp16', 0)\n print('export fp16 model')\n \n sm = mx.sym.load('resnet34_fp16-symbol.json')\n inputs = mx.sym.var('data', dtype='float64')\n net_fp16 = mx.gluon.SymbolBlock(sm, inputs)\n net_fp16.collect_params().load('resnet34_fp16-0000.params', ctx)\n pred = net_fp16.forward(data)\n </denchmark-code>\n \n Below are my findings:\n \n Casting worked fine.\n Saved parameters are in the correct format (fp64 in my sample code)\n sym.load worked fine. If I infer symbol's type (sym.infer_type(data='float64') I get correct inferred type (float64) for all params.\n \n Below is the issue:\n \n When you create mx.gluon.SymbolBlock(sm, input). It creates the parameters in the Block. Type is not passed for creating the parameter.\n See here - https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/block.py#L1058 and the behavior is \"If there is not parameter to get, it creates one and uses default type (fp32) https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/parameter.py#L688\n \n I am working on the fix.\n <denchmark-link:https://github.com/apeforest>@apeforest</denchmark-link>\n  <denchmark-link:https://github.com/ThomasDelteil>@ThomasDelteil</denchmark-link>\n   - FYI\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "wgchang", "commentT": "2018-09-18T22:47:15Z", "comment_text": "\n \t\tResolving as changes are merged.\n \t\t"}}}, "commit": {"commit_id": "acf309e87d48e47c7da7684a74a557aef5a76124", "commit_author": "Sandeep Krishnamurthy", "commitT": "2018-09-18 15:46:53-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\mxnet\\gluon\\block.py", "file_new_name": "python\\mxnet\\gluon\\block.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1095,1096,1097", "deleted_lines": null, "method_info": {"method_name": "cast", "method_params": "self,dtype", "method_startline": "1095", "method_endline": "1097"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162", "deleted_lines": null, "method_info": {"method_name": "_infer_param_types", "method_params": "in_params,out_params,arg_params,aux_params,default_dtype", "method_startline": "1102", "method_endline": "1162"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1057,1058,1059,1060,1062,1063,1064,1065,1066,1067,1068,1069,1070", "deleted_lines": "1056,1057,1058,1060,1061,1062", "method_info": {"method_name": "__init__", "method_params": "self,outputs,inputs,params", "method_startline": "1030", "method_endline": "1074"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\mxnet\\gluon\\parameter.py", "file_new_name": "python\\mxnet\\gluon\\parameter.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "730,731", "deleted_lines": null, "method_info": {"method_name": "get", "method_params": "self,name,kwargs", "method_startline": "685", "method_endline": "740"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\python\\gpu\\test_gluon_gpu.py", "file_new_name": "tests\\python\\gpu\\test_gluon_gpu.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234", "deleted_lines": null, "method_info": {"method_name": "test_symbol_block_fp16", "method_params": "", "method_startline": "207", "method_endline": "234"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\python\\unittest\\test_gluon.py", "file_new_name": "tests\\python\\unittest\\test_gluon.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375", "deleted_lines": null, "method_info": {"method_name": "test_symbol_block", "method_params": "", "method_startline": "302", "method_endline": "375"}}}}}}}