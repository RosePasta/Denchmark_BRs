{"BR": {"BR_id": "235", "BR_author": "stefan-it", "BRopenT": "2020-02-04T13:10:58Z", "BRcloseT": "2020-02-18T16:06:52Z", "BR_text": {"BRsummary": "German CoNLL-2003 data", "BRdescription": "\n Hi,\n it seems that the DOWNSTREAM_TASK_MAP uses the wrong data for the German CoNLL-2003.\n I had a look at the data that is specified e.g. for the conll03detrain task and I think they also include the data from the GermEval shared task.\n I compared the original CoNLL-2003 dataset (created from Reuters CD) and the correct data is located in this <denchmark-link:https://github.com/MaviccPRP/ger_ner_evals/tree/master/corpora/conll2003>conll2003 folder</denchmark-link>\n . Files are ,  and .\n However, they need a few preprocessing steps: conversion from iso-latin to utf-8 and the original dataset uses the IOB1 tagging scheme.\n I think we should use the original dataset for further benchmarks :)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "stefan-it", "commentT": "2020-02-04T14:03:01Z", "comment_text": "\n \t\tThanks for the heads up.\n We will double check and let you know asap.\n Do you maybe know another site that hosts the correctly transformed data? We would prefer to not host the copyrighted conll03 data ourselves.\n As last resort we will do the necessary transformations on FARM side.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "stefan-it", "commentT": "2020-02-05T14:56:41Z", "comment_text": "\n \t\tHey thanks for pointing this out. You are actually right - the conll dataset has at least some if not all of germeval14 in it. We are looking more deeply in to it now and will get back to you.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "stefan-it", "commentT": "2020-02-06T13:01:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/brandenchan>@brandenchan</denchmark-link>\n  at the moment I couldn't find the CoNLL data for German that is both in utf-8 and iob2. So I think we need to implement some function for encoding and fixing the tagging format.\n Here's a snippet for IOB1 to IOB2 (haven't test it yet), that we could use: <denchmark-link:https://gist.github.com/allanj/b9bd448dc9b70d71eb7c2b6dd33fe4ef>Gist</denchmark-link>\n  :)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "stefan-it", "commentT": "2020-02-06T17:38:34Z", "comment_text": "\n \t\tI was just about to ask you for that code - thanks! Will keep you updated on our progress\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "stefan-it", "commentT": "2020-02-07T14:28:34Z", "comment_text": "\n \t\tHey, I actually found a bug in the code. From what I can tell now, the links to CoNLL and Germeval datasets point to the right datasets. The first sentence in the CoNLL dataset starts \"Schartau sagte dem...\" and the first sentence in the Germeval dataset is \"Ereignis und Erz\u00e4hlung oder...\"\n I could not find any overlapping sentences between the datasets. Can you provide any examples of this?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "stefan-it", "commentT": "2020-02-14T17:47:59Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n \n I had time to look into this issue and can confirm this huge bug!\n The data that was downloaded as <denchmark-link:https://raw.githubusercontent.com/MaviccPRP/ger_ner_evals/master/corpora/training_data_for_Stanford_NER/NER-de-train-conll-formated.txt>\"conll03de\"</denchmark-link>\n  in FARM is text from more recent newspapers articles. I found examples pointing to n-tv.de articles from 2008 and a rp-online article from 2009. So our \"conll03de\" version cannot be the real deal.\n I created a PR <denchmark-link:https://github.com/deepset-ai/FARM/pull/248>#248</denchmark-link>\n  with your proposed changes.\n We updated our evaluation of the German Bert vs multilingual models in our <denchmark-link:https://deepset.ai/german-bert>blog post</denchmark-link>\n  as well. The post about XLM-R will be updated soon, too.\n Thanks for the hint! This was a big mess on our side : )\n \t\t"}}}, "commit": {"commit_id": "2bdf16ae7eb4d49d88bf88e571cef6905d61da02", "commit_author": "Timo Moeller", "commitT": "2020-02-18 17:06:52+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\doc_classification.py", "file_new_name": "examples\\doc_classification.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33,41", "deleted_lines": "40,41,42", "method_info": {"method_name": "doc_classifcation", "method_params": "", "method_startline": "16", "method_endline": "113"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\doc_classification_crossvalidation.py", "file_new_name": "examples\\doc_classification_crossvalidation.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "48,54", "deleted_lines": "53", "method_info": {"method_name": "doc_classification_crossvalidation", "method_params": "", "method_startline": "19", "method_endline": "229"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\doc_classification_custom_optimizer.py", "file_new_name": "examples\\doc_classification_custom_optimizer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33,67", "deleted_lines": "66", "method_info": {"method_name": "doc_classifcation", "method_params": "", "method_startline": "16", "method_endline": "141"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\doc_classification_multilabel_roberta.py", "file_new_name": "examples\\doc_classification_multilabel_roberta.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "36,39,40,41", "deleted_lines": "38,39", "method_info": {"method_name": "doc_classification_multilabel_roberta", "method_params": "", "method_startline": "17", "method_endline": "116"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\doc_classification_with_earlystopping.py", "file_new_name": "examples\\doc_classification_with_earlystopping.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "40,45", "deleted_lines": "44", "method_info": {"method_name": "doc_classification_with_earlystopping", "method_params": "", "method_startline": "19", "method_endline": "158"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\doc_regression.py", "file_new_name": "examples\\doc_regression.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "35,40", "deleted_lines": "39", "method_info": {"method_name": "doc_regression", "method_params": "", "method_startline": "17", "method_endline": "104"}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\lm_finetuning.py", "file_new_name": "examples\\lm_finetuning.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "36,40", "deleted_lines": "39", "method_info": {"method_name": "lm_finetuning", "method_params": "", "method_startline": "16", "method_endline": "92"}}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\ner.py", "file_new_name": "examples\\ner.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33,34,35,39,40,46", "deleted_lines": "33,34,38,39,45", "method_info": {"method_name": "ner", "method_params": "", "method_startline": "16", "method_endline": "103"}}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\question_answering.py", "file_new_name": "examples\\question_answering.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "37,38,44,66", "deleted_lines": "37,43,65", "method_info": {"method_name": "question_answering", "method_params": "", "method_startline": "19", "method_endline": "126"}}}}, "file_9": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "farm\\data_handler\\processor.py", "file_new_name": "farm\\data_handler\\processor.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "615", "deleted_lines": "615"}}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "farm\\data_handler\\utils.py", "file_new_name": "farm\\data_handler\\utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "78,79,80,81,89,90", "deleted_lines": "77,78", "method_info": {"method_name": "read_ner_file", "method_params": "filename,sep,proxies", "method_startline": "69", "method_endline": "104"}}, "hunk_1": {"Ismethod": 1, "added_lines": "107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129", "deleted_lines": null, "method_info": {"method_name": "_convertIOB1_to_IOB2", "method_params": "", "method_startline": "107", "method_endline": "129"}}}}}}}