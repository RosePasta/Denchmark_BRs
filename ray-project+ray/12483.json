{"BR": {"BR_id": "12483", "BR_author": "VenezianoMauro", "BRopenT": "2020-11-29T20:23:42Z", "BRcloseT": "2020-12-11T13:51:57Z", "BR_text": {"BRsummary": "[rllib] Discrete(20) Observation_space  gets converted to Box(-1,1,(20,)) inside agent", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Running tutorial:\n <denchmark-link:https://colab.research.google.com/github/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb>https://colab.research.google.com/github/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb</denchmark-link>\n \n Observation space is Discrete(20) in the environment. When training there is no problem but when  doing trainer.compute_action(state) raises ValueError: ('Observation ({}) outside given space ({})!', array(0), Box(-1.0, 1.0, (20,), float32))\n Ray version and other system information (Python version, TensorFlow version, OS):\n Ray version 1.0.1.post1\n Python version 3.7.9\n Pytorch version 1.7.0+cpu\n OS windows 10\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n <denchmark-code>import gym\n from gym import spaces\n import numpy as np\n import ray\n from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n ray.init(ignore_reinit_error=True, log_to_driver=False)\n \n class ChainEnv(gym.Env):\n     def __init__(self, env_config = None):\n         env_config = env_config or {}\n         self.n = env_config.get(\"n\", 20)\n         self.small_reward = env_config.get(\"small\", 2)  # payout for 'backwards' action\n         self.large_reward = env_config.get(\"large\", 10)  # payout at end of chain for 'forwards' action\n         self.state = 0  # Start at beginning of the chain\n         self._horizon = self.n\n         self._counter = 0  # For terminating the episode\n         self._setup_spaces()\n     \n     def _setup_spaces(self):\n         ##############\n         # TODO: Implement this so that it passes tests\n         self.action_space = spaces.Discrete(2)\n         self.observation_space = spaces.Discrete(self.n)\n         ##############\n \n     def step(self, action):\n         assert self.action_space.contains(action)\n         if action == 1:  # 'backwards': go back to the beginning, get small reward\n             ##############\n             # TODO 2: Implement this so that it passes tests\n             reward = self.small_reward\n             ##############\n             self.state = 0\n         elif self.state < self.n - 1:  # 'forwards': go up along the chain\n             ##############\n             # TODO 2: Implement this so that it passes tests\n             reward = 0\n             self.state += 1\n         else:  # 'forwards': stay at the end of the chain, collect large reward\n             ##############\n             # TODO 2: Implement this so that it passes tests\n             reward = self.large_reward\n             ##############\n         self._counter += 1\n         done = self._counter >= self._horizon\n         return self.state, reward, done, {}\n \n     def reset(self):\n         self.state = 0\n         self._counter = 0\n         return self.state\n     \n trainer_config = DEFAULT_CONFIG.copy()\n trainer_config['num_workers'] = 1\n trainer_config[\"train_batch_size\"] = 400\n trainer_config[\"sgd_minibatch_size\"] = 64\n trainer_config[\"num_sgd_iter\"] = 10\n trainer_config[\"framework\"] = \"torch\"\n \n trainer = PPOTrainer(trainer_config, ChainEnv);\n for i in range(2):\n     print(\"Training iteration {}...\".format(i))\n     trainer.train()\n \n env = ChainEnv({})\n state = env.reset()\n \n done = False\n max_state = -1\n cumulative_reward = 0\n \n while not done:\n     state = np.array(state)\n     action = trainer.compute_action(state)\n     state, reward, done, results = env.step(action)\n     max_state = max(max_state, state)\n     cumulative_reward += reward\n \n print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n print(\"Max state you've visited is: {}. This is out of {} states.\".format(max_state, env.n))\n </denchmark-code>\n \n If we cannot run your script, we cannot fix your issue.\n \n [yes ] I have verified my script runs in a clean environment and reproduces the issue.\n [ yes] I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "VenezianoMauro", "commentT": "2020-12-11T13:27:43Z", "comment_text": "\n \t\tThanks for filing this <denchmark-link:https://github.com/VenezianoMauro>@VenezianoMauro</denchmark-link>\n  ! Taking a look rn.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "VenezianoMauro", "commentT": "2020-12-11T13:48:15Z", "comment_text": "\n \t\tHmm, apparently our OneHotPreprocessor (the one that transforms int observations into one-hot Boxes) does not store the \"original_space\" property in the new space, leading the Trainer's local_worker to believe that its Policy has the Box space as its original one (instead of Discrete(20)).\n This is fixed with this PR: <denchmark-link:https://github.com/ray-project/ray/pull/12787>#12787</denchmark-link>\n \n As a workaround for right now, replace the if-block in line in rllib/models/preprocessors.py:~83\n <denchmark-code>        if (isinstance(self, TupleFlatteningPreprocessor)\n                 or isinstance(self, DictFlatteningPreprocessor)\n                 or isinstance(self, RepeatedValuesPreprocessor)):\n </denchmark-code>\n \n with this one here:\n <denchmark-code>        if isinstance(self, (DictFlatteningPreprocessor, OneHotPreprocessor,\n                              RepeatedValuesPreprocessor,\n                              TupleFlatteningPreprocessor)):\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "VenezianoMauro", "commentT": "2020-12-11T13:51:09Z", "comment_text": "\n \t\tThis is also related to this issue here: <denchmark-link:https://github.com/ray-project/ray/issues/12516>#12516</denchmark-link>\n \n Yet, slightly different bug.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "VenezianoMauro", "commentT": "2020-12-11T13:51:57Z", "comment_text": "\n \t\tI'm closing this issue. Please feel free to re-open it should the above solution/PR not work on your end.\n \t\t"}}}, "commit": {"commit_id": "abb1eefdc23f197b7ea7a0e54363a56408a86c61", "commit_author": "Sven Mika", "commitT": "2020-12-11 22:43:30+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\evaluation\\worker_set.py", "file_new_name": "rllib\\evaluation\\worker_set.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "77,266,267,268,270,271,272", "deleted_lines": "77,266,267,268,270,271,272"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\models\\preprocessors.py", "file_new_name": "rllib\\models\\preprocessors.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "83,84,85", "deleted_lines": "83,84,85", "method_info": {"method_name": "observation_space", "method_params": "self", "method_startline": "79", "method_endline": "87"}}}}}}}