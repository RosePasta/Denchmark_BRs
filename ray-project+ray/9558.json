{"BR": {"BR_id": "9558", "BR_author": "vish0910", "BRopenT": "2020-07-17T22:20:14Z", "BRcloseT": "2020-08-16T21:09:29Z", "BR_text": {"BRsummary": "[tune] Ray Cluster deployed in Kubernetes unable to sync checkpoint directories to the driver node", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Getting the following error (Stacktrace attached):\n <denchmark-code>2020-07-17 21:53:48,101\tERROR trial_runner.py:550 -- Trial TrainExample_fd24b_00001: Error handling checkpoint /root/ray_results/TrainExample/TrainExample_1_randomforestclassifier__n_estimators=5_2020-07-17_21-53-462l3hkjfs/checkpoint_1/\n Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 546, in _process_trial_save\n     trial.on_checkpoint(trial.saving_to)\n   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trial.py\", line 448, in on_checkpoint\n     self, checkpoint.value))\n ray.tune.error.TuneError: Trial TrainExample_fd24b_00001: Checkpoint path /root/ray_results/TrainExample/TrainExample_1_randomforestclassifier__n_estimators=5_2020-07-17_21-53-462l3hkjfs/checkpoint_1/ not found after successful sync down.\n </denchmark-code>\n \n This is output of ray_random_forest.py. From investigation it looks like the checkpoint directories ie. checkpoint_1, checkpoint_2 that were created in other nodes (workers) of the cluster do not get synced back to the driver node (from where I am running the python script).\n Rest of the files in the Trial directories seem to be in sync.\n This problem is not reproducible by running it on a single-machine with a cluster started up using bare init(). My guess is because all the workers point to the same file-system, and thus, the os.path.exist() returns True (This is reference to the line of code which checks and throws the above error)\n Taking a look at validate_save_restore() to validate a Trainable, I have written a bare-bone script which proves that checkpoint directories are not synced across workers.\n The stacktrace from checkpoint_validation_failed.py\n <denchmark-code>Traceback (most recent call last):\n   File \"code/checkpoint_validation_failed.py\", line 115, in <module>\n     _main()\n   File \"code/checkpoint_validation_failed.py\", line 94, in _main\n     print(f\"VALIDATE TRAINABLE CLASS: {validate_save_restore(TrainExample)}\")\n   File \"code/checkpoint_validation_failed.py\", line 67, in validate_save_restore\n     restore_check = ray.get(trainable_2.restore.remote(old_trainable))\n   File \"/opt/conda/lib/python3.6/site-packages/ray/worker.py\", line 1526, in get\n     raise value.as_instanceof_cause()\n ray.exceptions.RayTaskError(FileNotFoundError): ray::TrainExample.restore() (pid=2704, ip=172.17.0.8)\n   File \"python/ray/_raylet.pyx\", line 471, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 424, in ray._raylet.execute_task.function_executor\n   File \"/opt/conda/lib/python3.6/site-packages/ray/tune/trainable.py\", line 444, in restore\n     with open(checkpoint_path + \".tune_metadata\", \"rb\") as f:\n FileNotFoundError: [Errno 2] No such file or directory: '/root/ray_results/2020-07-17_21-43-17dbvo01tp/checkpoint_3/.tune_metadata'\n command terminated with exit code 1\n </denchmark-code>\n \n Here, Worker X is trying to restore checkpoint created by Worker Y.\n Ray version and other system information (Python version, TensorFlow version, OS):\n Setup:\n \n Ray cluster running in K8s.\n Ray version\n pip list | grep ray ray 0.9.0.dev0\n Docker image:\n REPOSITORY                                                       TAG                 IMAGE ID            CREATED             SIZE rayproject/autoscaler                                            latest              3f8d747cb8af        7 days ago          5.25GB\n \n Playing with sync_to_driver, sync_on_checkpoint parameters did not help.\n Thanks!\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Step 1. Spin-up a Ray cluster using ray-cluster.yaml in a namespace called ray. ie\n \n kubectl create -f ray-namespace.yaml\n kubectl apply -f ray-cluster.yaml\n \n Step 2. Copy the python script to any worker node.\n \n kubectl -n ray get pods.\n Copy WORKER_1_POD_NAME\n Exec into pod kubectl -n ray exec -it <WORKER_1_POD_NAME> -- bin/bash\n mkdir code\n Copy the python script kubectl -n ray cp checkpoint_validation_failed.py  <WORKER_1_POD_NAME>:/code/checkpoint_validation_failed.py\n Exit pod\n Execute script kubectl -n ray exec <WORKER_1_POD_NAME> -- bin/bash -c \"python code/checkpoint_validation_failed.py\n \n You can do the same steps to run .\n <denchmark-link:https://github.com/ray-project/ray/files/4940566/ray_issue_related_files.zip>ray_issue_related_files.zip</denchmark-link>\n \n The above zip contains:\n \n ray-cluster.yaml\n ray-namespace.yaml\n checkpoint_validation_failed.py\n ray_random_forest.py\n \n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "vish0910", "commentT": "2020-07-17T22:55:35Z", "comment_text": "\n \t\tDo you have an NFS for your kubernetes cluster? That might save you a lot of trouble.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "vish0910", "commentT": "2020-07-17T23:00:44Z", "comment_text": "\n \t\tHello <denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n ,\n Thank you for a quick reply.\n No, I do not have NFS for the cluster. Does it have to be a NFS? How does ray sync files like\n \n events.out.tfevents.1595022826.ray-worker-6d9f6b9b8d-g4zvk\n params.json\n params.pkl\n progress.csv\n result.json\n \n But not checkpoint_1, checkpoint_2 etc.?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "vish0910", "commentT": "2020-07-17T23:03:00Z", "comment_text": "\n \t\tThe files are actually generated on the driver, not on the node. Do you have access to S3? S3 could work too.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "vish0910", "commentT": "2020-07-17T23:38:23Z", "comment_text": "\n \t\tOkay. So what gets synced on sync_on_checkpoint?\n sync_on_checkpoint (bool) \u2013 Force sync-down of trial checkpoint to driver. If set to\n False, checkpoint syncing from worker to driver is asynchronous and best-effort. This does\n not affect persistent storage syncing. Defaults to True.\n In the above documentation snippet I am interpreting checkpoint as the checkpoint_N directories described above.\n Would the suggested S3 solution involve implementing DurableTrainable rather than Trainable?\n Also I wanted to clarify, is the above issue a bug in Ray Tune and a network/cloud storage a work-around for it?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "vish0910", "commentT": "2020-07-19T01:39:27Z", "comment_text": "\n \t\tCheckpoints (directories) are synced on sync_on_checkpoint, yep.\n The above is just a limitation of our file-transfer mechanism; we use rsync to move files between VMs, but I think that mechanism breaks down on kubernetes.\n The two approaches to workaround this limitation are to use 1. a networked storage or 2. to implement your own file-transfer protocol (check out sync_to_driver).\n Does that make sense? Let me know if you have any further questions!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "vish0910", "commentT": "2020-07-29T16:10:38Z", "comment_text": "\n \t\tThank you Richard for this clarification.\n Upon searching more, I found this: <denchmark-link:https://serverfault.com/questions/741670/rsync-files-to-a-kubernetes-pod>https://serverfault.com/questions/741670/rsync-files-to-a-kubernetes-pod</denchmark-link>\n \n I see similar solution applied to  in Ray. Maybe we can do the same for sync-checkpoint mechanism to resolve this issue. Or, we just implement it as a custom file-transfer protocol and make it part of the pallet.\n On a side note, I have taken your suggestion to use a NFS. It has unblocked me from aggregating all my checkpoints.\n Thanks once again.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "vish0910", "commentT": "2020-07-29T20:45:16Z", "comment_text": "\n \t\tGreat! yeah, KubernetesCommandRunner is what we will want to use. More than happy to accept a PR if you want to take a quick pass!\n Otherwise, we'll try to get to this in the next couple of weeks.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "vish0910", "commentT": "2020-07-30T15:08:08Z", "comment_text": "\n \t\tI spent sometime last-week to understand if I can fix it, but for me it was not a going to be a quick pass. So I will defer it to the team. Thanks once again Richard. :)\n \t\t"}}}, "commit": {"commit_id": "8f0f7371a055257b4c704bad49edccbfbfc01831", "commit_author": "krfricke", "commitT": "2020-08-16 14:09:28-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\cluster\\cloud.rst", "file_new_name": "doc\\source\\cluster\\cloud.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "181,182", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\tune\\user-guide.rst", "file_new_name": "doc\\source\\tune\\user-guide.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "python\\ray\\autoscaler\\command_runner.py", "file_new_name": "python\\ray\\autoscaler\\command_runner.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "244", "deleted_lines": "238,239,240,241", "method_info": {"method_name": "run_rsync_down", "method_params": "self,source,target", "method_startline": "229", "method_endline": "244"}}, "hunk_1": {"Ismethod": 1, "added_lines": "218", "deleted_lines": "218", "method_info": {"method_name": "run_rsync_up", "method_params": "self,source,target", "method_startline": "203", "method_endline": "218"}}, "hunk_2": {"Ismethod": 1, "added_lines": "220,221,222,223,224,225,226,227", "deleted_lines": "220,221", "method_info": {"method_name": "run_cp_up", "method_params": "self,source,target", "method_startline": "220", "method_endline": "227"}}, "hunk_3": {"Ismethod": 1, "added_lines": "246,247,248,249,250,251,252,253", "deleted_lines": null, "method_info": {"method_name": "run_cp_down", "method_params": "self,source,target", "method_startline": "246", "method_endline": "253"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\kubernetes\\kubectl-rsync.sh", "file_new_name": "python\\ray\\autoscaler\\kubernetes\\kubectl-rsync.sh", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "21,22", "deleted_lines": "21"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\tune\\BUILD", "file_new_name": "python\\ray\\tune\\BUILD", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "96,97,98,99,100,101,102,103", "deleted_lines": null}}}, "file_5": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "python\\ray\\tune\\integration\\kubernetes.py"}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\tune\\syncer.py", "file_new_name": "python\\ray\\tune\\syncer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "291,292,293", "deleted_lines": null, "method_info": {"method_name": "get_node_syncer", "method_params": "local_dir,remote_dir,sync_function", "method_startline": "276", "method_endline": "307"}}}}, "file_7": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "python\\ray\\tune\\tests\\test_integration_kubernetes.py"}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\requirements_tune.txt", "file_new_name": "python\\requirements_tune.txt", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "12", "deleted_lines": null}}}}}}