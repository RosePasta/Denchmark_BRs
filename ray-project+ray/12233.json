{"BR": {"BR_id": "12233", "BR_author": "tibogissel", "BRopenT": "2020-11-21T11:22:20Z", "BRcloseT": "2020-12-01T09:48:04Z", "BR_text": {"BRsummary": "[rllib] Weight sharing example does not seem to actually share weights", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Versions: Python 3.6  | tf2.1 | rllib1.0.0\n Weight sharing in tensorflow as implemented in rllib/examples/models/shared_weights_model.py creates two different sets of weights when it is supposed to reuse part of them.\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n <denchmark-code>import gym\n from ray.rllib.examples.models.shared_weights_model import SharedWeightsModel1, SharedWeightsModel2\n \n observation_space = gym.spaces.Box(low=-1, high=1, shape=(10,),)\n acition_space = gym.spaces.Discrete(3)\n \n m1 = SharedWeightsModel1(\n     observation_space=observation_space,\n     action_space=acition_space, \n     num_outputs=3,\n     model_config={},\n     name=\"m1\"\n )\n m2 = SharedWeightsModel2(\n     observation_space=observation_space,\n     action_space=acition_space, \n     num_outputs=3,\n     model_config={},\n     name=\"m2\"\n )\n \n v1 = m1.trainable_variables()\n v2 = m2.trainable_variables()\n for i in range(len(v1)):\n     print(v1[i].name,v2[i].name)\n </denchmark-code>\n \n Output\n <denchmark-code>fc1/kernel:0 fc1_1/kernel:0\n fc1/bias:0 fc1_1/bias:0\n fc_out/kernel:0 fc_out_1/kernel:0\n fc_out/bias:0 fc_out_1/bias:0\n value_out/kernel:0 value_out_1/kernel:0\n value_out/bias:0 value_out_1/bias:0\n </denchmark-code>\n \n Weight for fc_1 should eb the same but apparently they are not. It seems that tf.keras.layer is not affected by the scope and the solution could be to create the layer outside and use it in both classes. This script seems to work:\n <denchmark-code>import gym\n import numpy as np\n \n from ray.rllib.models.modelv2 import ModelV2\n from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n from ray.rllib.utils.annotations import override\n from ray.rllib.utils.framework import try_import_tf\n \n tf1, tf, tfv = try_import_tf()\n \n shared_last_layer = tf.keras.layers.Dense(units=64, activation=tf.nn.relu, name=\"fc1\")\n \n class SharedWeightsModel1(TFModelV2):\n     \"\"\"Example of weight sharing between two different TFModelV2s.\n     Here, we share the variables defined in the 'shared' variable scope\n     by entering it explicitly with tf1.AUTO_REUSE. This creates the\n     variables for the 'fc1' layer in a global scope called 'shared'\n     (outside of the Policy's normal variable scope).\n     \"\"\"\n \n     def __init__(self, observation_space, action_space, num_outputs,\n                  model_config, name):\n         super().__init__(observation_space, action_space, num_outputs,\n                          model_config, name)\n \n         inputs = tf.keras.layers.Input(observation_space.shape)\n         last_layer = shared_last_layer(inputs)\n         output = tf.keras.layers.Dense(\n             units=num_outputs, activation=None, name=\"fc_out\")(last_layer)\n         vf = tf.keras.layers.Dense(\n             units=1, activation=None, name=\"value_out\")(last_layer)\n         self.base_model = tf.keras.models.Model(inputs, [output, vf])\n         self.register_variables(self.base_model.variables)\n \n     @override(ModelV2)\n     def forward(self, input_dict, state, seq_lens):\n         out, self._value_out = self.base_model(input_dict[\"obs\"])\n         return out, []\n \n     @override(ModelV2)\n     def value_function(self):\n         return tf.reshape(self._value_out, [-1])\n \n \n class SharedWeightsModel2(TFModelV2):\n     \"\"\"The \"other\" TFModelV2 using the same shared space as the one above.\"\"\"\n \n     def __init__(self, observation_space, action_space, num_outputs,\n                  model_config, name):\n         super().__init__(observation_space, action_space, num_outputs,\n                          model_config, name)\n \n         inputs = tf.keras.layers.Input(observation_space.shape)\n \n         last_layer = shared_last_layer(inputs)\n         output = tf.keras.layers.Dense(\n             units=num_outputs, activation=None, name=\"fc_out\")(last_layer)\n         vf = tf.keras.layers.Dense(\n             units=1, activation=None, name=\"value_out\")(last_layer)\n         self.base_model = tf.keras.models.Model(inputs, [output, vf])\n         self.register_variables(self.base_model.variables)\n \n     @override(ModelV2)\n     def forward(self, input_dict, state, seq_lens):\n         out, self._value_out = self.base_model(input_dict[\"obs\"])\n         return out, []\n \n     @override(ModelV2)\n     def value_function(self):\n         return tf.reshape(self._value_out, [-1])\n     \n observation_space = gym.spaces.Box(low=-1, high=1, shape=(10,),)\n acition_space = gym.spaces.Discrete(3)\n \n m1 = SharedWeightsModel1(\n     observation_space=observation_space,\n     action_space=acition_space, \n     num_outputs=3,\n     model_config={},\n     name=\"m1\"\n )\n m2 = SharedWeightsModel2(\n     observation_space=observation_space,\n     action_space=acition_space, \n     num_outputs=3,\n     model_config={},\n     name=\"m2\"\n )\n \n v1 = m1.trainable_variables()\n v2 = m2.trainable_variables()\n for i in range(len(v1)):\n     print(v1[i].name,v2[i].name)\n </denchmark-code>\n \n Which output is:\n <denchmark-code>fc1/kernel:0 fc1/kernel:0\n fc1/bias:0 fc1/bias:0\n fc_out/kernel:0 fc_out_1/kernel:0\n fc_out/bias:0 fc_out_1/bias:0\n value_out/kernel:0 value_out_1/kernel:0\n value_out/bias:0 value_out_1/bias:0\n </denchmark-code>\n \n And we see that fc1 is well shared.\n \n [yes ] I have verified my script runs in a clean environment and reproduces the issue.\n [ yes] I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tibogissel", "commentT": "2020-11-24T01:19:23Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tibogissel", "commentT": "2020-11-25T11:39:36Z", "comment_text": "\n \t\tThanks for filing this issue <denchmark-link:https://github.com/tibogissel>@tibogissel</denchmark-link>\n  , and for providing a solution already! Will take a look now ...\n Yeah, I think this is also how we did this in torch (create a \"global\" layer outside and use that in the models).\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tibogissel", "commentT": "2020-11-25T12:24:38Z", "comment_text": "\n \t\tYeah, this is a typical tf1 vs tf2 problem. In tf1, we need the scope (global sharing will complain about different graphs being used for the shared layer). tf2 does not respect these scipes anymore, hence the non-sharing that you observed.\n Solution: I'll provide a third model for tf2 to make this work and then - in the example - disseminate between the different frameworks used.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "tibogissel", "commentT": "2020-11-25T12:26:11Z", "comment_text": "\n \t\tGreat! Thanks for taking the time to look at this!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "tibogissel", "commentT": "2020-11-25T12:27:47Z", "comment_text": "\n \t\tPR: <denchmark-link:https://github.com/ray-project/ray/pull/12399>#12399</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "tibogissel", "commentT": "2020-11-25T12:28:56Z", "comment_text": "\n \t\tNp, :)\n \t\t"}}}, "commit": {"commit_id": "841d93d366b1465959dba03512d64513c5ff568f", "commit_author": "Sven Mika", "commitT": "2020-11-25 11:27:19-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\BUILD", "file_new_name": "rllib\\BUILD", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1902", "deleted_lines": "1902"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "rllib\\examples\\models\\shared_weights_model.py", "file_new_name": "rllib\\examples\\models\\shared_weights_model.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "45,46,47", "deleted_lines": null, "method_info": {"method_name": "forward", "method_params": "self,input_dict,state,seq_lens", "method_startline": "45", "method_endline": "47"}}, "hunk_1": {"Ismethod": 1, "added_lines": "30,31", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,observation_space,action_space,num_outputs,model_config,name", "method_startline": "30", "method_endline": "31"}}, "hunk_2": {"Ismethod": 1, "added_lines": "50,51", "deleted_lines": null, "method_info": {"method_name": "value_function", "method_params": "self", "method_startline": "50", "method_endline": "51"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\examples\\multi_agent_cartpole.py", "file_new_name": "rllib\\examples\\multi_agent_cartpole.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "21,22,39,40,48,49,50,51,52,53,54,93", "deleted_lines": "21,38,46,47,86"}}}}}}