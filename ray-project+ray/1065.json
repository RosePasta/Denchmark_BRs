{"BR": {"BR_id": "1065", "BR_author": "ericl", "BRopenT": "2017-10-03T01:05:38Z", "BRcloseT": "2018-03-23T16:42:10Z", "BR_text": {"BRsummary": "GPU resources not released after killing actor", "BRdescription": "\n The following crashes with\n <denchmark-code>Exception: Could not find a node with enough GPUs or other resources to create this actor. The local scheduler information is [ {'ClientType': 'local_scheduler', 'Deleted': False, 'DBClientID': '31dc437d6df69857fea7a9eb6f04004421039e18', 'AuxAddress': '127.0.0.1:37853', 'NumCPUs': 32.0, 'NumGPUs': 1.0, 'LocalSchedulerSocketName': '/tmp/scheduler9534802'}].\n </denchmark-code>\n \n <denchmark-code>import ray\n import sys\n import time\n \n @ray.remote(num_gpus=1)\n class Actor(object):\n   def __init__(self):\n     pass\n \n ray.init(num_gpus=1)\n a = Actor.remote()\n a.__ray_terminate__.remote(a._ray_actor_id.id())\n \n time.sleep(5)\n a = Actor.remote()  # crashes with not enough gpus\n </denchmark-code>\n \n cc <denchmark-link:https://github.com/stephanie-wang>@stephanie-wang</denchmark-link>\n  <denchmark-link:https://github.com/robertnishihara>@robertnishihara</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ericl", "commentT": "2017-10-03T02:50:52Z", "comment_text": "\n \t\tThat's very surprising. I can reproduce it. However, this kind of thing should be covered by our tests.. I'll look into it.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ericl", "commentT": "2018-02-28T03:58:21Z", "comment_text": "\n \t\tIs this fixed? I still get this error when running the above.\n ray version:  0.3.1\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ericl", "commentT": "2018-02-28T04:01:29Z", "comment_text": "\n \t\tUpdate:  it will only work for me with time.sleep(20). It seems it takes about 15 seconds to disconnect. Is this expected?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ericl", "commentT": "2018-02-28T05:40:45Z", "comment_text": "\n \t\tThis should be fixed (and really shouldn't take 15 seconds).\n There are some bigger changes in the pipeline to handle actor creation more like the way we handle regular task placement, and when that happens the problem should definitely go away.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ericl", "commentT": "2018-02-28T18:11:37Z", "comment_text": "\n \t\tI had this error when I was tuning neural network hyperparameters using Ray Tune with Hyperband. I later switched to an older machine with only two GPUs, and it didn't give me error when running the above snippets (disconnected after a few seconds). But I still got error when running Ray Tune.  So in the end, only part of the intended hyper-param trials are actually tested.\n Any suggestions to debug this?\n Will these changes be included in the next version?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ericl", "commentT": "2018-02-28T19:52:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/haoyangz>@haoyangz</denchmark-link>\n  can you post the error and the minimal script you used?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ericl", "commentT": "2018-02-28T21:19:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n   Thanks. The following simple script produces the error for me:\n <denchmark-code>import torch\n import random\n from os.path import join\n \n from torch.autograd import Variable\n import ray\n from ray.tune import Trainable, TrainingResult, register_trainable, \\\n     run_experiments\n from ray.tune.hyperband import HyperBandScheduler\n \n class TwoLayerNet(torch.nn.Module):\n     def __init__(self, D_in, H, D_out):\n         super(TwoLayerNet, self).__init__()\n         self.linear1 = torch.nn.Linear(D_in, H)\n         self.linear2 = torch.nn.Linear(H, D_out)\n \n     def forward(self, x):\n         h_relu = self.linear1(x).clamp(min=0)\n         y_pred = self.linear2(h_relu)\n         return y_pred\n \n \n N, D_in, H, D_out = 64, 1000, 100, 10\n \n class MyTrainableClass(Trainable):\n     def _setup(self):\n         self.x = Variable(torch.randn(N, D_in).cuda())\n         self.y = Variable(torch.randn(N, D_out).cuda(), requires_grad=False)\n         self.model = TwoLayerNet(D_in, self.config['H'], D_out).cuda()\n         self.criterion = torch.nn.MSELoss(size_average=False)\n         self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-4)\n \n     def _train(self):\n         y_pred = self.model(self.x)\n         loss = self.criterion(y_pred, self.y)\n         self.optimizer.zero_grad()\n         loss.backward()\n         self.optimizer.step()\n         return TrainingResult(mean_loss=loss.data[0], timesteps_this_iter=1)\n \n     def _save(self, checkpoint_dir):\n         path = join(checkpoint_dir, \"checkpoint.pt\")\n         torch.save(self.model.state_dict(), path)\n         return path\n \n     def _restore(self, checkpoint_path):\n         self.model = TwoLayerNet(D_in, self.config['H'], D_out).cuda()\n         self.model.load_state_dict(torch.load(checkpoint_path))\n \n \n register_trainable(\"my_class\", MyTrainableClass)\n ray.init()\n hyperband = HyperBandScheduler(\n     time_attr=\"timesteps_total\", reward_attr=\"episode_reward_mean\",\n     max_t=100)\n \n run_experiments({\n     \"hyperband_test\": {\n         \"run\": \"my_class\",\n         \"stop\": {\"training_iteration\": 99999},\n         \"repeat\": 20,\n         \"resources\": {\"cpu\": 0, \"gpu\": 1},\n         \"config\": {\n             \"H\": lambda spec: 10 + int(90 * random.random()),\n         },\n     }\n }, scheduler=hyperband)\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ericl", "commentT": "2018-03-01T23:08:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  I checked that with the default FIFO scheduler,  there is no error.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ericl", "commentT": "2018-03-02T17:18:08Z", "comment_text": "\n \t\tWell with enough trials, it seems FIFO will also have the error but much less frequently.\n Another similar error observed under FIFO (maybe also under hyperband, not sure exactly):\n <denchmark-code>Traceback (most recent call last):\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/ray-0.3.1-py2.7-linux-x86_64.egg/ray/worker.py\", line 762, in _process_task\n     *arguments)\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/ray-0.3.1-py2.7-linux-x86_64.egg/ray/actor.py\", line 209, in actor_method_executor\n     method_returns = method(actor, *args)\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/ray-0.3.1-py2.7-linux-x86_64.egg/ray/tune/trainable.py\", line 89, in __init__\n     self._setup()\n   File \"/cluster/zeng/code/research/ray_tune_wrapper/mymodel.py\", line 51, in _setup\n     self.net = Net(self.config).cuda()\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 216, in cuda\n     return self._apply(lambda t: t.cuda(device))\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 146, in _apply\n     module._apply(fn)\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 152, in _apply\n     param.data = fn(param.data)\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 216, in <lambda>\n     return self._apply(lambda t: t.cuda(device))\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/torch/_utils.py\", line 69, in _cuda\n     return new_type(self.size()).copy_(self, async)\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 358, in _lazy_new\n     _lazy_init()\n   File \"/cluster/zeng/code/research/software/miniconda/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 121, in _lazy_init\n     torch._C._cuda_init()\n RuntimeError: cuda runtime error (38) : no CUDA-capable device is detected at /opt/conda/conda-bld/pytorch_1512378422383/work/torch/lib/THC/THCGeneral.c:70\n </denchmark-code>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "ericl", "commentT": "2018-03-02T18:00:09Z", "comment_text": "\n \t\tOK I'll try to reproduce this later today\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "ericl", "commentT": "2018-03-04T09:37:21Z", "comment_text": "\n \t\tOK unfortunately, I was unable to reproduce this error. Can you post the entire stdout for running the above script? I tried running the given script on a p3.2xlarge on EC2, using pytorch_p36 and pytorch_p27 and using a wheel built off of master.\n Here are the wheels that I used:\n <denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.3.1-cp36-cp36m-manylinux1_x86_64.whl>python3.6</denchmark-link>\n \n <denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.3.1-cp27-cp27mu-manylinux1_x86_64.whl>python2.7</denchmark-link>\n \n Perhaps try these out?\n PS, the above script has a typo - you'll need to change the HyperBand instantiation to:\n <denchmark-code>hyperband = HyperBandScheduler(\n     time_attr=\"timesteps_total\", reward_attr=\"mean_loss\",\n     max_t=100)\n </denchmark-code>\n \n And perhaps change the return of the training result, since hyperband expects monotonically increasing metrics:\n <denchmark-code>return TrainingResult(mean_loss=-loss.data[0], timesteps_this_iter=1)  #note that it is now negative\n </denchmark-code>\n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "ericl", "commentT": "2018-03-04T14:49:58Z", "comment_text": "\n \t\tThanks for looking into this!\n Yes I found these two typos after I posted it, but as expected unfortunately they are NOT relevant to this error.\n I tried uninstalling Ray and building from your wheels but the error is still there.\n The full stdout + stderr is <denchmark-link:https://s3.amazonaws.com/zengtest/log>here</denchmark-link>\n .\n And I am using CUDA9.0 + cudnn 7 with eight Nvidia 1080 Ti.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "ericl", "commentT": "2018-03-04T16:24:30Z", "comment_text": "\n \t\tOk after experimenting on AWS p3.2xlarge, it seems that it works fine for python 3.6 but NOT for python 2.7.\n What I did to setup for python 2.7:\n <denchmark-code>conda create -n py27 python=2.7\n source activate py27\n wget https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.3.1-cp27-cp27mu-manylinux1_x86_64.whl\n python install ray-0.3.1-cp27-cp27mu-manylinux1_x86_64.whl\n conda install pytorch torchvision cuda90 -c pytorch\n </denchmark-code>\n \n <denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n   Could you help look more into this? Thanks!\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "ericl", "commentT": "2018-03-04T20:42:35Z", "comment_text": "\n \t\tOK I was able to reproduce this now. Thanks! Will investigate this...\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "ericl", "commentT": "2018-03-05T21:22:54Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/haoyangz>@haoyangz</denchmark-link>\n , <denchmark-link:https://github.com/ray-project/ray/pull/1651>#1651</denchmark-link>\n  should fix your issue (it works on the p3.2xlarge test case you posted).\n Do you mind trying it out? You'd have to check out the branch and build from source.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "ericl", "commentT": "2018-03-06T01:25:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n   Thanks! I just tried on our cluster and it seems to still produce errors. Compiling from scratch gives error on the AMI I am using:\n <denchmark-code>+ cp /home/ubuntu/ray/thirdparty/pkg/arrow/cpp/build/cpp-install/bin/plasma_store /home/ubuntu/ray/python/ray/core/src/plasma/\n error: [Errno 2] No such file or directory: './ray/pyarrow_files/pyarrow'\n </denchmark-code>\n \n Could you give me the AMI that works for you and I will try on that.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "ericl", "commentT": "2018-03-06T04:01:26Z", "comment_text": "\n \t\tI'm just using the DL AMI 5.0\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "ericl", "commentT": "2018-03-06T04:06:49Z", "comment_text": "\n \t\tCould you post your output from running on cluster? Just to verify that the retries are in place.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "ericl", "commentT": "2018-03-06T04:22:16Z", "comment_text": "\n \t\tBuilding a wheel from source following instructions <denchmark-link:https://github.com/ray-project/ray/blob/master/python/README-building-wheels.md>here</denchmark-link>\n  and installing it resolves the compilation problem on p3.2xlarge, and it indeed works without any error.\n <denchmark-link:https://s3.amazonaws.com/zengtest/log10>Here</denchmark-link>\n  is the log fro running on our cluster. Thanks!\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "ericl", "commentT": "2018-03-06T08:28:45Z", "comment_text": "\n \t\tLooks like what is happening is that this error is thrown, probably because some GPU resources are being released asynchronously:\n <denchmark-code>RuntimeError: cuda runtime error (38) : no CUDA-capable device is detected at /opt/conda/conda-bld/pytorch_1512378422383/work/torch/lib/THC/THCGeneral.c:70\n </denchmark-code>\n \n and the handling for this is not very well isolated, causing a mismatch of resource bookkeeping and then subsequently the local scheduler issue on a couple experiments. We should probably improve the isolation, but it also seems like only a couple trials fail and the experiment actually continues.\n For debugging purposes, can you insert the following lines into your script at the beginning of def _setup(self):? It would be great to get the logs from this.\n <denchmark-code>print(\"Ray detects {}\".format(ray.utils.get_cuda_visible_devices()))\n print(\"Torch detects {}\".format(torch.cuda.device_count()))\n print(\"Torch ready? {}.format(torch.cuda.is_available()))\n </denchmark-code>\n \n BTW, as a quick workaround, you could try the following: in _setup(self), sleep until Torch detects an available GPU.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "ericl", "commentT": "2018-03-06T21:00:57Z", "comment_text": "\n \t\tSo I add the following lines at the begining of def _setup():\n <denchmark-code>        print(\"Ray detects {}\".format(ray.utils.get_cuda_visible_devices()))\n         print(\"Torch detects {}\".format(torch.cuda.device_count()))\n         print(\"Torch ready? {}\".format(torch.cuda.is_available()))\n \n         tried_time = 0\n         try_interval = 1\n         while not torch.cuda.is_available():\n             print(\"Torch is not ready, wait for {} s. Have already waited for   {} s\".format(try_interval, tried_time))\n             time.sleep(try_interval)\n             tried_time += try_interval\n \n         print(\"Ready now! Waited for {} s\".format(tried_time))\n </denchmark-code>\n \n And here is the <denchmark-link:https://s3.amazonaws.com/zengtest/error>stderr</denchmark-link>\n  and <denchmark-link:https://s3.amazonaws.com/zengtest/output>stdout</denchmark-link>\n . It seems that the ones that failed to get the resource initially never got it later, no matter how long it waited (the ready msg \"Ready now\" never shows up in stdout) . From stderr, it seems they are killed/ignored because Ray waited for too long? The whole program hung forever and I had to interrupt with ctrl+c.\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "ericl", "commentT": "2018-03-06T23:06:13Z", "comment_text": "\n \t\tFrom the stdout, only  - my_class_17_H=93 was never able to run. It looks like the rest of the trials ran successfully?\n Is that right? If so, it's weird that the program hung forever.\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "ericl", "commentT": "2018-03-06T23:13:01Z", "comment_text": "\n \t\tStdout errors aside, this line is very weird -\n Attempting to create an actor but there aren't enough available GPUs. We'll start the worker anyway without any GPUs, but this is incorrect behavior.\n Do you know where this is output?\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "ericl", "commentT": "2018-03-06T23:29:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n   Per you first comment, yes that seems to be the only trial that had an error. Overall the process is very random and sometimes all the trials are fine sometimes there are a few errors.\n Per your second question, I don't know but searching in this repository I found <denchmark-link:https://github.com/ray-project/ray/blob/0fcceef772a3126e027b0817988022ebda1a506e/src/local_scheduler/local_scheduler.cc#L964>this</denchmark-link>\n . Any idea what might be going on?\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "ericl", "commentT": "2018-03-07T02:03:57Z", "comment_text": "\n \t\tLooks like there's inconsistency between the local scheduler and the Redis state. <denchmark-link:https://github.com/robertnishihara>@robertnishihara</denchmark-link>\n  is currently working on a fix (and we'll keep you updated on a PR).\n As a temporary fix (if urgent), here are two steps that will be beneficial:\n \n \n based off of the aforementioned PR, you can modify this line to be for sec in range(1, 5), which should decrease chance of failure significantly.\n \n \n Consider putting this code in the beginning your train function - this will kill trials that were falsely started.\n \n \n <denchmark-code>cuda_devices = ray.utils.get_cuda_visible_devices()\n if not cuda_devices:\n     print(\"Ray detects {}\".format(cuda_devices))\n     raise Exception(\"No GPU devices allocated by Ray!\")\n </denchmark-code>\n \n Alternatively, if the GPU is not necessary for every, you could just if-else the cuda call based off of torch.cuda.is_available(). As you'd expect, certain trials would be slower, but you'll still get some of these results.\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "ericl", "commentT": "2018-03-20T03:07:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/haoyangz>@haoyangz</denchmark-link>\n  now that <denchmark-link:https://github.com/ray-project/ray/pull/1668>#1668</denchmark-link>\n  is in; do you still run into errors like this for HyperBand?\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "ericl", "commentT": "2018-03-23T15:53:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  No errors now. Thanks!\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "ericl", "commentT": "2018-03-23T16:42:10Z", "comment_text": "\n \t\tAwesome! Will close this issue for now.\n \t\t"}}}, "commit": {"commit_id": "4669c59fa8d0f071bbf5c2e31b3dc8018dd5ca44", "commit_author": "Robert Nishihara", "commitT": "2017-10-06 17:58:19-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\ray\\actor.py", "file_new_name": "python\\ray\\actor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "226,227,228,229,230,231,232,233", "deleted_lines": "217", "method_info": {"method_name": "make_actor", "method_params": "cls,num_cpus,num_gpus,checkpoint_interval", "method_startline": "216", "method_endline": "429"}}, "hunk_1": {"Ismethod": 1, "added_lines": "226,227,228,229,230,231,232,233", "deleted_lines": null, "method_info": {"method_name": "make_actor.__ray_terminate__", "method_params": "self,actor_id", "method_startline": "220", "method_endline": "238"}}, "hunk_2": {"Ismethod": 1, "added_lines": "122,123,124,138,139,140,141,142,143", "deleted_lines": "115", "method_info": {"method_name": "fetch_and_register_actor", "method_params": "actor_class_key,worker", "method_startline": "70", "method_endline": "143"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\utils.py", "file_new_name": "python\\ray\\utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184", "deleted_lines": null, "method_info": {"method_name": "release_gpus_in_use", "method_params": "driver_id,local_scheduler_id,gpu_ids,redis_client", "method_startline": "139", "method_endline": "184"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "test\\actor_test.py", "file_new_name": "test\\actor_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "326,327", "deleted_lines": null, "method_info": {"method_name": "testActorDeletionWithGPUs.getpid", "method_params": "self", "method_startline": "326", "method_endline": "327"}}, "hunk_1": {"Ismethod": 1, "added_lines": "318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343", "deleted_lines": null, "method_info": {"method_name": "testActorDeletionWithGPUs", "method_params": "self", "method_startline": "318", "method_endline": "343"}}}}}}}