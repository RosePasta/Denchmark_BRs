{"BR": {"BR_id": "1149", "BR_author": "robertnishihara", "BRopenT": "2017-10-20T22:10:43Z", "BRcloseT": "2020-03-05T23:07:21Z", "BR_text": {"BRsummary": "Hanging when using distributed actor handles.", "BRdescription": "\n When using distributed actor handles, I sometimes see calls hang when there are lots of tasks using the same actor handle. For example, the following script seems to hang.\n import ray\n import time\n \n ray.init()\n \n @ray.remote\n class StatusActor(object):\n     def __init__(self):\n         self.counter = 0\n \n     def inc_status(self):\n         self.counter += 1\n \n     def get_status(self):\n         return self.counter\n \n @ray.remote\n def run_incrementer(status_actor):\n     while True:\n         status_actor.inc_status.remote()\n         time.sleep(1)\n \n status_actor = StatusActor.remote()\n [run_incrementer.remote(status_actor) for _ in range(100)]\n \n time.sleep(2)\n \n ray.get(status_actor.get_status.remote())  # This line usually hangs.\n What could be happening is that the status_actor.get_status.remote() is getting starved because it is not prioritized and new tasks continually arrive at the actor which are prioritized.\n However, if it were as simple as that, I would expect it to eventually return when I replace the while True: line with for i in range(100):. But doing so, it still seems to hang. Inspecting the task table (with a different driver), I see\n tt = ray.global_state.task_table()\n states = [v['State'] for k, v in tt.items()]\n [(s, states.count(s)) for s in set(states)]  # [(16, 32), (8, 18), (4, 3582)]\n There should actually be 10000 tasks, so something is going wrong.\n EDIT: Ok it actually returned after a pretty long time (5 or so minutes) and printed 16. Then subsequent calls to ray.get(status_actor.get_status.remote()) return immediately and print 10000.\n <denchmark-link:https://github.com/stephanie-wang>@stephanie-wang</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "robertnishihara", "commentT": "2017-11-16T20:36:58Z", "comment_text": "\n \t\tThe issue seems to be that the tasks that call methods on the actor are getting scheduled before the actor process, so the order ends up being:\n \n Wait for all tasks to finish submitting tasks.\n Schedule the actor.\n Schedule the first task on the actor, which is usually the get_status call from the driver.\n \n You can see this easily if you call ray.get on the status_actor before submitting the tasks to make sure that the actor is scheduled before passing the handle. The script should then exit almost immediately.\n I'm not sure about the best way to handle this. Perhaps we should introduce a dependency on the actor's first task (the __init__ method) for forked handles?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "robertnishihara", "commentT": "2017-11-17T06:01:10Z", "comment_text": "\n \t\tOh that's very interesting. Hm, are you suggesting basically making the invocations of run_incrementer dependent on the actors __init__ task?\n When an actor handle A is passed to a task T, there seem to be a couple of options:\n \n Don't introduce any dependencies (current behavior).\n Make T depend on the actor creation event.\n Make T depend on the actor's __init__ task.\n Make T depend on the most recent task issued using the handle A.\n \n Options 2, 3, and 4 all solve the above problem.\n I think either 1, 2, or 4 make sense (maybe 2 makes the most sense). However 2 will not be possible until we start treating actor creation as a task.\n We already have 4 in some sense because the first method invoked on the actor from within T will depend on the last task issued using A.\n This could be a use case for \"execution edges\" outside of actor methods, right?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "robertnishihara", "commentT": "2019-12-07T01:04:33Z", "comment_text": "\n \t\tI think I'm seeing something like this kind of behavior in my code. I have an actor that stores a buffer of training samples, a couple of (remote actor) workers which are pushing samples to the buffer, and the main script pulls samples out of the buffer. After a while of working properly, and only sometimes, the workers (one at a time - ie they don't stop working at the same time) will hang seemingly on calling buffer.push_samples.remote(). The main training script continues to run fine, pulling samples from the buffer.\n Any recommendations for mitigations here?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "robertnishihara", "commentT": "2019-12-07T18:19:15Z", "comment_text": "\n \t\tI was able to reproduce this with a simple example, which I posted in <denchmark-link:https://github.com/ray-project/ray/issues/6392>#6392</denchmark-link>\n .\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "robertnishihara", "commentT": "2020-03-05T23:07:21Z", "comment_text": "\n \t\tStale\n \t\t"}}}, "commit": {"commit_id": "fc9f03cd96908bd2ab21847991807f5352e820f6", "commit_author": "Simon Mo", "commitT": "2019-09-13 12:35:44-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\tests\\test_actor.py", "file_new_name": "python\\ray\\tests\\test_actor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1787,1788", "method_info": {"method_name": "test_fork", "method_params": "setup_queue_actor", "method_startline": "1785", "method_endline": "1802"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1779,1780,1781,1782,1783", "deleted_lines": "1779", "method_info": {"method_name": "setup_queue_actor", "method_params": "", "method_startline": "1765", "method_endline": "1786"}}}}}}}