{"BR": {"BR_id": "10068", "BR_author": "mvindiola1", "BRopenT": "2020-08-12T16:11:13Z", "BRcloseT": "2020-11-04T08:33:56Z", "BR_text": {"BRsummary": "[rllib] MARWIL does not convert SampleBatch state_in_* values to torch tensors", "BRdescription": "\n Ray [0.6.8] and Ray nightly wheel: <denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.wh>https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.wh</denchmark-link>\n \n When training marwil with an LSTM layer using pytorch I get the following error:\n \n File \".../lib/python3.6/site-packages/ray/rllib/models/torch/recurrent_net.py\", line 155, in forward_rnn:\n [torch.unsqueeze(state[0], 0),\n TypeError: unsqueeze(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\n \n I do not get any errors in TF.\n This is the script I used to test and generate the attached error log.\n <denchmark-link:https://github.com/ray-project/ray/files/5064082/error.txt>torch_error.txt</denchmark-link>\n \n <denchmark-code>import ray\n import ray.rllib.agents.marwil as marwil\n from ray import tune\n \n ray.init(redis_port=6379)\n config = marwil.DEFAULT_CONFIG.copy()\n config[\"env\"] = \"CartPole-v0\"\n config[\"num_workers\"] = 1\n config[\"model\"][\"use_lstm\"] = True\n \n for framework in [\"tf\", \"torch\"]:\n     print(\"Testing MARWIL with framework: {}\".format(framework))\n     input_file = \"/tmp/marwil/cartpole_{}\".format(framework)\n     config[\"framework\"] = framework\n \n     # Create input file\n     config[\"input\"] = \"sampler\"\n     config[\"output\"] = input_file\n     config[\"output_max_file_size\"] = 50000000\n     trainer = marwil.MARWILTrainer(config=config, env=\"CartPole-v0\")\n     tune.run(\"MARWIL\", stop={\"training_iteration\": 10},\n              config=config, local_dir=\"/tmp/marwil/logs/create_{}\".format(framework), verbose=False)\n \n     # Train with input file\n     config[\"input\"] = input_file\n     config[\"output\"] = None\n     trainer = marwil.MARWILTrainer(config=config, env=\"CartPole-v0\")\n     tune.run(\"MARWIL\", stop={\"training_iteration\": 10},\n              config=config, local_dir=\"/tmp/marwil/logs/train_{}\".format(framework), verbose=False)\n ray.shutdown()\n \n \n \n - [X ] I have verified my script runs in a clean environment and reproduces the issue.\n - [X ] I have verified the issue also occurs with the [latest wheels](https://docs.ray.io/en/latest/installation.html).\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mvindiola1", "commentT": "2020-08-14T20:45:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mvindiola1", "commentT": "2020-10-28T21:16:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n  I just re-tested this with the nightly wheel. The result is the same as before. It works with tf but not with pytorch. The error seems to be in evaluation sampling. If I add this to the config  then pytorch trains without issue. I tried it with  and that failed with a different type of error.\n I am willing to hunt the pytorch issue down and prepare a PR if I am given a nudge in the direction to look.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "mvindiola1", "commentT": "2020-10-29T14:10:59Z", "comment_text": "\n \t\tThis change in torch_policy::compute_log_likelihoods fixed the issue for me:\n @@ -306,6 +306,10 @@ class TorchPolicy(Policy):\n              if prev_reward_batch is not None:\n                  input_dict[SampleBatch.PREV_REWARDS] = prev_reward_batch\n              seq_lens = torch.ones(len(obs_batch), dtype=torch.int32)\n +            state_batches = [\n +                convert_to_torch_tensor(s, self.device)\n +                for s in (state_batches or [])\n +            ]\n \n              # Exploration hook before each forward pass.\n              self.exploration.before_compute_actions(explore=False)\n It is the same thing you did in torch_policy::compute_actions here:\n \n \n \n ray/rllib/policy/torch_policy.py\n \n \n         Lines 163 to 170\n       in\n       91fa7e0\n \n \n \n \n \n \n  state_batches = [ \n \n \n \n  convert_to_torch_tensor(s, self.device) \n \n \n \n  for s in (state_batches or []) \n \n \n \n  ] \n \n \n \n  actions, state_out, extra_fetches, logp = \\ \n \n \n \n  self._compute_action_helper( \n \n \n \n  input_dict, state_batches, seq_lens, explore, timestep) \n \n \n \n  \n \n \n \n \n \n \t\t"}}}, "commit": {"commit_id": "4518fe790fd4d5a25ed44080b65ac23ebb1eadcc", "commit_author": "mvindiola1", "commitT": "2020-11-04 09:33:56+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\policy\\torch_policy.py", "file_new_name": "rllib\\policy\\torch_policy.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "309,310,311,312", "deleted_lines": null}}}}}}