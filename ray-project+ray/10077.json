{"BR": {"BR_id": "10077", "BR_author": "richardrl", "BRopenT": "2020-08-13T00:18:05Z", "BRcloseT": "2020-08-14T02:38:33Z", "BR_text": {"BRsummary": "Docker + AWS fails with no such container error", "BRdescription": "\n Latest dev version of ray\n <denchmark-code>(vanilla_ray_venv) richard@richard-desktop:~/improbable/vanillas/ray/python/ray/autoscaler/aws$ ray up aws_gpu_dummy.yaml \n 2020-08-12 20:12:39,383\tINFO config.py:268 -- _configure_iam_role: Role not specified for head node, using arn:aws:iam::179622923911:instance-profile/ray-autoscaler-v1\n 2020-08-12 20:12:39,612\tINFO config.py:346 -- _configure_key_pair: KeyName not specified for nodes, using ray-autoscaler_us-east-1\n 2020-08-12 20:12:39,745\tINFO config.py:407 -- _configure_subnet: SubnetIds not specified for head node, using [('subnet-f737f791', 'us-east-1a')]\n 2020-08-12 20:12:39,746\tINFO config.py:417 -- _configure_subnet: SubnetId not specified for workers, using [('subnet-f737f791', 'us-east-1a')]\n 2020-08-12 20:12:40,358\tINFO config.py:590 -- _create_security_group: Created new security group ray-autoscaler-richard_cluster_gpu_dummy (sg-0061ca6aff182c1bf)\n 2020-08-12 20:12:40,739\tINFO config.py:444 -- _configure_security_group: SecurityGroupIds not specified for head node, using ray-autoscaler-richard_cluster_gpu_dummy (sg-0061ca6aff182c1bf)\n 2020-08-12 20:12:40,739\tINFO config.py:454 -- _configure_security_group: SecurityGroupIds not specified for workers, using ray-autoscaler-richard_cluster_gpu_dummy (sg-0061ca6aff182c1bf)\n This will create a new cluster [y/N]: y\n 2020-08-12 20:12:42,619\tINFO commands.py:531 -- get_or_create_head_node: Launching new head node...\n 2020-08-12 20:12:42,620\tINFO node_provider.py:326 -- NodeProvider: calling create_instances with subnet-f737f791 (count=1).\n 2020-08-12 20:12:44,032\tINFO node_provider.py:354 -- NodeProvider: Created instance [id=i-0729c7a86355d5ff8, name=pending, info=pending]\n 2020-08-12 20:12:44,223\tINFO commands.py:570 -- get_or_create_head_node: Updating files on head node...\n 2020-08-12 20:12:44,320\tINFO command_runner.py:331 -- NodeUpdater: i-0729c7a86355d5ff8: Waiting for IP...\n 2020-08-12 20:12:54,409\tINFO command_runner.py:331 -- NodeUpdater: i-0729c7a86355d5ff8: Waiting for IP...\n 2020-08-12 20:12:54,534\tINFO log_timer.py:27 -- NodeUpdater: i-0729c7a86355d5ff8: Got IP  [LogTimer=10310ms]\n 2020-08-12 20:12:54,534\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && command -v docker'\n Warning: Permanently added '3.226.253.119' (ECDSA) to the list of known hosts.\n /usr/bin/docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:04,587\tINFO updater.py:71 -- NodeUpdater: i-0729c7a86355d5ff8: Updating to 6b5fc8ee8c5dcdf3cfabe0bf90ba4e844f65a7c9\n 2020-08-12 20:14:04,587\tINFO updater.py:180 -- NodeUpdater: i-0729c7a86355d5ff8: Waiting for remote shell...\n 2020-08-12 20:14:04,587\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n 2020-08-12 20:14:04,950\tINFO log_timer.py:27 -- AWSNodeProvider: Set tag ray-node-status=waiting-for-ssh on ['i-0729c7a86355d5ff8']  [LogTimer=361ms]\n                              Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:21,222\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:26,417\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:31,610\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:36,798\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:41,986\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:47,170\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:52,358\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:14:57,554\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:15:02,750\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:15:07,938\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:15:13,126\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:15:18,307\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:15:23,494\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:19:01,502\tINFO command_runner.py:468 -- NodeUpdater: i-0729c7a86355d5ff8: Running ssh -tt -i /home/richard/.ssh/ray-autoscaler_us-east-1.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_6ae199a93c/cfde1c79f1/%C -o ControlPersist=10s -o ConnectTimeout=120s ubuntu@3.226.253.119 bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && docker exec -it  pytorch_docker /bin/bash -c '\"'\"'bash --login -c -i '\"'\"'\"'\"'\"'\"'\"'\"'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && uptime'\"'\"'\"'\"'\"'\"'\"'\"''\"'\"' '\n Error: No such container: pytorch_docker\n Shared connection to 3.226.253.119 closed.\n 2020-08-12 20:19:06,689\tINFO log_timer.py:27 -- NodeUpdater: i-0729c7a86355d5ff8: Got remote shell  [LogTimer=302102ms]\n 2020-08-12 20:19:06,690\tINFO log_timer.py:27 -- NodeUpdater: i-0729c7a86355d5ff8: Applied config 6b5fc8ee8c5dcdf3cfabe0bf90ba4e844f65a7c9  [LogTimer=302103ms]\n 2020-08-12 20:19:06,690\tERROR updater.py:88 -- NodeUpdater: i-0729c7a86355d5ff8: Error executing: Unable to connect to node\n \n Exception in thread Thread-2:\n Traceback (most recent call last):\n   File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n     self.run()\n   File \"/home/richard/improbable/vanillas/ray/python/ray/autoscaler/updater.py\", line 76, in run\n     self.do_update()\n   File \"/home/richard/improbable/vanillas/ray/python/ray/autoscaler/updater.py\", line 232, in do_update\n     self.wait_ready(deadline)\n   File \"/home/richard/improbable/vanillas/ray/python/ray/autoscaler/updater.py\", line 224, in wait_ready\n     assert False, \"Unable to connect to node\"\n AssertionError: Unable to connect to node\n \n 2020-08-12 20:19:06,962\tERROR commands.py:650 -- get_or_create_head_node: Updating 3.226.253.119 failed\n 2020-08-12 20:19:07,002\tINFO log_timer.py:27 -- AWSNodeProvider: Set tag ray-node-status=update-failed on ['i-0729c7a86355d5ff8']  [LogTimer=312ms]\n \n </denchmark-code>\n \n YAML file for repo attached.\n <denchmark-code># An unique identifier for the head node and workers of this cluster.\n cluster_name: richard_cluster_gpu_dummy\n \n # The minimum number of workers nodes to launch in addition to the head\n # node. This number should be >= 0.\n min_workers: 1\n \n # The maximum number of workers nodes to launch in addition to the head\n # node. This takes precedence over min_workers.\n max_workers: 5\n \n # The initial number of worker nodes to launch in addition to the head\n # node. When the cluster is first brought up (or when it is refreshed with a\n # subsequent `ray up`) this number of nodes will be started.\n initial_workers: 1\n \n # Whether or not to autoscale aggressively. If this is enabled, if at any point\n #   we would start more workers, we start at least enough to bring us to\n #   initial_workers.\n autoscaling_mode: default\n \n # This executes all commands on all nodes in the docker efcontainer,\n # and opens all the necessary ports to support the Ray cluster.\n # Empty string means disabled.\n docker:\n   image: \"pytorch/pytorch:latest\" # e.g., tensorflow/tensorflow:1.5.0-py3\n   container_name: \"pytorch_docker\" # e.g. ray_docker\n   # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image\n   # if no cached version is present.\n   pull_before_run: True\n   run_options: []\n #    - $([ -d /proc/driver ] && echo -n --runtime-nvidia) # Use the nvidia runtime only if nvidia gpu's are installed\n   worker_run_options:\n     - --runtime=nvidia  # Extra options to pass into \"docker run\"\n \n   # Example of running a GPU head with CPU workers\n   # head_image: \"tensorflow/tensorflow:1.13.1-py3\"\n   # head_run_options:\n   #     - --runtime=nvidia\n \n   # worker_image: \"ubuntu:18.04\"\n   # worker_run_options: []\n \n # The autoscaler will scale up the cluster to this target fraction of resource\n # usage. For example, if a cluster of 10 nodes is 100% busy and\n # target_utilization is 0.8, it would resize the cluster to 13. This fraction\n # can be decreased to increase the aggressiveness of upscaling.\n # This value must be less than 1.0 for scaling to happen.\n target_utilization_fraction: 0.8\n \n # If a node is idle for this many minutes, it will be removed.\n idle_timeout_minutes: 5\n \n # Cloud-provider specific configuration.\n provider:\n   type: aws\n   region: us-east-1\n   # Availability zone(s), comma-separated, that nodes may be launched in.\n   # Nodes are currently spread between zones by a round-robin approach,\n   # however this implementation detail should not be relied upon.\n   availability_zone: us-east-1a, us-east-1b\n   cache_stopped_nodes: False\n \n # How Ray will authenticate with newly launched nodes.\n auth:\n   ssh_user: ubuntu\n # By default Ray creates a new private keypair, but you can also use your own.\n # If you do so, make sure to also set \"KeyName\" in the head and worker node\n # configurations below.\n #    ssh_private_key: /path/to/your/key.pem\n \n # Provider-specific config for the head node, e.g. instance type. By default\n # Ray will auto-configure unspecified fields such as SubnetId and KeyName.\n # For more documentation on available fields, see:\n # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances\n head_node:\n   InstanceType: c4.2xlarge\n   ImageId: ami-043f9aeaf108ebc37 # Deep Learning AMI (Ubuntu) Version 24.3\n #   You can provision additional disk space with a conf as follows\n   BlockDeviceMappings:\n     - DeviceName: /dev/sda1\n       Ebs:\n         VolumeSize: 100\n \n   # Additional options in the boto docs.\n \n # Provider-specific config for worker nodes, e.g. instance type. By default\n # Ray will auto-configure unspecified fields such as SubnetId and KeyName.\n # For more documentation on available fields, see:\n # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances\n worker_nodes:\n   InstanceType: p3.2xlarge\n   ImageId: ami-043f9aeaf108ebc37 # Deep Learning AMI (Ubuntu) Version 24.3\n \n   # Run workers on spot by default. Comment this out to use on-demand.\n   InstanceMarketOptions:\n     MarketType: spot\n     # Additional options can be found in the boto docs, e.g.\n     #   SpotOptions:\n     #       MaxPrice: MAX_HOURLY_PRICE\n \n   # Additional options in the boto docs.\n \n # Files or directories to copy to the head and worker nodes. The format is a\n # dictionary from REMOTE_PATH: LOCAL_PATH, e.g.\n file_mounts: {\n \n }\n \n \n \n # List of commands that will be run before `setup_commands`. If docker is\n # enabled, these commands will run outside the container and before docker\n # is setup.\n initialization_commands: []\n \n # List of shell commands to run to set up nodes.\n setup_commands:\n   # Note: if you're developing Ray, you probably want to create an AMI that\n   # has your Ray repo pre-cloned. Then, you can replace the pip installs\n   # below with a git checkout <your_sha> (and possibly a recompile).\n   - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl\n #  - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl\n   # Consider uncommenting these if you also want to run apt-get commands during setup\n   # - sudo pkill -9 apt-get || true\n   # - sudo pkill -9 dpkg || true\n   # - sudo dpkg --configure -a\n \n # Custom commands that will be run on the head node after common setup.\n head_setup_commands:\n   - pip install boto3  # 1.4.8 adds InstanceMarketOptions\n \n # Custom commands that will be run on worker nodes after common setup.\n worker_setup_commands:\n   - pip install boto3  # 1.4.8 adds InstanceMarketOptions\n \n # Command to start ray on the head node. You don't need to change this.\n head_start_ray_commands:\n   - ray stop\n   - ulimit -n 65536; ray start --num-cpus=0 --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml\n \n # Command to start ray on worker nodes. You don't need to change this.\n worker_start_ray_commands:\n   - ray stop\n   - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "richardrl", "commentT": "2020-08-14T02:38:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardrl>@richardrl</denchmark-link>\n  It looks like the issue is resolved, so I will close it. Please reopen it if I am wrong :)!\n \t\t"}}}, "commit": {"commit_id": "a252aa29da9feb2faec62a59c9cac3a1c2b72e34", "commit_author": "Ian Rodney", "commitT": "2020-08-13 20:35:06-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\autoscaler\\command_runner.py", "file_new_name": "python\\ray\\autoscaler\\command_runner.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "559", "deleted_lines": null, "method_info": {"method_name": "run_rsync_up", "method_params": "self,source,target", "method_startline": "558", "method_endline": "569"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\ray\\autoscaler\\updater.py", "file_new_name": "python\\ray\\autoscaler\\updater.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "148,149,150", "deleted_lines": "148,149", "method_info": {"method_name": "sync_file_mounts.do_sync", "method_params": "remote_path,local_path,allow_non_existing_paths", "method_startline": "131", "method_endline": "156"}}, "hunk_1": {"Ismethod": 1, "added_lines": "192", "deleted_lines": "191", "method_info": {"method_name": "wait_ready", "method_params": "self,deadline", "method_startline": "176", "method_endline": "225"}}, "hunk_2": {"Ismethod": 1, "added_lines": "148,149,150", "deleted_lines": "148,149", "method_info": {"method_name": "sync_file_mounts", "method_params": "self,sync_cmd,step_numbers,2", "method_startline": "121", "method_endline": "174"}}}}}}}