{"BR": {"BR_id": "11858", "BR_author": "karstenddwx", "BRopenT": "2020-11-06T16:28:45Z", "BRcloseT": "2020-11-25T12:39:23Z", "BR_text": {"BRsummary": "[rllib] How to handle missing rewards in a MultiAgentEnv", "BRdescription": "\n We have a MultiAgentEnv scenario where a single agent is selected per step that executes his action, and only one agent receives a reward, all others not.\n Now the question is: how to handle these non-existing rewards?\n The default is to return None (see multi_agent_env.py). This contradict with what is expected in the deploy-script rollout.py, where None is not allowed, as it crashes in line 433.\n The current advise is to use reward 0 instead of None as an equivalent (see <denchmark-link:10805>https://github.com/ray-project/ray/issues/10805</denchmark-link>\n )\n Our scenario aims to solve a technical problem, where actual rewards are in the range of  -0.9 and -0.4. The vast majority of rewards is somewhere between -0.8 and -0.5, so returning 0 would really be a drastic difference to None (i.e. no reward).\n So, why does rollout.py  not allow None (although see multi_agent_env.py uses it as default)?  Can we change that, so both are aligned?\n Or do we have a fundamental misunderstanding here?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "karstenddwx", "commentT": "2020-11-09T07:00:39Z", "comment_text": "\n \t\tI'm not sure I see why zero rewards is a problem. Since the agent is being trained to take the action maximizing the sum of future rewards (sum from t=0 to end of r(t)), won't adding extra zero reward terms have no effect on the optimization?\n Do you mean zero vs None as affect the temporal discounting (dynamically adjusting discount is something that seems commonly used, that we could support in RLlib).\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "karstenddwx", "commentT": "2020-11-09T08:20:27Z", "comment_text": "\n \t\tNo, temporal discounting is not used.\n The problem is that our episode (optimization scenario) consists of one step only. It's not like a game or such things.\n With one action being made to our environment, we can directly decide on good or bad action based on the single reward value.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "karstenddwx", "commentT": "2020-11-10T07:57:22Z", "comment_text": "\n \t\tCan you make it so only the one agent gets the observation? Or do all agents have to act, but only one gets a reward?\n I think another way of handling this is to define a custom postprocessor, or use the callback options, to rewrite the rollout batch to drop actions with noop rewards.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "karstenddwx", "commentT": "2020-11-10T08:27:19Z", "comment_text": "\n \t\tYes, only one agent acts per step, so only one observation and reward per step and episode (because the episode consists of one step only).\n I understand your proposals to workaround this issue.\n But why not just fixing the existing rollout batch line 433. It's a one liner just ignoring None's when summarizing rewards and\n it even better complies with the environment interfaces. I can create a pull request.\n \t\t"}}}, "commit": {"commit_id": "09d5413f70f4c8c952ab979d7f11a8c0168b1fed", "commit_author": "karstenddwx", "commitT": "2020-11-25 13:39:22+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\rollout.py", "file_new_name": "rllib\\rollout.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "454,455", "deleted_lines": "454"}}}}}}