{"BR": {"BR_id": "11294", "BR_author": "anqixu", "BRopenT": "2020-10-09T03:39:24Z", "BRcloseT": "2020-10-12T20:48:45Z", "BR_text": {"BRsummary": "[rllib] SAC (pytorch) log_alpha not optimizing with GPU enabled", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n The entropy regularization coefficient alpha (implemented as sac_model.log_alpha tensor) is not being updated by sac_policy.alpha_optim.\n Verify by running the attached script, load tensorboard, and confirm that the alpha value does not change over training iterations.\n <denchmark-h:h3>Suspected cause</denchmark-h>\n \n I believe the culprit is due to an incorrect order of execution:\n \n sac_model.log_alpha is initially created as a tensor on CPU device\n sac_policy.alpha_optim is initialized with the sac_model.log_alpha param\n but then sac_model.log_alpha is CLONED to the CUDA device, and this copy is used during backprop\n \n To verify, put a debugger breakpoint <denchmark-link:https://github.com/ray-project/ray/blob/b450cb030a5d316f04d4f6322e152335853d1a88/rllib/policy/torch_policy.py#L402>after opt.step()</denchmark-link>\n , then verify that  does not match :\n self._optimizers[3].param_groups[0][\"params\"][0]\n # DEBUGGER OUTPUT: tensor([-0.0003], requires_grad=True)\n \n self.model.log_alpha\n # DEBUGGER OUTPUT: tensor([0.], device='cuda:0', grad_fn=<CopyBackwards>)\n \n self._optimizers[3].param_groups[0][\"params\"][0] is self.model.log_alpha\n # DEBUGGER OUTPUT: False\n Unfortunately, I don't know RLLib internals enough to know how to properly solve this issue.\n <denchmark-h:h3>Related issue: unable to disable GPU usage in RLLib configs</denchmark-h>\n \n According to <denchmark-link:https://docs.ray.io/en/master/rllib-algorithms.html#sac>SAC configs</denchmark-link>\n , we should be able to disable GPU usage by the learner/driver by setting . I also set  just in case. But Ray still puts tensors onto CUDA. I don't know why this is the case.\n To sidestep this issue, we can force Ray to start without GPUs: ray.init(num_gpus=0, ...).\n Ray version and other system information (Python version, TensorFlow version, OS):\n \n Tested with ray 1.0.0 (via pip) and nightly (ray-1.1.0.dev0)\n Python 3.7.5 (default, Nov  7 2019, 10:50:52) [GCC 8.3.0] on linux\n Ubuntu 18.04: Linux MIMIC-5530-UB1804 5.4.0-48-generic #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n <denchmark-link:https://github.com/ray-project/ray/files/5352388/torch_sac_alpha_unchanged.py.txt>torch_sac_alpha_unchanged.py.txt</denchmark-link>\n \n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "anqixu", "commentT": "2020-10-09T06:33:22Z", "comment_text": "\n \t\tFor a quick fix (PR to follow today):\n Could try creating the sale.log_alpha property inside sac_torch_model.py like so?:\n <denchmark-code>        log_alpha = nn.Parameter(\n             torch.from_numpy(np.array([np.log(initial_alpha)])).float())\n         self.register_parameter(\"log_alpha\", log_alpha)\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "anqixu", "commentT": "2020-10-09T06:41:24Z", "comment_text": "\n \t\tPR: <denchmark-link:https://github.com/ray-project/ray/pull/11298>#11298</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "anqixu", "commentT": "2020-10-09T07:57:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/anqixu>@anqixu</denchmark-link>\n \n Thanks for filing this issue! :)\n \t\t"}}}, "commit": {"commit_id": "f5e2cda68ae6035793686780c952d2083d98cf54", "commit_author": "Sven Mika", "commitT": "2020-10-12 13:48:44-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\sac\\sac_torch_model.py", "file_new_name": "rllib\\agents\\sac\\sac_torch_model.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "134,135,136", "deleted_lines": "134,135,136,137"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\agents\\sac\\tests\\test_sac.py", "file_new_name": "rllib\\agents\\sac\\tests\\test_sac.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "521,522,524", "deleted_lines": null, "method_info": {"method_name": "_translate_weights_to_torch", "method_params": "self,weights_dict,map_", "method_startline": "518", "method_endline": "526"}}, "hunk_1": {"Ismethod": 1, "added_lines": "112,134,191,264,288,297,359,360,361,362,374,375,376,377", "deleted_lines": "189,262,286,295,357,369", "method_info": {"method_name": "test_sac_loss_function", "method_params": "self", "method_startline": "74", "method_endline": "379"}}}}}}}