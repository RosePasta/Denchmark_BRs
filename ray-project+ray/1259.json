{"BR": {"BR_id": "1259", "BR_author": "robertnishihara", "BRopenT": "2017-11-26T23:23:49Z", "BRcloseT": "2017-11-27T07:29:38Z", "BR_text": {"BRsummary": "Initial sets of tasks are slow on large cluster.", "BRdescription": "\n <denchmark-h:h3>System information</denchmark-h>\n \n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\n Ray installed from (source or binary): source\n Ray version: Testing this PR #1257, but I'm pretty sure it happens with the current master at e583d5a.\n Python version: Python 3.5.2, Anaconda 4.2.0 (64-bit)\n \n I first start Ray on the head node with\n <denchmark-code>ray start --head --redis-port=6379 --redis-max-clients=65504\n </denchmark-code>\n \n Then I start Ray on the workers with\n <denchmark-code>parallel-ssh -h workers.txt -P -I -p 200 < start_worker.sh\n </denchmark-code>\n \n using a start_worker.sh containing\n <denchmark-code>export PATH=/home/ubuntu/anaconda3/bin/:$PATH\n ray start --redis-address=<head-ip-address>:6379\n </denchmark-code>\n \n Then I start a driver (on the head node) with\n import ray\n import time\n \n ray.init(redis_address=<head-node-ip>)\n \n @ray.remote\n def f():\n     time.sleep(0.01)\n     return ray.services.get_node_ip_address()\n Then I run\n <denchmark-code>%time l = set(ray.get([f.remote() for _ in range(1000)]))\n </denchmark-code>\n \n in the driver a number of times. The first couple times take on the order of 50 seconds. Subsequent runs take around 170 milliseconds.\n In /tmp/raylogs, a number of processes contain lines like\n <denchmark-code>[WARN] (/home/ubuntu/ray/src/common/state/db_client_table.cc:54) calling redis_get_cached_db_client in a loop in with 1 manager IDs took 1038 milliseconds.\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "robertnishihara", "commentT": "2017-11-27T03:47:51Z", "comment_text": "\n \t\tThe problem seems to be that the primary Redis shard is overwhelmed (it's CPU utilization seems to grow linearly with the number of workers, reaching 100% utilization with around 15000 workers) even when no tasks are running and the cluster isn't doing anything. The CPU utilization is a combination of two things.\n \n \n Each worker has an import_thread, which calls\n worker.import_pubsub_client.psubscribe(\"__keyspace@0__:Exports\")\n \n \n \n Local schedulers and plasma managers send frequent heartbeats to the primary redis shard.\n \n \n I think the issue is that psubscribe is causing all of the local scheduler heartbeats to be more expensive than they should be (presumably the channel name is checked against all of the patterns). However, we aren't actually using patterns in these psubscribes, so the fix should be easy.\n \t\t"}}}, "commit": {"commit_id": "f7c4f41df80b5b0b57068138e87c728ee609e392", "commit_author": "Robert Nishihara", "commitT": "2017-11-26 23:29:37-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\worker.py", "file_new_name": "python\\ray\\worker.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1479,1481,1484", "deleted_lines": "1479,1481,1484", "method_info": {"method_name": "print_error_messages", "method_params": "worker", "method_startline": "1457", "method_endline": "1509"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1592,1594,1598,1630", "deleted_lines": "1592,1594,1598,1630", "method_info": {"method_name": "import_thread", "method_params": "worker,mode", "method_startline": "1589", "method_endline": "1671"}}}}}}}