{"BR": {"BR_id": "3494", "BR_author": "ericl", "BRopenT": "2018-12-07T21:04:44Z", "BRcloseT": "2019-01-05T06:30:35Z", "BR_text": {"BRsummary": "[rllib] Model self loss isn't included in all algorithms", "BRdescription": "\n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n <denchmark-link:https://groups.google.com/forum/#!topic/ray-dev/dk0erEEnkFY>https://groups.google.com/forum/#!topic/ray-dev/dk0erEEnkFY</denchmark-link>\n \n In DQN, DDPG, IMPALA, and A3C, the gradients() function for the tf policy graph is overriden, but does not include model.loss().\n The fix is to reference self._loss in gradients(), since that is always the total loss passed to TFPolicyGraph.\n We should also add a simple test that the model self loss is improving for each algorithm (for example by just adding custom model with a free TF variable that gets sent to zero).\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ericl", "commentT": "2018-12-07T21:33:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/megankawakami>@megankawakami</denchmark-link>\n  it would be great if you could look at this.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ericl", "commentT": "2018-12-08T22:18:12Z", "comment_text": "\n \t\tDoes it mean that for example A3C work wrongly?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ericl", "commentT": "2018-12-08T22:32:51Z", "comment_text": "\n \t\tOnly if you are using self supervised losses.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ericl", "commentT": "2018-12-09T03:58:01Z", "comment_text": "\n \t\tI got it , thanks.\n \t\t"}}}, "commit": {"commit_id": "03fe760616a70e7255f944168265b52efed27c6e", "commit_author": "Eric Liang", "commitT": "2019-01-04 22:30:35-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\rllib-offline.rst", "file_new_name": "doc\\source\\rllib-offline.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1,2", "deleted_lines": "1,2"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\rllib.rst", "file_new_name": "doc\\source\\rllib.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "86,87,92,93,100,101", "deleted_lines": "86,87,92,93,100,101"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\rllib\\agents\\a3c\\a3c_tf_policy_graph.py", "file_new_name": "python\\ray\\rllib\\agents\\a3c\\a3c_tf_policy_graph.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "145", "deleted_lines": "145", "method_info": {"method_name": "gradients", "method_params": "self,optimizer", "method_startline": "144", "method_endline": "148"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\rllib\\agents\\ddpg\\ddpg_policy_graph.py", "file_new_name": "python\\ray\\rllib\\agents\\ddpg\\ddpg_policy_graph.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "209,210,243,250,269,270,277,315,316,317,318,319,320,363", "deleted_lines": "210,211,244,251,270,271,278,358", "method_info": {"method_name": "__init__", "method_params": "self,observation_space,action_space,config", "method_startline": "183", "method_endline": "374"}}, "hunk_1": {"Ismethod": 1, "added_lines": "456,462,463", "deleted_lines": "457", "method_info": {"method_name": "_build_p_network", "method_params": "self,obs,obs_space", "method_startline": "455", "method_endline": "463"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\rllib\\agents\\dqn\\dqn_policy_graph.py", "file_new_name": "python\\ray\\rllib\\agents\\dqn\\dqn_policy_graph.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "406", "deleted_lines": "406", "method_info": {"method_name": "gradients", "method_params": "self,optimizer", "method_startline": "402", "method_endline": "413"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\rllib\\agents\\impala\\vtrace_policy_graph.py", "file_new_name": "python\\ray\\rllib\\agents\\impala\\vtrace_policy_graph.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "266", "deleted_lines": "266", "method_info": {"method_name": "gradients", "method_params": "self,optimizer", "method_startline": "265", "method_endline": "269"}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\rllib\\tuned_examples\\pendulum-ddpg.yaml", "file_new_name": "python\\ray\\rllib\\tuned_examples\\pendulum-ddpg.yaml", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7", "deleted_lines": "7"}}}}}}