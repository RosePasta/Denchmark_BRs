{"BR": {"BR_id": "7635", "BR_author": "fominskayage", "BRopenT": "2020-03-17T10:29:02Z", "BRcloseT": "2020-03-26T11:00:49Z", "BR_text": {"BRsummary": "[rllib] DQN module 'tensorflow._api.v1.compat.v1.initializers' has no attribute 'GlorotUniform'", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n I run rllib train --run=DQN --env=CartPole-v0 --config '{\"n_step\": 5, \"v_min\": -10.0, \"v_max\": 10.0, \"noisy\": true, \"num_atoms\": 10, \"dueling\": true, \"double_q\": true}'\n At first I get\n <denchmark-code>Traceback (most recent call last):\n   File \"/usr/local/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 459, in _process_trial\n     result = self.trial_executor.fetch_result(trial)\n   File \"/usr/local/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 377, in fetch_result\n     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n   File \"/usr/local/lib/python3.7/site-packages/ray/worker.py\", line 1504, in get\n     raise value.as_instanceof_cause()\n ray.exceptions.RayTaskError(AttributeError): ray::DQN.__init__() (pid=96308, ip=192.168.120.74)\n   File \"python/ray/_raylet.pyx\", line 437, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 86, in __init__\n     Trainer.__init__(self, config, env, logger_creator)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 447, in __init__\n     super().__init__(config, logger_creator)\n   File \"/usr/local/lib/python3.7/site-packages/ray/tune/trainable.py\", line 172, in __init__\n     self._setup(copy.deepcopy(self.config))\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 591, in _setup\n     self._init(self.config, self.env_creator)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 105, in _init\n     self.config[\"num_workers\"])\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 658, in _make_workers\n     logdir=self.logdir)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py\", line 60, in __init__\n     RolloutWorker, env_creator, policy, 0, self._local_config)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py\", line 262, in _make_worker\n     _fake_sampler=config.get(\"_fake_sampler\", False))\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 355, in __init__\n     self._build_policy_map(policy_dict, policy_config)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 820, in _build_policy_map\n     policy_map[name] = cls(obs_space, act_space, merged_conf)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/policy/tf_policy_template.py\", line 138, in __init__\n     obs_include_prev_action_reward=obs_include_prev_action_reward)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 137, in __init__\n     self.model = make_model(self, obs_space, action_space, config)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/dqn_policy.py\", line 183, in build_q_model\n     parameter_noise=config[\"parameter_noise\"])\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/models/catalog.py\", line 349, in get_model_v2\n     name, **model_kwargs)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py\", line 185, in __init__\n     q_out = build_action_value_in_scope(self.model_out)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py\", line 178, in build_action_value_in_scope\n     return build_action_value(model_out)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py\", line 68, in build_action_value\n     \"hidden_%d\" % i, action_out, q_hiddens[i], sigma0)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py\", line 259, in _noisy_layer\n     initializer=tf.initializers.GlorotUniform())\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 193, in __getattr__\n     attr = getattr(self._tfmw_wrapped_module, name)\n AttributeError: module 'tensorflow._api.v1.compat.v1.initializers' has no attribute 'GlorotUniform'\n </denchmark-code>\n \n When I tried to change line 259 in /usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py from tf.initializers.GlorotUniform() to tf.initializers.glorot_uniform() and now I get this\n <denchmark-code>Traceback (most recent call last):\n   File \"/usr/local/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 459, in _process_trial\n     result = self.trial_executor.fetch_result(trial)\n   File \"/usr/local/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 377, in fetch_result\n     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n   File \"/usr/local/lib/python3.7/site-packages/ray/worker.py\", line 1504, in get\n     raise value.as_instanceof_cause()\n ray.exceptions.RayTaskError(FailedPreconditionError): ray::DQN.__init__() (pid=96346, ip=192.168.120.74)\n   File \"python/ray/_raylet.pyx\", line 437, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 430, in ray._raylet.execute_task.function_executor\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 86, in __init__\n     Trainer.__init__(self, config, env, logger_creator)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 447, in __init__\n     super().__init__(config, logger_creator)\n   File \"/usr/local/lib/python3.7/site-packages/ray/tune/trainable.py\", line 172, in __init__\n     self._setup(copy.deepcopy(self.config))\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 591, in _setup\n     self._init(self.config, self.env_creator)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 105, in _init\n     self.config[\"num_workers\"])\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 658, in _make_workers\n     logdir=self.logdir)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py\", line 60, in __init__\n     RolloutWorker, env_creator, policy, 0, self._local_config)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py\", line 262, in _make_worker\n     _fake_sampler=config.get(\"_fake_sampler\", False))\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 355, in __init__\n     self._build_policy_map(policy_dict, policy_config)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 820, in _build_policy_map\n     policy_map[name] = cls(obs_space, act_space, merged_conf)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/policy/tf_policy_template.py\", line 138, in __init__\n     obs_include_prev_action_reward=obs_include_prev_action_reward)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 137, in __init__\n     self.model = make_model(self, obs_space, action_space, config)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/dqn_policy.py\", line 183, in build_q_model\n     parameter_noise=config[\"parameter_noise\"])\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/models/catalog.py\", line 349, in get_model_v2\n     name, **model_kwargs)\n   File \"/usr/local/lib/python3.7/site-packages/ray/rllib/agents/dqn/distributional_q_model.py\", line 186, in __init__\n     self.q_value_head = tf.keras.Model(self.model_out, q_out)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 147, in __init__\n     super(Model, self).__init__(*args, **kwargs)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 164, in __init__\n     self._init_graph_network(*args, **kwargs)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\n     result = method(self, *args, **kwargs)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 267, in _init_graph_network\n     base_layer_utils.create_keras_history(self._nested_outputs)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 184, in create_keras_history\n     _, created_layers = _create_keras_history_helper(tensors, set(), [])\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\n     layer_inputs, processed_ops, created_layers)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\n     layer_inputs, processed_ops, created_layers)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\n     layer_inputs, processed_ops, created_layers)\n   [Previous line repeated 1 more time]\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 229, in _create_keras_history_helper\n     constants[i] = backend.function([], op_input)([])\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 3476, in __call__\n     run_metadata=self.run_metadata)\n   File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\n     run_metadata_ptr)\n tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value default_policy/q_func/action_value/output_fc_b\n \t [[{{node default_policy/q_func/action_value/output_fc_b/read}}]]\n </denchmark-code>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n Ray version: 0.8.2\n Python version: 3.7\n TensorFlow version: 1.15.0\n OS: macOS 10.15.3\n <denchmark-h:h3>Reproduction</denchmark-h>\n \n rllib train --run=DQN --env=CartPole-v0 --config '{\"n_step\": 5, \"v_min\": -10.0, \"v_max\": 10.0, \"noisy\": true, \"num_atoms\": 10, \"dueling\": true, \"double_q\": true}'\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "fominskayage", "commentT": "2020-03-17T17:42:39Z", "comment_text": "\n \t\tI see this with TF2.0 as well.\n <denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n  will this be fixed by the param noise refactor?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "fominskayage", "commentT": "2020-03-23T16:20:40Z", "comment_text": "\n \t\tIt's the noisy layers, not the param noise. Should be an easy fix. It's on my current sprint. ...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "fominskayage", "commentT": "2020-03-26T09:32:59Z", "comment_text": "\n \t\tI'm thinking it's the non-\"keras.layers\" that goes into constructing the Keras model that's causing this.\n If you replace (in distributional_q_model.py):\n <denchmark-code>action_activation = tf.nn.xw_plus_b(action_in, w + sigma_w * epsilon_w, b + epsilon_b)\n </denchmark-code>\n \n with:\n <denchmark-code>action_activation = tf.keras.layers.Lambda(lambda x: tf.matmul(x, w + sigma_w * epsilon_w) + b + sigma_b * epsilon_b)(action_in)\n </denchmark-code>\n \n it should work. I'll create a PR right now...\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "fominskayage", "commentT": "2020-03-26T11:00:49Z", "comment_text": "\n \t\tHere is the PR that fixes this issue. There is also a new test case for a complete rainbow-compiled DQN agent in .\n <denchmark-link:https://github.com/ray-project/ray/pull/7750>#7750</denchmark-link>\n \n Closing this issue. Please feel free to re-open in case it still does not work for you.\n Thanks!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "fominskayage", "commentT": "2020-05-09T03:14:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n  <denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n  Somehow this fix hasn't made it into the nightly build, and this bug is causing a problem for me. Can you please fix this?\n \t\t"}}}, "commit": {"commit_id": "93b5c38b7dc75b64a4812c1b300f48f9f1bf5d2b", "commit_author": "Sven Mika", "commitT": "2020-03-26 22:08:34-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "rllib\\agents\\dqn\\distributional_q_model.py", "file_new_name": "rllib\\agents\\dqn\\distributional_q_model.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "104,105,106,107,108,109,110,111,112,113", "deleted_lines": "104,105,106,107,108,109,110,111,112,113", "method_info": {"method_name": "build_action_value._layer", "method_params": "x", "method_startline": "104", "method_endline": "113"}}, "hunk_1": {"Ismethod": 1, "added_lines": "132,135,145", "deleted_lines": "132,133,136,137", "method_info": {"method_name": "build_state_score", "method_params": "model_out", "method_startline": "121", "method_endline": "146"}}, "hunk_2": {"Ismethod": 1, "added_lines": "73,74,79,94,97,103,104,105,106,107,108,109,110,111,112,113,114,115", "deleted_lines": "73,74,79,80,95,96,104,105,106,107,108,109,110,111,112,113,114,115", "method_info": {"method_name": "build_action_value", "method_params": "model_out", "method_startline": "62", "method_endline": "119"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\dqn\\tests\\test_dqn.py", "file_new_name": "rllib\\agents\\dqn\\tests\\test_dqn.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,34,35,41,42,43,44,45,46,51", "deleted_lines": "18,19,25,26", "method_info": {"method_name": "test_dqn_compilation", "method_params": "self", "method_startline": "13", "method_endline": "51"}}}}}}}