{"BR": {"BR_id": "1717", "BR_author": "st2yang", "BRopenT": "2020-07-07T03:45:15Z", "BRcloseT": "2020-07-13T15:41:55Z", "BR_text": {"BRsummary": "Failed to reproduce example her_ddpg_fetchreach", "BRdescription": "\n Hi,\n I was trying to run examples/tf/<denchmark-link:https://github.com/rlworkgroup/garage/blob/master/examples/tf/her_ddpg_fetchreach.py>her_ddpg_fetchreach.py</denchmark-link>\n  but got a much worse performance. I attached the results as follows, and it seems that it's not working at all. Do you have any idea how to make it work? Though the default parameters look reasonable, should I try to tune some parameters?\n Thank you in advance.\n \n AverageSuccessRate                       0\n Epoch                                   49\n Evaluation/AverageDiscountedReturn      -9.94112\n Evaluation/AverageReturn              -999.93\n Evaluation/CompletionRate                0\n Evaluation/Iteration                   980\n Evaluation/MaxReturn                  -998\n Evaluation/MinReturn                 -1000\n Evaluation/NumTrajs                    100\n Evaluation/StdReturn                     0.324191\n Policy/AveragePolicyLoss                 4.17451\n QFunction/AverageAbsQ                    4.19709\n QFunction/AverageAbsY                    4.19412\n QFunction/AverageQ                      -4.16916\n QFunction/AverageQFunctionLoss           0.0232313\n QFunction/AverageY                      -4.16946\n QFunction/MaxQ                           2.87869\n QFunction/MaxY                           2.62668\n TotalEnvSteps                       100000\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "st2yang", "commentT": "2020-07-07T18:05:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/st2yang>@st2yang</denchmark-link>\n  thanks for the bug report! we will investigate in the next few weeks and get back to you.\n in the meantime, if you find working parameters or the root cause, please let us know!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "st2yang", "commentT": "2020-07-07T19:41:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ryanjulian>@ryanjulian</denchmark-link>\n  Sure, thanks for your help. I tried to change some parameters and might get a better result. But I'm not sure how good it is. Since I am new to garage, I am wondering if there is a way to evaluate the trained policy by rendering the environment?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "st2yang", "commentT": "2020-07-07T19:43:59Z", "comment_text": "\n \t\tYou can either set  in  or follow <denchmark-link:https://garage--1642.org.readthedocs.build/en/1642/user/reuse_garage_policy.html>this guide</denchmark-link>\n  (it is in a pull request now, but will appear on garage.readthedocs.io in the next day or two)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "st2yang", "commentT": "2020-07-07T20:13:03Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/ryanjulian>@ryanjulian</denchmark-link>\n  , I'll try. BTW, when I tried the second option, it has some conflict error regarding tf\n \n data = snapshotter.load('path/to/snapshot/dir')\n File \"/homes/yangyang/.local/lib/python3.6/site-packages/garage/experiment/snapshotter.py\", line 153, in load\n return joblib.load(file)\n File \"/homes/yangyang/.local/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 588, in load\n obj = _unpickle(fobj)\n File \"/homes/yangyang/.local/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 526, in _unpickle\n obj = unpickler.load()\n File \"/homes/yangyang/miniconda3/envs/mujoco-gym/lib/python3.6/pickle.py\", line 1050, in load\n dispatchkey[0]\n File \"/homes/yangyang/.local/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 339, in load_build\n Unpickler.load_build(self)\n File \"/homes/yangyang/miniconda3/envs/mujoco-gym/lib/python3.6/pickle.py\", line 1507, in load_build\n setstate(state)\n File \"/homes/yangyang/.local/lib/python3.6/site-packages/garage/tf/policies/continuous_mlp_policy.py\", line 209, in setstate\n self._initialize()\n File \"/homes/yangyang/.local/lib/python3.6/site-packages/garage/tf/policies/continuous_mlp_policy.py\", line 85, in _initialize\n shape=(None, self._obs_dim))\n \n \n File \"/homes/yangyang/miniconda3/envs/mujoco-gym/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 3023, in placeholder\n raise RuntimeError(\"tf.placeholder() is not compatible with \"\n RuntimeError: tf.placeholder() is not compatible with eager execution.\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "st2yang", "commentT": "2020-07-07T20:20:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/st2yang>@st2yang</denchmark-link>\n  can you post the full error message? if it doesn't fit in github, you can use pastebin or similar.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "st2yang", "commentT": "2020-07-07T20:39:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ryanjulian>@ryanjulian</denchmark-link>\n  Sorry I updated the error message. The error happens when loading tf policy in snapshotter.load('path/to/snapshot/dir').\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "st2yang", "commentT": "2020-07-07T20:41:10Z", "comment_text": "\n \t\tCan you post the full text of the script you're running? It looks like TF2 has been initialized in eager mode (rather than graph mode), which garage doesn't support.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "st2yang", "commentT": "2020-07-07T20:50:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ryanjulian>@ryanjulian</denchmark-link>\n  I am using the exact example script as you mentioned. Disabling eager by disable_eager_execution() will throw another error. I think it's tf-related issue, I might use pytorch first?\n \n from garage.experiment import Snapshotter\n snapshotter = Snapshotter()\n data = snapshotter.load('path/to/snapshot/dir')\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "st2yang", "commentT": "2020-07-07T22:35:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/st2yang>@st2yang</denchmark-link>\n  yeah -- if you want an immediate solution, try the PyTorch version. However, I don't think HER is currently implemented in the torch version.\n I don't think it's a difficult modification.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "st2yang", "commentT": "2020-07-07T22:46:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ryanjulian>@ryanjulian</denchmark-link>\n  I thought HERBuffer is independent of the backend, and I can actually run pytorch code with HER. You mean HERBuffer wouldn't boost policy training in the torch vision? Like HERBuffer would be treated as a regular buffer?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "st2yang", "commentT": "2020-07-07T22:51:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/st2yang>@st2yang</denchmark-link>\n  you're right! we recently made HERReplayBuffer can be used independent of the algorithm.\n the only caveat i'll add is that we haven't tested HERReplayBuffer+torch/DDPG -- let us know how it goes!\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "st2yang", "commentT": "2020-07-07T22:53:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/st2yang>@st2yang</denchmark-link>\n  in your original attempt, did you replace  with the actual path to your algorithm's snapshot?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "st2yang", "commentT": "2020-07-07T23:13:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ryanjulian>@ryanjulian</denchmark-link>\n \n \n \n snapshot loading\n Yes, I replaced it. And the snapshot works in torch version. So there's some minor bug/conflict in tf version.\n \n \n HER\n HERBuffer + torch/ddpg is at least runnable and give some reasonable results. Since FetchReach is a simple task, I am not sure how much HERBuffer contributes in this case.\n \n \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "st2yang", "commentT": "2020-07-07T23:20:01Z", "comment_text": "\n \t\tSome updates regarding example script her_ddpg_fetchreach:\n After some experiments, I think there're two types of bugs or errors\n (1) the original parameters are not working at all. After I change some parameters, the training is indeed much better and give reasonable performance.\n (2) statistic calculation might be wrong. Under a reasonably good performance (in terms of other statistics and evaluation rendering), AverageSuccessRate and Evaluation/CompletionRate are always 0, which doesn't make sense.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "st2yang", "commentT": "2020-07-07T23:39:39Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/st2yang>@st2yang</denchmark-link>\n  feel free to send us a PR with the updated params. We would love to have it!\n As for (2), there might be a misunderstanding here around the definition of those metrics:\n \n \n Evaluation/AverageSuccessRate calculates the average number of trajectories which have 'success=True' in their env_info field (see here). This applies only to metaworld environments, as far as I know. If you seeing this metrics in your log while using the fetch environments, that's a bug. To compute this for the gym Fetch environments, you would need to look for the key 'is_success' (see here). Feel free to send us a PR for that.\n \n \n Evaluation/CompletionRate measures the average number of trajectories which end in a termination condition (i.e. done==True in the gym.Env interface). However, not all environments send termination signals, so it's quite common for this to be 0 for a working agent. As far as I can tell, the Fetch environments never send termination signals (see here), so this always being 0 is normal.\n \n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "st2yang", "commentT": "2020-07-08T00:40:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ryanjulian>@ryanjulian</denchmark-link>\n  I see. Thanks a lot for your detailed explanation! I'll try to look into these and do more experiments, then send a PR about the issues.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "st2yang", "commentT": "2020-07-08T06:51:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/st2yang>@st2yang</denchmark-link>\n  Thanks for bringing this up. For now, could you try entering a  and see if loading tf policy works?\n <denchmark-code>from garage.experiment import Snapshotter\n import tensorflow as tf //import tf\n \n snapshotter = Snapshotter()\n with tf.compat.v1.Session(): //enter a tf session\n         data = snapshotter.load('path/to/snapshot/dir')\n \tpolicy = data['algo'].policy\n \tenv = data['env']\n \n \t# See what the trained policy can accomplish\n \tfrom garage.sampler.utils import rollout\n \tpath = rollout(env, policy, animated=True)\n \tprint(path)\n </denchmark-code>\n \n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "st2yang", "commentT": "2020-07-08T14:43:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ahtsan>@ahtsan</denchmark-link>\n  Thanks it works!\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "st2yang", "commentT": "2020-07-08T14:50:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ahtsan>@ahtsan</denchmark-link>\n  can you update the docs page accordingly?\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "st2yang", "commentT": "2020-07-13T15:41:55Z", "comment_text": "\n \t\tclose this issue, related to PR <denchmark-link:https://github.com/rlworkgroup/garage/pull/1739>#1739</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "00c92ce90d9209f91b5a9570a03ae2cfd5d405a6", "commit_author": "Ryan Julian", "commitT": "2020-07-09 15:45:54+00:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\user\\experiments.rst", "file_new_name": "docs\\user\\experiments.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "115", "deleted_lines": "115"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\user\\save_load_resume_exp.md", "file_new_name": "docs\\user\\save_load_resume_exp.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "185", "deleted_lines": "185"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\user\\training_a_policy.md", "file_new_name": "docs\\user\\training_a_policy.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "183", "deleted_lines": "183"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "examples\\jupyter\\custom_env.ipynb", "file_new_name": "examples\\jupyter\\custom_env.ipynb", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "561,584,649,672", "deleted_lines": "561,584,649,672"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "examples\\jupyter\\trpo_gym_tf_cartpole.ipynb", "file_new_name": "examples\\jupyter\\trpo_gym_tf_cartpole.ipynb", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "411,455,499,543,593,637,681,725,775,819", "deleted_lines": "411,455,499,543,593,637,681,725,775,819"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\garage\\_functions.py", "file_new_name": "src\\garage\\_functions.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "157", "deleted_lines": "157", "method_info": {"method_name": "log_performance", "method_params": "itr,batch,discount,prefix", "method_startline": "122", "method_endline": "161"}}, "hunk_1": {"Ismethod": 1, "added_lines": "116", "deleted_lines": "116", "method_info": {"method_name": "log_multitask_performance", "method_params": "itr,batch,discount,name_map", "method_startline": "66", "method_endline": "119"}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\garage\\tf\\algos\\te_npo.py", "file_new_name": "src\\garage\\tf\\algos\\te_npo.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "903", "deleted_lines": "903", "method_info": {"method_name": "evaluate", "method_params": "self,policy_opt_input_values,samples_data", "method_startline": "815", "method_endline": "907"}}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\garage\\experiment\\test_meta_evaluator.py", "file_new_name": "tests\\garage\\experiment\\test_meta_evaluator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "95,96", "deleted_lines": "95", "method_info": {"method_name": "test_meta_evaluator", "method_params": "", "method_startline": "68", "method_endline": "102"}}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\garage\\test_functions.py", "file_new_name": "tests\\garage\\test_functions.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "64", "deleted_lines": "64", "method_info": {"method_name": "test_log_performance", "method_params": "", "method_startline": "28", "method_endline": "70"}}}}}}}